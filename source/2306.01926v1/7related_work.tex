\section{Related work}
\label{sec.related}


\subsection{Timeseries Analytics}
There is a great deal of prior work on timeseries analytics methods. This work can be divided into three categories: (1) non-deep learning methods; (2) CNN/RNN-based deep learning methods; and (3) Transformer-based deep learning methods.

\noindent\textbf{Traditional Methods.} 
These methods, such as TS-CHIEF~\cite{Shifaz2020TSCHIEFAS}, HIVE-COTE~\cite{10.1145/3182382},  ROCKET~\cite{DBLP:journals/datamine/DempsterPW20} have achieved notable performance on public datasets. Despite that, traditional methods suffer from one or more issues: they (1) rely on expert knowledge for feature extraction;
%,  scaling poorly to timeseries from other domains or in different lengths;
(2) incur heavy computation cost and are inappropriate for GPU devices; (3) support only uni-variate timeseries; (4) perform classification solely. 
Some work~\cite{DBLP:conf/kdd/ZerveasJPBE21} shows that the transformed-based methods outperform these traditional methods especially on multi-variate timeseries. 

%ROCKET~\cite{DBLP:journals/datamine/DempsterPW20}, one of the best uni-variate classification models, involves training a classifier with features extracted by a massive number (10,000) of convolution kernels. HIVE-COTE~\cite{10.1145/3182382} and TS-CHIEF~\cite{Shifaz2020TSCHIEFAS} are heterogeneous ensemble models designed with expert knowledge, involving classifiers based on shaplet transformation, elastic similarity measures, etc. 

In particular, as the SOTA of timeseries {\bf representation learning}, GRAIL~\cite{paparrizos2019grail} extracts landmarks from data and computes the representations with the combination of the landmarks. However, GRAIL only supports uni-variate timeseries. Our experiments (Sec.~\ref{sec.exp.univariate}) show that \system significantly outperforms GRAIL in both effectiveness and efficiency on uni-variate timeseries.

%spectral features, random interval and dictionary-based methods

%\srm{Why didn't we compare against these?}

\noindent\textbf{CNN/RNN-based Deep Learning Methods.}
CNN-based methods, such as InceptionTime~\cite{ismail2020inceptiontime} and Resnet~\cite{he2016deep}, are good at classification tasks, but can not handle generative tasks such as forecasting because of the inductive bias of convolution networks. 
RNN-based methods, such as Brit~\cite{cao2018brits} and deepAR~\cite{salinas2020deepar}, are capable for classification, regression and generation. However, the recurrent structure brings a lot of problems: (1) limiting the model's ability in capturing long-range correlation; (2) notoriously difficult to train~\cite{pascanu2013difficulty} because of gradient vanishing and exploding problem. As a result, such methods can hardly scale to very long timeseries. 
%(2) the sequential computation architecture is not friendly to GPUs; 

%, and the results showed Transformer-based method's superiority over traditional methods and RNN-based methods.
\noindent\textbf{Transformer-based Deep Learning Methods.}
Given that Transformer is the best choice for backbone in almost all sequence modeling tasks, some effort has been made to apply Transformer to timeseries analytics.
%Being the state-of-the-art model in NLP,  
%And thanks to the pairwise attention computation, Transformer is especially good at modeling long-range dependencies. 
Targeting forecasting of uni-variate timeseries, LogTrans~\cite{li2019enhancing} introduced a log sparsity assumption to attention computation.
%like ARIMA~\cite{arima}, DeepAR~\cite{salinas2020deepar} and DeepState~\cite{rangapuram2018deep}.
Informer~\cite{zhou2021informer} pushes LogTrans a step further and scales forecasting to multi-variate timeseries. Autoformer~\cite{wu2021autoformer} performs forecasting by decomposing timeseries into two parts, i.e. the trend part and the seasonal part.
%Moreover, some researchers explored other Transformer architecture customized for forecasting~\cite{liu2021pyraformer,zhou2022fedformer}, such as hierarchical attention~\cite{liu2021pyraformer} and Fourier Transformation-based attention~\cite{zhou2022fedformer}.

%At the meantime, they introduced the log sparsity implicitly by performing approximated attention computation with $O(log L)$ dominant queries. 
For imputation tasks, CDSA~\cite{ma2019cdsa} outperforms statistical methods and the SOTA of RNN-based method Brit~\cite{cao2018brits} on 3 public and 2 competition datasets. 
For timeseries classification, AutoTransformer~\cite{ren2022autotransformer} performs architecture search to adapt to the tasks in different domains. 
For timeseries anomaly detection, Anomaly Transformer~\cite{xu2021anomaly} outperforms many widely-used methods such as OmniAnomaly~\cite{su2019robust}, assuming the attention score maps show Gaussian distribution.

%as in \system.
All of these works are designed for specific tasks, rather than functioning as a {\bf representation learning} framework to serve different downstream tasks. To fill this gap, some researchers proposed a Transformer-based architecture, called TST~\cite{DBLP:conf/kdd/ZerveasJPBE21}. Like \system, TST supports regression, classification, and unsupervised learning through the ``cloze test'' pretraining task on timeseries. 
However, TST directly uses the classical Vanilla self-attention, thus not scalable to long timeseries as shown in our experiments (Sec.~\ref{sec.exp.efficiency.length}).

\subsection{Efficient Transformers}
The need of improving the scalability of Transformers has led to more efficient variations of Transformers, especially for accommodating long text data in NLP~\cite{tay2020efficient}. 

Introducing fixed/random patterns to self-attention mechanism is an intuitive idea. Sparse Transformer~\cite{child2019generating} and Longformer~\cite{beltagy2020longformer} only compute attention at fixed intervals. ETC~\cite{ainslie2020etc} and BigBird~\cite{zaheer2020big} use global-local attention: the attention computation is limited within a fixed radius, while some auxiliary tokens are added to attend/get attended globally. 
The deficiencies of fixed attention patterns are obvious: it heavily depends on users to give an optimal setting.
  
%proposed doing random attention besides global attention and local attention. The deficiencies of fixed attention patterns are obvious: it requires an optimal setting from users to the effectiveness is heavily dependent on the data distribution of specific datasets, requiring the users to give an optimal setting. 

To decrease the reliance on human labor, some works seek to introduce learnable/adaptive attention patterns instead of fixed patterns. Reformer~\cite{kitaev2020reformer} proposed  only computing the dominant attention terms based on their observation of sparsity in attention matrix from language/image data. Such sparsity is intuitive in language data, in which a word's attention mainly focuses on the nearby sentences. However, attention in timeseries data shows strong seasonal patterns rather than sparse patterns, mainly as result of  the periodicity of timeseries data. Therefore, such works do not work well for timeseries.

%Cluster Attention~\cite{vyas2020fast} computes the dominant terms accurately and the other terms approximately. However, it relies on the assumption that there exists a small number of critical terms in the data. Although this assumption might hold in natural language due to its sparse property, it does not fit the timesereis data which often produces many dense sub-areas in the latent feature space.

Apart from introducing attention patterns, some works seek to solve this problem with 
applied mathematics techniques. Linformer~\cite{wang2020linformer} performs a projection to decrease the size of query, key and value matrices before attention computation, because the attention matrix tends to be low-ranked. 
Performer~\cite{choromanski2020rethinking} uses linear functions to approximate the kernel function \textit{softmax}, making attention computation commutative. When the sequence length is far greater than the dimension of embedding vectors, Performer benefits from changing the order of matrix multiplication.
Linformer and Performer do not depend on the unique properties of language data, thus potentially fitting timeseries better than other techniques, which is why we compared against them in our experiments. 
However as shown in Sec.~\ref{sec.exp}, our group attention significantly outperforms them in both accuracy and efficiency (training time), because group attention fully leverages the periodicity of timeseries.  


% \begin{comment}
% %\subsection{Transformer-based Architecture Customized to Forecasting}
% Some works~\cite{li2019enhancing,zhou2021informer,liu2021pyraformer,zhou2022fedformer} pay their attention to design Transformer-based architectures customized to time series forecasting. Leveraging the unique properties of the forecasting task, they use the similar key ideas to improve the forecasting accuracy and reduce the memory consumption: (1) forecasting is forward looking only; (2) an event tends to play a larger role in forecasting if it is closer to the `to be forecasted event'. Although these ideas seem reasonable for forecasting task, they do not fit other analytics tasks such as classification which requires accurately capturing the global features of the `to-be-classified' timeseries segments. 
% Further, although they are effective in reducing the memory consumption, these complex methods are not necessarily faster than the classical self-attention due to the heavy computation overhead. 
% Our Group Attention instead targets generally supporting various timeseries analytics tasks and is both memory and computation efficient, as confirmed in the experiments.

% \end{comment}

% LogTrans~\cite{li2019enhancing} introduced a log sparsity assumption, in which attention are only computed at an exponential interval. Informer~\cite{zhou2021informer} combined the idea of log sparsity from LogTrans and the idea of dominant terms from Reformer, proposing to compute attention for $O(log L)$ dominant queries. Pyraformer\cite{liu2021pyraformer} designed a hierarchical attention mechanism based on m-ary tree structure. FEDformer~\cite{zhou2022fedformer} proposed to leverage the signal nature of time series. They transform time series to frequency domain representation and do sampling to decreased the size of attention matrix. After attention computation, the time series is transformed back to time domain representation by an inverse transform.























