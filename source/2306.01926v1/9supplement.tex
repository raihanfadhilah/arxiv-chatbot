
\begin{sloppypar}
\section{Appendix: Supplementary Material}
\subsection{Experiment Configuration and Hyper-parameter Settings}
\label{appendix.exp}
\noindent\textbf{Configuration.} All models were trained on an NVIDIA Tesla V100 16GB GPU. All the methods are optimized with AdamW~\cite{loshchilov2017decoupled} of which the starting learning rate and weight decay parameter are both $1e^{-4}$. In full-label training scenario, we train the models for 100 epochs. In ``pretraining + few-label finetuning scenario'', as the pretrained models require fewer epochs to converge~\cite{DBLP:conf/kdd/ZerveasJPBE21}, we train the model for 50 epochs. For a fair comparison, the baselines use a maximal batch size within GPU's capacity during training.

As for model hyper-parameter setting, \system and the baselines use a Transformer structure balancing \textit{\textbf{Vanilla}} 's accuracy and efficiency: 8-layer stack of 2-head attention with hidden vectors in dimension of 64. Convolution kernel size is set to 5 by default.
We set the error bound threshold ($\epsilon$, Sec.~\ref{sec.scheduler.group}) of Group Attention to 2, as it balances the accuracy and the efficiency in general on all datasets.
Because Linformer requires the users to set the sizes of projection matrix, in different settings we choose an accuracy-efficiency balancing one among \{64,128,256,512\}.

\subsection{Efficient Computation of Group Attention}
\label{appendix.groupAttention}
\begin{algorithm}
    \caption{Efficient Computation of Group Attention}
    \label{algo.grpattn}
    \small
    \begin{algorithmic}[1] 
    \Require $Q,V,R,COUNT,BELONG$
    \Ensure $Q,V \in \mathbb{R}^{n*d}$,$ R \in \mathbb{R}^{N*d}$,$COUNT \in \mathbb{N}^{N}$,$BELONG \in \mathbb{N}^{n}$
            \Function {group\_attention}{$Q,V,R$}
                \For{$i = 0 \to N-1$}
                    \State $\widetilde{v}_i \gets \sum_{j=0}^{n-1}(BELONG_j==i) v_j$
                \EndFor
                
                \State $\widetilde{P} \gets QR^T$
                
                \For{$i = 0 \to n-1$}
                    \For{$j = 0 \to N-1$}
                    \State $w_{i,j} \gets exp(\widetilde{P}_{i,j})COUNT_j$
                    \EndFor
                \EndFor
                
                \For{$i = 0 \to n-1$}
                    \State $s_i \gets \sum_{j=0}^{N-1} w_{i,j}$ 
                \EndFor
                
                \For{$i = 0 \to n-1$}
                    \State $o_{i} \gets \sum_{j=0}^{N-1}\frac{exp(\widetilde{P}_{i,j})}{s_i}\widetilde{v}_j$
                \EndFor
                    
                \State \Return{$O$}
            \EndFunction
            
        \end{algorithmic}
\end{algorithm}

In Alg.~\ref{algo.grpattn}, we denote $COUNT_i$ to be the size of the $i^{th}$ group, $N$ to be the number of groups, $\mathbf{r}_i$ to be the representative key of the $i^{th}$ group and $\mathbf{R}$ to be the matrix consisting of all $\mathbf{r}_i$, $BELONG_i$ to be the group that $\mathbf{k}_i$ belongs to. $Q,V$ are the packing matrices of query vectors and value vectors as described in Sec.\ref{sec.preliminary}. Alg.~\ref{algo.grpattn} outputs the packing matrix $O$ for new feature emebddings $\{o_1,...,o_n\}$, where $o_i$ corresponds to the feature embedding of $win_i$. 
Lines 2-3 implement the embedding aggregation operation, while Lines 8-11 implement the group softmax function.

\subsection{The Algorithms and Optimality Proof for Dynamically Determing Batch Size}
\label{appendix.batch}

\begin{algorithm}[h]
    \caption{Binary Search for Batch Size}
    \label{algo.binsearch}
    \begin{algorithmic}[1] 
    \Require $L,N$
    \Ensure $1 \leq L \leq L_{max}, 1\leq N \leq L$
            \Function {binary\_search}{$L,N$}
                \State $L \gets 1$
                \State $R \gets MaxBatchSize$
                \State $data \gets RandomTimeSeries\ in\ length\  L$
                \State $B_{temporal}$
                \While{$L \leq R$}
                     
                    \State $Input \gets data \times B_{temporal}$
                    \State $ModelForward(Input)$
                    \State $ModelBackward$
                    \State $u \gets  \frac{PeakMemoryUsage}{TotalMemory}$
                    
                    \If{$0.9 > u$} 
                    \State {$L \gets B_{temporal}+1$}
                    \State {$B \gets B_{temporal}$}
                    \Else 
                    \State{$R \gets B_{temporal}-1$}
                    \EndIf
                    \State $B_{temporal}\gets \frac{\lfloor L+R \rfloor}{2}$
                \EndWhile
                
                \State \Return{$B$}
            \EndFunction
            
        \end{algorithmic}
\end{algorithm}


\begin{algorithm}[h]
    \caption{Dynamic Programming for Plane Division}
    \label{algo.dpdiv}
    \footnotesize
    \begin{algorithmic}[1] 
    \Require $L_i,N_i,B_i,L_{max}$
    \Ensure $1 \leq L_i \leq L_{max}, 1\leq N_i \leq L_i$
            \Function{cost}{S}
                \If{$|S|<M$}
                    \Return{$+\infty$}
                \EndIf
                \State{$L,N,B \gets points\ in\ S$}
                \State{$f \gets function\ fitting(B|L,N)$}

\Return{$E(B,L,N|f)$}
            \EndFunction
                
            \Function {dynamic\_programming}{$L_i,N_i,L_{max}$}
                \For {$l_1=1 \to L_{max}$}
                    \For {$l_2=1 \to l_1$}
                        \For{$n=1 \to l_1$}
                            \State{$S \gets points\ set\ in\ \{l_2 \leq L \leq l_1,N \leq n\}$}
                            \State{$g(n) \gets COST(S)$ }
                            \For{$i=1 \to n$}
                                \State {$S \gets  points\ set\ in\ \{l_2 \leq L \leq l_1, i\leq N \leq n\}$}
                                \State{$g(n) \gets min(g(n),g(i)+COST(S))$}
                            \EndFor
                        \EndFor
                    \State{$f_{l_2,l_1} \gets g(l_1)$}
                    \EndFor
                \EndFor
            
                \State{}
            
                \For {$l=1 \to L_{max}$}
                    \State {$dp(l) \gets f(1,l)$}
                    \For {$i=1 \to l$}
                        \State {$dp(l) \gets min(dp(l),dp(i)+f(i,l))$}
                    \EndFor
                \EndFor
                \Return{$dp(L_{max})$}
            \EndFunction
            
        \end{algorithmic}
\end{algorithm}

We describe Alg.~\ref{algo.dpdiv} and intuitively show its optimality. 
We assume that Scipy~\cite{2020SciPy-NMeth} learns an optimal function in Line 4 so that function COST gives the optimal estimation error when fitting the points in set $S$. 
When fitting very few points, we assign an infinite cost to prevent a biased fitting function (Line 2).  
$g(n)$ denotes the minimal estimation error for points in sub-plane $\{l_2 \leq L \leq l_1, N \leq n\}$. In Lines 11-13, we enumerate all possible ways of cutting $\{l_2 \leq L \leq l_1, N \leq n\}$ horizontally into two sub-plane $\{l_2 \leq L \leq l_1, N \leq i\}$ and $\{l_2 \leq L \leq l_1, i \leq N \leq n\}$ by iterating $i$ from 1 to n. 
Choosing the cutting strategy that minimizes estimation error gets us a $g(l_1)$ with minimal estimation error for sub-plane $\{l_2 \leq L \leq l_1, N \leq l_1\}$, which is recorded as $f_{l_1,l_2}$ in Line 14. 
$dp(l)$ denotes the minimal estimation error for sub-plane $\{L \leq l\}$. 
We enumerate all the possible ways of cutting $\{ L \leq l\}$ vertically into two sub-plane $\{  L \leq i\}$ and $\{i \leq L \leq l\}$ by iterating $i$ from 1 to $l$ (Line 17-19). Finally, we have the minimal estimation error for the whole plane as $dp(L_{max})$. 
Based on the above discussion, this algorithm guarantees to not miss any better solution, hence optimal.

% \noindent\textbf{Learning the Prediction Function.} We use \textit{curve fit}    
% The estimation error is defined as the difference between estimated batch size and measured batch size $E(B,L,N | f)=\sum |B_i-f(L_i,N_i)|$. 

% The number of sub-planes $m$ is defined by users and the division is determined by a dynamic programming algorithm
% (Alg.~\ref{algo.dpdiv}). It is {\bf optimal} in minimizing the total estimation error on all sub-planes, i.e. $\mathit{min \  E(B,L,N | f_1,f_2,...,f_m)=\sum_{i=1}^{m}\sum |B_{i_j}-f_i(L_{i_j},N_{i_j})|}$.

\subsection{The Correctness of Group Attention}
\label{appendix.proof.groupAttention}
\begin{lemma}
\label{lm.grpattnalgo}
\sloppypar
Assuming the windows belonging to the same group $G_i$ have the same key vector, i.e. $k_j=r_i (win_j \in G_i)$, then the feature embedding $O$ produced by the original self-attention mechanism is identical to the output of our group attention mechanism implemented in Algorithm~\ref{algo.grpattn}.

%outputs the same \textbf{o} with \eqref{eq.grpattn1} \eqref{eq.grpattn2} \eqref{eq.grpattn3}.
\end{lemma}

\begin{proof}
Denote $\widetilde{k_j}$ to be the representative vectors of $k_j$, i.e. $\widetilde{k_j}=r_i=k_j (win_j \in G_i)$. Algorithm~\ref{algo.grpattn} gives that
\begin{equation}
\label{eq.grpres}
\small
\begin{aligned}
    \widetilde{v}_i&=\sum_{j=0}^{n-1}(BELONG_j==i)\mathbf{v}_j, \ \widetilde{P}_{i,j}=\mathbf{q}_i \cdot \mathbf{r}_j\\
    s_i&=\sum_{j=0}^{N-1}exp(\widetilde{P}_{i,j})COUNT_j, \ \widetilde{o}_i=\sum_{j=0}^{N-1}\frac{\widetilde{P}_{i,j}}{s_i}\widetilde{v}_j
\end{aligned}
\end{equation}

By the canonical self-attention mechanism introduced in Sec.~\ref{sec.preliminary}, we get:
\begin{equation}
\small
\label{eq.grpattn1}
P_{i,j}=\mathbf{q}_i \cdot \mathbf{k_j},\ A_{i,j}=\frac{exp(P_{i,j})}{\sum_{k=0}^{n-1}exp(P_{i,k})}, \ \mathbf{o}_i=\sum_{j=0}^{n-1}A_{i,j}\mathbf{v}_j
\end{equation}

% \begin{equation}
% \label{eq.grpattn2}
% A_{i,j}=\frac{exp(P_{i,j})}{\sum_{k=0}^{n-1}exp(P_{i,k})}
% \end{equation}

% \begin{equation}
% \label{eq.grpattn3}
% \mathbf{o}_i=\sum_{j=0}^{n-1}A_{i,j}\mathbf{v}_j
% \end{equation}

With \ref{eq.grpres} and \ref{eq.grpattn1}, we have
\begin{equation}
\label{eq.sumexpahat}
\small
\begin{aligned}
    \sum_{j=0}^{n-1}exp(P_{i,j})&=\sum_{j=0}^{n-1}exp(\mathbf{q}_i \cdot \mathbf{k}_j)\\
    &=\sum_{j=0}^{N-1} \sum_{x=0}^{n-1}(BELONG_x==j)exp(\mathbf{q}_i \cdot \mathbf{k}_x)\\
    &=\sum_{j=0}^{N-1} exp(\mathbf{q}_i \cdot \mathbf{r}_j) \sum_{x=0}^{n-1} (BELONG_x==j) \\
    &=\sum_{j=0}^{N-1} exp(\mathbf{q}_i \cdot \mathbf{r}_j) COUNT_j
    \\
    &=\sum_{j=0}^{N-1} exp(\widetilde{P}_{i,j}) COUNT_j\\
    &=s_i\\
\end{aligned}
\end{equation}


Further,
\begin{equation}
\label{eq.output}
\small
\begin{aligned}
\mathbf{o}_i&=\sum_{j=0}^{n-1} A_{i,j}\mathbf{v_j} \\
&=\sum_{j=0}^{N-1}\sum_{x=0}^{n-1} (BELONG_x==j)A_{i,x}\mathbf{v}_x \\
&=\sum_{j=0}^{N-1}\sum_{x=0}^{n-1}(BELONG_x==j)\frac{exp(P_{i,x})}{\sum_{k=0}^{n-1}exp(P_{i,k})}\mathbf{v}_x\\
&=\sum_{j=0}^{N-1}\sum_{x=0}^{n-1}(BELONG_x==j)\frac{exp(\mathbf{q}_i \cdot \mathbf{k}_x)}{\sum_{k=0}^{n-1}exp(P_{i,k})}\mathbf{v}_x\\
&=\sum_{j=0}^{N-1}\sum_{x=0}^{n-1}(BELONG_x==j)\frac{exp(\mathbf{q}_i \cdot \mathbf{r_j})}{\sum_{k=0}^{n-1}exp(P_{i,k})}\mathbf{v}_x\\
&=\sum_{j=0}^{N-1} \frac{exp(\mathbf{q}_i \cdot \mathbf{r_j})}{\sum_{k=0}^{n-1}exp(P_{i,k})} \sum_{x=0}^{n-1}(BELONG_x==j)\mathbf{v}_x\\
&=\sum_{j=0}^{N-1} \frac{exp(\mathbf{q}_i \cdot \mathbf{r_j})}{\sum_{k=0}^{n-1}exp(P_{i,k})} \widetilde{v_j}\\
\end{aligned}
\end{equation}

Combining \eqref{eq.grpres}, \eqref{eq.sumexpahat} \eqref{eq.output}, we have
$\mathit{\mathbf{o}_i=\sum_{j=0}^{N-1}\frac{\widetilde{P}_{i,j}}{s_i}\widetilde{v}_j=\widetilde{o}_i}$.

This concludes that the output of our group attention is identical to vanilla self-attention's.
\end{proof}


\subsection{The Proof of Error Bound (Lemma 1)}
\label{appendix.proof.errorBound}
\begin{proof}
\renewcommand{\qedsymbol}{}
We have
\begin{equation}
\begin{aligned}
    \frac{exp(\overline{P}_{i,j})}{exp(P_{i,j})}&=\frac{exp({\mathbf{q}}_i \cdot \widetilde{\mathbf{k}}_j)}{exp(\mathbf{q}_i \cdot \mathbf{k}_j)} = exp({\mathbf{q}}_i \cdot (\widetilde{ \mathbf{k}}_j-\mathbf{k}_j))\\
%   &= exp({\mathbf{q}}_i \cdot (\widetilde{ \mathbf{k}}_j-\mathbf{k}_j))\\
   &=exp(||\mathbf{q}_i|| \cdot ||\widetilde{\mathbf{k}}_j-\mathbf{k}_j||\cdot cos(\mathbf{q}_i,\widetilde{\mathbf{k}}_j-\mathbf{k}_j))
\end{aligned}
\end{equation}

So
\begin{equation}
\label{equ.abound}
exp(-dR) \leq \frac{exp(\overline{P}_{i,j})}{exp(P_{i,j})} \leq exp(dR)
\end{equation}

Then we have:
\begin{equation}
\label{equ.Aequ}
\begin{aligned}
\frac{\overline{A}_{i,j}}{A_{i,j}}&=\frac{exp(\overline{P}_{i,j})}{\sum_{k=1}^{n} exp(\overline{P}_{i,k})} / \frac{exp({P}_{i,j})}{\sum_{k=1}^{n} exp({P}_{i,k})}\\
&=\frac{exp(\overline{P}_{i,j})}{exp({P}_{i,j})} \frac{\sum_{k=1}^{n} exp({P}_{i,k})}{\sum_{k=1}^{n} exp(\overline{P}_{i,k})}
\end{aligned}
\end{equation}

Combining (\ref{equ.abound}) (\ref{equ.Aequ}), the error is bounded by
\begin{equation}
\label{equ.Abound}
exp(-2dR) \leq \frac{\overline{A}_{i,j}}{A_{i,j}} \leq exp(2dR)
\end{equation}

Thus, if $\mathit{d \leq \frac{\ln(\epsilon)}{2R}}$, $\mathit{\frac{1}{\epsilon} \leq \frac{\overline{A}_{i,j}}{A_{i,j}} \leq \epsilon}$. This proves Lemma~\ref{lm.grperrorbound}.
\end{proof}



\subsection{The Proof of Merge Operation (Lemma 2)}
\label{appendix.proof.merge}
\begin{proof}
\renewcommand{\qedsymbol}{}
Denote the cluster size of $cluster_k$ to be $n_k$.After mergeing, the new center will be: $$c'= \frac{\sum_{i=1}^m n_{k_i}c_{k_i}}{\sum_{i=1}^m n_{k_i}}$$
For $\forall i \in [1,m],\forall x \in cluster_{k_i}$, it holds that:
\begin{equation}
\small
\label{eq.tri}
\begin{aligned}
        |x-c'| &\leq |x-c_{k_i}|+|c_{k_i}-c'|\  (Triangle\  inequality)\\
        &=|x-c_{k_i}|+| \frac{\sum_{j=1}^m n_{k_j}}{\sum_{j=1}^m n_{k_j}}c_{k_i}-
        \frac{\sum_{j=1}^m n_{k_j}c_{k_j}}{\sum_{j=1}^m n_{k_j}}|\\
        &=|x-c_{k_i}|+| 
         \frac{\sum_{j=1}^m n_{k_j}(c_{k_i}-c_{k_j})}{\sum_{j=1}^m n_{k_j}}|\\
        &=|x-c_{k_i}|+\frac{|\sum_{j=1}^m n_{k_j}(c_{k_i}-c_{k_j})|}{\sum_{j=1}^m n_{k_j}}\\
        &\leq |x-c_{k_i}|+\frac{\sum_{j=1}^m n_{k_j}|c_{k_i}-c_{k_j}|}{\sum_{j=1}^m n_{k_j}}\\
        &= \frac{\sum_{j=1}^m n_{k_j}(|c_{k_i}-c_{k_j}|+|x-c_{k_i}|)}{\sum_{j=1}^m n_{k_j}}\\
        &\leq \frac{\sum_{j=1}^m n_{k_j}d}{\sum_{j=1}^m n_{k_j}} =d
\end{aligned}
\end{equation}
\end{proof}

\subsection{Downstream Tasks}
\label{appendix.downstream}
\system supports a variety of downstream tasks. In this section, we show that with minimal modification \system can effectively support classification, imputation and forecasting tasks. 
Other unsupervised tasks such as similarity search or clustering are naturally supported by extracting feature embeddings from \system.

\subsubsection{\textbf{Classification}\nopunct}\ \\
To classify timeseries, we input timeseries to the model as described in Sec.~\ref{sec.rita} and attach a special token \textbf{[CLS]} as the first input embedding. \textbf{[CLS]}'s embedding acts as the embedding for the entire timeseries, and the output representation of \textbf{[CLS]} is fed into a classifier: $\mathit{y=Softmax(W_{cls}Z_{[CLS]}+B_{cls})}$, where $Z_{[CLS]}\in \mathbb{R}^d$ is the output representation of \textbf{[CLS]}, C is the number of classes, and $\mathit{W_{cls} \in \mathbb{R}^{C \times d}, B_{cls} \in \mathbb{R}^{C}}$ are learnable parameters for classification task. 
The result vector $y\in \mathbb{R}^{C}$ represents the possibility that the input timeseries belongs to each class.

We apply Cross Entropy Loss as the loss function of the classification task~\cite{cox1958regression}:
$\mathit{L=\frac{1}{C}\sum_{i=1}^C -\hat{y}(i)log(y(i))}$, where $\hat{y}$ is a binary indicator for ground truth label:
\vspace{-0.5mm}
\begin{eqnarray}
\hat{y}(i) =
\begin{cases}
1   & i\  \text{is ground truth label} \\
0   & otherwise
\end{cases}
\end{eqnarray}

%or replaced with reasonable values
\subsubsection{\textbf{Imputation}\nopunct}\ \\
\label{sec.transformer.imputation}
Timeseries are mainly generated by sensors, a common problem of which is missing values. This becomes a challenge when many downstream analytics require the missing values to be recovered. The recovering task is imputation. 

Denote the real timeseries as $T_{r} \in \mathbb{R}^{t \times m}$, the observed timeseries with missing values as $T_{o} \in \mathbb{R}^{t \times m}$, and the set of missing values' positions as $M$. We scale the values of all timeseries to non-negative and use a special value (-1) to indicate missing values:
\vspace{-0.5mm}
\begin{eqnarray}
\label{eq.imp_task}
T_{o}(i,j) =
\begin{cases}
-1   & (i,j) \in M\\
T_{r}(i,j)   & (i,j) \notin M \\
\end{cases}
\end{eqnarray}

$T_{o}$ is fed into the \system as input, and the output representations are concatenated and fed into a {\it Transpose Convolution} layer which decodes the output embedding vectors from hidden space to timeseries values, corresponding to the convolution operation in the input stage, i.e., 
$\mathit{Y=TransposeCNN(Z_1 \textcircled{+} Z_2 \textcircled{+} ... \textcircled{+} Z_n)}$, where $Y \in \mathbb{R}^{t \times m}$ is the recovered timeseries, and $Z_i \in \mathbb{R}^d$ is the output of each position.

Here Mean Square Error is chosen as the loss function~\cite{thompson1990mse}:
$L=\frac{1}{|M|}\sum_{(i,j) \in M} (Y(i,j)-T_{r}(i,j))^2$.

\subsubsection{\textbf{Forecasting}\nopunct}\ \\
Forecasting can be regarded as a special case of imputation, in which all missing values are at the end of timeseries. 

So like in imputation task, we scale the timeseries to non-negative and use a special value (-1) to indicate the values to be predicted:
\begin{eqnarray}
T_{observed}(i,j) =
\begin{cases}
T_{real}(i,j)   & i \leq t_{observed} \\
-1   & otherwise
\end{cases}
\end{eqnarray}

Where $t_{observed}$ is the observed timestamp. Then the output representations are fed into a Transpose Convolution layer using Mean Squared Error as loss function, as described above.

\subsubsection{\textbf{Other Unsupervised Tasks}\nopunct}\ \\
\system naturally supports other unsupervised tasks, such as similarity search and clustering~\cite{lin1995fast,keogh2001dimensionality,liao2005clustering}, by producing the embedding of one timeseries (output representation of the special token \textbf{[CLS]}).
Clustering can be performed on the embeddings with flexible choice of distance metrics. Similarly, a high dimensional similarity search system~\cite{johnson2019billion, malkov2018efficient, jegou2010product} can be built on the embeddings. 


\subsection{Inference Time}
\label{sec.sup.evaltime}





\begin{table}[htbp]
\centering
\small
\scalebox{0.88}{
\begin{tabular}{cc|c|c|c|c|c}
\toprule
Dataset &  Length & {TST\cite{DBLP:conf/kdd/ZerveasJPBE21}}& Vanilla & Performer & Linformer & Group Attn.\\
 \cline{3-7}
 \hline
 WISDM & 200 & 2.18 & 2.26 & 2.35 & 2.22 & 2.17\\
 HHAR & 200  & 1.19 & 1.23 & 1.28 & 1.21 & 1.18\\
 RWHAR & 200 & 1.32 & 1.37 & 1.42 & 1.34 & 1.31\\
ECG & 2000 & 18.44 & 15.26 & 5.80 & 6.08 & 5.16\\
 \bottomrule
\end{tabular}}
\caption{Inference time:  Classification on multi-variate data \quad \quad \quad (seconds).}
\label{tab.infcls}
\end{table}







\begin{table}[htbp]
\centering
\small
\scalebox{0.88}{
\begin{tabular}{cc|c|c|c|c|c}
\toprule
Dataset &  Length & 
 {TST\cite{DBLP:conf/kdd/ZerveasJPBE21}} & Vanilla & Performer & Linformer & Group Attn.\\
 \cline{3-7}
 \hline
 WISDM & 200 & 2.03 & 2.11 & 2.19 & 2.07 & 2.02\\
 HHAR & 200 & 1.11 & 1.14 & 1.19 & 1.12 & 1.10\\
 RWHAR & 200 & 1.23 & 1.27& 1.32& 1.25& 1.22 \\
ECG & 2000 & 17.22 & 14.32 & 4.73 & 4.99 & 4.11\\
MGH & 10000 & N/A & N/A & 6.58 & 6.88 & 1.35\\
 \bottomrule
\end{tabular}}
\caption{Inference time: Imputation on multi-variate data (seconds). }
\label{tab.infimp}
\vspace{-5mm}
\end{table}


In this section, we present the average inference time on validation sets. The results in Table.~\ref{tab.infcls} and ~\ref{tab.infimp} correspond to the average inference time on validation sets of classification and imputation tasks, respectively. Consistent with the results in Section.~\ref{sec.exp.efficiency}, our method Group Attn. outperforms the baselines on both classification and imputation tasks, particularly on the datasets comprising long timeseries (ECG and MGH).


\end{sloppypar}



 










