\section{\system overview}
\label{sec.rita}
Given a collection of {\it unlabeled} timeseries, \system first pre-trains a Transformer-style model to produce high quality feature embeddings for timeseries data. This pre-trained model is then used to support various downstream tasks, similar to BERT~\cite{DBLP:conf/naacl/DevlinCLT19}. Next, we overview the model architecture of \system. We show how \system supports various downstream tasks in Appendix~\ref{appendix.downstream}. 

\begin{sloppypar}
% \subsection{Model Architecture}

As shown in Fig.~\ref{fig.convolution},
RITA is consist of two components: (1) Time-aware Convolution Layer (2) RITA Encoder.

\noindent\textbf{Time-aware Convolution Layer} fills the gap between timeseries and natural language. Despite their high-level similarity, there is a big gap between timeseries and natural language. 
First, in natural language each word, as a discrete semantic unit, has an independent meaning, while each element in a timeseries is a continuous, numerical value and does not necessarily constitute an independent event. 
Furthermore, the input sequences are single-channeled in NLP, but often multi-channeled in timeseries (i.e., sensor data often consists of several related channels).

\system leverages the classical convolution~\cite{NIPS2012_c399862d} strategy to solve this problem. Convolution is widely used to capture the local structures of an image. We use convolution to chunk one input timeseries into a sequence of windows and learn the local structure of each window, similar to the discrete semantic units in natural language. 
It also discovers the correlations across different channels, thus naturally solving the multi-channel problem. 

More specifically, treating a multi-variate timeseries of length $n$ and with $m$ variables as an $\mathit{n \times m}$ matrix $T$, \system uses $d$ convolution kernels to chunk $T$ into \textbf{n} windows and produce one d-dimensional embedding per window using the convolution operation~\cite{NIPS2012_c399862d}. Each convolution kernel corresponds to a $\mathit{w \times m}$ matrix, where $w$ defines the number of timestamps that each convolution kernel covers, identical to the window size in sliding window. 

% Formally, we denote a raw timeseries as $T \in R^{t*m}$, corresponding to a multi-variate timeseries of length $t$ and with $m$ variables. The feature embeddings of $n$ windows $E_1,..,E_n (E_i \in R^d)$ are generated by:
% \vspace{-1mm}
% \begin{equation}
% \label{eq.convolution}
% E_{i,j}=\sum_{k=0}^m  \sum_{l=0}^{KernelSize}W_{j,k,l}*T_{i+l,k}+ B_{j,k} 
% \end{equation}

% In Eq.~\ref{eq.convolution}, $W_i$ $\in$ $\mathbb{R}^{ m \times KernelSize}, B_i \in \mathbb{R}^{m} (i=0,1,...,d-1)$ corresponds to the weight and bias of $d$ convolution kernels. $KernelSize$ defines the number of timestamps that each convolution kernel covers, identical to the window size in sliding window. Such time-aware convolution are proved to be effective in generating embeddings for windows \cite{DBLP:conf/kdd/ZerveasJPBE21}.

% Finally, as mentioned before, temporal order is important in timeseries analytics. We thus use position embedding to make \system aware of the time ordering among the local embeddings extracted by convolution. 
% In addition, a special embedding vector \textbf{[CLS]} is attached at the start of input, serving as a global information aggregator.  
% To be specific, input $X$ is generated by $\mathit{X=[CLS] \textcircled{+} (E+P)}$, where $\mathit{P=[p_1,p_2,...,p_n]}$, $p_i$ $\in$ $\mathbb{R}^d$ is the embedding of position $i$ and \textcircled{+} denotes concatenation. 
% As is widely adopted in NLP, we use learnable position embedding~\cite{DBLP:conf/naacl/DevlinCLT19}, which means all $p_i$ are trainable parameters. 

\noindent\textbf{RITA Encoder} functions as Transformer Encoder as described in the original Transformer work\cite{DBLP:conf/nips/VaswaniSPUJGKP17}. It takes the embeddings of $n$ semantic units $X_1,X_2,...,X_n (X_i \in R^d)$ as input (e.g. embeddings of $n$ windows for a timeseries), then models the correlations between the semantic units and outputs $Y_1,...,Y_n (Y_i \in R^d)$ as the context-aware embedding of each unit. %We don't use the Decoder part like Transformer because it is for generative tasks solely such as translation in NLP.

What makes RITA Encoder different from Transformer Encoder is that: at the core of Transformer Encoder lies self-attention mechanism which incurs a $O(n^2)$ time complexity and memory usage. This quadratic cost becomes prohibitive for long timeseries and limits the scalablity of Transformer-based models. To make the attention computation efficient yet high-quality, we replace the canonical self-attention with our proposed {\bf group attention}.
%as illustrated in Sec.~\ref{sec.group}

\noindent\textbf{Self-supervised Pretraining.}
% %\label{sec.transformer.pretraining}
Inspired by the ``cloze text'' pretraining task in NLP, we designed a mask-and-predict task as the pretraining task for our model. The timeseries is randomly masked and the model should recover the masked values based on corresponding contextual information.

To be specific, we generate masks on time-stamps, with a mask rate $p$. The timeseries is scaled to be non-negative and the values across all the channels on the masked timestamps are set to be -1, an impossible value on normal timestamps. Then the masked timeseries is fed into \system and the output representation is translated to the recovered timeseries by a Transpose Convolution layer.

\begin{comment}
\subsection{Downstream Tasks}
\label{sec.transformer.downstream}

\system supports a variety of downstream tasks, as shown in Fig.~\ref{fig.overview}. In this section, we show that with minimal modification \system can effectively support classification, imputation and forecasting tasks. 
Other unsupervised tasks such as similarity search or clustering are naturally supported by extracting feature embeddings from \system.

\subsubsection{Classification\nopunct}\ \\ 
To classify timeseries, we input timeseries to the model as described in Sec.~\ref{sec.rita.encoder}, and feed the output representation of \textbf{[CLS]} into a classifier: $\mathit{y=Softmax(W_{cls}Z_{[CLS]}+B_{cls})}$, where $Z_{[CLS]}\in \mathbb{R}^d$ is the output representation of \textbf{[CLS]}, C is the number of classes, and $\mathit{W_{cls} \in \mathbb{R}^{C \times d}, B_{cls} \in \mathbb{R}^{C}}$ are learnable parameters for classification task. 
The result vector $y\in \mathbb{R}^{C}$ represents the possibility that the input timeseries belongs to each class.

We apply Cross Entropy Loss as the loss function of the classification task~\cite{cox1958regression}:
$\mathit{L=\frac{1}{C}\sum_{i=1}^C -\hat{y}(i)log(y(i))}$, where $\hat{y}$ is a binary indicator for ground truth label:
\vspace{-1mm}
\begin{eqnarray}
\hat{y}(i) =
\begin{cases}
1   & i\  \text{is ground truth label} \\
0   & otherwise
\end{cases}
\end{eqnarray}

%or replaced with reasonable values
\subsubsection{Imputation\nopunct}\ \\
\label{sec.transformer.imputation}
Timeseries are mainly generated by sensors, a common problem of which is missing values. This becomes a challenge when many downstream analytics require the missing values to be recovered. The recovering task is imputation. 

Denote the real timeseries as $T_{r} \in \mathbb{R}^{t \times m}$, the observed timeseries with missing values as $T_{o} \in \mathbb{R}^{t \times m}$, and the set of missing values' positions as $M$. We scale the values of all timeseries to non-negative and use a special value (-1) to indicate missing values:
\vspace{-1mm}
\begin{eqnarray}
\label{eq.imp_task}
T_{o}(i,j) =
\begin{cases}
-1   & (i,j) \in M\\
T_{r}(i,j)   & (i,j) \notin M \\
\end{cases}
\end{eqnarray}

$T_{o}$ is fed into the \system as input, and the output representations are concatenated and fed into a {\it Transpose Convolution} layer which decodes the output embedding vectors from hidden space to timeseries values, corresponding to the convolution operation in the input stage, i.e., 
$\mathit{Y=TransposeCNN(Z_1 \textcircled{+} Z_2 \textcircled{+} ... \textcircled{+} Z_n)}$, where $Y \in \mathbb{R}^{t \times m}$ is the recovered timeseries, and $Z_i \in \mathbb{R}^d$ is the output of each position.

Here Mean Square Error is chosen as the loss function~\cite{thompson1990mse}:
$L=\frac{1}{|M|}\sum_{(i,j) \in M} (Y(i,j)-T_{r}(i,j))^2$.

\vspace{-2mm}
\subsubsection{Forecasting\nopunct}\ \\
%\vspace{-0.5mm}
Forecasting can be regarded as a special case of imputation, in which all missing values are at the end of timeseries. 
Due to the space limit, please refer to the supplementary materialwe for the details.

\vspace{-2mm}
\subsubsection{Other Unsupervised Tasks\nopunct}\ \\
%\vspace{-0.5mm}

\system naturally supports other unsupervised tasks, such as similarity search and clustering~\cite{lin1995fast,keogh2001dimensionality,liao2005clustering}, by producing the embedding of one timeseries (output representation of the special token \textbf{[CLS]}).
Clustering can be performed on the embeddings with flexible choice of distance metrics. Similarly, a high dimensional similarity search system~\cite{johnson2019billion, malkov2018efficient, jegou2010product} can be built on the embeddings. 

So like in imputation task, we scale the timeseries to non-negative and use a special value (-1) to indicate the values to be predicted:
\begin{eqnarray}
T_{observed}(i,j) =
\begin{cases}
T_{real}(i,j)   & i \leq t_{observed} \\
-1   & otherwise
\end{cases}
\end{eqnarray}

Where $t_{observed}$ is the observed timestamp. Then the output representations are fed into a Transpose Convolution layer using Mean Squared Error as loss function, as described above.
\end{comment}

\begin{comment}
\vspace{-2mm}
\subsection{Self-supervised Pretraining}
\label{sec.transformer.pretraining}
Inspired by the ``cloze text'' pretraining task in NLP, we designed a mask-and-predict task as the pretraining task for our model. The timeseries is randomly masked and the model should recover the masked values based on corresponding contextual information.

To be specific, we generate masks on time-stamps, with an expected mask rate $p$. The timeseries is scaled to be non-negative and the values across all the channels on the masked timestamps are set to be -1, an impossible value on normal timestamps. Then the masked timeseries is fed into \system and the output representation is translated to the recovered timeseries by a Transpose Convolution layer.

Pretraining encourages \system to learn the dependencies across different channels over time. Meanwhile, pretraining on unlabeled timeseries gives the model a prior knowledge of data distribution of the corresponding domain, thus mitigating the issue of overfitting when training with few labeled data on downstream tasks.
\end{comment}

\end{sloppypar}














