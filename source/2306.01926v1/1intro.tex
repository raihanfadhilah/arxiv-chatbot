%\vspace{-3mm}
\section{Introduction}
\label{sec.intro}
\noindent\textbf{Motivation.} Many data driven applications involve processing massive timeseries data, including IoT~\cite{cook2019anomaly}, medical AI~\cite{crabtree1990individual}, stock market~\cite{kraft1977determinants}, and so on. As such, there is a great need for timeseries analytics, such as forecasting~\cite{chatfield2000time}, classification~\cite{ismail2019deep}, clustering~\cite{liao2005clustering}, similarity search~\cite{negi2005time}, and anomaly detection~\cite{teng2010anomaly}, with applications ranging from automatically diagnosing diseases~\cite{bui2017time}, recognizing human activities~\cite{lara2012survey}, to stopping financial fraud~\cite{yue2007review}. 


%However, to date, applications still mainly use traditional techniques such as ARIMA~\cite{arima}, TS-CHIEF~\cite{Shifaz2020TSCHIEFAS}, HIVE-COTE~\cite{10.1145/3182382}, and ROCKET~\cite{DBLP:journals/datamine/DempsterPW20}, to analyze timeseries data, which either heavily rely on humans to {\it manually extract features} based on their domain knowledge. %or are inadequate in handling complex multi-variate timeseries data. 

Effective feature extraction~\cite{paparrizos2019grail} lies at the core of almost all these timeseries analytics tasks. Recently researchers~\cite{DBLP:conf/kdd/ZerveasJPBE21} have started leveraging the {\it self-supervised pre-training} methodology of Transformers~\cite{DBLP:conf/nips/VaswaniSPUJGKP17,DBLP:conf/naacl/DevlinCLT19,brown2020language}, which have proven remarkably successful in natural language processing (NLP), to automatically learn high quality feature embeddings from timeseries. In NLP, self-supervised pre-training exploits the sequential patterns (correlations) among the words in sentences to produce {\it contextualized} feature embeddings. Timeseries bear similarity to natural language, because in timeseries data the sequential order among the values (stock  price, volume, etc.) over time matters. That is, each value is highly correlated with other values observed before or after it. Therefore, pre-training a Transformer model which takes the correlations among different observations into account is a natural idea to learn feature embeddings from timeseries. Indeed, the experiments in \cite{DBLP:conf/kdd/ZerveasJPBE21} confirm that Transformer-based methods outperform traditional timeseries analytics techniques.
 
%Recent attempts \cite{DBLP:conf/kdd/ZerveasJPBE21} have been made to apply the SOTA model in NLP -- Transformer\cite{DBLP:conf/nips/VaswaniSPUJGKP17}, also known as the foundation model of BERT/GPT-3\cite{DBLP:conf/naacl/DevlinCLT19,brown2020language}. 

However, existing work~\cite{DBLP:conf/kdd/ZerveasJPBE21} that directly applies Transformers to learn features from timeseries data have been shown not to be scalable to {\it long} timeseries~\cite{li2019enhancing}. The idea of self-attention~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} is central to pre-training methods in NLP: It computes pairwise correlations among different semantic units in a sequence (in NLP, a sentence); as such, it has {\it quadratic time and space} complexity in the length of the input sequence. 
Such an approach places limits on the model's scalability, especially when handling large sequences,  which are common in real-world timeseries applications such as IoT, medical AI, and finance~\cite{zhou2021informer,DBLP:journals/pvldb/CaoTAJYLGSBSCWM19,liu2018open}. Predictions about timeseries may need to look at months or years of historical data to make accurate predictions, spanning hundreds of thousands of samples.  
As an example, in collaboration with a research hospital we have been developing a seizure classifier that automatically detects seizures based on EEG signals (timeseries) collected during the clinical observation of patients. As seizures last only a few seconds, we chunk long EEG data into many 2 second segments and detect seizures at a segment level. However, the classification of a particular segment depends on up to 12 hours of prior signal to determine if one 2 second segment indicates seizure or not, because seizure diagnosis needs to consider long-term trends in the EEG data~\cite{DBLP:journals/pvldb/CaoTAJYLGSBSCWM19}. The number of segments in 12 hours is more than 21k. 
This is far larger than the number of semantic units the typical NLP tasks expect. For example, BERT~\cite{DBLP:conf/naacl/DevlinCLT19} limits the number of units to 512 and even massive models like GPT-3~\cite{brown2020language} limit the number of units to 2048.

Although in NLP some lower-complexity methods have been proposed to {\it approximately} compute self-attention~\cite{kitaev2020reformer,choromanski2020rethinking,wang2020linformer}, their performance degrades dramatically when used on timeseries, due to the gap between natural language and timeseries, as we will show in our experiments.

%\noindent\textbf{The \system Strategy.} 


% To tackle the aforementioned problems, we propose \textbf{RITA}, a general-purpose timeseries analytics tool. RITA is {\bf automatic} -- freeing humans from manual feature extraction, {\bf self-supervised} -- leveraging the existing plethora of unlabeled data to offer higher accuracy, and {\bf scalable} -- efficiently handling highly complex, massive-scale timeseries data. \system consists of two components: (1) a time-aware convolution to generate \textbf{\textit{context-free}} embeddings for units, i.e. windows, in timeseries; (2) an efficient variant Transformer-based encoder to model the correlations over time and generate the \textbf{\textit{context-aware}} embeddings. 


% Given a collection of {\it unlabeled} timeseries, \system 
%  firstly pre-trains the model by randomly masking some of the values from the input timeseries and predicting the masked values based on their contexts, inspired by the {\it{mask and predict}} task in NLP\cite{DBLP:conf/naacl/DevlinCLT19}. This pre-trained model is then used to support various downstream tasks. For example, as with BERT, \system supports {\it supervised tasks} such as classification by fine-tuning the pre-trained parameters to achieve high accuracy with {\it small number} of labels.
% \system also naturally supports other {\it analytics} tasks such as forecasting, missing value imputation. In addition, outliers can be identified as the observations that significantly deviate from their corresponding predictions~\cite{10.1007/978-3-030-77964-1_17}. {\it Unsupervised analytics tasks} such as clustering and similarity search can run directly on the feature embeddings produced by \system, which encode the long term trends of the timeseries and tend to be more informative than the raw features traditionally used in modeling the similarity of different timeseries objects~\cite{DBLP:conf/kdd/WangP21}.



%This is far more than the number of semantic units the typical NLP tasks expect. For example, GPT-3, which took tens of years of GPU time to train, limits the number of units to 2048.
%\vspace{-3mm}
\noindent\textbf{Proposed Approach.} 
To tackle the aforementioned problem, we develop \textbf{RITA}, a Transformer-based timeseries analytics tool, which uses a novel attention mechanism, called {\bf group attention}, to scale to long timeseries.

Leveraging the periodicity of timeseries, \system chunks the input timeseries into segments and dynamically clusters the segments into a small number (denoted as $N$) of groups. Segments in the same group possess similar feature embeddings during the current training iteration, thus enabling them to approximately share the computation of attention. As the timeseries increases in length, more sharing opportunities become available. \system then computes the self-attention at a group level and produces a {\it compressed group attention matrix}.
In this way, group attention eliminates both computation and memory bottlenecks in Transformer-style models and thus more scalable to long timeseries.

However, making this idea effective and efficient in Transformer architectures is {\it challenging} for several reasons:

\begin{compactitem}
\item \textbf{Efficiently Producing High Quality Feature Embeddings.} Although \system computes the attention matrix at a group level, to preserve the quality of the feature embeddings, it still has to produce different embeddings for different segments. This is because even if some segments share the attention score temporally, it does not mean they should have the same feature embedding. However, using the group attention matrix, the existing self-attention mechanism will only produce a single feature vector for each group. A naive solution would be to restore the original attention matrix from the group attention matrix. 
However, in this case we again get an attention matrix with quadratic space complexity. Because GPUs have limited memory, GPU memory will remain a {\it bottleneck} in group attention. 

%Note the attention matrix A is an $n \times n$ matrix, where $n$ represents the number of elements in the input sequence (e.g. words in NLP). 
%Because GPUs have limited memory, self-attention is not scalable to long sequence due to its quadratic space complexity.

\item \textbf{The Number of Groups N.} In \system, the number of groups $N$ is a crucial factor that balances the speed up and the quality of attention approximation. A small $N$ will lead to a large speedup, but the approximation errors can also be significant. On the other hand, although a large $N$ tends to produce high-quality approximations, it inevitably slows down the training process. Therefore, an appropriate $N$ is essential to the performance of group attention. However, $N$ depends on the distributional properties of the dataset. Furthermore, like the classical transformer models, \system stacks multiple attention layers to produce better embeddings. Ideally, different layers should also use different values of $N$. In addition, during the model training phrase, group attention should use different values of $N$ at different iterations to adapt to the varying feature embeddings.
This makes manually setting appropriate $N$ almost impossible. 

%Moreover, as we want to dynamically adjusts $N$ during training, the batch size should also vary.
\item \textbf{Batch Size.} Moreover, as we want to dynamically adjust $N$ during training, a fixed batch size is sub-optimal: as $N$ decreases, the memory usage of a single sample decreases. This allows a larger batch size which is beneficial, because: 
(1) it makes full use of GPU memory; (2) high-parallelism across the samples in a big batch brings better performance. 
Our experimental study shows that doubling the batch size reduces the training time by 30\%, while still preserving the quality of the model. Thus, \system should dynamically adjust batch size as $N$ changes.
\end{compactitem}

% In order to ensure successful group attention that is both effective and efficient, several challenges need to be addressed:

% (1) How do we guarantee good accuracy?

% (2) What are the optimal group sizes? What if the ideal groups sizes aren't even constant? 

%  (3) How do we efficiently compute group attention under GPU architectures?

% 
To address the above problems, we first propose an {\it embedding aggregation} strategy and a customized {\it group softmax function} to replace the classical softmax function~\cite{DBLP:conf/nips/VaswaniSPUJGKP17}. Together they ensure \system is able to directly use the compressed attention matrix to produce different feature embeddings for different segments. We theoretically show the embeddings \system produces in this way are identical to those produced by first re-storing the original large attention matrix. Thus \system is able to produce high quality embeddings without introducing extra overhead.
Further, we design a GPU friendly algorithm to group the segments {\it in parallel}, effectively minimizing the grouping cost.

% \textbf{To preserve the accuracy} on the downstream tasks, \system allows users to set an error bound $\epsilon$ according to their expected approximation quality of attention computation. Then RITA controls the grouping strategies to ensure the error bound is met. Moreover, we designed an {\it embedding aggregation} operation and a customized {\it group softmax function} to accurately produce feature embeddings from the compressed attention matrix. Notably, $\epsilon$ is the only parameter that needs to be supplied by users, thus making RITA user-friendly.

Second, we design an {\it adaptive scheduler} which dynamically decides an appropriate $N$ for each group attention layer during the training process. It starts with a large $N$ and iteratively merges groups that are similar to each other. Guided by an error bound on the approximated self-attention that users can tolerate, it automatically determines if two groups are mergeable, performing merging efficiently in a GPU-friendly way.

% \textbf{To make RITA efficient,}
% we design a GPU friendly strategy to group the segments {\it in parallel}, effectively minimizing the grouping overhead. 

Moreover, we propose a {\it learning-based method} to model the correlation between the number of groups $N$ and the batch size $B$. This model is used to predict $B$ for a given $N$ when training \system.
Specifically, we first sample some $N$ values in a reasonable range. For each sampled $N$, we find a batch size that consumes up to a certain percentage of GPU memory in a cost-efficient way. Using a small set of mathematical functions as a prior, \system learns a model with only a few <N, B> pairs as ground truth labels.    

Our experiments on public timeseries benchmarks and the MGH EEG data~\cite{DBLP:journals/pvldb/CaoTAJYLGSBSCWM19} confirm that \system outperforms  state-of-the-art methods in accuracy on various timeseries analytics tasks, while our group attention mechanism achieves a 63X speedup with much less memory required, compared to existing self-attention mechanisms~\cite{DBLP:conf/nips/VaswaniSPUJGKP17,choromanski2020rethinking,wang2020linformer}.

\noindent\textbf{Contributions.} The key contributions of this work include:

\begin{compactitem}
% \item We build \system, a general-purpose timeseries analytics tool that is automatic, self-supervised, scalable to high complexity, massive-scale timeseries data.

% \item We design a time-aware convolution operation to bridge the gap between timeseries and natural language. 

\item Our group attention mechanism leverages the periodicity of timeseries, reducing the time and space complexity of the self-attention mechanism with accuracy guarantees, allowing \system to scale to long timeseries data.

\item Guided by an approximation error bound, our adaptive scheduler dynamically adapts the number of groups and the batch size to the distribution properties of the evolving feature embeddings, making group attention efficient and easily tunable.

\item We conduct experiments on various datasets and different analytics tasks, demonstrating that \system is 4 to 63 times faster than the state-of-the-art while achieving better accuracy when handling long timeseries (length $\geq$ 2000).
%\jiaming{For an observable instance, \system is able to analyze the risk of seizures for 100 patients within a seconds, while the current SOTAs over a minute.}

%\item We conduct experiments on various datasets and different analytics tasks, confirming that \system is up to 63 times faster than the state-of-the-art, yet better in accuracy. 

\end{compactitem}
