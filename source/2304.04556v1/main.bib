
@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\QEHE3QJJ\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\BIPP7S7W\\1706.html:text/html},
}

@misc{ramsauer_hopfield_2021,
	title = {Hopfield {Networks} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/2008.02217},
	doi = {10.48550/arXiv.2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	month = apr,
	year = {2021},
	note = {arXiv:2008.02217 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\QVZB3WRG\\Ramsauer et al. - 2021 - Hopfield Networks is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\Q6ZSHDJ4\\2008.html:text/html},
}

@misc{millidge_universal_2022,
	title = {Universal {Hopfield} {Networks}: {A} {General} {Framework} for {Single}-{Shot} {Associative} {Memory} {Models}},
	shorttitle = {Universal {Hopfield} {Networks}},
	url = {http://arxiv.org/abs/2202.04557},
	abstract = {A large number of neural network models of associative memory have been proposed in the literature. These include the classical Hopfield networks (HNs), sparse distributed memories (SDMs), and more recently the modern continuous Hopfield networks (MCHNs), which possesses close links with self-attention in machine learning. In this paper, we propose a general framework for understanding the operation of such memory networks as a sequence of three operations: similarity, separation, and projection. We derive all these memory models as instances of our general framework with differing similarity and separation functions. We extend the mathematical framework of Krotov et al (2020) to express general associative memory models using neural network dynamics with only second-order interactions between neurons, and derive a general energy function that is a Lyapunov function of the dynamics. Finally, using our framework, we empirically investigate the capacity of using different similarity functions for these associative memory models, beyond the dot product similarity measure, and demonstrate empirically that Euclidean or Manhattan distance similarity metrics perform substantially better in practice on many tasks, enabling a more robust retrieval and higher memory capacity than existing models.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Millidge, Beren and Salvatori, Tommaso and Song, Yuhang and Lukasiewicz, Thomas and Bogacz, Rafal},
	month = jun,
	year = {2022},
	note = {arXiv:2202.04557 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\VXBI27IM\\Millidge et al. - 2022 - Universal Hopfield Networks A General Framework f.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\SYTBVGWU\\2202.html:text/html},
}

@misc{annabi_relationship_2022,
	title = {On the {Relationship} {Between} {Variational} {Inference} and {Auto}-{Associative} {Memory}},
	url = {http://arxiv.org/abs/2210.08013},
	abstract = {In this article, we propose a variational inference formulation of auto-associative memories, allowing us to combine perceptual inference and memory retrieval into the same mathematical framework. In this formulation, the prior probability distribution onto latent representations is made memory dependent, thus pulling the inference process towards previously stored representations. We then study how different neural network approaches to variational inference can be applied in this framework. We compare methods relying on amortized inference such as Variational Auto Encoders and methods relying on iterative inference such as Predictive Coding and suggest combining both approaches to design new auto-associative memory models. We evaluate the obtained algorithms on the CIFAR10 and CLEVR image datasets and compare them with other associative memory models such as Hopfield Networks, End-to-End Memory Networks and Neural Turing Machines.},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Annabi, Louis and Pitti, Alexandre and Quoy, Mathias},
	month = oct,
	year = {2022},
	note = {arXiv:2210.08013 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\3LB48ELU\\Annabi et al. - 2022 - On the Relationship Between Variational Inference .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\B46LWZGB\\2210.html:text/html},
}

@misc{locatello_object-centric_2020,
	title = {Object-{Centric} {Learning} with {Slot} {Attention}},
	url = {http://arxiv.org/abs/2006.15055},
	abstract = {Learning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
	month = oct,
	year = {2020},
	note = {arXiv:2006.15055 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\MSVMMAD5\\Locatello et al. - 2020 - Object-Centric Learning with Slot Attention.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\LMICZU6G\\2006.html:text/html},
}

@misc{singh_neural_2022,
	title = {Neural {Block}-{Slot} {Representations}},
	url = {http://arxiv.org/abs/2211.01177},
	abstract = {In this paper, we propose a novel object-centric representation, called Block-Slot Representation. Unlike the conventional slot representation, the Block-Slot Representation provides concept-level disentanglement within a slot. A block-slot is constructed by composing a set of modular concept representations, called blocks, generated from a learned memory of abstract concept prototypes. We call this block-slot construction process Block-Slot Attention. Block-Slot Attention facilitates the emergence of abstract concept blocks within a slot such as color, position, and texture, without any supervision. This brings the benefits of disentanglement into slots and the representation becomes more interpretable. Similar to Slot Attention, this mechanism can be used as a drop-in module in any arbitrary neural architecture. In experiments, we show that our model disentangles object properties significantly better than the previous methods, including complex textured scenes. We also demonstrate the ability to compose novel scenes by composing slots at the block-level.},
	urldate = {2022-11-07},
	publisher = {arXiv},
	author = {Singh, Gautam and Kim, Yeongbin and Ahn, Sungjin},
	month = nov,
	year = {2022},
	note = {arXiv:2211.01177 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\ARQB2UXH\\Singh et al. - 2022 - Neural Block-Slot Representations.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\UI3P5G9S\\2211.html:text/html},
}

@misc{grathwohl_your_2020,
	title = {Your {Classifier} is {Secretly} an {Energy} {Based} {Model} and {You} {Should} {Treat} it {Like} {One}},
	url = {http://arxiv.org/abs/1912.03263},
	abstract = {We propose to reinterpret a standard discriminative classifier of p(y{\textbar}x) as an energy based model for the joint distribution p(x,y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x{\textbar}y). Within this framework, standard discriminative architectures may beused and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, andout-of-distribution detection while also enabling our models to generate samplesrivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and presentan approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-artin both generative and discriminative learning within one hybrid model.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, Jörn-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin},
	month = sep,
	year = {2020},
	note = {arXiv:1912.03263 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\G3DT89SM\\Grathwohl et al. - 2020 - Your Classifier is Secretly an Energy Based Model .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\E6EQ33NY\\1912.html:text/html},
}

@inproceedings{jaegle_perceiver_2021,
	title = {Perceiver: {General} {Perception} with {Iterative} {Attention}},
	shorttitle = {Perceiver},
	url = {https://proceedings.mlr.press/v139/jaegle21a.html},
	abstract = {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver \{–\} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
	language = {en},
	urldate = {2022-11-29},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {4651--4664},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\FC28FN8N\\Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf:application/pdf},
}

@inproceedings{deng_latent_2018,
	title = {Latent {Alignment} and {Variational} {Attention}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/b691334ccf10d4ab144d672f7783c8a3-Abstract.html},
	abstract = {Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks, alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training. On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.},
	urldate = {2022-12-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Deng, Yuntian and Kim, Yoon and Chiu, Justin and Guo, Demi and Rush, Alexander},
	year = {2018},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\EGP6FTZ9\\Deng et al. - 2018 - Latent Alignment and Variational Attention.pdf:application/pdf},
}

@inproceedings{nguyen_improving_2022,
	title = {Improving {Transformers} with {Probabilistic} {Attention} {Keys}},
	url = {https://proceedings.mlr.press/v162/nguyen22c.html},
	abstract = {Multi-head attention is a driving force behind state-of-the-art transformers, which achieve remarkable performance across a variety of natural language processing (NLP) and computer vision tasks. It has been observed that for many applications, those attention heads learn redundant embedding, and most of them can be removed without degrading the performance of the model. Inspired by this observation, we propose Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, Transformer-MGK accelerates training and inference, has fewer parameters, and requires fewer FLOPs to compute while achieving comparable or better accuracy across tasks. Transformer-MGK can also be easily extended to use with linear attention. We empirically demonstrate the advantage of Transformer-MGK in a range of practical applications, including language modeling and tasks that involve very long sequences. On the Wikitext-103 and Long Range Arena benchmark, Transformer-MGKs with 4 heads attain comparable or better performance to the baseline transformers with 8 heads.},
	language = {en},
	urldate = {2022-12-15},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nguyen, Tam Minh and Nguyen, Tan Minh and Le, Dung D. D. and Nguyen, Duy Khuong and Tran, Viet-Anh and Baraniuk, Richard and Ho, Nhat and Osher, Stanley},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {16595--16621},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\HNNZ9JA2\\Nguyen et al. - 2022 - Improving Transformers with Probabilistic Attentio.pdf:application/pdf},
}

@misc{kim_structured_2017,
	title = {Structured {Attention} {Networks}},
	url = {http://arxiv.org/abs/1702.00887},
	abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
	month = feb,
	year = {2017},
	note = {arXiv:1702.00887 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\GK23FA2I\\Kim et al. - 2017 - Structured Attention Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\REKCIKPG\\1702.html:text/html},
}

@misc{gabbur_probabilistic_2021,
	title = {Probabilistic {Attention} for {Interactive} {Segmentation}},
	url = {http://arxiv.org/abs/2106.15338},
	abstract = {We provide a probabilistic interpretation of attention and show that the standard dot-product attention in transformers is a special case of Maximum A Posteriori (MAP) inference. The proposed approach suggests the use of Expectation Maximization algorithms for online adaptation of key and value model parameters. This approach is useful for cases in which external agents, e.g., annotators, provide inference-time information about the correct values of some tokens, e.g, the semantic category of some pixels, and we need for this new information to propagate to other tokens in a principled manner. We illustrate the approach on an interactive semantic segmentation task in which annotators and models collaborate online to improve annotation efficiency. Using standard benchmarks, we observe that key adaptation boosts model performance (\${\textbackslash}sim10{\textbackslash}\%\$ mIoU) in the low feedback regime and value propagation improves model responsiveness in the high feedback regime. A PyTorch layer implementation of our probabilistic attention model will be made publicly available here: https://github.com/apple/ml-probabilistic-attention.},
	urldate = {2022-12-19},
	publisher = {arXiv},
	author = {Gabbur, Prasad and Bilkhu, Manjot and Movellan, Javier},
	month = jul,
	year = {2021},
	note = {arXiv:2106.15338 [cs]},
	keywords = {68T07, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, I.4.6, I.5.1, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\MDHFTRDQ\\Gabbur et al. - 2021 - Probabilistic Attention for Interactive Segmentati.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\ANJJ5XHQ\\2106.html:text/html},
}

@misc{ding_attention_2020,
	title = {Attention that does not {Explain} {Away}},
	url = {http://arxiv.org/abs/2009.14308},
	abstract = {Models based on the Transformer architecture have achieved better accuracy than the ones based on competing architectures for a large set of tasks. A unique feature of the Transformer is its universal application of a self-attention mechanism, which allows for free information flow at arbitrary distances. Following a probabilistic view of the attention via the Gaussian mixture model, we find empirical evidence that the Transformer attention tends to "explain away" certain input neurons. To compensate for this, we propose a doubly-normalized attention scheme that is simple to implement and provides theoretical guarantees for avoiding the "explaining away" effect without introducing significant computational or memory cost. Empirically, we show that the new attention schemes result in improved performance on several well-known benchmarks.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Ding, Nan and Fan, Xinjie and Lan, Zhenzhong and Schuurmans, Dale and Soricut, Radu},
	month = sep,
	year = {2020},
	note = {arXiv:2009.14308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\KRHDEYSV\\Ding et al. - 2020 - Attention that does not Explain Away.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\FSGVHGMI\\2009.html:text/html},
}

@article{lindsay_attention_2020,
	title = {Attention in {Psychology}, {Neuroscience}, and {Machine} {Learning}},
	volume = {14},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2020.00029},
	abstract = {Attention is the important ability to flexibly control limited computational resources. It has been studied in conjunction with many other topics in neuroscience and psychology including awareness, vigilance, saliency, executive control, and learning. It has also recently been applied in several domains in machine learning. The relationship between the study of biological attention and its use as a tool to enhance artificial neural networks is not always clear. This review starts by providing an overview of how attention is conceptualized in the neuroscience and psychology literature. It then covers several use cases of attention in machine learning, indicating their biological counterparts where they exist. Finally, the ways in which artificial attention can be further inspired by biology for the production of complex and integrative systems is explored.},
	urldate = {2022-12-26},
	journal = {Frontiers in Computational Neuroscience},
	author = {Lindsay, Grace W.},
	year = {2020},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\QBPTYD4P\\Lindsay - 2020 - Attention in Psychology, Neuroscience, and Machine.pdf:application/pdf},
}

@inproceedings{yuille_concave-convex_2001,
	title = {The {Concave}-{Convex} {Procedure} ({CCCP})},
	volume = {14},
	url = {https://proceedings.neurips.cc/paper/2001/hash/a012869311d64a44b5a0d567cd20de04-Abstract.html},
	abstract = {We  introduce the  Concave-Convex procedure  (CCCP)  which  con(cid:173) structs discrete  time  iterative  dynamical  systems  which  are  guar(cid:173) anteed to monotonically decrease global optimization/energy func(cid:173) tions.  It can be applied to  (almost)  any optimization problem and  many existing algorithms can be interpreted in terms of CCCP.  In  particular, we  prove relationships to some applications of Legendre  transform techniques.  We  then illustrate CCCP by applications to  Potts models, linear assignment,  EM  algorithms,  and  Generalized  Iterative Scaling  (GIS).  CCCP  can be  used  both as  a  new  way  to  understand existing optimization algorithms and as a procedure for  generating new algorithms.},
	urldate = {2023-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Yuille, Alan L and Rangarajan, Anand},
	year = {2001},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\SVYXAS23\\Yuille and Rangarajan - 2001 - The Concave-Convex Procedure (CCCP).pdf:application/pdf},
}

@misc{yang_transformers_2022,
	title = {Transformers from an {Optimization} {Perspective}},
	url = {http://arxiv.org/abs/2205.13891},
	abstract = {Deep learning models such as the Transformer are often constructed by heuristics and experience. To provide a complementary foundation, in this work we study the following problem: Is it possible to find an energy function underlying the Transformer model, such that descent steps along this energy correspond with the Transformer forward pass? By finding such a function, we can reinterpret Transformers as the unfolding of an interpretable optimization process across iterations. This unfolding perspective has been frequently adopted in the past to elucidate more straightforward deep models such as MLPs and CNNs; however, it has thus far remained elusive obtaining a similar equivalence for more complex models with self-attention mechanisms like the Transformer. To this end, we first outline several major obstacles before providing companion techniques to at least partially address them, demonstrating for the first time a close association between energy function minimization and deep layers with self-attention. This interpretation contributes to our intuition and understanding of Transformers, while potentially laying the ground-work for new model designs.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Yang, Yongyi and Huang, Zengfeng and Wipf, David},
	month = may,
	year = {2022},
	note = {arXiv:2205.13891 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\ZZ5K9HTB\\Yang et al. - 2022 - Transformers from an Optimization Perspective.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\EKESQ9PZ\\2205.html:text/html},
}

@misc{tay_efficient_2022,
	title = {Efficient {Transformers}: {A} {Survey}},
	shorttitle = {Efficient {Transformers}},
	url = {http://arxiv.org/abs/2009.06732},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = mar,
	year = {2022},
	note = {arXiv:2009.06732 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\HUMTXX4L\\Tay et al. - 2022 - Efficient Transformers A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\R34KK986\\2009.html:text/html},
}

@misc{millidge_theoretical_2022,
	title = {A {Theoretical} {Framework} for {Inference} and {Learning} in {Predictive} {Coding} {Networks}},
	url = {http://arxiv.org/abs/2207.12316},
	abstract = {Predictive coding (PC) is an influential theory in computational neuroscience, which argues that the cortex forms unsupervised world models by implementing a hierarchical process of prediction error minimization. PC networks (PCNs) are trained in two phases. First, neural activities are updated to optimize the network's response to external stimuli. Second, synaptic weights are updated to consolidate this change in activity -- an algorithm called {\textbackslash}emph\{prospective configuration\}. While previous work has shown how in various limits, PCNs can be found to approximate backpropagation (BP), recent work has demonstrated that PCNs operating in this standard regime, which does not approximate BP, nevertheless obtain competitive training and generalization performance to BP-trained networks while outperforming them on tasks such as online, few-shot, and continual learning, where brains are known to excel. Despite this promising empirical performance, little is understood theoretically about the properties and dynamics of PCNs in this regime. In this paper, we provide a comprehensive theoretical analysis of the properties of PCNs trained with prospective configuration. We first derive analytical results concerning the inference equilibrium for PCNs and a previously unknown close connection relationship to target propagation (TP). Secondly, we provide a theoretical analysis of learning in PCNs as a variant of generalized expectation-maximization and use that to prove the convergence of PCNs to critical points of the BP loss function, thus showing that deep PCNs can, in theory, achieve the same generalization performance as BP, while maintaining their unique advantages.},
	urldate = {2023-02-16},
	publisher = {arXiv},
	author = {Millidge, Beren and Song, Yuhang and Salvatori, Tommaso and Lukasiewicz, Thomas and Bogacz, Rafal},
	month = aug,
	year = {2022},
	note = {arXiv:2207.12316 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\WDMJ6T7Y\\Millidge et al. - 2022 - A Theoretical Framework for Inference and Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\637A7IMF\\2207.html:text/html},
}

@article{feldman_attention_2010,
	title = {Attention, {Uncertainty}, and {Free}-{Energy}},
	volume = {4},
	issn = {1662-5161},
	url = {https://www.frontiersin.org/articles/10.3389/fnhum.2010.00215},
	abstract = {We suggested recently that attention can be understood as inferring the level of uncertainty or precision during hierarchical perception. In this paper, we try to substantiate this claim using neuronal simulations of directed spatial attention and biased competition. These simulations assume that neuronal activity encodes a probabilistic representation of the world that optimizes free-energy in a Bayesian fashion. Because free-energy bounds surprise or the (negative) log-evidence for internal models of the world, this optimization can be regarded as evidence accumulation or (generalized) predictive coding. Crucially, both predictions about the state of the world generating sensory data and the precision of those data have to be optimized. Here, we show that if the precision depends on the states, one can explain many aspects of attention. We illustrate this in the context of the Posner paradigm, using the simulations to generate both psychophysical and electrophysiological responses. These simulated responses are consistent with attentional bias or gating, competition for attentional resources, attentional capture and associated speed-accuracy trade-offs. Furthermore, if we present both attended and non-attended stimuli simultaneously, biased competition for neuronal representation emerges as a principled and straightforward property of Bayes-optimal perception.},
	urldate = {2023-03-19},
	journal = {Frontiers in Human Neuroscience},
	author = {Feldman, Harriet and Friston, Karl},
	year = {2010},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\39SWBJEA\\Feldman and Friston - 2010 - Attention, Uncertainty, and Free-Energy.pdf:application/pdf},
}

@article{buckley_free_2017,
	title = {The free energy principle for action and perception: {A} mathematical review},
	volume = {81},
	issn = {0022-2496},
	shorttitle = {The free energy principle for action and perception},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249617300962},
	doi = {10.1016/j.jmp.2017.09.004},
	abstract = {The ‘free energy principle’ (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian ‘perception as inference’, machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.},
	language = {en},
	urldate = {2023-03-27},
	journal = {Journal of Mathematical Psychology},
	author = {Buckley, Christopher L. and Kim, Chang Sub and McGregor, Simon and Seth, Anil K.},
	month = dec,
	year = {2017},
	keywords = {Action, Agent-based model, Bayesian brain, Free energy principle, Inference, Perception},
	pages = {55--79},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\W5AKXCJ3\\Buckley et al. - 2017 - The free energy principle for action and perceptio.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\2EQRTUVI\\S0022249617300962.html:text/html},
}

@article{rao_predictive_1999,
	title = {Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects},
	volume = {2},
	copyright = {1999 Nature America Inc.},
	issn = {1546-1726},
	shorttitle = {Predictive coding in the visual cortex},
	url = {https://www.nature.com/articles/nn0199_79},
	doi = {10.1038/4580},
	abstract = {We describe a model of visual processing in which feedback connections from a higher- to a lower-order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These results suggest that rather than being exclusively feedforward phenomena, nonclassical surround effects in the visual cortex may also result from cortico-cortical feedback as a consequence of the visual system using an efficient hierarchical strategy for encoding natural images.},
	language = {en},
	number = {1},
	urldate = {2023-03-27},
	journal = {Nature Neuroscience},
	author = {Rao, Rajesh P. N. and Ballard, Dana H.},
	month = jan,
	year = {1999},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, general, Neurobiology, Neurosciences},
	pages = {79--87},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\LNMUQ48U\\Rao and Ballard - 1999 - Predictive coding in the visual cortex a function.pdf:application/pdf},
}

@article{friston_predictive_2009,
	title = {Predictive coding under the free-energy principle},
	volume = {364},
	issn = {0962-8436},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2666703/},
	doi = {10.1098/rstb.2008.0300},
	abstract = {This paper considers prediction and perceptual categorization as an inference problem that is solved by the brain. We assume that the brain models the world as a hierarchy or cascade of dynamical systems that encode causal structure in the sensorium. Perception is equated with the optimization or inversion of these internal models, to explain sensory data. Given a model of how sensory data are generated, we can invoke a generic approach to model inversion, based on a free energy bound on the model's evidence. The ensuing free-energy formulation furnishes equations that prescribe the process of recognition, i.e. the dynamics of neuronal activity that represent the causes of sensory input. Here, we focus on a very general model, whose hierarchical and dynamical structure enables simulated brains to recognize and predict trajectories or sequences of sensory states. We first review hierarchical dynamical models and their inversion. We then show that the brain has the necessary infrastructure to implement this inversion and illustrate this point using synthetic birds that can recognize and categorize birdsongs.},
	number = {1521},
	urldate = {2023-03-27},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Friston, Karl and Kiebel, Stefan},
	month = may,
	year = {2009},
	pmid = {19528002},
	pmcid = {PMC2666703},
	pages = {1211--1221},
	file = {PubMed Central Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\6QQLBP6X\\Friston and Kiebel - 2009 - Predictive coding under the free-energy principle.pdf:application/pdf},
}

@inproceedings{teh_collapsed_2006,
	title = {A {Collapsed} {Variational} {Bayesian} {Inference} {Algorithm} for {Latent} {Dirichlet} {Allocation}},
	volume = {19},
	url = {https://proceedings.neurips.cc/paper_files/paper/2006/hash/532b7cbe070a3579f424988a040752f2-Abstract.html},
	abstract = {Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference pro- cedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efﬁcient, easy to implement and signiﬁ- cantly more accurate than standard variational Bayesian inference for LDA.},
	urldate = {2023-03-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Teh, Yee and Newman, David and Welling, Max},
	year = {2006},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\7ZLJLQVJ\\Teh et al. - 2006 - A Collapsed Variational Bayesian Inference Algorit.pdf:application/pdf},
}

@inproceedings{frecon_bregman_2022,
	title = {Bregman {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v162/frecon22a.html},
	abstract = {We present a framework based on bilevel optimization for learning multilayer, deep data representations. On the one hand, the lower-level problem finds a representation by successively minimizing layer-wise objectives made of the sum of a prescribed regularizer as well as a fidelity term and some linear function both depending on the representation found at the previous layer. On the other hand, the upper-level problem optimizes over the linear functions to yield a linearly separable final representation. We show that, by choosing the fidelity term as the quadratic distance between two successive layer-wise representations, the bilevel problem reduces to the training of a feed-forward neural network. Instead, by elaborating on Bregman distances, we devise a novel neural network architecture additionally involving the inverse of the activation function reminiscent of the skip connection used in ResNets. Numerical experiments suggest that the proposed Bregman variant benefits from better learning properties and more robust prediction performance.},
	language = {en},
	urldate = {2023-04-02},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Frecon, Jordan and Gasso, Gilles and Pontil, Massimiliano and Salzo, Saverio},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {6779--6792},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\LIRDX968\\Frecon et al. - 2022 - Bregman Neural Networks.pdf:application/pdf},
}

@article{baxter_model_2000,
	title = {A {Model} of {Inductive} {Bias} {Learning}},
	volume = {12},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/10253},
	doi = {10.1613/jair.731},
	abstract = {A major problem in machine learning is that of inductive    bias: how to choose a learner's hypothesis space so that it is large    enough to contain a solution to the problem being learnt, yet small    enough to ensure reliable generalization from reasonably-sized    training sets.  Typically such bias is supplied by hand through the    skill and insights of experts. In this paper a model for automatically    learning bias is investigated. The central assumption of the model is    that the learner is embedded within an environment of related learning    tasks. Within such an environment the learner can sample from multiple    tasks, and hence it can search for a hypothesis space that contains    good solutions to many of the problems in the environment. Under    certain restrictions on the set of all hypothesis spaces available to    the learner, we show that a hypothesis space that performs well on a    sufficiently large number of training tasks will also perform well    when learning novel tasks in the same environment.  Explicit bounds    are also derived demonstrating that learning multiple tasks within an    environment of related tasks can potentially give much better    generalization than learning a single task.},
	language = {en},
	urldate = {2023-04-07},
	journal = {Journal of Artificial Intelligence Research},
	author = {Baxter, J.},
	month = mar,
	year = {2000},
	pages = {149--198},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\6U2I9V7M\\Baxter - 2000 - A Model of Inductive Bias Learning.pdf:application/pdf},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\THUXXITH\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\BZQNNYV9\\2010.html:text/html},
}

@misc{wang_image_2022,
	title = {Image as a {Foreign} {Language}: {BEiT} {Pretraining} for {All} {Vision} and {Vision}-{Language} {Tasks}},
	shorttitle = {Image as a {Foreign} {Language}},
	url = {http://arxiv.org/abs/2208.10442},
	abstract = {A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEIT-3, which achieves state-of-the-art transfer performance on both vision and visionlanguage tasks. Speciﬁcally, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-speciﬁc encoding. Based on the shared backbone, we perform masked “language” modeling on images (Imglish), texts (English), and image-text pairs (“parallel sentences”) in a uniﬁed manner. Experimental results show that BEIT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classiﬁcation (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).},
	language = {en},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and Wei, Furu},
	month = aug,
	year = {2022},
	note = {arXiv:2208.10442 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Wang et al. - 2022 - Image as a Foreign Language BEiT Pretraining for .pdf:C\:\\Users\\ryans\\Zotero\\storage\\ENKTCNMT\\Wang et al. - 2022 - Image as a Foreign Language BEiT Pretraining for .pdf:application/pdf},
}

@misc{li_can_2021,
	title = {Can {Vision} {Transformers} {Perform} {Convolution}?},
	url = {http://arxiv.org/abs/2111.01353},
	abstract = {Several recent studies have demonstrated that attention-based networks, such as Vision Transformer (ViT), can outperform Convolutional Neural Networks (CNNs) on several computer vision tasks without using convolutional layers. This naturally leads to the following questions: Can a self-attention layer of ViT express any convolution operation? In this work, we prove that a single ViT layer with image patches as the input can perform any convolution operation constructively, where the multi-head attention mechanism and the relative positional encoding play essential roles. We further provide a lower bound on the number of heads for Vision Transformers to express CNNs. Corresponding with our analysis, experimental results show that the construction in our proof can help inject convolutional bias into Transformers and significantly improve the performance of ViT in low data regimes.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Li, Shanda and Chen, Xiangning and He, Di and Hsieh, Cho-Jui},
	month = nov,
	year = {2021},
	note = {arXiv:2111.01353 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ryans\\Zotero\\storage\\ENL28CMM\\Li et al. - 2021 - Can Vision Transformers Perform Convolution.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ryans\\Zotero\\storage\\QFQMHZ8V\\2111.html:text/html},
}

@inproceedings{shankar_surprisingly_2018,
	address = {Brussels, Belgium},
	title = {Surprisingly {Easy} {Hard}-{Attention} for {Sequence} to {Sequence} {Learning}},
	url = {https://aclanthology.org/D18-1065},
	doi = {10.18653/v1/D18-1065},
	abstract = {In this paper we show that a simple beam approximation of the joint distribution between attention and output is an easy, accurate, and efficient attention mechanism for sequence to sequence learning. The method combines the advantage of sharp focus in hard attention and the implementation ease of soft attention. On five translation tasks we show effortless and consistent gains in BLEU compared to existing attention mechanisms.},
	urldate = {2023-04-07},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Shankar, Shiv and Garg, Siddhant and Sarawagi, Sunita},
	month = oct,
	year = {2018},
	pages = {640--645},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\9MIJ5R5X\\Shankar et al. - 2018 - Surprisingly Easy Hard-Attention for Sequence to S.pdf:application/pdf},
}

@inproceedings{shah_learning_2021,
	title = {On {Learning} {Continuous} {Pairwise} {Markov} {Random} {Fields}},
	url = {https://proceedings.mlr.press/v130/shah21a.html},
	abstract = {We consider learning a sparse pairwise Markov Random Field (MRF) with continuous-valued variables from i.i.d samples. We adapt the algorithm of Vuffray et al. (2019) to this setting and provide finite-sample analysis revealing sample complexity scaling logarithmically with the number of variables, as in the discrete and Gaussian settings. Our approach is applicable to a large class of pairwise MRFs with continuous variables and also has desirable asymptotic properties, including consistency and normality under mild conditions. Further, we establish that the population version of the optimization criterion employed in Vuffray et al. (2019) can be interpreted as local maximum likelihood estimation (MLE). As part of our analysis, we introduce a robust variation of sparse linear regression a‘ la Lasso, which may be of interest in its own right.},
	language = {en},
	urldate = {2023-04-07},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Shah, Abhin and Shah, Devavrat and Wornell, Gregory},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1153--1161},
	file = {Full Text PDF:C\:\\Users\\ryans\\Zotero\\storage\\32NGXZ9V\\Shah et al. - 2021 - On Learning Continuous Pairwise Markov Random Fiel.pdf:application/pdf;Supplementary PDF:C\:\\Users\\ryans\\Zotero\\storage\\CWEYHRW6\\Shah et al. - 2021 - On Learning Continuous Pairwise Markov Random Fiel.pdf:application/pdf},
}
