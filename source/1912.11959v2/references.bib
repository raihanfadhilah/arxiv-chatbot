@article{kaiser2015,
	title={Neural GPUs Learn Algorithms.},
	author={Kaiser, Lukasz and Sutskever, Ilya},
	journal={arXiv preprint arXiv:1511.08228},
	year={2015}
}

@article{kaiser2016,
    title={Can Active Memory Replace Attention},
    author={Kaiser, Lukasz and Bengio, Samy},
    journal={arXiv preprint arXiv:1610.08613},
    year={2016}
}

@article{vaswani2017,
    title={Attention Is All You Need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, ≈Åukasz and Polosukhin, Illia},
    journal={arXiv preprint arXiv:1706.03762},
    year={2017}
}

@article{sukbaatar2019,
    title={Augmenting Self-Attention with Persistent Memory.},
    author={Sukhbaatar, Sainbayar and Grave, Edouard and Lample, Guillaume and Jegou, Herve and Joulin, Armand},
    journal={arXiv preprint arXiv:1907.01470},
    year={2019}
}

@article{srivastava2015,
    title={Highway Networks},
    author={Srivastava, Rupseh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
    journal={arXiv preprint arXiv:1505.00387},
    year={2015}
}

@article{kingma2014,
    title={Adam: A Method for Stochastic Optimization},
    author={Kingma, Diederik P. and Ba, Jimmy},
    journal={arXiv preprint arXiv:1412.6980},
    year={2014}
}

@article{gehring2017,
    title={Convolutional Sequence to Sequence Learning},
    author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann},
    journal={arXiv preprint arXiv:1705.03122},
    year={2017}
}

@article{merity2016,
    title={Pointer Sentinel Mixture Models},
    author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
    journal={arXiv preprint arXiv:1609.07843},
    year={2016},
}

@article{sennrich2016,
    title={Neural Machine Translation of Rare Words with Subword Units},
    author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
    journal={arXiv preprint arXiv:1508.07909},
    year={2016},
}

@article{yang2019,
    title={Convolutional Self-Attention Networks},
    author={Yang, Baosong and Wang, Longyue and Wong, Derek F. and Chao, Lidia S. and Tu, Zhaopeng},
    journal={arXix preprint arXiv:1904.03107},
    year={2019},
}

@article{wu2019,
    title={Pay Less Attention With Lightweight and Dynamic Attention},
    author={Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann N. and Auli, Michael},
    journal={arXiv preprint arXiv:1901.10430},
    year={2019},
}

@article{shoeybi2019,
    title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
    author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
    journal={arXiv preprint arXiv:1909.08053},
    year={2019},
}

@article{glu2017,
    title={Language Modeling with Gated Convolutional Networks},
    author={Dauphin, Yann N. and Fan, Angela and Auli, Michael and Grangier, David},
    journal={arXiv preprint arXiv:1612.08083},
    year={2016},
}

@article{wang2019,
    title={R-Transformer: Recurrent Neural Network Enhanced Transformer},
    author={Wang, Zhiwei and Ma, Yao and Liu, Zitao and Tang, Jiliang},
    journal={arXiv preprint arXiv:1907.05572},
    year={2019}
}

@article{bai,
    title={An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
    author={Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
    journal={arXiv preprint arXiv:1803.01271},
    year={2019}
    
}

@article{hao,
    title={Modeling Recurrence for Transformer},
    author={Hao, Jie and Wang, Xing and Yang, Baosong and Wang, Longyue and Zhang, Jinfeng and Tu, Zhaopeng},
    journal={arXiv preprint arXiv:1904.03092},
    year={2019}
}

@article{freivalds,
    title= {Improving the Neural GPU Architecture for Algorithm Learning},
    author= {Freivalds, Karlis and Liepins, Renars},
    journal = {arXiv preprint arXiv:1702.08727},
    year={2017}
}

@article{one_shot_draw,
    title= {One Shot Generalization in Deep Generative Models},
    author = {Resende, Danilo J. and Mohamed, Shakir and Danihelka, Ivo and Gregor, Karol and Wiestra, Daan},
    journal = {arXiv preprint arXiv:1603.05106},
    year={2016}
}

@article{bert,
    title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author = {Delvin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    journal = {arXiv preprint arXiv:1810.04805},
    year = {2018}
}

@article{xlnet,
    title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
    author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
    journal = {arXiv preprint arXiv:1906.08237},
    year = {2019},
}

@article{albert,
    title = {Albert: A Lite Bert For Self-Supervised Learning  Of Language Representations},
    author = {Zhenzhong, Lan and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
    journal = {arXiv preprint arXiv:1909.11942},
    year = {2019}
}

@article{transformerxl,
    title = {Transformer-{XL}: Attentive Language Models Beyond a Fixed-Length Context},
    author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
    journal = {arXiv preprint arXiv:1901.02860},
    year = {2019}
}

@article{historyofdeeplearning,
    title = {The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches},
    author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Christopher and Westbery, Stefan and Sidike, Paheding and Nasrin, Mst Sharmina and Esesn, Brian C Van and Awwal, Abdul A S. and Asari, Vijayan K.},
    journal = {arXiv preprint arXiv:1803.01164},
    year = {2018}

}