{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'abstract': 'The effect of the electron-electron cusp on the convergence of configuration\\ninteraction (CI) wave functions is examined. By analogy with the\\npseudopotential approach for electron-ion interactions, an effective\\nelectron-electron interaction is developed which closely reproduces the\\nscattering of the Coulomb interaction but is smooth and finite at zero\\nelectron-electron separation. The exact many-electron wave function for this\\nsmooth effective interaction has no cusp at zero electron-electron separation.\\nWe perform CI and quantum Monte Carlo calculations for He and Be atoms, both\\nwith the Coulomb electron-electron interaction and with the smooth effective\\nelectron-electron interaction. We find that convergence of the CI expansion of\\nthe wave function for the smooth electron-electron interaction is not\\nsignificantly improved compared with that for the divergent Coulomb interaction\\nfor energy differences on the order of 1 mHartree. This shows that, contrary to\\npopular belief, description of the electron-electron cusp is not a limiting\\nfactor, to within chemical accuracy, for CI calculations.', 'title': 'Impact of Electron-Electron Cusp on Configuration Interaction Energies', 'authors': ['David Prendergast', 'M. Nolan', 'Claudia Filippi', 'Stephen Fahy', 'J. C. Greer']}, {'abstract': 'We calculate the thermal conductivity of electrons produced by\\nelectron-electron Coulomb scattering in a strongly degenerate electron gas\\ntaking into account the Landau damping of transverse plasmons. The Landau\\ndamping strongly reduces this conductivity in the domain of ultrarelativistic\\nelectrons at temperatures below the electron plasma temperature. In the inner\\ncrust of a neutron star at temperatures T < 1e7 K this thermal conductivity\\ncompletely dominates over the electron conductivity due to electron-ion\\n(electron-phonon) scattering and becomes competitive with the the electron\\nconductivity due to scattering of electrons by impurity ions.', 'title': 'Electron thermal conductivity owing to collisions between degenerate\\n  electrons', 'authors': ['P. S. Shternin', 'D. G. Yakovlev']}, {'abstract': 'Starting from the shell structure in atoms and the significant correlation\\nwithin electron pairs, we distinguish the exchange-correlation effects between\\ntwo electrons of opposite spins occupying the same orbital from the average\\ncorrelation among many electrons in a crystal. In the periodic potential of the\\ncrystal with lattice constant larger than the effective Bohr radius of the\\nvalence electrons, these correlated electron pairs can form a metastable energy\\nband above the corresponding single-electron band separated by an energy gap.\\nIn order to determine if these metastable electron pairs can be stabilized, we\\ncalculate the many-electron exchange-correlation renormalization and the\\npolaron correction to the two-band system with single electrons and electron\\npairs. We find that the electron-phonon interaction is essential to\\ncounterbalance the Coulomb repulsion and to stabilize the electron pairs. The\\ninterplay of the electron-electron and electron-phonon interactions, manifested\\nin the exchange-correlation energies, polaron effects, and screening, is\\nresponsible for the formation of electron pairs (bipolarons) that are located\\non the Fermi surface of the single-electron band.', 'title': 'Electron pairing: from metastable electron pair to bipolaron', 'authors': ['Guo-Qiang Hai', 'Ladir Cândido', 'Braulio G. A. Brito', 'François M. Peeters']}]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "url = 'http://export.arxiv.org/api/query?search_query=all:electron&start=0&max_results=100'\n",
    "data = urllib.request.urlopen(url)\n",
    "\n",
    "# Parse the XML data\n",
    "soup = BeautifulSoup(data, 'xml')\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over each entry in the XML data\n",
    "for entry in soup.find_all('entry'):\n",
    "    # Initialize an empty dictionary for each entry\n",
    "    result = {}\n",
    "    \n",
    "    # Extract the abstract, title, and authors\n",
    "    result['abstract'] = entry.find('summary').text.strip()\n",
    "    result['title'] = entry.find('title').text.strip()\n",
    "    \n",
    "    # Extract the author names and get only the first name\n",
    "    authors = entry.find_all('author')\n",
    "    result['authors'] = [author.find('name').text.strip().split('\\n')[0] for author in authors]\n",
    "    \n",
    "    # Append the result to the list of results\n",
    "    results.append(result)\n",
    "\n",
    "# Print the list of results\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using arxiv package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv, logging, PyPDF2, requests, io, os\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "class ArxivScraper(object):\n",
    "  def __init__(self, query, max_results=10):\n",
    "    \n",
    "    self.query = query\n",
    "\n",
    "  # Construct the default API client.\n",
    "    self.client = arxiv.Client()\n",
    "\n",
    "    # Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "    self.search = arxiv.Search(\n",
    "      query = query,\n",
    "      max_results = max_results,\n",
    "    )\n",
    "    \n",
    "  def scrape(self):\n",
    "\n",
    "    papers = []\n",
    "    for r in self.client.results(self.search):\n",
    "      \n",
    "      url = \"https://export.\" + r.pdf_url.split(\"://\")[-1]\n",
    "      req = requests.get(url)\n",
    "      \n",
    "      file_bin = io.BytesIO(req.content)\n",
    "      pdf_reader = PyPDF2.PdfReader(file_bin)\n",
    "      text = \"\\n\".join([pdf_reader.pages[i].extract_text() for i in range(len(pdf_reader.pages))])\n",
    "      \n",
    "      file_name = f\"{url.split('pdf/')[-1]}.txt\"\n",
    "      #save text as txt where the name is the entry_id contained in folder named by query\n",
    "      if not os.path.exists(self.query):\n",
    "        os.makedirs(self.query)\n",
    "        \n",
    "      with open(f\"{self.query}/{file_name}\", \"w\", encoding = 'utf-8') as f:\n",
    "       f.write(text)\n",
    "      \n",
    "      r.\n",
    "      papers.append({\n",
    "        'entry_id': r.entry_id,\n",
    "        'title': r.title,\n",
    "        'abstract': r.summary,\n",
    "        'authors': r.authors,\n",
    "        'pdf_url': r.pdf_url,\n",
    "        'doi': r.doi,\n",
    "        'updated': r.updated,\n",
    "        'published': r.published,\n",
    "        'categories': r.categories,\n",
    "        'text': text,\n",
    "        'ref': text.split(\"\\nReferences\\n\")[-1]\n",
    "      })\n",
    "      \n",
    "    return papers\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:arxiv:Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=Mixture+of+Experts&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): export.arxiv.org:443\n",
      "DEBUG:urllib3.connectionpool:https://export.arxiv.org:443 \"GET /api/query?search_query=Mixture+of+Experts&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100 HTTP/1.1\" 200 56454\n",
      "INFO:arxiv:Got first page: 100 of 2367007 total results\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): export.arxiv.org:443\n",
      "DEBUG:urllib3.connectionpool:https://export.arxiv.org:443 \"GET /pdf/1806.08200v1 HTTP/1.1\" 200 1363398\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): export.arxiv.org:443\n",
      "DEBUG:urllib3.connectionpool:https://export.arxiv.org:443 \"GET /pdf/1312.4314v3 HTTP/1.1\" 200 1928905\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): export.arxiv.org:443\n",
      "DEBUG:urllib3.connectionpool:https://export.arxiv.org:443 \"GET /pdf/2008.09662v1 HTTP/1.1\" 200 2350483\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): export.arxiv.org:443\n",
      "DEBUG:urllib3.connectionpool:https://export.arxiv.org:443 \"GET /pdf/2302.02043v1 HTTP/1.1\" 200 1018476\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): export.arxiv.org:443\n",
      "DEBUG:urllib3.connectionpool:https://export.arxiv.org:443 \"GET /pdf/2207.09094v1 HTTP/1.1\" 200 3172551\n"
     ]
    }
   ],
   "source": [
    "papers = ArxivScraper(\"Mixture of Experts\", 5).scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entry_id': 'http://arxiv.org/abs/1806.08200v1',\n",
       "  'title': 'Mixtures of Experts Models',\n",
       "  'abstract': 'Mixtures of experts models provide a framework in which covariates may be\\nincluded in mixture models. This is achieved by modelling the parameters of the\\nmixture model as functions of the concomitant covariates. Given their mixture\\nmodel foundation, mixtures of experts models possess a diverse range of\\nanalytic uses, from clustering observations to capturing parameter\\nheterogeneity in cross-sectional data. This chapter focuses on delineating the\\nmixture of experts modelling framework and demonstrates the utility and\\nflexibility of mixtures of experts models as an analytic tool.',\n",
       "  'authors': [arxiv.Result.Author('Isobel Claire Gormley'),\n",
       "   arxiv.Result.Author('Sylvia Frühwirth-Schnatter')],\n",
       "  'pdf_url': 'http://arxiv.org/pdf/1806.08200v1',\n",
       "  'doi': None,\n",
       "  'updated': datetime.datetime(2018, 6, 21, 12, 30, 12, tzinfo=datetime.timezone.utc),\n",
       "  'published': datetime.datetime(2018, 6, 21, 12, 30, 12, tzinfo=datetime.timezone.utc),\n",
       "  'categories': ['stat.ME'],\n",
       "  'text': 'Mixtures of Experts Models∗\\nIsobel Claire Gormley†and Sylvia Frühwirth-Schnatter‡\\nAbstract\\nMixtures of experts models provide a framework in which covariates may be in-\\ncluded in mixture models. This is achieved by modelling the parameters of the mix-\\nture model as functions of the concomitant covariates. Given their mixture model\\nfoundation, mixtures of experts models possess a diverse range of analytic uses, from\\nclustering observations to capturing parameter heterogeneity in cross-sectional data.\\nThis chapter focuses on delineating the mixture of experts modelling framework and\\ndemonstrates the utility and ﬂexibility of mixtures of experts models as an analytic\\ntool.\\n1 Introduction\\nThe terminology mixtures of experts models encapsulates a broad class of mixture models\\nin which the model parameters are modelled as functions of concomitant covariates. While\\nthe response variable yis modelled via a mixture model, model parameters are modelled as\\nfunctions of other, related, covariates xfrom the context under study.\\nThe mixture of experts nomenclature (ME) has its origins in the machine-learning lit-\\nerature (Jacobs et al., 1991), but mixtures of experts models appear in many diﬀerent\\nguises, including switching regression models (Quandt, 1972), concomitant variable latent-\\nclass models (Dayton & Macready, 1988), latent class regression models (DeSarbo & Cron,\\n1988),andmixedmodels(Wang et al.,1996).Li et al.(2011)discussﬁnitesmoothmixtures,\\na special case of ME modelling. McLachlan & Peel (2000) and Frühwirth-Schnatter (2006)\\nprovide background to a range of mixtures of experts models; Masoudnia & Ebrahimpour\\n(2014) survey the ME literature from a machine learning perspective.\\nThe mixture of experts framework facilitates ﬂexible modelling, allowing a wide range of\\napplication. ME models for rank data (Gormley & Murphy, 2008b, 2010a), ME models for\\nnetwork data (Gormley & Murphy, 2010b), for time series data (Waterhouse et al., 1996;\\nHuertaet al., 2003; Frühwirth-Schnatter et al., 2012), for non-normal data (Villani et al.\\n, 2009; Chamroukhi, 2015) and for longitudinal data (Tang & Qu, 2015), among others,\\nhave been developed. Peng et al.(1996) employed a hierarchical mixture of experts model\\nin a speech recognition context. The general ME framework has also been incorporated\\nin the mixed membership model setting, giving rise to a mixed membership of experts\\nmodel (White & Murphy, 2016), and into the inﬁnite mixture model setting (Rasmussen &\\n∗A chapter prepared for the forthcoming Handbook of Mixture Analysis\\n†School of Mathematics and Statistics, Insight Centre for Data Analytics, University College Dublin,\\nIreland. claire.gormley@ucd.ie\\n‡Institute for Statistics and Mathematics, Vienna University of Economics and Business, Austria.\\nsfruehwi@wu.ac.at\\n1arXiv:1806.08200v1  [stat.ME]  21 Jun 2018\\nGhahramani, 2002). Cluster weighted models (Ingrassia et al., 2015; Subedi et al., 2013;\\nGershenfeld, 1997) are also closely related to ME models.\\nThis chapter introduces the generic mixture of experts framework, in Section 2, and\\ndescribes approaches to inference for ME models in Section 3. A broad range of illustrative\\ndata analyses are given in Section 4, and an overview of existing softwares which ﬁt ME\\nmodels is provided. Section 5 discusses identiﬁability issues for mixtures of experts models.\\nThe chapter concludes with some discussion of the beneﬁts and issues of the ME framework,\\nand of some areas ripe for future development.\\n2 The Mixture of Experts Framework\\nAny mixture model which incorporates covariates or concomitant variables falls within the\\nmixture of experts framework.\\n2.1 A mixture of experts model\\nLety1,...,ynbe an independent and identically distributed sample of outcome variables\\nfrom a population modelled by a Gcomponent ﬁnite mixture model. Depending on the\\napplication context, the outcome variable can be univariate or multivariate, discrete or con-\\ntinuous, or of a more general structure such as time series or network data. Each component\\ng(forg= 1,...,G) is modelled by the probability density function fg(·|θg)with parameters\\ndenoted by θg, and has weight ηgwhere/summationtextG\\ng=1ηg= 1. Observation yi(i= 1,...,n) has\\nqassociated covariates, which are denoted xi. The ME model extends the standard ﬁnite\\nmixture model introduced in Chapter 1 of this volume by allowing model parameters to be\\nfunctions of the concomitant variables xi:\\np(yi|xi) =G/summationdisplay\\ng=1ηg(xi)fg(yi|θg(xi)). (1)\\nME models can be considered as a member of the class of conditional mixture models\\n(Bishop, 2006); for a given set of covariates xi, the distribution of yiis a ﬁnite mixture\\nmodel. Jacobs et al.(1991) consider the component densities fg(yi|θg(xi))as theexperts,\\nwhich model diﬀerent parts of the input space, and the component weights ηg(xi)as the\\ngating networks , hence the mixture of experts terminology.\\nThe models for ηg(xi)and forθg(xi)in (1) vary and are typically application speciﬁc. For\\nexample,Jacobs et al.(1991)modelthecomponentweightsusingamultinomiallogit(MNL)\\nregression model, and the component densities using generalized linear models. Young &\\nHunter (2010) provide further ﬂexibility by allowing the mixing proportions to be modelled\\nnonparametrically, as a function of the covariates.\\n2.2 An illustration\\nA simple simulated data set is employed here to introduce the mixture of experts framework.\\nFigure 1 shows n= 200two-dimensional continuously valued observations, y1,...,yn, sim-\\nulated from an ME model with G= 2components. A single ( q= 1) categorical covariate xi\\nis associated with each observation representing, for example, gender where level 0 denotes\\nfemale. Interest lies in clustering the observations and exploring any relations between the\\nresulting clusters and the associated covariate.\\n2\\n−3 −2 −1 0 1 2 3−3 −2 −1 0 1 2 3\\nVariable 1Variable 2Figure1: Atwo-dimensionalsimulateddataset, froma G= 2MEmodel. Blackobservations\\nbelong to cluster 1, and grey to cluster 2, based on the MAP clustering from ﬁtting a G= 2\\nmixture of bivariate Gaussian distributions.\\nTable 1: Cross tabulation of MAP cluster memberships and the gender covariate for the\\nsimulated data of Figure 1\\nFemale Male\\nCluster 1 75 17\\nCluster 2 23 85\\nIt is common that a clustering method is implemented on the outcome variables of in-\\nterest,y1,...,yn, without reference to the covariate information. Once a clustering has been\\nproduced, the user typically probes the clusters to investigate their structure. Interpreta-\\ntions of the clusters are produced with reference to values of the model parameters within\\neach cluster and with reference to the covariates that were not used in the construction of\\nthe clusters. Therefore, a natural approach to modelling the data in Figure 1 is to cluster\\nthem by ﬁtting a two component mixture of bivariate Gaussian distributions to y1,...,yn.\\nThemaximum a posteriori (MAP) cluster membership of each observation resulting from\\nﬁtting such a model is also illustrated in Figure 1.\\nA cross tabulation of the MAP cluster memberships and the gender covariate is given\\nin Table 1. It is clear that females have a strong presence in cluster 1, and males in cluster\\n2. However, the mixture of Gaussians model ﬁtted does not incorporate or quantify this\\nrelationship or its associated uncertainty. It is in such a setting that an ME model is useful.\\nThe model from which the data in Figure 1 are simulated is an ME model where\\nfg(yi|θg) =φ(yi|µg,Σg)is the density of a bivariate Normal distribution and in which the\\ncomponent weights arise from a multinomial logit model with Gcategories with gender as\\ncovariatexi, i.e.\\nlog/bracketleftBiggηg(xi)\\nη1(xi)/bracketrightBigg\\n=γg0+γg1xi, (2)\\nwhere cluster 1 is the baseline cluster with γ1= (γ10,γ11)/latticetop= (0,0)/latticetop, andg= 2,...,G.\\nIn our example, where G= 2, model (2) reduces to a binary logit model. The parameter\\n3\\nγg1(and its associated uncertainty) quantiﬁes the relationship between the gender covariate\\nand membership of cluster g, withγg1= 0corresponding to independence between cluster\\nmembership and the gender covariate. Note that such a model easily extends to q > 1\\ncovariatesxi= (xi1,...,xiq)with associated parameter γg= (γg0,...,γgq)/latticetopfor clusterg.\\nFitting such an ME model to the simulated data results in a MAP clustering unchanged\\nfrom that reported in Table 1 and gives the maximum likelihood estimate ˆγ21= 2.79, with\\nstandard error 0.36. (Details of the maximum likelihood estimation process and standard\\nerror derivation follow in Section 3.1.) Thus, the odds of a male belonging to cluster 2 are\\nexp(2.79)≈16times greater than the odds of a female belonging to cluster 2. Thus the ME\\nmodel has clustering capabilities and provides insight into the type of observation which\\ncharacterises each cluster.\\n2.3 The suite of ME models\\nThe ME model outlined in Section 2.2 involves modelling the component weights as a func-\\ntion of covariates. This is one model type (termed a simple mixture of experts model ) from\\ntheMEframework.Figure2showsagraphicalmodelrepresentationofthesuiteoffourmod-\\nels in the ME framework, based on a latent variable representation of the mixture model\\n(1), involving the latent cluster membership of each observation, denoted zi, wherezi=g\\nif observation yibelongs to cluster g. The indicator variable zitherefore has a multinomial\\ndistribution with a single trial and probabilities equal to ηg(xi)forg= 1,...,Gand the\\nlatent variable representation reads:\\nyi|xi,zi=g∼fg(yi|θg(xi)),P(zi=g|xi) =ηg(xi). (3)\\nThis suite of models ranges from a standard mixture of experts regression model (in which\\nall model parameters are functions of covariates) to the special cases where some of the\\nmodel parameters do not depend on covariates. The four models in the ME framework have\\nthe following interpretations, see also Figure 2:\\n(a) Mixture models, where the outcome variable distribution depends on the latent cluster\\nmembership, denoted z. The model is independent of the covariates x; i.e.p(yi,zi|xi) =\\nfzi(yi|θzi)ηzi.\\n(b) Mixtures of regression models, where the outcome variable distribution depends on both\\nthe covariates xand the latent cluster membership variable z; the distribution of the\\nlatent variable is independent of the covariates; i.e. p(yi,zi|xi) =fzi(yi|θzi(xi))ηzi.\\n(c) Simple mixtures of experts models, where the outcome variable distribution depends on\\nthe latent cluster membership variable zand the distribution of the latent variable z\\ndepends on the covariates x; i.e.p(yi,zi|xi) =fzi(yi|θzi)ηzi(xi).\\n(d) Standardmixturesofexpertsregressionmodels,wheretheoutcomevariabledistribution\\ndepends on both the covariates xand on the latent cluster membership variable z.\\nAdditionally the distribution of the latent variable zdepends on the covariates x; i.e.\\np(yi,zi|xi) =fzi(yi|θzi(xi))ηzi(xi).\\nThemannerinwhichthediﬀerentmodelswithintheMEframeworkdependonthecovariates\\nis typically application speciﬁc. The component weights are usually modelled using a MNL\\nmodel, but this need not be the case; Geweke & Keane (2007) employ a model similar to\\nan ME model, where the component weights have a multinomial probit structure. The form\\n4\\nyz x\\nθη\\n(a) Mixture modelyz\\nθη x\\n(b) Mixture of regressions\\nmodel\\nyx z\\nθη\\n(c) Simple mixture of ex-\\nperts modelyz\\nθη x\\n(d) Standard mixture of\\nexperts regression model\\nFigure 2: The graphical model representation of mixtures of experts models. The diﬀerences\\nbetween the four special cases are due to the presence or absence of edges between the\\ncovariatesxand the latent variable zand response variable y. For model (a) p(y,z|x) =\\np(y|z)p(z), for model (b) p(y,z|x) =p(y|x,z)p(z), for model (c) p(y,z|x) =p(y|z)p(z|x),\\nwhereas for model (d) p(y,z|x) =p(y|x,z)p(z|x).\\nof the distribution fg(yi|θg(xi))depends on the type of outcome data under study. The\\napplications of the ME framework outlined in Section 4 include cases where the outcome\\ndata range from a categorical time series, to rank data, to network data.\\n3 Statistical Inference for Mixtures of Experts Models\\nBefore illustrating the breadth of the ME framework through illustrative applications in\\nSection 4, the issue of inference for ME models is addressed. For any ME model that is\\nunderpinned by a ﬁnite mixture model, the approaches to inference outlined in Chapter 2\\nand Chapter 5 in this volume are applicable. Jacobs et al.(1991) and Jordan & Jacobs\\n(1994) derive maximum likelihood estimates (MLEs) for ME models via the expectation-\\nmaximisation (EM) algorithm; Gormley & Murphy (2008a) employ the closely related\\nexpectation-minorisation-maximisation (EMM) algorithm. Estimation of the ME model\\nwithin the Bayesian framework is detailed, among others, in Peng et al.(1996), Frühwirth-\\nSchnatter & Kaufmann (2008), Villani et al.(2009), Gormley & Murphy (2010a) and in\\nFrühwirth-Schnatter et al.(2012) in which Markov chain Monte Carlo methods (Tanner,\\n1996) are used; Bishop & Svenskn (2003) use variational methods in the Bayesian paradigm\\nto perform inference for a hierarchical mixture of experts model. Hunter & Young (2012)\\npresent an algorithm for parameter estimation in a semiparametric mixtures of regressions\\nmodel setting.\\nIn this section, a general overview of approaches to inference in the ME framework is\\nprovided. Throughout the section, y= (y1,...,yn)will denote the collection of outcome\\nvariables and x= (x1,...,xn)the associated covariates. The latent cluster membership\\nindicators introduced in (3) are denoted by z= (z1,...,zn), whereasθ={θ1,...,θG}refers\\nto the collection of the Gcomponent parameters and γ={γ2,...,γG}to the unknown\\n5\\nparameters in the Gcomponent weights.\\nThe exact manner in which an ME model is estimated again depends on the nature of the\\nME model and the outcome variable. The simple simulated data example of Section 2.2 is\\nused here to delineate approaches to inference; more detailed application speciﬁc estimation\\napproaches are outlined in Section 4.\\n3.1 Maximum likelihood estimation\\nThe EM algorithm (Dempster et al., 1977) provides an eﬃcient approach to deriving MLEs\\nin ME models. The EM algorithm is most commonly known as a technique to produce MLEs\\nin settings where the data under study are incomplete or when optimisation of the likelihood\\nwould be simpliﬁed if an additional set of variables were known. The iterative EM algorithm\\nconsists of an expectation (E) step followed by a maximisation (M) step. Generally, during\\nthe E step the conditional expectation of the complete (i.e. observed and unobserved) data\\nlog likelihood is computed, given the data and current parameter values. In the M step the\\nexpected log likelihood is maximised with respect to the model parameters. The imputation\\nof latent variables often makes maximisation of the expected log likelihood more feasible.\\nThe parameter estimates produced in the M step are then used in a new E step and the\\ncycle continues until convergence. The parameter estimates produced on convergence are\\nestimates that achieve a stationary point of the likelihood function of the data, which is at\\nleast a local maximum but may be a saddle point.\\nThe component weights of the simple mixture of experts model outlined in Section 2.2\\nare given by\\nηg(xi|γ) = exp (˜xiγg)/G/summationdisplay\\ng/prime=1exp (˜xiγg/prime) (4)\\nwhere ˜xi= (1,xi)andγg= (γg0,γg1)/latticetop. Note that this is a special case of the multinomial\\nlogit model. For the Normal distribution θg={µg,Σg}and the likelihood function of the\\nsimple mixture of experts model is\\nL(γ,θ;G) =p(y|x,γ,θ) =n/productdisplay\\ni=1G/summationdisplay\\ng=1ηg(xi|γ)φ(yi|µg,Σg),\\nwhereφ(yi|µg,Σg)is the pdf of the d-variate Normal distribution and d= dim(yi). It is\\ndiﬃcult to directly obtain MLEs from this likelihood. To alleviate this, the data are aug-\\nmented by imputing for each observation yi,i= 1,...,n, the latent group membership\\nindicatorzi. For the EM algorithm, this latent variable is represented through Gbinary\\nvariables (zi1,...,ziG)wherezig=I(zi=g)takes the value 1 if observation yiis a member\\nof component gand the value 0 otherwise. This provides the complete data likelihood\\nLc(γ,θ,z;G) =p(y,z|x,γ,θ) =n/productdisplay\\ni=1G/productdisplay\\ng=1{ηg(xi|γ)φ(yi|µg,Σg)}zig, (5)\\nthe expectation of (the log of) which is obtained in the E step of the EM algorithm. As\\nthe complete data log likelihood is linear in the latent variable, the E step simply consists\\nof replacing for each i= 1,...,nthe missing data ziwith their expected values ˆzi.In the\\nM step the complete data log likelihood, computed with the estimates ˆz= (ˆz1,..., ˆzn), is\\nmaximised to provide estimates of the component weight parameters ˆγand the component\\nparameters ˆθ.\\n6\\nAlgorithm 1 EM algorithm for a simple Gaussian mixture of experts model\\nLets= 0. Choose initial estimates for the component weight parameters\\nγ(0)= (0,γ(0)\\n2,...,γ(0)\\nG)and for the component parameters µ(0)\\ngand Σ(0)\\ngforg=\\n1,...,G.\\n1E step: fori= 1,...,nandg= 1,...,Gcompute the estimates:\\nz(s+1)\\nig =η(s)\\ng(xi|γ(s))φ(yi|µ(s)\\ng,Σ(s)\\ng)/G/summationdisplay\\ng/prime=1η(s)\\ng/prime(xi|γ(s))φ(yi|µ(s)\\ng/prime,Σ(s)\\ng/prime).\\n2M step: Substituting the z(s+1)\\nigvalues obtained in the E step into the log of the complete\\ndata likelihood (5) forms the so called ‘Q function’\\nQ=n/summationdisplay\\ni=1G/summationdisplay\\ng=1z(s+1)\\nig\\uf8ee\\n\\uf8f0˜xiγg−log\\uf8f1\\n\\uf8f2\\n\\uf8f3G/summationdisplay\\ng/prime=1exp(˜xiγg/prime)\\uf8fc\\n\\uf8fd\\n\\uf8fe\\n−d/2 log(2π)−1/2 log|Σg|−1/2(yi−µg)/latticetopΣ−1\\ng(yi−µg)\\uf8f9\\n\\uf8fb\\nwithd= dim(yi), which is maximised with respect to the model parameters.\\n(a) The updates of the g= 1,...,Gcomponent means and covariances are, respectively:\\nµ(s+1)\\ng =n/summationdisplay\\ni=1z(s+1)\\nigyi/n/summationdisplay\\ni=1z(s+1)\\nig\\nΣ(s+1)\\ng =n/summationdisplay\\ni=1z(s+1)\\nig(yi−µ(s+1)\\ng)(yi−µ(s+1)\\ng)/latticetop/n/summationdisplay\\ni=1z(s+1)\\nig.\\n(b) The update for the component weight parameters is obtained via a numerical opti-\\nmisation step, such as a Newton-Raphson step, where for g= 2,...,G\\nγ(s+1)\\ng =γ(s)\\ng−(H(γ(s)\\ng))−1Q/prime(γ(s)\\ng)\\nandQ/primeandHdenote the ﬁrst and second derivatives of Qwith respect to γg\\nrespectively. Note that this M step is equivalent to ﬁtting a generalised linear model\\nwith weights provided by the E step.\\n3 If converged, stop. Otherwise, increment sand return to Step 1.\\nThe EM algorithm for ﬁtting ME models is straightforward in principle, but the M step\\nis often diﬃcult in practice. This is usually due to a complex component density and/or\\ncomponent weights model, or a large parameter set. A modiﬁed version of the EM algorithm,\\nthe Expectation and Conditional Maximisation (ECM) algorithm (Meng & Rubin, 1993) is\\nthereforeoftenemployed.IntheECMalgorithm,theMstepconsistsofaseriesofconditional\\nmaximisation steps. In the context of the simple mixture of experts example considered\\nhere, these maximisations are not straightforward with regard to the γparameters; as in\\nany MNL model, no closed form expression for the parameter MLEs is available. Thus,\\nwhile the conditional M steps for µgandΣg∀g= 1,...,Gare available in closed form,\\nthe conditional M step for γrequires the use of a numerical optimisation technique, or\\nas in Gormley & Murphy (2008b) the MM algorithm (Hunter & Lange, 2004) in which a\\n7\\nminorising function is iteratively maximised and updated. In summary, to ﬁt the simple\\nmixture of experts example outlined in Section 2.2 the EM algorithm proceeds as described\\nin Algorithm 1. In the simulated data example, d= 2.\\nMcLachlan & Peel (2000) outline a number of approaches to assessing convergence in\\nStep 3; typically it is assessed by tracking the change in the log likelihood as the algorithm\\nproceeds. Standard errors of the resulting parameter estimates are not automatically pro-\\nduced by the EM algorithm, but they can be approximately computed after convergence,\\nfor example, by computing and inverting the observed information matrix (McLachlan &\\nPeel, 2000). For a detailed discussion of EM algorithms in a mixture context see Chapter 2\\nand 3 of this volume.\\n3.2 Bayesian estimation\\nEstimation of ME models can be achieved within the Bayesian paradigm, either using a\\nMarkov chain Monte Carlo (MCMC) algorithm or via a variational approach. The reader\\nis directed to Bishop & Svenskn (2003) for details on the variational approach; this section\\nfocuses on inference using MCMC methods. Both the Gibbs sampler (Geman & Geman,\\n1984) and the Metropolis-Hastings algorithm (Chib & Greenberg, 1995; Metropolis et al.,\\n1953) are typically required. Again, the speciﬁc MCMC algorithm, and the form of the prior\\ndistributions, depend on the nature of the ME model under study and on the type of the\\nresponse data. As is standard in Bayesian estimation of mixture models (Diebolt & Robert,\\n1994; Hurn et al., 2003) ﬁtting ME models is greatly simpliﬁed by augmenting the observed\\ndata with the latent group indicator variable zifor each observation yi.\\nPerforming inference on the illustrative simple mixture of experts model of Section 2.2\\nis again straightforward in principle, but can be diﬃcult in practice. To begin, priors for\\nthe model parameters µg,Σgforg= 1,...,Gandγg(g= 2,...,G) require speciﬁcation.\\nPositing a conditional d-variate normal prior N(µ0,Λ0)on the group means µg, and an\\ninverse Wishart prior IW(ν0,S0)on the group covariances Σgprovides conjugacy for these\\nparameters (Hoﬀ, 2009). The full conditional distributions for these parameters are therefore\\navailable in closed form, and thus Gibbs sampling can be used to draw samples.\\nA(q+ 1)-variatenormalN(µγ,Λγ)isanintuitivepriorforthecomponentweightparam-\\netersγg, but it is non-conjugate. Hence the full conditional distribution is not available in\\nclosed form and a Metropolis-Hastings (MH) step can be applied to sample the component\\nweight parameters. One sweep of such a Metropolis-within-Gibbs sampler required to ﬁt the\\nsimple mixture of experts model of Section 2.2 in a Bayesian framework is outlined below.\\nNote that the full conditional distribution of the latent indicator variable zifori= 1,...,n\\nis also available in closed form and thus a Gibbs step is available, see Algorithm 2.\\nSampling the component weight parameters in Step 4 through a MH-algorithm brings\\nissues such as choosing suitable proposal distributions q(γ∗\\ng|γg,γ−g)and tuning parameters,\\nwhich may make ﬁtting ME models troublesome. Gormley & Murphy (2010b) detail an\\napproach to deriving proposal distributions with attractive properties, within the context of\\nan ME model for network data.\\nAlternatively, Frühwirth-Schnatter et al.(2012) exploit data augmentation of the MNL\\nmodel (4) based on the diﬀerenced random utility model representation in the context of\\nME models to implement Step 4. As shown by Frühwirth-Schnatter & Frühwirth (2010), for\\neachg= 1,...,Gthe MNL model has the following representation as a binary logit model\\n8\\nAlgorithm 2 MH-within-Gibbs MCMC inference for a simple Gaussian mixture of experts\\nmodel\\nIterate the following steps for m= 1,...,M:\\n1 Forg= 1,...,G, drawµgfrom thed-variate normal posterior N(µng,Λng)where\\nΛng= (Λ−1\\n0+ngΣ−1\\ng)−1andµng= Λng(Λ−1\\n0µ0+ Σ−1\\ngng¯yg)andng=/summationtextn\\ni=1I(zi=g)and\\nng¯yg=/summationtextn\\ni=1yiI(zi=g).\\n2 Forg= 1,...,G, draw ΣgfromIW(νng,Sng)whereνng=ν0+ngandSng=S0+/summationtextn\\ni=1I(zi=g)(yi−µg)(yi−µg)/latticetop.\\n3 Fori= 1,...,ndrawzifrom a multinomial distribution M(1,pi1,...,piG)with success\\nprobabilities (pi1,...,piG)where\\npig=ηg(xi|γ)φ(yi|µg,Σg)/G/summationdisplay\\ng/prime=1ηg/prime(xi|γ)φ(yi|µg/prime,Σg/prime).\\n4 Forg= 2,...,G, the component weight parameters γgare updated via a Metropolis-\\nHastings step, while holding the remaining component weight parameters γ−gﬁxed. Typ-\\nically, a multivariate normal proposal distribution q(γ∗\\ng|γg,γ−g)is employed:\\n(a) Propose γ∗\\ng∼N (˜µγ,˜Λγ)from a (q+ 1)-variate Normal distribution where ˜µγand\\n˜Λγare user speciﬁed and might depend on the current value of γ.\\n(b) IfU∼U[0,1]is such that\\nU≤min/braceleftBiggp(z|γ∗\\ng,γ−g,x)p(γ∗\\ng)q(γg|γ∗\\ng,γ−g)\\np(z|γg,γ−g,x)p(γg)q(γ∗\\ng|γg,γ−g),1/bracerightBigg\\n,\\nthen setγg=γ∗\\ng; otherwise leave γgunchanged.\\nconditional on knowing λhi= exp (˜xiγh)for allh/negationslash=g:\\nugi= ˜xiγg−log(/summationdisplay\\nh/negationslash=gλhi) +εgi, (6)\\nDg\\ni=I(ugi≥0)\\nwhereugiis a latent variable, εgiare i.i.d. errors following a logistic distribution, and Dg\\ni=\\nI(zi=g)is a binary outcome variable indicating whether the group indicator ziis equal to\\ng. Note that γ1= 0for the baseline, hence λ1i= 1. In a data augmented implementation of\\nStep 4, the latent variables (u2i,...,uGi)are introduced for each i= 1,...,nas unknowns.\\nGivenλ2i,...,λGiandzi,(u2i,...,uGi)can be sampled in closed form from exponentially\\ndistributed random variables. Following Scott (2011), natural proposal distributions are\\navailable to implement an MH-step to sample γg|γ−g,z,ugfor allg= 2,...,Gconditional\\nonug={ug1,...,ugn}from the linear, non-Gaussian regression model (6).\\nTo avoid any MH-step, Frühwirth-Schnatter et al.(2012) apply auxiliary mixture sam-\\npling as introduced by Frühwirth-Schnatter & Frühwirth (2010) to (6) and approximate for\\neachεgithe logistic distribution by a 10-component scale mixture of Normal distributions\\nwith zero means and parameters (s2\\nr,wr),r= 1,..., 10. In a second step of data augmenta-\\ntion, the component indicator rgiis introduced as yet another latent variable. Conditional\\non the latent variables ugand the indicators rg={rg1,...,rgn}the binary logit model (6)\\nreduces to a linear Gaussian regression model. Hence, the posterior γg|γ−g,z,ug,rgis Gaus-\\n9\\nsian and a Gibbs step is available to sample γgfor allg= 2,...,Gconditional on ugand\\nrg. Finally, each component indicator rgiis sampled from a discrete distribution conditional\\nonugiandγ.\\nChapter 13 in this volume details Bayesian estimation of informative regime switching\\nmodels which can be regarded as an extension of ME models to hidden Markov models in\\ntime series analysis.\\nAs in any mixture model setting, the so called label switching problem (Stephens, 2000;\\nFrühwirth-Schnatter, 2011a) must be considered when employing such Gibbs based algo-\\nrithms, see Chapter 5. This identiﬁability issue, along with others, is discussed in Section 5.\\n3.3 Model selection\\nWithin the suite of ME models outlined in Section 2.3 the question of which, how and where\\ncovariates are used naturally arises. This is a challenging problem as the space of ME models\\nis potentially very large, once variable selection for the covariates entering the component\\nweights and the mixture components is considered. Thus in practice only models where\\ncovariates enter all mixture components and/or all component weights as main eﬀects are\\ntypically considered in order to restrict the size of the model search space. In fact, even for\\nthis reduced model space, there are a maximum of G×2q×2qpossible models to consider.\\nIn ME models involving generalised linear models of covariates, standard variable selection\\napproaches can be used to ﬁnd the optimal model. Practical approaches to this issue are\\ndetailedintheillustrativeapplicationsofSection4.Notethatthemannerinwhichcovariates\\nenter the ME model may also be guided by the question of interest in the application under\\nstudy.\\nIf the number of components Gis unknown, the model search space increases again.\\nApproaches such as marginal likelihood evaluation, or information criteria, are useful for\\nchoosing the optimal Gin ME models; the reader is referred to Chapter 7 in this volume\\nwhich addresses model selection and selecting the number of components in a mixture model\\nin great detail.\\nMarginal likelihood computation for mixtures of experts models\\nAs discussed in Chapter 7, Section 7.2.3.2, highly accurate sampling-based approximations\\nto the marginal likelihood are available, if Gis not too large. For instance, Frühwirth-\\nSchnatter & Kaufmann (2008) apply bridge sampling (Frühwirth-Schnatter, 2004) to com-\\npute marginal likelihoods for a mixture of experts model with a single covariate (that is\\nq= 1) with up to four components. Frühwirth-Schnatter (2011b) combines auxiliary mix-\\nture sampling (Frühwirth-Schnatter & Wagner, 2008) with importance sampling to compute\\nmarginal likelihoods for mixture of experts models. A detailed summary of this approach is\\nprovided below.\\nPermutation sampling is applied to ensure that all equivalent modes of the posterior\\ndistribution are visited. Consider a permutation σ∈S(G), where S(G)denotes the set of\\ntheG!permutations of{1,...,G}. To relabel all parameters in a mixture of experts model\\naccording to the permutation σ, deﬁneθ⋆\\ng=θσ(g)andη⋆\\ng(˜xi) =ησ(g)(˜xi)forg= 1,...,G.\\nSpecial attention has to be given to the correct relabelling of the coeﬃcients γgin the\\nMNL model when applying the permutation σ. The coeﬃcients (γ1,...,γG)and(γ⋆\\n1,...,γ⋆\\nG)\\n10\\ndeﬁning, respectively, the MNL models ηg(˜xi)andη⋆\\ng(˜xi)are related through:\\n˜xiγ⋆\\ng= log/bracketleftBiggη⋆\\ng(˜xi)\\nη⋆\\ng0(˜xi)/bracketrightBigg\\n= log/bracketleftBiggησ(g)(˜xi)\\nησ(g0)(˜xi)/bracketrightBigg\\n= log/bracketleftBiggησ(g)(˜xi)\\nηg0(˜xi)/bracketrightBigg\\n−log/bracketleftBiggησ(g0)(˜xi)\\nηg0(˜xi)/bracketrightBigg\\n= ˜xi(γσ(g)−γσ(g0)).\\nTo ensure that the baseline g0(assumed to be equal to g0= 1throughout this chapter)\\nremains the same, despite relabeling, the coeﬃcients are permuted in the following way:\\nγ⋆\\ng=γσ(g)−γσ(g0), g = 1,...,G,\\nwhich indeed implies that γ⋆\\ng0= 0. ForG= 2, the sign of all coeﬃcients of γ2is simply\\nﬂipped, ifσ= (2,1)and remains unchanged, otherwise.\\nFrühwirth-Schnatter & Wagner (2008) discuss various importance sampling estimators\\nof the marginal likelihood for non-Gaussian models such as logistic models. Using auxiliary\\nmixture sampling, one of their approaches constructs the importance density from the Gaus-\\nsian full conditional densities appearing in the augmented Gibbs sampler. This approach is\\neasily extended to mixture of experts models. As discussed in Section 3.2, auxiliary mixture\\nsampling yields Gaussian posteriors p(γg|γ−g,z,ug,rg)for the MNL coeﬃcients γgin a mix-\\ntureofexpertsmodels,conditionalonthelatentutilities ugandthelatentindicators rg.This\\nallows construction of an importance density qG(θ)as in Chapter 7, Section 7.2.3.2, however\\nit is essential that qG(θ)covers all symmetric modes of the mixture posterior. A successful\\nstrategy is to apply random permutation sampling, where each sampling step is concluded\\nby relabelling as described above, using a randomly selected permutation σ∈S(G). The\\ncorresponding importance density reads:\\nqG(θ) =1\\nSS/summationdisplay\\ns=1G/productdisplay\\ng=2p(γg|γ(s)\\n−g,u(s)\\ng,r(s)\\ng,z(s))G/productdisplay\\ng=1p(θg|z(s),y), (7)\\nwhere{γ(s),u(s)\\n2,..., u(s)\\nG,r(s)\\n2,..., r(s)\\nG,z(s)},s= 1,...,Sisasubsequenceofposteriordraws.\\nOnlyifSislargecomparedto G!,thenallsymmetricmodesarecoveredbyrandompermuta-\\ntion sampling, with the number of visits per mode being on average S/G!. The construction\\nof this importance density is fully automatic and it is suﬃcient to store the moments of\\nthe various conditional densities (rather than the allocations zand the latent utilities ug\\nand indicators rgthemselves) during MCMC sampling for later evaluation. This importance\\ndensity is used to compute importance sampling estimators of the marginal likelihood, see\\nthe illustrative application in Section 4.1.\\n4 Illustrative Applications\\nThe utility of ME models is illustrated in this section through the use of several applications.\\nME Markov chain models for categorical time series, ME models for ranked preference data,\\nand ME models for network data, all of which are members of the ME model framework,\\nare applied.\\n4.1 Analysing marijuana use through ME Markov chain models\\nLanget al.(1999) studied data on the marijuana use of 237 teenagers taken from ﬁve annual\\nwaves (1976-80) of the National Youth Survey. The respondents were 13 years old in 1976\\n11\\nand reported for ﬁve consecutive years their marijuana use in the past year as a categorical\\nvariable with the three categories “never”, “not more than once a month” and “more than\\nonce a month”. Hence, for i= 1,..., 237, the outcome variable is a categorical time series\\nyi= (yi0,yi1,...,yi4)with three states, labeled 1 for never-user, 2 for light and 3 for heavy\\nusers.\\nTo identify groups of teenagers with similar marijuana use behaviour, Frühwirth-\\nSchnatter (2011b) applied a ME approach based on Markov chain models (Frühwirth-\\nSchnatter et al., 2012) and considered each time series yias a single entity belonging\\nto one ofGunderlying classes. Various types of ME Markov chain models were applied\\nto capture dependence in marijuana use over time and to investigate if the gender of the\\nteenagers can be associated with a certain type of marijuana use.\\nGiven the times series nature of the categorical outcome variable yi, the component\\ndensityfg(·)in the mixture of experts model (1) must have an appropriate form and various\\nmodels are considered. Model M1is a standard ﬁnite mixture of time-homogeneous Markov\\nchainmodelsoforderone(Pamminger&Frühwirth-Schnatter,2010)whereeachcomponent-\\nspeciﬁc density fg(·)in (1) is characterized by a transition matrix ξgwithJ= 3rows\\nand the weight distribution η1,...,ηGis independent of any covariates. Each row ξg,j·=\\n(ξg,j1,...,ξg,j3),j= 1,...,J, of the matrix ξgrepresents a probability distribution over the\\nthree categories of marijuana use with\\nξg,jk=P(yit=k|yi,t−1=j,zi=g), k = 1,..., 3.\\nThis model is extended in various ways to include covariate information into the transition\\nbehaviour. First, an inhomogeneous model (labelled model M2) is considered, where the\\ntransitionmatrixineachgroupdependsonthegender xioftheteenager.Ifall J= 6possible\\ncombinationsHit= (yi,t−1,xi)of the immediate past yi,t−1at timetand the gender xiare\\nindexedby j= 1,...,J,thenthecomponent-speciﬁcdensity fg(yi|ξg)in(1)canbedescribed\\nby a generalized transition matrix ξgwith six rows, with the jth rowξg,j·= (ξg,j1,...,ξg,j3)\\ndescribing again the conditional distribution of yit, given that the state of the history Hit\\nequalsj:\\nξg,jk=P(yit=k|Hit=j,zi=g), k = 1,..., 3.\\nEvidently, the component speciﬁc distribution reads:\\nfg(yi|ξg) =J/productdisplay\\nj=13/productdisplay\\nk=1ξni,jk\\ng,jk (8)\\nwhere, for each time series i,ni,jk=/summationtext4\\nt=1I(yit=k,Hit=j)is the number of transitions\\ninto statekgiven a history of type j. Note that (8) is formulated conditional on the ﬁrst\\nobservation yi0.\\nAlternative component-speciﬁc distributions can be constructed, by deﬁning the history\\nHitthrough diﬀerent combinations of past values and covariates. Choosing Hit= (yi,t−1,t),\\nfor instance, deﬁnes a time-inhomogeneous Markov chain model, labeled model M3, with\\nJ= 12diﬀerent covariate combinations. This model is able to capture the eﬀect that the\\ntransition behaviour between the states might change as the teenagers grow older.\\nThemost complexmodel, labelled model M4, extendsmodelM3byassuming additional\\ndependence on gender, i.e. Hit= (yi,t−1,t,xi), withJ= 24diﬀerent covariate combinations.\\nBoth modelM3andM4are characterised by component-speciﬁc generalized transition\\nmatricesξgwith, respectively, 12 and 24 rows. For each of the models M2,M3,M4, it is\\n12\\nTable2:Marijuanadata;marginallikelihood logp(y|Mk)forvariousﬁnitemixturesofhomo-\\ngeneous (M1) and inhomogeneous ( M2,M3,M4) Markov chain models with an increasing\\nnumberGof classes (best values for each model in bold font)\\nG\\nModel Covariates J1 2 3\\nM1 - 3 -605.5 -600.0 -600.3\\nM2xi6 -610.0 -601.3 -603.6\\nM3t12 -613.7 -596.5 -599.4\\nM4t,xi24 -619.8 -602.7 -601.1\\nassumed that the weight distribution η1,...,ηGis independent of any covariate, leading to\\nvarious ﬁnite mixtures of inhomogeneous Markov chain models.\\nBayesian inference is carried out for all models M1,...,M4for an increasing number\\nG= 1,2,3of classes. MCMC estimation as described in Section 3.2 is easily applied, as the\\nJrowsξg,j·ofξgare conditionally independent under the conditionally conjugate Dirichlet\\npriorξg,j·∼D(d0,j1,...,d 0,j3). Given zandy, the generalized transition matrix ξgis sampled\\nrow-by-row from a total of JGDirichlet distributions:\\nξg,j·|z,y∼D(d0,j1+ng\\nj1,...,d 0,j3+ng\\nj3), j = 1,...,J, g = 1,...,G, (9)\\nwhereng\\njk=/summationtext\\ni:zi=gni,jkis the total number of transitions into state kobserved in class g\\ngiven a history of type j.\\nFor model comparison, the marginal likelihood is computed explicitly for G= 1, while\\nimportancesamplingasdescribedinSection3.3isappliedfor G= 2,3,usingtheimportance\\ndensity:\\nqG(θ) =1\\nSS/summationdisplay\\ns=1p(η|z(s))G/productdisplay\\ng=1J/productdisplay\\nj=1p(ξg,j·|z(s),y),\\nwherep(ξg,j·|z,y)is equal to the full conditional Dirichlet posterior of ξg,j·given in (9).\\nRandom permutation sampling is applied to ensure that all G!symmetric modes are visited\\nandS= 10,000. The marginal likelihoods reported in Table 2 select G= 2for all models\\nexcept forM4, whereG= 3is selected. Among all models, the marginal likelihood is the\\nhighest for model M3withG= 2classes.\\nHence,atime-inhomogeneousMarkovchainmodelwhichdoesnotdependongenderbest\\ndescribes the transition behaviour in each class. Table 3 reports the corresponding posterior\\nmeans E (ξg,·|y)and E (ηg|y)for each of the two groups. Label switching was resolved by\\napplyingk-means clustering to a vector constructed from all persistence probabilities at all\\ntime points. Both groups are roughly of equal size, with the ﬁrst group being slightly larger.\\nA characteristic diﬀerence is evident for the two groups of teenagers. In group 1, never-users\\nhave a high probability ξt,11to remain never-users throughout the whole observation period,\\nwhereas this probability is much smaller for the second group right from the beginning and\\ndrops to only 45% in the last year.\\nTo investigate if gender is associated with group membership, model M3withG= 2\\nclasses is combined with the ME model (4), by including gender as subject-speciﬁc covariate\\nxias in the example in Section 2.2. This model is labelled model M5. Additionally, a dummy\\nvariableDi0is included, indicating if the teenager used marijuana, light or heavy, in the ﬁrst\\nyear. AsG= 2, the MEmodel (4)reduces to abinary logitmodel withregressioncoeﬃcients\\nγ2= (γ20,γ21,γ22), each assumed to follow a standard normal prior distribution.\\n13\\nTable 3: Marijuana data; ﬁnite mixture of time-inhomogeneous Markov chain models (model\\nM3) withG= 2classes; the estimated posterior mean E (ξg,·|y)is arranged for each t=\\n1,..., 4as a3×3matrix; the estimated class sizes ˆηgare equal to the posterior mean E (ηg|y)\\nt= 1 t= 2 t= 3 t= 4\\nGroup 1 0.93 0.04 0.03 0.89 0.09 0.02 0.90 0.04 0.06 0.93 0.04 0.03\\n(ˆη1= 0.56)0.50 0.17 0.34 0.10 0.33 0.57 0.17 0.65 0.18 0.20 0.64 0.16\\n0.22 0.18 0.60 0.10 0.17 0.73 0.04 0.27 0.69 0.15 0.12 0.74\\nGroup 2 0.76 0.21 0.03 0.70 0.24 0.06 0.75 0.18 0.07 0.45 0.43 0.12\\n(ˆη2= 0.44)0.34 0.15 0.51 0.23 0.39 0.38 0.31 0.41 0.28 0.46 0.43 0.11\\n0.18 0.23 0.59 0.10 0.22 0.68 0.13 0.15 0.72 0.05 0.10 0.85\\nTable 4: Marijuana data; ME model with ˜xi= (1,xi,Di0)(modelM5), extending model M3\\nwithG= 2classes. Posterior expectation and 95% HPD region of the component weight\\nparameters γ2jin the ME model (4)\\nCovariate ˜xij E(γ2j|y)95% HPD region of γ2j\\nconstant -0.69 (-1.75,0.35)\\nmale (baseline: female) 0.28 (-0.71,1.22)\\nmarijuana use in 1976 (baseline: no) -0.07 (-1.70,1.43)\\nlogp(y|M 5) -598.5\\nFrom posterior inference in Table 4, we ﬁnd that male teenagers have a slightly higher\\nprobability to belong to the second group, because E (γ21|y)>0, however, the coeﬃcient γ21\\nis not signiﬁcantly diﬀerent from 0. Similarly, the initial state from which a teenager started\\nin 1976 does not have a signiﬁcant inﬂuence on the probability to belong to the second\\ngroup. This suggests that the ME time-inhomogeneous Markov chain model actually reduces\\nto a standard mixture of time-inhomogeneous Markov chain models which is conﬁrmed by\\ncomparing the log marginal likelihood of both models, being equal to -596.5 for a standard\\nmixture model with G= 2groups, see Table 2, and being equal to -598.5 for an ME model\\nwithG= 2groups, see Table 4.\\nThe marginal likelihood estimator for the ME model is based on importance sampling\\nusing the importance density (7) derived from auxiliary mixture sampling:\\nqG(θ) =1\\nSS/summationdisplay\\ns=1p(γ2|u(s)\\n2,r(s)\\n2,z(s))2/productdisplay\\ng=1J/productdisplay\\nj=1p(ξg,j·|z(s),y),\\nwherep(γ2|u2,r2,z)is conditionally Gaussian and p(ξg,j·|z,y)is equal to the full conditional\\nDirichlet posterior of ξg,j·given in (9). Again, random permutation sampling is applied to\\nensure that the two equivalent modes are visited.\\nTo sum up, this investigation shows that teenagers may, indeed, be clustered into two\\ngroups with diﬀerent behaviour with respect to marijuana use, one being a never-user group,\\nwhile the second group has a much higher risk to become a user. Preference for a standard\\nmixture of Markov chain models over a mixture of experts Markov chain model based on\\ngender shows that the two types of marijuana use cannot be associated with the gender of\\nthe teenager. Both male and female teenagers have about the same risk to belong to the\\nsecond group. Unobserved factors, not the gender, are relevant for membership of a teenager\\nto one group or the other.\\n14\\nTable 5: Covariates recorded for each respondent in the Irish Marketing Surveys poll.\\nAge Area Gender Government Marital Social\\nsatisfaction status class\\n– City Housewife No opinion Married AB\\nRural Male Not satisﬁed Single C1\\nTown Non-housewife Satisﬁed Widowed C2\\nDE\\nF50+\\nF50-\\n4.2 A mixture of experts model for ranked preference data\\nMary McAleese served as the eighth President of Ireland from 1997 to 2011 and was elected\\nunder the Single Transferable Vote electoral system. Under this system voters rank, in order\\nof their preference, some or all of the electoral candidates. The vote counting system which\\nresults in the elimination of candidates and the subsequent election of the President is an\\nintricateprocessinvolvingthetransferofvotesbetweencandidatesasspeciﬁedbythevoters’\\nballots. Details of the electoral system, the counting process and the 1997 Irish presidential\\nelection are given in Coakley & Gallagher (2004), Sinnott (1995), Sinnott (1999) and Marsh\\n(1999).\\nThe 1997 presidential election race involved ﬁve candidates: Mary Banotti, Mary\\nMcAleese, Derek Nally, Adi Roche and Rosemary Scallon. Derek Nally and Rosemary Scal-\\nlon were independent candidates while Mary Banotti and Adi Roche were endorsed by the\\nthen current opposition parties Fine Gael and Labour respectively. Mary McAleese was en-\\ndorsed by the Fianna Fáil party who were in power at that time. In terms of candidate type,\\nMcAleese and Scallon were deemed to be conservative candidates with the other candidates\\nregarded as liberal. Gormley & Murphy (2008a,b, 2010a,b) provide further details on the\\n1997 presidential election and on the candidates.\\nOne month prior to election day a survey was conducted by Irish Marketing Surveys on\\n1083 respondents. Respondents were asked to list some or all of the candidates in order of\\npreference, as if they were voting on the day of the poll. In addition, pollsters gathered data\\non attributes of the respondents as detailed in Table 5.\\nInterest lies in determining if groups of voters with similar preferences (i.e. voting blocs)\\nexist within the electorate. If such voting blocs do exist, the inﬂuence the recorded socio-\\neconomic variables may have on the clustering structure and/or on the preferences which\\ncharacterize a voting bloc is also of interest. Jointly modelling the rank preference votes and\\nthe covariates through a mixture of experts model for rank preference data when clustering\\nthe electorate provides this insight.\\nGiven the rank nature of the outcome variables or votes yi(i= 1,...,n = 1083) the\\ncomponent density fg(·)in the mixture of experts model (1) must have an appropriate form.\\nThe Plackett-Luce model (Plackett, 1975; Gormley & Murphy, 2006) (or exploded logit\\nmodel) for rank data provides a suitable model; Benter’s model (Benter, 1994) provides\\nanother alternative. Let yi= [c(i,1),...,c (i,mi)]denote the ranked ballot of voter iwhere\\nc(i,j)denotes the candidate ranked in jth position by voter iandmiis the number of\\ncandidates ranked by voter i. Under the Plackett-Luce model, given that voter iis a member\\nof voting bloc gand given the ‘support parameter’ pg= (pg1,...,pgM), the probability of\\n15\\nTable 6: The model with smallest BIC within each type of mixture of experts model for\\nranked preference data applied to the 1997 Irish presidential election data\\nBICGCovariates\\nSimple mixture of experts model 8491 4 ηg: Government satisfaction, Age.\\nStandard mixture of experts 8512 3 ηg: Government satisfaction, Age.\\nregression model pg: Age\\nMixture model 8513 3 –\\nMixture of regressions model 8528 1 pg: Government satisfaction\\nvoteri’s ballot is\\np(yi|pg) =pg,c(i,1)/summationtextM\\ns=1pg,c(i,s)·pg,c(i,2)/summationtextM\\ns=2pg,c(i,s)···pg,c(i,mi)/summationtextM\\ns=mipg,c(i,s),\\nwhereM= 5denotes the number of candidates in the electoral race. The support parameter\\npgj(typically restricted such that/summationtextM\\nj=1pgj= 1) can be interpreted as the probability of\\nranking candidate jﬁrst, out of the currently available choice set. Hence, the Plackett-Luce\\nmodel models the ranking of candidates by a voter as a set of independent choices by the\\nvoter, conditional on the cardinality of the choice set being reduced by one after each choice\\nis made.\\nIn the standard mixture of experts regression model, the parameters of the component\\ndensities are modelled as a function of covariates. Here the support parameters are modelled\\nas a logistic function of the covariates\\nlog/bracketleftBiggpgj(xi)\\npg1(xi)/bracketrightBigg\\n=βgj0+βgj1xi1+···+βgjqxiq\\nwherexi= (xi1,...,xiq)is the set of qcovariates associated with voter iandβgj=\\n(βgj0,...,βgjq)/latticetopare unknown parameters for j= 2,...,M. Note that for identiﬁability\\nreasons candidate 1 is used as the baseline choice and βg1= (0,..., 0)for allg= 1,...,G.\\nIn the standard mixture of experts regression model, the component weights are also\\nmodelled as a function of covariates, in a similar vein to the example used in Section 2.2,\\ni.e.\\nlog/bracketleftBiggηg(xi)\\nη1(xi)/bracketrightBigg\\n=γg0+γg1xi1+···+γgqxiq,\\nwhere voting bloc 1 is used as the baseline voting bloc.\\nThe suite of four ME models in the ME framework (Figure 2) arise from modelling\\nthe component parameters and/or the component weights as functions of covariates, or as\\nconstant with respect to covariates. In this application, each model is ﬁtted in a maximum\\nlikelihood framework using the EM algorithm; approximate standard errors for the model\\nparameters are derived from the empirical information matrix (McLachlan & Peel, 2000)\\nafter the EM algorithm has converged. Model ﬁtting details for each model are outlined in\\nGormley & Murphy (2008a,b, 2010a,b).\\nEach of the four ME models for rank preference data were ﬁtted to the data from\\nthe electorate in the Irish presidential election poll. A range of models with G= 1,..., 5\\nwas considered and a forward step-wise selection method was employed to choose inﬂuen-\\ntial covariates. The Bayesian Information Criterion (BIC) (Kass & Raftery, 1995; Schwarz,\\n16\\nVoting Bloc 1 Voting Bloc 2 Voting Bloc 3 Voting Bloc 4\\nBanotti\\nMcAleese\\nNally\\nRoche\\nScallon\\nτ^1=0.19 τ^2=0.16 τ^3=0.35 τ^4=0.30.11 (0.01)\\n0.28 (0.03)\\n0.15 (0.02)\\n0.15 (0.04)\\n0.31 (0.06)0.13 (0.01)\\n0.13 (0.01)\\n0.03 (<0.01)\\n0.70 (0.01)\\n0.01 (<0.01)0.17 (<0.01)\\n0.72 (0.03)\\n0.04 (<0.01)\\n0.06 (<0.01)\\n0.01 (<0.01)0.52 (<0.01)\\n0.14 (0.01)\\n0.13 (0.01)\\n0.15 (0.01)\\n0.05 (<0.01)Figure 3: A mosaic plot representation of the parameters of the component densities of\\nthe simple mixture of experts model for rank preference data. The width of each block is\\nproportional to the marginal probability of component membership ( ˆηg=/summationtextn\\ni=1ηg(xi|ˆγ)/n).\\nThe blocks are divided in proportion to the Plackett-Luce support parameters which are\\ndetailed therein. Standard errors are provided in parentheses.\\n1978) was used to select the optimal model; this criterion is a penalized likelihood criterion\\nwhich rewards model ﬁt while penalizing non-parsimonious models, see also Chapter 7, Sec-\\ntion 7.2.2 of this volume. Small BIC values indicate a preferable model. Table 6 details the\\noptimal models for each type of ME model ﬁtted.\\nBased on the BIC values, the optimal model is a simple mixture of experts model with\\nfour groups where “age“ and “government satisfaction” are important covariates for deter-\\nmining group or “voting bloc” membership. Under this simple mixture of experts model,\\nthe covariates are not informative within voting blocs, but only in determining voting bloc\\nmembership. The maximum likelihood estimates of the model parameters are reported in\\nFigure 3 and in Table 7.\\nThe support parameter estimates illustrated in Figure 3 have an interpretation in the\\ncontext of the 1997 Irish presidential election. Voting bloc 1 could be characterized as the\\n“conservative voting bloc” due to its large support parameters for McAleese and Scallon.\\nVoting bloc 2 has large support for the liberal candidate Adi Roche. Voting bloc 3 is the\\n17\\nTable 7: Odds ratios ( exp(γg)/exp(γ1)) for the component weight parameters in the simple\\nME model for rank preference data (95% conﬁdence intervals are given in parentheses). The\\ncovariates ‘age’ and ‘government satisfaction level’ were selected as inﬂuential\\nAge Not satisﬁed Satisﬁed\\nVoting bloc 2 0.01 (0.00, 0.05) 2.80 (0.77, 10.15) 1.14 (0.42, 3.11)\\nVoting bloc 3 0.95 (0.32, 2.81) 3.81 (0.90, 16.13) 3.12 (0.94, 10.31)\\nVoting bloc 4 1.56 (0.35, 6.91) 3.50 (1.07, 11.43) 0.35 (0.12, 0.98)\\nlargestvotingblocintermsofmarginalcomponentweightsandintuitivelyhaslargersupport\\nparameters for the high proﬁle candidates McAleese and Banotti. These candidates were\\nendorsed by the two largest political parties in the country at that time. Voters belonging to\\nvoting bloc 4 favor Banotti and have more uniform levels of support for the other candidates.\\nA detailed discussion of this optimal model is also given in Gormley & Murphy (2008b).\\nTable 7 details the odds ratios computed from the component weight parameters γ=\\n{γ2,γ3,γ4}. In the model, voting bloc 1 (the conservative voting bloc) is the baseline voting\\nbloc andγ1= (0,..., 0)/latticetop. Two covariates were selected as inﬂuential: age and government\\nsatisfaction levels. In the “government satisfaction” covariate, the baseline was chosen to be\\n“no opinion“.\\nInterpreting the odds ratios provides insight to the type of voter which characterises\\neach voting bloc. For example, older (and generally more conservative) voters are much\\nless likely to belong to the liberal voting bloc 2 than to the conservative voting bloc 1\\n(exp(γ21) = 0.01). Also, voters with some interest in government are more likely to belong\\nto voting bloc 3 ( exp(γ32) = 3.81andexp(γ33) = 3.12), the bloc favouring candidates backed\\nby large government parties, than to belong to the conservative voting bloc 1. Voting bloc\\n1 had high levels of support for the independent candidate Scallon. The component weight\\nparameterestimates furtherindicate thatvotersdissatisﬁed with thecurrentgovernmentare\\nmore likely to belong to voting bloc 4 than to voting bloc 1 ( exp(γ42) = 3.50). This is again\\nintuitive as voting bloc 4 favours Mary Banotti who was backed by the main government\\noppositionparty,whilevotingbloc1favoursthegovernmentbackedMaryMcAleese.Further\\ninterpretation of the component weight parameters are given in Gormley & Murphy (2008b).\\n4.3 A mixture of experts latent position cluster model\\nThe latent position cluster model (Handcock et al., 2007) develops the idea of the latent\\nsocialspace(Hoﬀ et al.,2002)byextendingittoaccommodateclustersofactorsinthelatent\\nspace. Under the latent position cluster model, the latent location of each actor is assumed\\nto be drawn from a ﬁnite normal mixture model, each component of which represents a\\ncluster of actors. In contrast, the model outlined in Hoﬀ et al.(2002) assumes that the\\nlatent positions were normally distributed. Thus, the latent position cluster model oﬀers a\\nmore ﬂexible version of the latent space model for modelling heterogeneous social networks.\\nThe latent position cluster model provides a framework in which actor covariates may\\nbe explicitly included in the model – the probability of a link between two actors may be\\nmodelled as a function of both their separation in the latent space and of their relative\\ncovariates. However, the covariates may contribute more to the structure of the network\\nthan solely through the link probabilities – the covariates may inﬂuence both the cluster\\nmembership of an actor and their link probabilities. A latent position cluster model in which\\nthe cluster membership of an actor is modelled as a function of their covariates lies within\\n18\\nTable 8: Covariates associated with the 71 lawyers in the US corporate law ﬁrm. The last\\ncategory in each categorical covariate is treated as the baseline category in all analyses.\\nCovariate Levels\\nAge –\\nGender 1 = male\\n2 = female\\nLaw school 1 = Harvard or Yale\\n2 = University of Connecticut\\n3 = other\\nOﬃce 1 = Boston\\n2 = Hartford\\n3 = Providence\\nPractice 1 = litigation\\n2 = corporate\\nSeniority 1 = partner\\n2 = associate\\nYears with the ﬁrm –\\nthe mixture of experts framework.\\nSpeciﬁcally, social network data take the form of a set of relations {yi,j}between a group\\nofi,j= 1,...,nactors, represented by an n×nsociomatrix y. Here it is assumed that the\\nrelationyi,jbetween actors iandjis a binary relation, indicating the presence or absence of\\na link between the two actors; the mixture of experts latent position cluster model is easily\\nextended to other forms of relation (such as count data). Covariate data xi= (xi1,...,xiq)\\nassociated with actor iare assumed to be available, where qdenotes the number of observed\\ncovariates.\\nEach actor iis assumed to have a location wi= (wi1,...,wiD)in theDdimensional\\nlatent social space. The probability of a link between any two actors is assumed to be\\nindependent of all other links in the network, given the latent locations of the actors. Let\\nxi,j= (xij1,...,xijq)denote anqvector of dyadic speciﬁc covariates where xijk=d(xik,xjk)\\nis a measure of the similarity in the value of the kth covariate for actors iandj. Given the\\nlink probabilities parameter vector β, the likelihood function is then\\np(y|w,x,β) =n/productdisplay\\ni=1/productdisplay\\nj/negationslash=ip(yi,j|wi,wj,xi,j,β)\\nwhere wis then×Dmatrix of latent locations and xis the matrix of dyadic speciﬁc\\ncovariates. The probability of a link between actors iandjis then modelled using a logistic\\nregression model where both dyadic speciﬁc covariates and Euclidean distance in the latent\\nspace are covariates:\\nlog/bracketleftBiggP(yi,j= 1)\\nP(yi,j= 0)/bracketrightBigg\\n=β0+β1xij1+···+βqxijq−||wi−wj||.\\nTo account for clustering of actor locations in the latent space, it is assumed that the latent\\nlocationswiare drawn from a ﬁnite mixture model. Moreover, in the mixture of experts\\nlatent position cluster model, the latent locations are assumed drawn from a ﬁnite mixture\\n19\\nmodel in which actor covariates may inﬂuence the mixing proportions:\\nwi∼G/summationdisplay\\ng=1ηg(xi|γ)φ(wi|µg,σ2\\ngI)\\nwhere\\nηg(xi|γ) =exp(γg0+γg1xi1+···+γgqxiq)\\n/summationtextG\\ng/prime=1exp(γg/prime0+γg/prime1xi1+···+γg/primeqxiq)\\nandγ1= (0,..., 0)/latticetop. This model has an intuitive motivation: the covariates of an actor may\\ninﬂuence their cluster membership, their cluster membership inﬂuences their latent location,\\nand in turn their latent location determines their link probabilities.\\nThe mixture of experts latent position cluster model can be ﬁtted within the Bayesian\\nparadigm; as outlined in Section 3.2 a Metropolis-within-Gibbs sampler can be employed\\nto draw samples from the posterior distribution of interest. Model issues such as likelihood\\ninvariance to distance preserving transformations of the latent space and label switching\\nmustbeconsideredduringthemodelﬁttingprocess–anapproachtodealingwithsuchmodel\\nidentiﬁability and full model ﬁtting details are available in Gormley & Murphy (2010b). In\\nthis application, model choice concerns not only the number Gof clusters, but also the\\ndimensionDof the latent space.\\nAn example of the mixture of experts latent position cluster model methodology is pro-\\nvided here through the analysis of a network data set detailing interactions between a set\\nof 71 lawyers in a corporate law ﬁrm in the USA (Lazega, 2001). The data include mea-\\nsurements of the coworker network, an advice network and a friendship network. Covariates\\nassociated with each lawyer in the ﬁrm are also included and are detailed in Table 8. Interest\\nlies in identifying social processes within the ﬁrm such as knowledge sharing and organisa-\\ntional structures, and examining the potential inﬂuence of covariates on such processes.\\nUnder the ME model framework outlined in Section 2.3, a suite of four mixtures of\\nexperts latent position cluster models is available. This suite of models was ﬁtted to the\\nadvice network; data in this network detail links between lawyers who sought basic profes-\\nsional advice from each other over the previous twelve months. Gormley & Murphy (2010b)\\nexplore the coworkers network data set and the friendship network data set using similar\\nmethodology. Figure 4 illustrates the resulting latent space locations of the lawyers under\\neach ﬁtted model with (G,D ) = (2,2). These values were selected using BIC after ﬁtting a\\nrange of latent position cluster models (with no covariates) to the network data only (Hand-\\ncocket al., 2007). Table 9 details the resulting regression parameter estimates and their\\nassociated uncertainty for the four ﬁtted models.\\nThe models are compared through the AICM, the posterior simulation-based analogue\\nof Akaike’s Information Criterion (AIC) (Akaike, 1973; Raftery et al., 2007). In this imple-\\nmentation the optimal model is that with the highest AICM and is the model with covariates\\nin the link probabilities and in the component weights. The results of the analysis show some\\ninteresting patterns. The coeﬃcients of the covariates in the link probabilities are very simi-\\nlar in the models (b) and (d) in Table 9. These coeﬃcients indicate that a number of factors\\nhave a positive or negative eﬀect on whether a lawyer asks another for advice. In summary,\\nlawyers who are similar in seniority, gender, oﬃce location and practice type are more likely\\nto ask each other for advice. The eﬀects of years and age seem to have a negative eﬀect, but\\nthese variables are correlated with seniority and with each other, so their marginal eﬀects\\nare more diﬃcult to interpret.\\nImportantly, the latent positions are very similar in models (a) and (c) which do not\\nhave covariates in the link probabilities and models (b) and (d) which do have covariates\\n20\\n−5 0 5−4 −2 0 2 4\\nDimension 1Dimension 2(a) Latent position cluster model.\\n−5 0 5−4 −2 0 2 4\\nDimension 1Dimension 2 (b) Mixture of regressions latent position\\ncluster model.\\n−5 0 5−4 −2 0 2 4\\nDimension 1Dimension 2\\n(c) Simple latent position cluster model.\\n−5 0 5−4 −2 0 2 4\\nDimension 1Dimension 2 (d) Standard mixture of experts latent po-\\nsition cluster model.\\nFigure 4: Estimates of clusters and latent positions of the lawyers from the advice network\\ndata. The ellipses are 50 %posterior sets illustrating the uncertainty in the latent locations.\\nLawyers who are members of the same cluster are illustrated using the same shade and\\nsymbol. Observed links between lawyers are also illustrated.\\nin the link probabilities. This can be explained because of the diﬀerent role that the latent\\nspace plays in the models with covariates in the link probabilities and those that do not\\nhave such covariates. When the covariates are in the link probabilities, the latent space is\\nmodelling the network structure that could not be explained by the link covariates, whereas\\nin the other case the latent space is modelling much of the network structure.\\nInterestingly, in the model with the highest AICM value, there are covariates in the\\ncluster membership probabilities as well as in the link probabilities. This means that the\\nstructure in the latent space, which is modelling what could not be explained directly in\\nthe link probabilities, has structure that can be further explained using the covariates. The\\noﬃce location, practice and age of the lawyers retain explanatory power in explaining the\\nclustering found in the latent social space.\\nThe diﬀerence in the cluster membership coeﬃcients in models (c) and (d) is due to the\\ndiﬀerent interpretation of the latent space in these models. However, it is interesting to note\\nthat in this application the signs of the coeﬃcients are identical because the cluster member-\\nships shown for these models in Figure 4(c) and Figure 4(d) are similar; this phenomenon\\n21\\nTable9:Posteriormeanparameterestimatesforthefourmixturesofexpertsmodelsﬁttedto\\nthe lawyers advice data as detailed in Figure 4. Standard deviations are given in parentheses.\\nNote that cluster 1 was used as the baseline cluster in the case of the cluster membership\\nparameters. Baseline categories for the covariates are detailed in Table 8\\nModel (a) Model (b) Model (c) Model (d)\\nLink Probabilities\\nIntercept 1.26 (0.10) -2.87 (0.17) 1.23 (0.10) -2.65 (0.17)\\nAge -0.02 (0.004) -0.02 (0.004)\\nGender 0.60 (0.09) 0.62 (0.09)\\nOﬃce 2.02 (0.10) 1.97 (0.10)\\nPractice 1.63 (0.10) 1.57 (0.10)\\nSeniority 0.89 (0.11) 0.81 (0.11)\\nYears -0.04 (0.005) -0.04 (0.005)\\nCluster Memberships\\nIntercept -1.05 (1.75) 0.94 (0.79) -0.62 (1.23) 1.27 (1.29)\\nAge -0.09 (0.04) -0.14 (0.06)\\nOﬃce (=1) 1.94 (1.02) 2.40 (1.14)\\nOﬃce (=2) -2.08 (1.09) -0.97 (1.19)\\nPractice 3.18 (0.85) 2.14 (1.08)\\nLatent Space Model\\nCluster 1 mean -0.50 (0.52) 0.09 (0.19) -1.09 (0.31) -0.54 (0.21)\\n0.21 (0.58) -0.09 (0.26) 0.40 (0.28) 0.40 (0.20)\\nCluster 1 variance 3.35 (1.29) 2.12 (0.77) 3.19 (0.58) 1.25 (0.34)\\nCluster 2 mean 1.66 (0.92) -0.24 (0.20) 2.10 (0.30) 1.32 (0.51)\\n-0.67 (0.58) 0.35 (0.23) -0.77 (0.30) -0.98 (0.47)\\nCluster 2 variance 1.29 (1.58) 0.27 (0.68) 1.16 (0.40) 1.63 (0.69)\\nAICM -3644.24 -3346.87 -3682.71 -3325.95\\ndoes not hold generally (see Gormley & Murphy, 2010b, Section 5.3).\\nThe results of this analysis oﬀer a cautionary message in automatically selecting the\\ntype of mixture of experts latent position cluster model for analyzing the lawyer advice\\nnetwork. The role of the latent space in the model is very diﬀerent depending on how the\\ncovariates enter the model. So, if the latent space is to be interpreted as a social space that\\nexplains network structure, then the covariates should not directly enter the link probabili-\\nties. However, if the latent space is being used to ﬁnd interesting or anomalous structure in\\nthe network that cannot be explained by the covariates, then one should consider allowing\\nthe covariates enter the cluster membership probabilities.\\n4.4 Software\\nAs demonstrated in this section, the approach to ﬁtting an ME model depends on the\\napplication setting and on the form of the ME model itself. Therefore, a single software\\ncapable of ﬁtting any ME model is not currently available.\\n22\\nIn R (R Core Team, 2018), the MEclustnet package (Gormley & Murphy, 2018) ﬁts\\nthe mixture of experts latent position cluster model detailed in Section 4.3. The flexmix\\npackage (Grün & Leisch, 2008b) has model ﬁtting capabilities for a range of mixture of\\nregression models, which include covariates (or concomitant variables), as does the mixreg\\npackage (Turner, 2014). Additionally, mixtools (Benaglia et al., 2009) facilitates ﬁtting\\nof aG= 2mixture of regressions model in which the component weights are modelled as\\nan inverse logit function of the covariates. The cluster weighted models which are closely\\nrelated to ME models can be ﬁtted using the flexCWM package (Mazza et al., 2017). All\\npackages are freely available through the Comprehensive R Archive Network (CRAN) at\\nhttps://cran.r-project.org .\\nInMATLAB,the bayesfpackage(Frühwirth-Schnatter,2018)allowstoestimateabroad\\nrangeofmixturemodelsusingeitherﬁnitemixtures,mixturesofexpertsorMarkovswitching\\nmodels as a model for the hidden group indicators z.\\nIn terms of other softwares, the FMMprocedure in SAS also facilitates ME model ﬁtting,\\nand stand alone softwares such as Latent GOLD (Vermunt & Magidson, 2005) and Mplus\\n(Muthén & Muthén, 2011) ﬁt closely related latent class models.\\n5 Identiﬁability of Mixtures of Experts Models\\nFor a ﬁnite mixture distribution one has to distinguish three types of non-identiﬁability\\n(Frühwirth-Schnatter, 2006, Section 1.3): invariance to relabelling the components of the\\nmixture distribution (the so-called label switching problem), non-identiﬁability due to po-\\ntential overﬁtting and generic non-identiﬁability which occurs only for certain classes of\\nmixture distributions.\\nConsider a standard mixture distribution with Gcomponents with non-zero weights\\nη1,...,ηGgenerated by distinct parameters θ1,...,θG. Assume that for all possible realisa-\\ntionsyfrom this mixture distribution the identity\\nG/summationdisplay\\ng=1ηgfg(y|θg) =G⋆/summationdisplay\\ng=1η⋆\\ngfg(y|θ⋆\\ng)\\nholds where the right-hand side is a mixture distribution from the same family with G⋆\\ncomponents with non-zero weights η⋆\\n1,...,η⋆\\nG⋆generated by distinct parameters θ⋆\\n1,...,θ⋆\\nG⋆.\\nThen generic identiﬁability implies that G⋆=Gand the two mixtures’ parameters\\nθ= (η1,...,ηG,θ1,...,θG)andθ⋆= (η⋆\\n1,...,η⋆\\nG,θ⋆\\n1,...,θ⋆\\nG)are identical up to relabelling\\nthe component indices. Common ﬁnite mixture distributions such as Gaussian and Poisson\\nmixtures are generically identiﬁed, see Teicher (1963), Yakowitz & Spragins (1968), and\\nChandra (1977) for a detailed discussion.\\nDiscrete mixtures often suﬀer from generic non-identiﬁability for certain parameter con-\\nﬁgurations, well-known examples being mixtures of binomial distributions (see Section 5.1)\\nand mixtures of multinomial distributions (Grün & Leisch, 2008c). Somewhat unexpect-\\nedly, mixtures of regression models suﬀer from generic non-identiﬁability (Hennig, 2000;\\nGrün & Leisch, 2008a), as will be discussed in more detail in Section 5.2. Little is known\\nabout generic identiﬁability of mixtures of experts models and some results are presented\\nin Section 5.3. However, ensuring generic identiﬁability for general ME models remains a\\nchallenging issue.\\nIdentiﬁability problems for mixture with nonparametric components are discussed in\\nChapter 14 of this volume.\\n23\\n5.1 Identiﬁability for mixtures of binomials\\nFor binomial mixtures the component densities arise from B(N,π)-distributions, where N\\nis commonly assumed to be known, whereas πis heterogeneous across the components:\\nY∼η1B(N,π 1) +···+ηGB(N,πG). (10)\\nThe probability mass function (pmf) of this mixture takes on N+1diﬀerent support points:\\np(y|θ) =P(Y=y|θ) =G/summationdisplay\\ng=1ηg/parenleftBigg\\nN\\ny/parenrightBigg\\nπy\\ng(1−πg)N−y, y = 0,1,...,N, (11)\\nwith 2G−1independent parameters θ= (π1,...,πG,η1,...,ηG), withηG= 1−/summationtextG−1\\ng=1ηg.\\nGiven data y= (y1,...,yn)from mixture (10), the only information available to estimate\\nθareN(among the N+ 1observed) relative frequencies hn(Y=y)(y= 0,1,...,N). As\\nn→∞(whileNis ﬁxed),hn(Y=y)converges to P (Y=y|θ)by the law of large numbers,\\nbut the number of support points remains ﬁxed. Hence, the data provide only Nstatistics,\\ngiven by the relative frequencies, to estimate 2G−1parameters. Simple counting yields\\nthe following necessary condition for identiﬁability for a binomial mixture, which has been\\nshown by Teicher (1961) to be also suﬃcient:\\n2G−1≤N⇔G≤(N+ 1)/2. (12)\\nConsider, for illustration, a mixture of two binomial distributions,\\nY∼η×B(N,π 1) + (1−η)×B(N,π 2), (13)\\nwith three unknown parameters θ= (η,π 1,π2)and assume that the population indeed\\ncontains two diﬀerent groups, i.e. π1/negationslash=π2andη >0. Assuming N= 2obviously violates\\ncondition (12). Lack of identiﬁcation can be veriﬁed directly from the pmf which is diﬀerent\\nfrom zero only for the three outcomes y∈{0,1,2}:\\nP(Y= 0|θ) =η(1−π1)2+ (1−η)(1−π2)2, (14)\\nP(Y= 1|θ) = 2ηπ1(1−π1) + 2(1−η)π2(1−π2),\\nP(Y= 2|θ) =ηπ2\\n1+ (1−η)π2\\n2.\\nSince/summationtext\\nyP(Y=y|θ) = 1, only two linearly independent equations remain to identify the\\nthree parameters (η,π 1,π2). Hence parameters θ= (π1,π2,η)/negationslash=θ⋆= (π⋆\\n1,π⋆\\n2,η⋆)fulﬁlling\\nequations(14)existwhichimplythesamedistributionfor Y,i.e.:P (Y=y|θ) =P(Y=y|θ⋆),\\n∀y= 0,1,2, but are not related to each other by simple relabelling of the component indices.\\nSuch generic non-identiﬁability severely impacts statistical estimation of the mixture\\nparameters θfrom observations y= (y1,...,yn), even ifGis known, and goes far be-\\nyond label switching. Assume, for illustration, that yis the realisation of a random sam-\\nple(Y1,...,Yn)from the two-component binomial mixture (13) with N= 2and true pa-\\nrameterθtrue= (πtrue\\n1,πtrue\\n2,ηtrue)and consider the corresponding observed-data likelihood\\np(y|θ) =/producttextn\\ni=1P(Yi=yi|θ).Genericnon-identiﬁabilityoftheunderlyingmixturedistribution\\nimplies that the observed-data likelihood is the same for any pair θ/negationslash=θ⋆of distinct param-\\neters satisfying (14), for any possible sample yin the sampling space Y={0,1,2}n, i.e.:\\np(y|θ) =p(y|θ⋆),∀y∈Y.Since this holds for arbitrary sample size n= 1,2,..., the true pa-\\nrameterθtruecannot be recovered, even if n→∞, and both maximum likelihood estimation\\nas well Bayesian inference suﬀer from non-identiﬁability problems for such a mixture.\\n24\\nFigure 5: MCMC inference for data simulated from a mixture of two binomial distributions\\nwithN= 2(left-hand side) and N= 5(right-hand side). Top: scatter plot of π1versusπ2\\n(truevaluesindicatedbyacircle).Bottom:posteriordrawsofthegroup-speciﬁcprobabilities\\nπ1andπ2after resolving label switching in the scatter plot of π1versusπ2throughk-means\\nclustering.\\nThisexamplemotivatesthefollowingmoreformaldeﬁnitionofgenericnon-identiﬁability.\\nFor a given θ, any subset U(θ)of the parameter space Θof a mixture model, deﬁned as\\nU(θ) ={θ⋆∈Θ :p(y|θ⋆) =p(y|θ),∀y∈Y},is called a non-identiﬁability set, if it contains\\nat least one point θ⋆which is not related to θby simple relabelling of the component indices.\\nLetθtruebe the true parameter value of a mixture model with Gdistinct parameters (i.e.\\nθg/negationslash=θg/prime, forg/negationslash=g/prime). IfU(θtrue)is a non-identiﬁability set in the sense deﬁned above, then\\nθtruecannot be recovered from data, even as ngoes to inﬁnity.\\nSuchgenericnon-identiﬁabilityhasimportantimplicationsforpracticalmixtureanalysis.\\nFor ﬁniten, the observed-data likelihood function p(y|θ)has a ridge close to U(θtrue)instead\\nofG!isolated modes and no unique maximum, leading to inconsistent estimates of θtrue. In a\\nBayesian framework, this leads to a posterior distribution that does not concentrate around\\nG!isolated, equivalent modes as nincreases, as for identiﬁable models (see Chapter 4, Sec-\\ntion 4.3). Rather, the posterior concentrates over the entire non-identiﬁability set U(θtrue)\\nwhich has a complex geometry and can be represented as the union of G!symmetric sub-\\nspaces, see e.g. Figure 5 for a binomial mixture with G= 2andN= 2. The prior p(θ)\\nprovides information beyond the data and might inﬂuence how the posterior concentrates\\non each of these G!subspacesU(θtrue), in particular, if the prior p(θ)is not constant over\\nU(θ).\\nWhile generic non-identiﬁability has important practical implications for mixture anal-\\nysis, it is rarely as easily diagnosed as for mixtures of binomial distributions and can easily\\ngo unnoticed for more complex mixture models, in particular for maximum likelihood es-\\ntimation, whereas MCMC based Bayesian inference often provides indications of potential\\nidentiﬁability problems, as the following example demonstrates.\\n25\\nMCMC inference for an example: a mixture of binomial distributions\\nFor further illustration, we perform MCMC inference (based on 10,000 draws after a burn-in\\nof 5,000 iterations) for two data sets simulated from a mixture of two binomial distributions\\nwith logitπ1=−1and logitπ2= 1.5using random permutation sampling as explained\\nin Chapter 5, Section 5.2. We assume that G= 2andNis known, whereas all other pa-\\nrameters in mixture (13) are unknown. Bayesian inference is based on the following priors:\\nπg∼U(0,1), and (η1,η2)∼D(1,1). The two data sets were generated with, respectively,\\nN= 2,n= 250andN= 5,n= 100, implying the same total number n×N= 500of\\nexperiments.\\nMCMC inference is summarised in Figure 5, showing scatter plot of π1versusπ2for\\nboth values of N. ForN= 5, the mixture is generically identiﬁed and the posterior draws\\nconcentrate around two symmetric modes, centered at the true values (0.269,0.818)and\\n(0.818,0.269). Non-identiﬁability due to label switching is resolved by applying k-means\\nclustering to the posterior draws, see the lower part of Figure 5 showing identiﬁed posterior\\ndraws of the group-speciﬁc success probabilities π1andπ2.\\nForN= 2, a similar scatter plot of π1versusπ2clearly indicates severe identiﬁability\\nissues, showing that the posterior draws arise from two symmetric unidentiﬁability sets,\\nrather than concentrating around two symmetric modes centered at the true values. When\\nwe applyk-means clustering to resolve label switching, we obtain the posterior draws of the\\nsuccess probabilities π1andπ2shown in the lower part of Figure 5, also indicating problems\\nwith identifying π1andπ2from the data for N= 2.\\n5.2 Identiﬁability for mixtures of regression models\\nConsider a mixture of Gregression models for i= 1,...,noutcomesyi, arising from G\\ndiﬀerent groups,\\nyi|˜xi∼G/summationdisplay\\ng=1ηgφ(y|µi,g(˜xi),σ2\\ng) (15)\\nwhere for each g= 1,...,G, the group-speciﬁc mean µi,g(˜xi) = ˜xiβgdepends on a group-\\nspeciﬁc regression parameter βgand on the (1×(q+1))-dimensional row vector ˜xicontaining\\ntheqcovariatesxiand a constant. For a ﬁxed design point x= ˜xi, (15) is a standard ﬁnite\\nGaussian mixture distribution and as such generically identiﬁed. Hence, if the identity\\nG/summationdisplay\\ng=1ηgφ(y|µi,g(x),σ2\\ng) =G/summationdisplay\\ng=1η⋆\\ngφ(y|µ⋆\\ni,g(x),σ2,⋆\\ng), (16)\\nholds, then the two mixtures are related to each other by relabelling, i.e. µ⋆\\ni,g(x) =\\nµi,σx(g)(x) =xβσx(g),σ2,⋆\\ng=σ2\\nσx(g), andη⋆\\ng=ησx(g)forg= 1,...,G, for some permuta-\\ntionσx∈S(G), where S(G)denotes the set of the G!permutations of{1,...,G}. Note\\nthatσxdepends on the covariate xand that there is no guarantee that σxis identical across\\ndiﬀerent values of xwhich can cause intra-component label switching . One such example is\\ndisplayed on the left-hand side of Figure 6.\\nNevertheless, assume for the moment that σx≡σ⋆is the same for all possible covariates\\nx. Then (16) implies ˜xiβ⋆\\ng= ˜xiβσ⋆(g)for alli= 1,...,nandXβ⋆\\ng=Xβσ⋆(g)where the rows\\nof the matrix Xare equal to ˜x1,..., ˜xn. If the usual condition in regression modelling is\\nsatisﬁed that X/latticetopXhas full rank, then it follows immediately that the regression coeﬃcients\\nare determined up to relabelling: β⋆\\ng=βσ⋆(g).\\n26\\nFigure 6: Data simulated from a mixture of two regression lines under Design 1 (left-hand\\nside) and Design 2 (right-hand side). The full lines indicate the true underlying model used\\nto generate 100 data points (black dots). For the unidentiﬁed Design 1 , a second solution\\nexists which is indicated by the dashed lines.\\nHence, generic identiﬁability for a mixture of regressions model can be veriﬁed through\\nsuﬃcient conditions guaranteeing that σxis indeed identical across all values of x. Mathe-\\nmatically, one such condition is the assumption that either the error variances σ2\\n1,...,σ2\\nGor\\nthe weights η1,...,ηGsatisfy a strict order constraint. However, in practice such constraints\\nare rarely fulﬁlled and forcing an order constraint on one coeﬃcient does not necessarily\\nprevent label switching for the other coeﬃcients in Bayesian posterior sampling, see e.g.\\nFrühwirth-Schnatter (2006, Section 2.4).\\nHence, several papers focused on conditions for generic identiﬁability through the regres-\\nsion part of the model (Hennig, 2000; Grün & Leisch, 2008c,a). Assume that the covariates\\n˜xitakepdiﬀerent values in a design space {x1,...,xp}for the observed outcome yi, for\\ni= 1,...,n. Identiﬁability through the regression part requires enough variability in the de-\\nsign space and is guaranteed under so-called coverage conditions . These conditions require\\nthat the number of clusters Gis exceeded by the minimum number of distinct q-dimensional\\nhyperplanes needed to cover the covariates (excluding the constant). For q= 1, for instance,\\nthe coverage condition is satisﬁed, if the number of design points p(i.e. the number of\\ndistinct values of the univariate covariate) is larger than the number of clusters G. These\\nidentiﬁability conditions go far beyond the usual condition that X/latticetopXhas full rank and are\\noften violated for regression models with too few design points, a common example being\\nregression models with 0/1 dummy variables as covariates which are identiﬁable for G= 1,\\nbut not for G> 1, as the following examples with q= 1demonstrate.\\nFor illustration, we consider the following special case of the mixture of regressions model\\n(15) investigated in Grün & Leisch (2008a, Section 3.1):\\nyi∼0.5φ(y|µi,1(˜xi),0.1) + 0.5φ(y|µi,2(˜xi),0.1), (17)\\nwith covariate vector ˜xi= (1di)and group-speciﬁc regression parameters β1= (2 2)/latticetopand\\nβ2= (1−2)/latticetop. We consider two diﬀerent regression designs, Design 1 wherediis a 0/1\\ndummy variable capturing the eﬀect of gender (with female as baseline) and Design 2 where\\ndicaptures a time eﬀect over 3 periods (with t= 0serving as baseline):\\nDesign 1 :x1=/parenleftBig\\n1 0/parenrightBig\\n, x2=/parenleftBig\\n1 1/parenrightBig\\n,\\nDesign 2 :x1=/parenleftBig\\n1 0/parenrightBig\\n, x2=/parenleftBig\\n1 1/parenrightBig\\n, x3=/parenleftBig\\n1 2/parenrightBig\\n.\\nIn the following, it is veriﬁed that mixture (17) is generically identiﬁed under Design 2\\n(which contains three design points), but generically unidentiﬁed under Design 1 (which\\ncontains only two design points).\\n27\\nWe ﬁrst consider Design 1 . According to (16), µj,1andµj,2are identiﬁed for j= 1and\\nj= 2up to label switching arising from two permutations σ1andσ2, where we may assume\\nwithout loss of generality that σ1is equal to the identity:\\nx1β1=µ1,1, x 1β2=µ1,2, (18)\\nx2β1=µ2,σ2(1), x 2β2=µ2,σ2(2).\\nIfσ2is identical to σ1, then the original values β1andβ2are recovered through:\\nβ1=X−1\\n1,2/parenleftBigg\\nµ1,1\\nµ2,1/parenrightBigg\\n=/parenleftBigg\\n2\\n2/parenrightBigg\\n, β 2=X−1\\n1,2/parenleftBigg\\nµ1,2\\nµ2,2/parenrightBigg\\n=/parenleftBigg\\n1\\n−2/parenrightBigg\\n, (19)\\nsince the design matrix\\nX1,2=/parenleftBigg\\nx1\\nx2/parenrightBigg\\n=/parenleftBigg\\n1 0\\n1 1/parenrightBigg\\nis invertible. However, as mentioned above, σ2need not be identical to σ1, in which case\\nσ2(1) = 2,σ2(2) = 1, and a second solution emerges:\\nβ⋆\\n1=X−1\\n1,2/parenleftBigg\\nµ1,1\\nµ2,2/parenrightBigg\\n=/parenleftBigg\\n2\\n−3/parenrightBigg\\n, β⋆\\n2=X−1\\n1,2/parenleftBigg\\nµ1,2\\nµ2,1/parenrightBigg\\n=/parenleftBigg\\n1\\n3/parenrightBigg\\n.\\nEvidently, the group-speciﬁc slopes of this second solution are diﬀerent from the original\\nones and the un-identiﬁability set U(θtrue)contains two points. The two possible solutions\\nare depicted in the left-hand side of Figure 6 which also shows a balanced sample of n= 100\\nobservations simulated from mixture (17) under Design 1 .\\nForDesign 2 , the ﬁrst two design points are as before and a third point is added with\\nµ3,1andµ3,2being identiﬁed up to label switching according to a permutation σ3:\\nx3β1=µ3,σ3(1), x 3β2=µ3,σ3(2). (20)\\nAs only two diﬀerent permutations exist for G= 2, at least two of the three permutations\\nσ1,σ2, andσ3in (18) and (20) have to be identical (assuming again without loss of gener-\\nality thatσ1is equal to the identity). Assume, for example, that σ1=σ2. Then the true\\nparameters β1andβ2are recovered from (µj,1,µj,2),j= 1,2,as in (19) and can be used to\\nuniquely predict µ3,1=x3β1andµ3,2=x3β2in both groups. Comparing these predictions\\nwith (20), it is clear that σ3(1) = 1andσ3(2) = 2, henceσ1=σ2=σ3. A similar proof can\\nbe performed for any pair of identical permutations σj=σl,j/negationslash=l, as long as the matrix\\nX/latticetop\\nj,l= (x/latticetop\\njx/latticetop\\nl)is invertible and generic identiﬁability of Design 2 follows.\\nThe only possible solution under Design 2 is depicted in the right-hand side of Figure 6\\nwhich also shows a balanced sample of n= 100observations simulated from mixture (17)\\nunder this design.\\nMCMC inference for an example: a mixture of regressions model\\nFor further illustration, we perform MCMC inference (based on 10,000 draws after a burn-in\\nof 5,000 iterations) for both data sets shown in Figure 6 using random permutation sam-\\npling as explained in Chapter 5, Section 5.2. We assume that G= 2is known, whereas\\nall other parameters in mixture (15) are unknown. Bayesian inference is based on the pri-\\nors(η1,η2)∼D (4,4)andβg∼N (0,100×I),σ2\\ng∼IG (2.5,1.25s2\\ny)forg= 1,2, where\\n28\\nFigure 7: MCMC inference for data simulated from a mixture of two regression models\\nunder the generically un-identiﬁed Design 1 (left-hand side) and under the generically iden-\\ntiﬁedDesign 2 (right-hand side). Top: scatter plot of the group-speciﬁc slopes β1,2versus\\nβ2,2. Bottom: posterior draws of the group-speciﬁc slopes β1,2andβ2,2after resolving label\\nswitching through k-means clustering in the posterior draws.\\ns2\\nyis the data variance of (y1,...,yn). The upper part of Figure 7 shows scatter plots of\\nthe group-speciﬁc slopes β1,2versusβ2,2for both designs which are symmetric due to label\\nswitching.\\nAs expected from Chapter 4, Section 4.3, the posterior draws shown in the upper right-\\nhand part for the generically identiﬁed Design 2 concentrate around two symmetric modes\\ncorresponding to the true values (2,−2)and(−2,2). Label switching is easily resolved by\\napplyingk-means clustering to the posterior draws, see the lower right-hand part of Figure 7\\nshowing identiﬁed posterior draws of the group-speciﬁc slopes β1,2andβ2,2for this design.\\nForDesign 1 , a similar scatter plot of β1,2versusβ2,2in the upper left-hand part clearly\\nindicates severe identiﬁability issues. The posterior draws concentrate around four rather\\nthan two modes, with two of them being the symmetric modes corresponding to the true\\nvalues (2,−2)and(−2,2). The other two symmetric modes correspond to the second solu-\\ntion(−3,3)and(3,−3), resulting from generic non-identiﬁability. When we apply k-means\\nclustering to these posterior draws to resolve label switching, we obtain the posterior draws\\nof the group-speciﬁc slopes β1,2andβ2,2in the lower left-hand part of Figure 7, showing\\nintra-component label switching and indicating identiﬁability problems for this design. Since\\nβg,2switches sign between the two solutions in both groups, it is not possible to recover that\\n“gender” has a strong positive eﬀect on the outcome in one group and a strong negative\\neﬀect in the other group.\\n5.3 Identiﬁability for mixtures of experts models\\nHennig (2000) considers mixtures of regression models where the component sizes can arbi-\\ntrarily depend on covariates and establishes identiﬁability results in the case that the joint\\nobservations of covariates and dependent variable are assumed to be iid and gives suﬃcient\\n29\\nidentiﬁability conditions for this model. For such a model, the covariates are not assumed\\nﬁxed or to occur for a ﬁxed design, but random with a speciﬁc distribution. As opposed\\nto this, the ME model is deﬁned conditional on the covariates without speciﬁc assumptions\\nconcerning their distribution. We will discuss identiﬁcation for this case.\\nConsider, as a ﬁrst example, a simple mixture of experts model of Gunivariate Gaussian\\ndistributions for i= 1,...,noutcomesyiarising from Gdiﬀerent groups,\\nyi|˜xi∼G/summationdisplay\\ng=1ηg(˜xi)φ(y|µg,σ2\\ng) (21)\\nwhere the group weights ηgdepend on a covariate ˜xiwith group-speciﬁc regression param-\\neters, i.e.:\\nlog/bracketleftBiggηg(˜xi)\\nηg0(˜xi)/bracketrightBigg\\n= ˜xiγg, g = 1,...,G, (22)\\nwith baseline g0, whereγg0= 0. Assume that the component densities diﬀer, i.e. θg/negationslash=θ/prime\\ng,\\nforg/negationslash=g/prime, whereθg= (µg,σ2\\ng).\\nFor each ﬁxed design point x= ˜xi, (21) is a standard ﬁnite Gaussian mixture and\\ntherefore generically identiﬁed. Therefore, if the identity\\nG/summationdisplay\\ng=1ηg(x)φ(y|µg,σ2\\ng) =G/summationdisplay\\ng=1η⋆\\ng(x)φ(y|µ⋆\\ng,σ2,⋆\\ng), (23)\\nholds, then the two mixtures are related to each other by relabelling, i.e. µ⋆\\ng=µσx(g),σ2,⋆\\ng=\\nσ2\\nσx(g), andη⋆\\ng(x) =ησx(g)(x)forg= 1,...,G, for some permutation σx∈S(G). As opposed\\nto mixtures of regression models, one can show that σx≡σ⋆for all covariate values x.\\nAssume that σxi/negationslash=σxjfor two covariates ˜xi/negationslash= ˜xjand assume, without loss of generality,\\nthatσxiis equal to the identity. Consider ﬁrst the case of G= 2. Then (23) implies for\\nx= ˜xi:\\nθ⋆\\n1=/parenleftBigg\\nµ⋆\\n1\\nσ2,⋆\\n1/parenrightBigg\\n=θ1, θ⋆\\n2=/parenleftBigg\\nµ⋆\\n2\\nσ2,⋆\\n2/parenrightBigg\\n=θ2,\\nwhereas for x= ˜xj:\\nθ⋆\\n1=/parenleftBigg\\nµ⋆\\n2\\nσ2,⋆\\n2/parenrightBigg\\n=θ2, θ⋆\\n2=/parenleftBigg\\nµ⋆\\n1\\nσ2,⋆\\n1/parenrightBigg\\n=θ1,\\ncontradicting the assumptions that θ1/negationslash=θ2. A similar proof is possible for G> 2, where the\\nassumption σxi/negationslash=σxj(assuming again that σ1is equal to the identity) implies for x= ˜xi\\nthatθ⋆\\ng=θgfor all components g= 1,...,G, whereas for x= ˜xjat least one component\\ngiexists with θ⋆\\ngi=θgj, wheregj=σxj(gi)/negationslash=gi. Hence,θgi=θgj, which contradicts the\\nassumptions that θgi/negationslash=θgj. This implies that σx≡σ⋆for all covariate values x.\\nTherefore, the weight distribution η1(x),...,ηG(x)is identiﬁed up to relabelling the com-\\nponents and identiﬁcation depends on whether γgcan be recovered from the correspond-\\ning MNL model (22) given the design matrix X, constructed row-wise from the covariates\\n˜xi,i= 1,...,n. Standard conditions for identiﬁcation in a MNL model apply, e.g. that\\n(X/latticetopX)−1exists (McCullagh & Nelder, 1999). It is well-known that identiﬁcation in a logit\\nand more generally in a MNL model fails under complete separation, see e.g. Heinze (2006).\\nHence, a situation where a mixture of experts model is not generically identiﬁed occurs, if\\n30\\ncertain clusters do not share covariate values with other clusters, see Example 4.2 in Hennig\\n(2000) for illustration. A rather strong condition ensuring generic identiﬁability for this type\\nof models is an extended coverage condition (Hennig, 2000) requiring that the number of\\nclustersGis exceeded by the minimum number of distinct q-dimensional hyperplanes needed\\nto cover the covariate values (excluding the constant) for eachcluster.\\nSimilar arguments as above apply in general for simple mixtures of experts models of G\\nprobability distributions,\\nyi∼G/summationdisplay\\ng=1ηg(˜xi)p(yi|θg). (24)\\nProvided that the parameters in the MNL model (22) are identiﬁed, it can be shown that\\na mixture of experts model is generically identiﬁed, if the corresponding standard ﬁnite\\nmixture distribution is generically identiﬁed. In this case, any other mixture representation\\n(24) with parameters θ⋆\\ngandη⋆\\ng(˜xi)is identiﬁed up to (the same) label switching according to\\na permutation σfor all possible values ˜xi:θ⋆\\ng=θσ(g)andη⋆\\ng(˜xi) =ησ(g)(˜xi)forg= 1,...,G.\\nIt follows that mixtures of experts of multivariate Gaussian distributions (as considered\\nin Section 2.2) and Poisson distributions, among many others, are generically identiﬁed,\\nprovided that parameters in the MNL model (22) are identiﬁed. Since a standard ﬁnite\\nmixture model is that special case of a mixture of experts model where ˜xi≡1is equal to\\nthe intercept, special care must be exercised when the underlying standard ﬁnite mixture\\ndistribution is generically unidentiﬁed, as might be the case when modelling discrete data. It\\nis interesting to note that including ˜xiinto the weight function ηg(˜xi)in mixtures of experts\\nmodels is possible for models where including ˜xiin the component density p(yi|˜xi,θg)yields\\na generically non-identiﬁed model, an example being the regressor ˜xi= (1di), wherediis a\\n0/1 dummy variable, see Section 5.2.\\nThe situation gets rather complex, when covariates ˜xi(or subsets of these) are included\\nasregressorsbothintheoutcomedistribution p(yi|˜xi,θg)aswellasintheweightdistribution\\nηg(˜xi). The presence of a covariate ˜xiinηg(˜xi)could introduce high discriminative power\\namong the groups and might lead to identiﬁcation of mixture of regression models which are\\nnot identiﬁed, if ηgis assumed to be independent of the covariates. To our knowledge, generic\\nidentiﬁcation for general mixtures of experts models has not been studied systematically and\\nwould be an interesting venue for future research.\\nAs it is, the only way to investigate, if the chosen mixture model suﬀers from identiﬁabil-\\nityproblems is toanalyze theresults obtainedfrom ﬁttingthese models tothe datacarefully.\\nAs the examples in Section 5.1 and 5.2 have shown, weird behaviour of the MCMC draws\\nin a Bayesian framework are often a sign of identiﬁability problems. On the other hand,\\nmarginal posterior concentration around pronounced modes, veriﬁed for instance through\\nappropriate scatter plots of MCMC draws for the parameters of interest, indicates that\\nidentiﬁcation might not be an issue for that speciﬁc application.\\n6 Concluding Remarks\\nThis chapter has outlined the deﬁnition, estimation and application of ME models in a num-\\nber of settings clearly demonstrating their utility as an analytical tool. Their demonstrated\\nuse to cluster observations, and to appropriately capture heterogeneity in cross sectional\\ndata, provides only a glimpse of their potential ﬂexibility and utility in a wide range of\\nsettings. The ability of ME models to jointly model response and concomitant variables pro-\\nvides deeper and more principled insight into the relations between such data in a mixture\\nmodel based analysis.\\n31\\nOn a cautionary note however, when an ME model is employed as an analytic tool,\\ncare must be exercised in how and where covariates enter the ME model framework. The\\ninterpretation of the analysis fundamentally depends on which of the suite of ME models is\\ninvoked. Further, as outlined herein, the identiﬁability of an ME model must be carefully\\nconsidered;establishingidentiﬁabilityforMEmodelsisanoutstanding,challengingproblem.\\nReferences\\nAkaike, Hirotogu. 1973. Information theory and an extension of the maximum likelihood\\nprinciple. Pages 267–281 of: Petrov, B. N., & Csáki, F. (eds), 2nd International Sympo-\\nsium Symp. Information Theory . Budapest: Akadémiai Kiadó.\\nBenaglia, T., Chauveau, D., Hunter, D.R., & Young, D. 2009. mixtools: An R Package for\\nAnalyzing Finite Mixture Models. Journal of Statistical Software ,32(6), 1–29.\\nBenter, W. 1994. Computer-based Horse Race Handicapping and Wagering Systems: A\\nReport.Pages 183–198 of: Ziemba, William T., Lo, Victor S., & Haush, Donald B. (eds),\\nEﬃciency of Racetrack Betting Markets . San Diego and London: Academic Press.\\nBishop, C. M., & Svenskn, M. 2003. Bayesian Hierarchical Mixtures of Experts. Pages 57–\\n64 of: Proceedings of the Nineteenth Conference on Uncertainty in Artiﬁcial Intelligence .\\nUAI’03. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\\nBishop, C.M. 2006. Pattern Recognition and Machine Learning . New York: Springer.\\nChamroukhi, F. 2015. Non-Normal Mixtures of Experts. ArXiv preprints 1506.06707 , June.\\nChandra, Satish. 1977. On the Mixtures of Probability Distributions. Scandinavian Journal\\nof Statistics ,4, 105–112.\\nChib, S., & Greenberg, E. 1995. Understanding the Metropolis-Hastings Algorithm. The\\nAmerican Statistician ,49, 327–335.\\nCoakley, J., & Gallagher, M. 2004. Politics in the Republic of Ireland . 4th edn. London:\\nRoutledge in association with PSAI Press.\\nDayton, C. M., & Macready, G. B. 1988. Concomitant-variable latent-class models. Journal\\nof the American Statistical Association ,83(401), 173–178.\\nDempster, A. P., Laird, N. M., & Rubin, D. B. 1977. Maximum likelihood from incomplete\\ndata via the EM algorithm (with discussion). Journal of the Royal Statistical Society\\nSeries B,39, 1–38.\\nDeSarbo, W.S., & Cron, W.L. 1988. A maximum likelihood methodology for clusterwise\\nlinear regression. Journal of Classiﬁcation ,5, 248–282.\\nDiebolt, J., & Robert, Christian P. 1994. Estimation of Finite Mixture Distributions by\\nBayesian Sampling. Journal of the Royal Statistical Society Series B ,56, 363–375.\\nFrühwirth-Schnatter, S. 2004. Estimating marginal likelihoods for mixture and Markov\\nswitching models using bridge sampling techniques. The Econometrics Journal ,7(1),\\n143–167.\\n32\\nFrühwirth-Schnatter, S. 2011a. Dealing with label switching under model uncertainty. Chap.\\n10, pages 213–239 of: Mengersen, K., Robert, C. P., & Titterington, D. (eds), Mixture\\nEstimation and Applications . Chichester: Wiley.\\nFrühwirth-Schnatter, S. 2011b. Panel Data Analysis - A Survey on Model-Based Clustering\\nof Time Series. Advances in Data Analysis and Classiﬁcation ,5, 251–280.\\nFrühwirth-Schnatter, S., & Frühwirth, R. 2010. Data augmentation and MCMC for bi-\\nnary and multinomial logit models. Pages 111–132 of: Kneib, Thomas, & Tutz, Gerhard\\n(eds),Statistical Modelling and Regression Structures – Festschrift in Honour of Ludwig\\nFahrmeir . Heidelberg: Physica-Verlag.\\nFrühwirth-Schnatter, S., & Kaufmann, Sylvia. 2008. Model-based clustering of multiple time\\nseries.Journal of Business &Economic Statistics ,26, 78–89.\\nFrühwirth-Schnatter, S., & Wagner, Helga. 2008. Marginal Likelihoods for Non-Gaussian\\nModels Using Auxiliary Mixture Sampling. Computational Statistics and Data Analysis ,\\n52, 4608–4624.\\nFrühwirth-Schnatter, S., Pamminger, C., Weber, A., & Winter-Ebmer, R. 2012. Labor\\nmarketentryandearningsdynamics:Bayesianinferenceusingmixtures-of-expertsMarkov\\nchain clustering. Journal of Applied Econometrics ,27(7), 1116–1137.\\nFrühwirth-Schnatter, Sylvia. 2006. Finite Mixture and Markov Switching Models . New York:\\nSpringer-Verlag.\\nFrühwirth-Schnatter, Sylvia. 2018. Applied Bayesian Mixture Modelling. Implementations\\nin MATLAB using the package bayesf Version 4.0 .\\nGeman, S., & Geman, D. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian\\nrestoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence ,\\n6, 721–741.\\nGershenfeld, Neil. 1997. Nonlinear Inference and Cluster-Weighted Modeling. Annals of the\\nNew York Academy of Sciences ,808(1), 18–24.\\nGeweke, J., & Keane, M. 2007. Smoothly mixing regressions. Journal of Econometrics ,\\n136(1), 252–290.\\nGormley, I. C., & Murphy, T. B. 2006. Analysis of Irish third-level college applications data.\\nJournal of the Royal Statistical Society Series A ,169(2), 361–379.\\nGormley, I. C., & Murphy, T. B. 2008a. Exploring voting blocs within the Irish electorate:\\nA mixture modeling approach. Journal of the American Statistical Association ,103(483),\\n1014–1027.\\nGormley, I. C., & Murphy, T. B. 2008b. A mixture of experts model for rank data with\\napplications in election studies. The Annals of Applied Statistics ,2(4), 1452–1477.\\nGormley, I. C., & Murphy, T. B. 2010a. Clustering ranked preference data using sociode-\\nmographic covariates. Pages 543–569 of: Hess, S., & Daly, A. (eds), Choice Modelling:\\nThe State-of-the-Art and the State-of-Practice . United Kingdom: Emerald.\\n33\\nGormley, I. C., & Murphy, T. B. 2010b. A Mixture of Experts Latent Position Cluster\\nModel for Social Network Data. Statistical Methodology ,7(3), 385–405.\\nGormley, I. C., & Murphy, T. B. 2018. MEclustnet: ﬁtting the mixture of experts latent\\nposition cluster model. R package version 1.0.\\nGrün, B., & Leisch, F. 2008a. Finite Mixtures of Generalized Linear Regression Models.\\nPages 205–230 of: Shalabh, & Heumann, Christian (eds), Recent Advances in Linear\\nModels and Related Areas . Springer.\\nGrün,B.,&Leisch,F.2008b. FlexMixVersion2:Finitemixtureswithconcomitantvariables\\nand varying and constant parameters. Journal of Statistical Software ,28, 1–35.\\nGrün, B., & Leisch, F. 2008c. Identiﬁability of Finite Mixtures of Multinomial Logit Models\\nwith Varying and Fixed Eﬀects. Journal of Classiﬁcation ,25, 225–247.\\nHandcock, M., Raftery, A.E., & Tantrum, J. M. 2007. Model-based clustering for social\\nnetworks. Journal of the Royal Statistical Society Series A ,170(2), 301 – 354.\\nHeinze, Georg. 2006. A comparative investigation of methods for logistic regression with\\nseparated or nearly separated data. Statistics in Medicine ,25, 4216–4226.\\nHennig, C. 2000. Identiﬁability of models for clusterwise linear regression. Journal of\\nClassiﬁcation ,17, 273–296.\\nHoﬀ, P. D., Raftery, A. E., & Handcock, M. S. 2002. Latent Space Approaches to Social\\nNetwork Analysis. Journal of the American Statistical Association ,97, 1090–1098.\\nHoﬀ, P.D. 2009. A First Course in Bayesian Statistical Methods . Springer-Verlag, New\\nYork.\\nHuerta,G.,Jiang,W.,&Tanner,M.A.2003. Timeseriesmodelingviahierarchicalmixtures.\\nStatistica Sinica ,13(4), 1097–1118.\\nHunter, D. R., & Lange, K. 2004. A tutorial on MM algorithms. The American Statistician ,\\n58(1), 30–37.\\nHunter, David R, & Young, Derek S. 2012. Semiparametric mixtures of regressions. Journal\\nof Nonparametric Statistics ,24(1), 19–38.\\nHurn, M., Justel, A., & Robert, C.P. 2003. Estimating mixtures of regressions. Journal of\\nCompututional and Graphical Statistics ,12, 1–25.\\nIngrassia, Salvatore, Punzo, Antonio, Vittadini, Giorgio, & Minotti, Simona C. 2015. The\\ngeneralized linear mixed cluster-weighted model. Journal of Classiﬁcation ,32(1), 85–113.\\nJacobs, R.A., Jordan, M.I., Nowlan, S.J., & Hinton, G.E. 1991. Adaptive mixtures of local\\nexperts. Neural Computation ,3, 79–87.\\nJordan, M.I., & Jacobs, R.A. 1994. Hierarchical mixtures of experts and the EM algorithm.\\nNeural Computation ,6, 181–214.\\nKass, R.E., & Raftery, A.E. 1995. Bayes factors. Journal of the American Statistical Asso-\\nciation,90, 773–795.\\n34\\nLang, J. B., McDonald, J. W., & Smith, P. W. F. 1999. Association-marginal modelling\\nof multivariate categorical responses: A maximim likelihood approach. Journal of the\\nAmerican Statistical Association ,94, 1161–71.\\nLazega, E. 2001. The Collegial Phenomenon: The Social Mechanisms of Cooperation Among\\nPeers in a Corporate Law Partnership . Oxford University Press.\\nLi, F., Villani, M., & Kohn, R. 2011. Modeling conditional densities using ﬁnite smooth\\nmixtures. Chap. 6, pages 123–144 of: Mengersen, K., Robert, C., & Titterington, M.\\n(eds),Mixtures: Estimation and Applications . Wiley.\\nMarsh, M. 1999. The Making of the Eighth President. Pages 215–242 of: Marsh, Michael,\\n& Mitchell, Paul (eds), How Ireland Voted 1997 . Boulder, CO: Westview and PSAI Press.\\nMasoudnia, S., & Ebrahimpour, R. 2014. Mixture of experts: a literature survey. Artiﬁcial\\nIntelligence Review ,42(2), 275–293.\\nMazza, A., Punzo, A., & Ingrassia, S. 2017. ﬂexCWM: Flexible Cluster-Weighted Modeling.\\nR package version 1.7.\\nMcCullagh, P., & Nelder, John A. 1999. Generalized Linear Models . London: Chapman &\\nHall.\\nMcLachlan, G, & Peel, D. 2000. Finite Mixture Models . New York: John Wiley.\\nMeng, X.-L., & Rubin, D. B. 1993. Maximum likelihood estimation via the ECM algorithm:\\nA general framework. Biometrika ,80(2), 267–278.\\nMetropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H., & Teller, E. 1953. Equa-\\ntions of state calculations by fast computing machines. The Journal of Chemical Physics ,\\n21, 1087–1092.\\nMuthén, L. K., & Muthén, B. O. 2011. Mplus User’s Guide . 6 edn. Los Angeles, CA: Muthén\\nand Muthén.\\nPamminger, Christoph, & Frühwirth-Schnatter, Sylvia. 2010. Model-based Clustering of\\nCategorical Time Series. Bayesian Analysis ,5, 345–368.\\nPeng, F., Jacobs, R. A., & Tanner, M. A. 1996. Bayesian inference in mixtures-of-experts\\nand hierarchical mixtures-of-experts models with an application to speech recognition.\\nJournal of the American Statistical Association ,91(435), 953–960.\\nPlackett, R. L. 1975. The analysis of permutations. Applied Statistics ,24(2), 193–202.\\nQuandt, R.E. 1972. A new approach to estimating switching regressions. Journal of the\\nAmerican Statistical Association ,67, 306–310.\\nR Core Team. 2018. R: A Language and Environment for Statistical Computing . R Foun-\\ndation for Statistical Computing, Vienna, Austria.\\nRaftery, A.E., Newton, M.A., Satagopan, J.M., & Krivitsky, P. 2007. Estimating the in-\\ntegrated likelihood via posterior simulation using the harmonic mean identity (with dis-\\ncussion). Pages 371–416 of: Bernardo, J.M., Bayarri, M.J., Berger, J.O., Dawid, A.P.,\\nHeckerman, D., Smith, A.F.M., , & West, M. (eds), Bayesian Statistics 8 . Oxford Uni-\\nversity Press.\\n35\\nRasmussen, C. E., & Ghahramani, Z. 2002. The inﬁnite mixtures of Gaussian process\\nexperts. Pages 554–560 of: Advances in Neural Information Processing Systems , vol. 12.\\nMIT Press.\\nSchwarz, G. 1978. Estimating the dimension of a model. Annals of Statistics ,6, 461–464.\\nScott, Steven L. 2011. Data augmentation, frequentist estimation, and the Bayesian analysis\\nof multinomial logit models. Statistical Papers ,52, 87–109.\\nSinnott, R. 1995. Irish voters decide: Voting behaviour in elections and referendums since\\n1918. Manchester: Manchester University Press.\\nSinnott, R. 1999. The Electoral System. Pages 99–126 of: Coakley, John, & Gallagher,\\nMichael (eds), Politics in the Republic of Ireland , 3rd edn. London: Routledge & PSAI\\nPress.\\nStephens, M. 2000. Bayesian analysis of mixture models with an unknown number of\\ncomponents—an alternative to reversible jump methods. Annals of Statistics ,28, 40–\\n74.\\nSubedi, Sanjeena, Punzo, Antonio, Ingrassia, Salvatore, & McNicholas, Paul D. 2013. Clus-\\ntering and classiﬁcation via cluster-weighted factor analyzers. Advances in Data Analysis\\nand Classiﬁcation ,7(1), 5–40.\\nTang, X., & Qu, A. 2015. Mixture Modeling for Longitudinal Data. Journal of Computa-\\ntional and Graphical Statistics ,25, 1117–1137.\\nTanner, M. A. 1996. Tools for Statistical Inference: Observed Data and Data Augmentation\\nMethods. 3 edn. New York: Springer-Verlag.\\nTeicher, Henry. 1961. Identiﬁability of mixtures. The Annals of Mathematical Statistics ,32,\\n244–248.\\nTeicher, Henry. 1963. Identiﬁability of ﬁnite mixtures. The Annals of Mathematical Statis-\\ntics,34, 1265–1269.\\nTurner, Rolf. 2014. mixreg: functions to ﬁt mixtures of regressions. R package version 0.0-5.\\nVermunt, Jeroen K, & Magidson, Jay. 2005. Latent GOLD 4.0 User’s Guide . Statistical\\nInnovations Inc.\\nVillani, Mattias, Kohn, Robert, & Giordani, Paolo. 2009. Regression density Estimation\\nusing smooth adaptive Gaussian mixtures. Journal of Econometrics ,153, 155–173.\\nWang, P., Puterman, M.L., Cockburn, I., & Le, N. 1996. Mixed Poisson regression models\\nwith covariate dependent rates. Biometrics ,52, 381–400.\\nWaterhouse, S., MacKay, D., & Robinson, T. 1996. Bayesian methods for mixtures of\\nexperts. Pages 351–357 of: Advances in Neural Information Processing Systems . Morgan\\nKaufmann Publishers.\\nWhite, A., & Murphy, T. B. 2016. Mixed-Membership of Experts Stochastic Blockmodel.\\nNetwork Science ,4(Apr.), 48–80.\\n36\\nYakowitz, S. J., & Spragins, J. D. 1968. On the Identiﬁability of ﬁnite mixtures. The Annals\\nof Mathematical Statistics ,39, 209–214.\\nYoung, D. S., & Hunter, D. R. 2010. Mixtures of regressions with predictor-dependent\\nmixing proportions. Computational Statistics & Data Analysis ,54(10), 2253–2266.\\n37',\n",
       "  'ref': 'Akaike, Hirotogu. 1973. Information theory and an extension of the maximum likelihood\\nprinciple. Pages 267–281 of: Petrov, B. N., & Csáki, F. (eds), 2nd International Sympo-\\nsium Symp. Information Theory . Budapest: Akadémiai Kiadó.\\nBenaglia, T., Chauveau, D., Hunter, D.R., & Young, D. 2009. mixtools: An R Package for\\nAnalyzing Finite Mixture Models. Journal of Statistical Software ,32(6), 1–29.\\nBenter, W. 1994. Computer-based Horse Race Handicapping and Wagering Systems: A\\nReport.Pages 183–198 of: Ziemba, William T., Lo, Victor S., & Haush, Donald B. (eds),\\nEﬃciency of Racetrack Betting Markets . San Diego and London: Academic Press.\\nBishop, C. M., & Svenskn, M. 2003. Bayesian Hierarchical Mixtures of Experts. Pages 57–\\n64 of: Proceedings of the Nineteenth Conference on Uncertainty in Artiﬁcial Intelligence .\\nUAI’03. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\\nBishop, C.M. 2006. Pattern Recognition and Machine Learning . New York: Springer.\\nChamroukhi, F. 2015. Non-Normal Mixtures of Experts. ArXiv preprints 1506.06707 , June.\\nChandra, Satish. 1977. On the Mixtures of Probability Distributions. Scandinavian Journal\\nof Statistics ,4, 105–112.\\nChib, S., & Greenberg, E. 1995. Understanding the Metropolis-Hastings Algorithm. The\\nAmerican Statistician ,49, 327–335.\\nCoakley, J., & Gallagher, M. 2004. Politics in the Republic of Ireland . 4th edn. London:\\nRoutledge in association with PSAI Press.\\nDayton, C. M., & Macready, G. B. 1988. Concomitant-variable latent-class models. Journal\\nof the American Statistical Association ,83(401), 173–178.\\nDempster, A. P., Laird, N. M., & Rubin, D. B. 1977. Maximum likelihood from incomplete\\ndata via the EM algorithm (with discussion). Journal of the Royal Statistical Society\\nSeries B,39, 1–38.\\nDeSarbo, W.S., & Cron, W.L. 1988. A maximum likelihood methodology for clusterwise\\nlinear regression. Journal of Classiﬁcation ,5, 248–282.\\nDiebolt, J., & Robert, Christian P. 1994. Estimation of Finite Mixture Distributions by\\nBayesian Sampling. Journal of the Royal Statistical Society Series B ,56, 363–375.\\nFrühwirth-Schnatter, S. 2004. Estimating marginal likelihoods for mixture and Markov\\nswitching models using bridge sampling techniques. The Econometrics Journal ,7(1),\\n143–167.\\n32\\nFrühwirth-Schnatter, S. 2011a. Dealing with label switching under model uncertainty. Chap.\\n10, pages 213–239 of: Mengersen, K., Robert, C. P., & Titterington, D. (eds), Mixture\\nEstimation and Applications . Chichester: Wiley.\\nFrühwirth-Schnatter, S. 2011b. Panel Data Analysis - A Survey on Model-Based Clustering\\nof Time Series. Advances in Data Analysis and Classiﬁcation ,5, 251–280.\\nFrühwirth-Schnatter, S., & Frühwirth, R. 2010. Data augmentation and MCMC for bi-\\nnary and multinomial logit models. Pages 111–132 of: Kneib, Thomas, & Tutz, Gerhard\\n(eds),Statistical Modelling and Regression Structures – Festschrift in Honour of Ludwig\\nFahrmeir . Heidelberg: Physica-Verlag.\\nFrühwirth-Schnatter, S., & Kaufmann, Sylvia. 2008. Model-based clustering of multiple time\\nseries.Journal of Business &Economic Statistics ,26, 78–89.\\nFrühwirth-Schnatter, S., & Wagner, Helga. 2008. Marginal Likelihoods for Non-Gaussian\\nModels Using Auxiliary Mixture Sampling. Computational Statistics and Data Analysis ,\\n52, 4608–4624.\\nFrühwirth-Schnatter, S., Pamminger, C., Weber, A., & Winter-Ebmer, R. 2012. Labor\\nmarketentryandearningsdynamics:Bayesianinferenceusingmixtures-of-expertsMarkov\\nchain clustering. Journal of Applied Econometrics ,27(7), 1116–1137.\\nFrühwirth-Schnatter, Sylvia. 2006. Finite Mixture and Markov Switching Models . New York:\\nSpringer-Verlag.\\nFrühwirth-Schnatter, Sylvia. 2018. Applied Bayesian Mixture Modelling. Implementations\\nin MATLAB using the package bayesf Version 4.0 .\\nGeman, S., & Geman, D. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian\\nrestoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence ,\\n6, 721–741.\\nGershenfeld, Neil. 1997. Nonlinear Inference and Cluster-Weighted Modeling. Annals of the\\nNew York Academy of Sciences ,808(1), 18–24.\\nGeweke, J., & Keane, M. 2007. Smoothly mixing regressions. Journal of Econometrics ,\\n136(1), 252–290.\\nGormley, I. C., & Murphy, T. B. 2006. Analysis of Irish third-level college applications data.\\nJournal of the Royal Statistical Society Series A ,169(2), 361–379.\\nGormley, I. C., & Murphy, T. B. 2008a. Exploring voting blocs within the Irish electorate:\\nA mixture modeling approach. Journal of the American Statistical Association ,103(483),\\n1014–1027.\\nGormley, I. C., & Murphy, T. B. 2008b. A mixture of experts model for rank data with\\napplications in election studies. The Annals of Applied Statistics ,2(4), 1452–1477.\\nGormley, I. C., & Murphy, T. B. 2010a. Clustering ranked preference data using sociode-\\nmographic covariates. Pages 543–569 of: Hess, S., & Daly, A. (eds), Choice Modelling:\\nThe State-of-the-Art and the State-of-Practice . United Kingdom: Emerald.\\n33\\nGormley, I. C., & Murphy, T. B. 2010b. A Mixture of Experts Latent Position Cluster\\nModel for Social Network Data. Statistical Methodology ,7(3), 385–405.\\nGormley, I. C., & Murphy, T. B. 2018. MEclustnet: ﬁtting the mixture of experts latent\\nposition cluster model. R package version 1.0.\\nGrün, B., & Leisch, F. 2008a. Finite Mixtures of Generalized Linear Regression Models.\\nPages 205–230 of: Shalabh, & Heumann, Christian (eds), Recent Advances in Linear\\nModels and Related Areas . Springer.\\nGrün,B.,&Leisch,F.2008b. FlexMixVersion2:Finitemixtureswithconcomitantvariables\\nand varying and constant parameters. Journal of Statistical Software ,28, 1–35.\\nGrün, B., & Leisch, F. 2008c. Identiﬁability of Finite Mixtures of Multinomial Logit Models\\nwith Varying and Fixed Eﬀects. Journal of Classiﬁcation ,25, 225–247.\\nHandcock, M., Raftery, A.E., & Tantrum, J. M. 2007. Model-based clustering for social\\nnetworks. Journal of the Royal Statistical Society Series A ,170(2), 301 – 354.\\nHeinze, Georg. 2006. A comparative investigation of methods for logistic regression with\\nseparated or nearly separated data. Statistics in Medicine ,25, 4216–4226.\\nHennig, C. 2000. Identiﬁability of models for clusterwise linear regression. Journal of\\nClassiﬁcation ,17, 273–296.\\nHoﬀ, P. D., Raftery, A. E., & Handcock, M. S. 2002. Latent Space Approaches to Social\\nNetwork Analysis. Journal of the American Statistical Association ,97, 1090–1098.\\nHoﬀ, P.D. 2009. A First Course in Bayesian Statistical Methods . Springer-Verlag, New\\nYork.\\nHuerta,G.,Jiang,W.,&Tanner,M.A.2003. Timeseriesmodelingviahierarchicalmixtures.\\nStatistica Sinica ,13(4), 1097–1118.\\nHunter, D. R., & Lange, K. 2004. A tutorial on MM algorithms. The American Statistician ,\\n58(1), 30–37.\\nHunter, David R, & Young, Derek S. 2012. Semiparametric mixtures of regressions. Journal\\nof Nonparametric Statistics ,24(1), 19–38.\\nHurn, M., Justel, A., & Robert, C.P. 2003. Estimating mixtures of regressions. Journal of\\nCompututional and Graphical Statistics ,12, 1–25.\\nIngrassia, Salvatore, Punzo, Antonio, Vittadini, Giorgio, & Minotti, Simona C. 2015. The\\ngeneralized linear mixed cluster-weighted model. Journal of Classiﬁcation ,32(1), 85–113.\\nJacobs, R.A., Jordan, M.I., Nowlan, S.J., & Hinton, G.E. 1991. Adaptive mixtures of local\\nexperts. Neural Computation ,3, 79–87.\\nJordan, M.I., & Jacobs, R.A. 1994. Hierarchical mixtures of experts and the EM algorithm.\\nNeural Computation ,6, 181–214.\\nKass, R.E., & Raftery, A.E. 1995. Bayes factors. Journal of the American Statistical Asso-\\nciation,90, 773–795.\\n34\\nLang, J. B., McDonald, J. W., & Smith, P. W. F. 1999. Association-marginal modelling\\nof multivariate categorical responses: A maximim likelihood approach. Journal of the\\nAmerican Statistical Association ,94, 1161–71.\\nLazega, E. 2001. The Collegial Phenomenon: The Social Mechanisms of Cooperation Among\\nPeers in a Corporate Law Partnership . Oxford University Press.\\nLi, F., Villani, M., & Kohn, R. 2011. Modeling conditional densities using ﬁnite smooth\\nmixtures. Chap. 6, pages 123–144 of: Mengersen, K., Robert, C., & Titterington, M.\\n(eds),Mixtures: Estimation and Applications . Wiley.\\nMarsh, M. 1999. The Making of the Eighth President. Pages 215–242 of: Marsh, Michael,\\n& Mitchell, Paul (eds), How Ireland Voted 1997 . Boulder, CO: Westview and PSAI Press.\\nMasoudnia, S., & Ebrahimpour, R. 2014. Mixture of experts: a literature survey. Artiﬁcial\\nIntelligence Review ,42(2), 275–293.\\nMazza, A., Punzo, A., & Ingrassia, S. 2017. ﬂexCWM: Flexible Cluster-Weighted Modeling.\\nR package version 1.7.\\nMcCullagh, P., & Nelder, John A. 1999. Generalized Linear Models . London: Chapman &\\nHall.\\nMcLachlan, G, & Peel, D. 2000. Finite Mixture Models . New York: John Wiley.\\nMeng, X.-L., & Rubin, D. B. 1993. Maximum likelihood estimation via the ECM algorithm:\\nA general framework. Biometrika ,80(2), 267–278.\\nMetropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H., & Teller, E. 1953. Equa-\\ntions of state calculations by fast computing machines. The Journal of Chemical Physics ,\\n21, 1087–1092.\\nMuthén, L. K., & Muthén, B. O. 2011. Mplus User’s Guide . 6 edn. Los Angeles, CA: Muthén\\nand Muthén.\\nPamminger, Christoph, & Frühwirth-Schnatter, Sylvia. 2010. Model-based Clustering of\\nCategorical Time Series. Bayesian Analysis ,5, 345–368.\\nPeng, F., Jacobs, R. A., & Tanner, M. A. 1996. Bayesian inference in mixtures-of-experts\\nand hierarchical mixtures-of-experts models with an application to speech recognition.\\nJournal of the American Statistical Association ,91(435), 953–960.\\nPlackett, R. L. 1975. The analysis of permutations. Applied Statistics ,24(2), 193–202.\\nQuandt, R.E. 1972. A new approach to estimating switching regressions. Journal of the\\nAmerican Statistical Association ,67, 306–310.\\nR Core Team. 2018. R: A Language and Environment for Statistical Computing . R Foun-\\ndation for Statistical Computing, Vienna, Austria.\\nRaftery, A.E., Newton, M.A., Satagopan, J.M., & Krivitsky, P. 2007. Estimating the in-\\ntegrated likelihood via posterior simulation using the harmonic mean identity (with dis-\\ncussion). Pages 371–416 of: Bernardo, J.M., Bayarri, M.J., Berger, J.O., Dawid, A.P.,\\nHeckerman, D., Smith, A.F.M., , & West, M. (eds), Bayesian Statistics 8 . Oxford Uni-\\nversity Press.\\n35\\nRasmussen, C. E., & Ghahramani, Z. 2002. The inﬁnite mixtures of Gaussian process\\nexperts. Pages 554–560 of: Advances in Neural Information Processing Systems , vol. 12.\\nMIT Press.\\nSchwarz, G. 1978. Estimating the dimension of a model. Annals of Statistics ,6, 461–464.\\nScott, Steven L. 2011. Data augmentation, frequentist estimation, and the Bayesian analysis\\nof multinomial logit models. Statistical Papers ,52, 87–109.\\nSinnott, R. 1995. Irish voters decide: Voting behaviour in elections and referendums since\\n1918. Manchester: Manchester University Press.\\nSinnott, R. 1999. The Electoral System. Pages 99–126 of: Coakley, John, & Gallagher,\\nMichael (eds), Politics in the Republic of Ireland , 3rd edn. London: Routledge & PSAI\\nPress.\\nStephens, M. 2000. Bayesian analysis of mixture models with an unknown number of\\ncomponents—an alternative to reversible jump methods. Annals of Statistics ,28, 40–\\n74.\\nSubedi, Sanjeena, Punzo, Antonio, Ingrassia, Salvatore, & McNicholas, Paul D. 2013. Clus-\\ntering and classiﬁcation via cluster-weighted factor analyzers. Advances in Data Analysis\\nand Classiﬁcation ,7(1), 5–40.\\nTang, X., & Qu, A. 2015. Mixture Modeling for Longitudinal Data. Journal of Computa-\\ntional and Graphical Statistics ,25, 1117–1137.\\nTanner, M. A. 1996. Tools for Statistical Inference: Observed Data and Data Augmentation\\nMethods. 3 edn. New York: Springer-Verlag.\\nTeicher, Henry. 1961. Identiﬁability of mixtures. The Annals of Mathematical Statistics ,32,\\n244–248.\\nTeicher, Henry. 1963. Identiﬁability of ﬁnite mixtures. The Annals of Mathematical Statis-\\ntics,34, 1265–1269.\\nTurner, Rolf. 2014. mixreg: functions to ﬁt mixtures of regressions. R package version 0.0-5.\\nVermunt, Jeroen K, & Magidson, Jay. 2005. Latent GOLD 4.0 User’s Guide . Statistical\\nInnovations Inc.\\nVillani, Mattias, Kohn, Robert, & Giordani, Paolo. 2009. Regression density Estimation\\nusing smooth adaptive Gaussian mixtures. Journal of Econometrics ,153, 155–173.\\nWang, P., Puterman, M.L., Cockburn, I., & Le, N. 1996. Mixed Poisson regression models\\nwith covariate dependent rates. Biometrics ,52, 381–400.\\nWaterhouse, S., MacKay, D., & Robinson, T. 1996. Bayesian methods for mixtures of\\nexperts. Pages 351–357 of: Advances in Neural Information Processing Systems . Morgan\\nKaufmann Publishers.\\nWhite, A., & Murphy, T. B. 2016. Mixed-Membership of Experts Stochastic Blockmodel.\\nNetwork Science ,4(Apr.), 48–80.\\n36\\nYakowitz, S. J., & Spragins, J. D. 1968. On the Identiﬁability of ﬁnite mixtures. The Annals\\nof Mathematical Statistics ,39, 209–214.\\nYoung, D. S., & Hunter, D. R. 2010. Mixtures of regressions with predictor-dependent\\nmixing proportions. Computational Statistics & Data Analysis ,54(10), 2253–2266.\\n37'},\n",
       " {'entry_id': 'http://arxiv.org/abs/1312.4314v3',\n",
       "  'title': 'Learning Factored Representations in a Deep Mixture of Experts',\n",
       "  'abstract': 'Mixtures of Experts combine the outputs of several \"expert\" networks, each of\\nwhich specializes in a different part of the input space. This is achieved by\\ntraining a \"gating\" network that maps each input to a distribution over the\\nexperts. Such models show promise for building larger networks that are still\\ncheap to compute at test time, and more parallelizable at training time. In\\nthis this work, we extend the Mixture of Experts to a stacked model, the Deep\\nMixture of Experts, with multiple sets of gating and experts. This\\nexponentially increases the number of effective experts by associating each\\ninput with a combination of experts at each layer, yet maintains a modest model\\nsize. On a randomly translated version of the MNIST dataset, we find that the\\nDeep Mixture of Experts automatically learns to develop location-dependent\\n(\"where\") experts at the first layer, and class-specific (\"what\") experts at\\nthe second layer. In addition, we see that the different combinations are in\\nuse when the model is applied to a dataset of speech monophones. These\\ndemonstrate effective use of all expert combinations.',\n",
       "  'authors': [arxiv.Result.Author('David Eigen'),\n",
       "   arxiv.Result.Author(\"Marc'Aurelio Ranzato\"),\n",
       "   arxiv.Result.Author('Ilya Sutskever')],\n",
       "  'pdf_url': 'http://arxiv.org/pdf/1312.4314v3',\n",
       "  'doi': None,\n",
       "  'updated': datetime.datetime(2014, 3, 9, 20, 15, 3, tzinfo=datetime.timezone.utc),\n",
       "  'published': datetime.datetime(2013, 12, 16, 11, 15, 10, tzinfo=datetime.timezone.utc),\n",
       "  'categories': ['cs.LG'],\n",
       "  'text': 'Learning Factored Representations in a\\nDeep Mixture of Experts\\nDavid Eigen1;2Marc’Aurelio Ranzato1\\x03Ilya Sutskever1\\n1Google, Inc.\\n2Dept. of Computer Science, Courant Institute, NYU\\ndeigen@cs.nyu.edu ranzato@fb.com ilyasu@google.com\\nAbstract\\nMixtures of Experts combine the outputs of several “expert” networks, each of\\nwhich specializes in a different part of the input space. This is achieved by train-\\ning a “gating” network that maps each input to a distribution over the experts. Such\\nmodels show promise for building larger networks that are still cheap to compute\\nat test time, and more parallelizable at training time. In this this work, we ex-\\ntend the Mixture of Experts to a stacked model, the Deep Mixture of Experts , with\\nmultiple sets of gating and experts. This exponentially increases the number of\\neffective experts by associating each input with a combination of experts at each\\nlayer, yet maintains a modest model size. On a randomly translated version of the\\nMNIST dataset, we ﬁnd that the Deep Mixture of Experts automatically learns to\\ndevelop location-dependent (“where”) experts at the ﬁrst layer, and class-speciﬁc\\n(“what”) experts at the second layer. In addition, we see that the different combi-\\nnations are in use when the model is applied to a dataset of speech monophones.\\nThese demonstrate effective use of all expert combinations.\\n1 Introduction\\nDeep networks have achieved very good performance in a variety of tasks, e.g.[10, 5, 3]. However,\\na fundamental limitation of these architectures is that the entire network must be executed for all\\ninputs. This computational burden imposes limits network size. One way to scale these networks up\\nwhile keeping the computational cost low is to increase the overall number of parameters and hidden\\nunits, but use only a small portion of the network for each given input. Then, learn a computationally\\ncheap mapping function from input to the appropriate portions of the network.\\nThe Mixture of Experts model [7] is a continuous version of this: A learned gating network mixes\\nthe outputs of N“expert” networks to produce a ﬁnal output. While this model does not itself\\nachieve the computational beneﬁts outlined above, it shows promise as a stepping stone towards\\nnetworks that can realize this goal.\\nIn this work, we extend the Mixture of Experts to use a different gating network at each layer in\\na multilayer network, forming a Deep Mixture of Experts (DMoE). This increases the number of\\neffective experts by introducing an exponential number of paths through different combinations of\\nexperts at each layer. By associating each input with one such combination, our model uses different\\nsubsets of its units for different inputs. Thus it can be both large and efﬁcient at the same time.\\nWe demonstrate the effectiveness of this approach by evaluating it on two datasets. Using a jittered\\nMNIST dataset, we show that the DMoE learns to factor different aspects of the data representation\\nat each layer (speciﬁcally, location and class), making effective use of all paths. We also ﬁnd that all\\ncombinations are used when applying our model to a dataset of speech monophones.\\n\\x03Marc’Aurelio Ranzato currently works at the Facebook AI Group.\\n1arXiv:1312.4314v3  [cs.LG]  9 Mar 2014\\n2 Related Work\\nA standard Mixture of Experts (MoE) [7] learns a set of expert networks fialong with a gating\\nnetwork g. Each fimaps the input xtoCoutputs (one for each class c= 1; : : : ; C ), while g(x)is a\\ndistribution over experts i= 1; : : : ; N that sums to 1. The ﬁnal output is then given by Eqn. 1\\nFMoE(x) =NX\\ni=1gi(x)softmax( fi(x)) (1)\\n=NX\\ni=1p(eijx)p(cjei; x) =p(cjx) (2)\\nThis can also be seen as a probability model, where the ﬁnal probability over classes is marginalized\\nover the selection of expert: setting p(eijx) =gi(x)andp(cjei; x) = softmax( fi(x)), we have\\nEqn. 2.\\nA product of experts (PoE) [6] is similar, but instead combines log probabilities to form a product:\\nFPoE(x)/NY\\ni=1softmax( fi(x)) =NY\\ni=1pi(cjx) (3)\\nAlso closely related to our work is the Hierarchical Mixture of Experts [9], which learns a hierarchy\\nof gating networks in a tree structure. Each expert network’s output corresponds to a leaf in the tree;\\nthe outputs are then mixed according to the gating weights at each node.\\nOur model differs from each of these three models because it dynamically assembles a suitable\\nexpert combination for each input. This is an instance of the concept of conditional computation\\nput forward by Bengio [1] and examined in a single-layer stochastic setting by Bengio, Leonard and\\nCourville [2]. By conditioning our gating and expert networks on the output of the previous layer,\\nour model can express an exponentially large number of effective experts.\\n3 Approach\\nTo extend MoE to a DMoE, we introduce two sets of experts with gating networks (g1; f1\\ni)and\\n(g2; f2\\nj), along with a ﬁnal linear layer f3(see Fig. 1). The ﬁnal output is produced by composing\\nthe mixtures at each layer:\\nz1=NX\\ni=1g1\\ni(x)f1\\ni(x)\\nz2=MX\\nj=1g2\\nj(z1)f2\\nj(z1)\\nF(x) = z3= softmax( f3(z2))\\nWe set each fl\\nito a single linear map with rectiﬁcation, and each gl\\nito two layers of linear maps with\\nrectiﬁcation (but with few hidden units); f3is a single linear layer. See Section 4 for details.\\nWe train the network using stochastic gradient descent (SGD) with an additional constraint on gating\\nassignments (described below). SGD by itself results in a degenerate local minimum: The experts at\\neach layer that perform best for the ﬁrst few examples end up overpowering the remaining experts.\\nThis happens because the ﬁrst examples increase the gating weights of these experts, which in turn\\ncauses them to be selected with high gating weights more frequently. This causes them to train more,\\nand their gating weights to increase again, ad inﬁnitum .\\nTo combat this, we place a constraint on the relative gating assignments to each expert during train-\\ning. Let Gl\\ni(t) =Pt\\nt0=1gl\\ni(xt0)be the running total assignment to expert iof layer lat step t, and\\nlet\\x16Gl(t) =1\\nNPN\\ni=1Gl\\ni(t)be their mean (here, xt0is the training example at step t0). Then for each\\nexpert i, we set gl\\ni(xt) = 0 ifGl\\ni(t)\\x00\\x16Gl(t)> m for a margin threshold m, and renormalize the\\n2\\nx\\t\\n\\rf11(x)\\t\\n\\rf21(x)\\t\\n\\rfN1(x)\\t\\n\\rg1(x)\\t\\n\\rz1\\t\\n\\r\\n. . .\\t\\n\\r\\nx\\t\\n\\rf11(x)\\t\\n\\rf21(x)\\t\\n\\rfN1(x)\\t\\n\\rg1(x)\\t\\n\\rz1\\t\\n\\r\\n. . .\\t\\n\\rf12(x)\\t\\n\\rf22(x)\\t\\n\\rfM2(x)\\t\\n\\rg2(x)\\t\\n\\rz2\\t\\n\\r\\n. . .\\t\\n\\rz3\\t\\n\\r(a) (b)\\nFigure 1: (a) Mixture of Experts; (b) Deep Mixture of Experts with two layers.\\ndistribution gl(xt)to sum to 1 over experts i. This prevents experts from being overused initially,\\nresulting in balanced assignments. After training with the constraint in place, we lift it and further\\ntrain in a second ﬁne-tuning phase.\\n4 Experiments\\n4.1 Jittered MNIST\\nWe trained and tested our model on MNIST with random uniform translations of \\x064pixels, resulting\\nin grayscale images of size 36\\x0236. As explained above, the model was trained to classify digits\\ninto ten classes.\\nFor this task, we set all f1\\niandf2\\njto one-layer linear models with rectiﬁcation, f1\\ni(x) =\\nmax(0 ; W1\\nix+b1\\ni), and similarly for f2\\nj. We set f3to a linear layer, f3(z2) = W3z2+b3.\\nWe varied the number of output hidden units of f1\\niandf2\\njbetween 20 and 100. The ﬁnal output\\nfromf3has 10 units (one for each class).\\nThe gating networks g1andg2are each composed of two linear+rectiﬁcation layers with either 50\\nor 20 hidden units, and 4 output units (one for each expert), i.e.g1(x) = softmax( B\\x01max(0 ; Ax+\\na) +b), and similarly for g2.\\nWe evaluate the effect of using a mixture at the second layer by comparing against using only a single\\nﬁxed expert at the second layer, or concatenating the output of all experts. Note that for a mixture\\nwithhhidden units, the corresponding concatenated model has N\\x01hhidden units. Thus we expect\\nthe concatenated model to perform better than the mixture, and the mixture to perform better than\\nthe single network. It is best for the mixture to be as close as possible to the concatenated-experts\\nbound. In each case, we keep the ﬁrst layer architecture the same (a mixture).\\nWe also compare the two-layer model against a one-layer model in which the hidden layer z1is\\nmapped to the ﬁnal output through linear layer and softmax. Finally, we compare against a fully-\\nconnected deep network with the same total number of parameters. This was constructed using the\\nsame number of second-layer units z2, but expanding the number ﬁrst layer units z1such that the\\ntotal number of parameters is the same as the DMoE (including its gating network parameters).\\n3\\n4.2 Monophone Speech\\nIn addition, we ran our model on a dataset of monophone speech samples. This dataset is a random\\nsubset of approximately one million samples from a larger proprietary database of several hundred\\nhours of US English data collected using V oice Search, V oice Typing and read data [8]. For our\\nexperiments, each sample was limited to 11 frames spaced 10ms apart, and had 40 frequency bins.\\nEach input was fed to the network as a 440-dimensional vector. There were 40 possible output\\nphoneme classes.\\nWe trained a model with 4 experts at the ﬁrst layer and 16 at the second layer. Both layers had\\n128 hidden units. The gating networks were each two layers, with 64 units in the hidden layer. As\\nbefore, we evaluate the effect of using a mixture at the second layer by comparing against using only\\na single expert at the second layer, or concatenating the output of all experts.\\n5 Results\\n5.1 Jittered MNIST\\nTable 1 shows the error on the training and test sets for each model size (the test set is the MNIST test\\nset with a single random translation per image). In most cases, the deeply stacked experts performs\\nbetween the single and concatenated experts baselines on the training set, as expected. However, the\\ndeep models often suffer from overﬁtting: the mixture’s error on the test set is worse than that of the\\nsingle expert for two of the four model sizes. Encouragingly, the DMoE performs almost as well as\\na fully-connected network (DNN) with the same number of parameters, even though this network\\nimposes fewer constraints on its structure.\\nIn Fig. 2, we show the mean assignment to each expert ( i.e.the mean gating output), both by input\\ntranslation and by class. The ﬁrst layer assigns experts according to translation, while assignment is\\nuniform by class. Conversely, the second layer assigns experts by class, but is uniform according to\\ntranslation. This shows that the two layers of experts are indeed being used in complementary ways,\\nso that all combinations of experts are effective. The ﬁrst layer experts become selective to where\\nthe digit appears, regardless of its membership class, while the second layer experts are selective to\\nwhat the digit class is, irrespective of the digit’s location.\\nFinally, Fig. 3 shows the nine test examples with highest gating value for each expert combination.\\nFirst-layer assignments run over the rows, while the second-layer runs over columns. Note the\\ntranslation of each digit varies by rows but is constant over columns, while the opposite is true for\\nthe class of the digit. Furthermore, easily confused classes tend to be grouped together, e.g.3 and 5.\\nTest Set Error: Jittered MNIST\\nModel Gate Hids Single Expert DMoE Concat Layer2 DNN\\n4\\x02100\\x004\\x02100 50\\x0050 1.33 1.42 1.30 1.30\\n4\\x02100\\x004\\x0220 50\\x0050 1.58 1.50 1.30 1.41\\n4\\x02100\\x004\\x0220 50\\x0020 1.41 1.39 1.30 1.40\\n4\\x0250\\x004\\x0220 20\\x0020 1.63 1.77 1.50 1.67\\n4\\x02100(one layer) 50 2.86 1.72 1.69 –\\nTraining Set Error: Jittered MNIST\\nModel Gate Hids Single Expert DMoE Concat Layer2 DNN\\n4\\x02100\\x004\\x02100 50\\x0050 0.85 0.91 0.77 0.60\\n4\\x02100\\x004\\x0220 50\\x0050 1.05 0.96 0.85 0.90\\n4\\x02100\\x004\\x0220 50\\x0020 1.04 0.98 0.87 0.87\\n4\\x0250\\x004\\x0220 20\\x0020 1.60 1.41 1.33 1.32\\n4\\x02100(one layer) 50 2.99 1.78 1.59 –\\nTable 1: Comparison of DMoE for MNIST with random translations, against baselines (i) using\\nonly one second layer expert, (ii) concatenating all second layer experts, and (iii) a DNN with same\\ntotal number of parameters. For both (i) and (ii), experts in the ﬁrst layer are mixed to form z1.\\nModels are annotated with “# experts \\x02# hidden units” for each layer.\\n4\\nJittered MNIST: Two-Layer Deep Model\\nby Translation by Class\\nLayer 1\\nOD\\\\HU\\x14\\x03DVVLJQPHQWV\\x1d\\nOD\\\\HU\\x15\\x03DVVLJQPHQWV\\x1d\\nOD\\\\HU\\x14\\x03DVVLJQPHQWV\\x1d\\nOD\\\\HU\\x15\\x03DVVLJQPHQWV\\x1d\\nLayer 2\\nMRLQW\\x03LQLW\\x03UDQGRPJDWH\\x03WUDLQHGEDVHOLQH\\x03\\x14EORFN\\x03OD\\\\HU\\x15EDVHOLQH\\x03WDUJHW\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x14\\x13\\x13\\x03EDODQFHGU\\x1c\\x1c\\x19U\\x1c\\x1c\\x1a\\x03\\x0b\\x17[\\x14\\x13\\x13\\x10\\x14[\\x14\\x13\\x13\\x0cU\\x1c\\x1c\\x1b\\x03\\x0b\\x17[\\x14\\x13\\x13\\x10\\x14[\\x17\\x13\\x13\\x0c³\\x03ILQHWXQHU\\x14\\x13\\x16\\x1aU\\x14\\x13\\x16\\x1cU\\x14\\x13\\x17\\x13\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03EDODQFHGU\\x14\\x13\\x19\\x17U\\x14\\x13\\x19\\x18U\\x14\\x13\\x19\\x19³\\x03ILQHWXQHU\\x14\\x13\\x1a\\x1bU\\x14\\x13\\x1a\\x1cU\\x14\\x13\\x1b\\x13\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x03EDODQFHGU\\x14\\x13\\x19\\x1aU\\x14\\x13\\x19\\x1bU\\x14\\x13\\x19\\x1c³\\x03ILQHWXQHU\\x14\\x13\\x1b\\x14U\\x14\\x13\\x1b\\x15U\\x14\\x13\\x1b\\x16\\x17[\\x18\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x03EDODQFHGU\\x14\\x13\\x1a\\x13U\\x14\\x13\\x1a\\x14U\\x14\\x13\\x1a\\x15³\\x03ILQHWXQHU\\x14\\x13\\x1b\\x17U\\x14\\x13\\x1b\\x18U\\x14\\x13\\x1b\\x19\\x17[\\x15\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x10\\x15\\x13\\x03EDOU\\x14\\x13\\x1a\\x16U\\x14\\x13\\x1a\\x17U\\x14\\x13\\x1a\\x18³\\x03ILQHWXQHU\\x14\\x13\\x1b\\x1aU\\x14\\x13\\x1b\\x1bU\\x14\\x13\\x1b\\x1c\\x17[\\x15\\x13\\x03\\x10\\x03\\x1b[\\x15\\x13\\x03JDWH\\x15\\x13\\x10\\x15\\x13\\x03EDOU\\x14\\x13\\x1a\\x19U\\x14\\x13\\x1a\\x17U\\x14\\x13\\x1a\\x1a³\\x03ILQHWXQHU\\x14\\x13\\x1c\\x13U\\x14\\x13\\x1b\\x1bU\\x14\\x13\\x1c\\x14\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x1b[\\x15\\x13\\x03JDWH\\x18\\x13\\x15\\x13\\x03EDOU\\x14\\x14\\x17\\x14U\\x14\\x14\\x17\\x15U\\x14\\x14\\x17\\x16\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x14\\x19[\\x15\\x13\\x03JDWH\\x18\\x13\\x15\\x13\\x03EDOU\\x14\\x14\\x17\\x17U\\x14\\x14\\x17\\x15U\\x14\\x14\\x17\\x18ILUVW\\x03WUDLQ\\x03SDVV\\x0f\\x03ERWK\\x03OD\\\\HUV\\x03EDODQFHG\\x03\\x0bEHIRUH\\x03ILQHWXQLQJ\\x0c\\x1d\\nMRLQW\\x03LQLW\\x03UDQGRPJDWH\\x03WUDLQHGEDVHOLQH\\x03\\x14EORFN\\x03OD\\\\HU\\x15EDVHOLQH\\x03WDUJHW\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x14\\x13\\x13\\x03EDODQFHGU\\x1c\\x1c\\x19U\\x1c\\x1c\\x1a\\x03\\x0b\\x17[\\x14\\x13\\x13\\x10\\x14[\\x14\\x13\\x13\\x0cU\\x1c\\x1c\\x1b\\x03\\x0b\\x17[\\x14\\x13\\x13\\x10\\x14[\\x17\\x13\\x13\\x0c³\\x03ILQHWXQHU\\x14\\x13\\x16\\x1aU\\x14\\x13\\x16\\x1cU\\x14\\x13\\x17\\x13\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03EDODQFHGU\\x14\\x13\\x19\\x17U\\x14\\x13\\x19\\x18U\\x14\\x13\\x19\\x19³\\x03ILQHWXQHU\\x14\\x13\\x1a\\x1bU\\x14\\x13\\x1a\\x1cU\\x14\\x13\\x1b\\x13\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x03EDODQFHGU\\x14\\x13\\x19\\x1aU\\x14\\x13\\x19\\x1bU\\x14\\x13\\x19\\x1c³\\x03ILQHWXQHU\\x14\\x13\\x1b\\x14U\\x14\\x13\\x1b\\x15U\\x14\\x13\\x1b\\x16\\x17[\\x18\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x03EDODQFHGU\\x14\\x13\\x1a\\x13U\\x14\\x13\\x1a\\x14U\\x14\\x13\\x1a\\x15³\\x03ILQHWXQHU\\x14\\x13\\x1b\\x17U\\x14\\x13\\x1b\\x18U\\x14\\x13\\x1b\\x19\\x17[\\x15\\x13\\x03\\x10\\x03\\x17[\\x15\\x13\\x03JDWH\\x15\\x13\\x10\\x15\\x13\\x03EDOU\\x14\\x13\\x1a\\x16U\\x14\\x13\\x1a\\x17U\\x14\\x13\\x1a\\x18³\\x03ILQHWXQHU\\x14\\x13\\x1b\\x1aU\\x14\\x13\\x1b\\x1bU\\x14\\x13\\x1b\\x1c\\x17[\\x15\\x13\\x03\\x10\\x03\\x1b[\\x15\\x13\\x03JDWH\\x15\\x13\\x10\\x15\\x13\\x03EDOU\\x14\\x13\\x1a\\x19U\\x14\\x13\\x1a\\x17U\\x14\\x13\\x1a\\x1a³\\x03ILQHWXQHU\\x14\\x13\\x1c\\x13U\\x14\\x13\\x1b\\x1bU\\x14\\x13\\x1c\\x14\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x1b[\\x15\\x13\\x03JDWH\\x18\\x13\\x15\\x13\\x03EDOU\\x14\\x14\\x17\\x14U\\x14\\x14\\x17\\x15U\\x14\\x14\\x17\\x16\\x17[\\x14\\x13\\x13\\x03\\x10\\x03\\x14\\x19[\\x15\\x13\\x03JDWH\\x18\\x13\\x15\\x13\\x03EDOU\\x14\\x14\\x17\\x17U\\x14\\x14\\x17\\x15U\\x14\\x14\\x17\\x18ILUVW\\x03WUDLQ\\x03SDVV\\x0f\\x03ERWK\\x03OD\\\\HUV\\x03EDODQFHG\\x03\\x0bEHIRUH\\x03ILQHWXQLQJ\\x0c\\x1d\\n1-Layer\\nMoE\\nwithout\\njitters—\\nPRH\\x03ZLWK\\x03UHOX\\x03LQ\\x03HDFK\\x03H[SHUW\\x0f\\x03WUDLQ\\x12WHVW\\x03ZLWK\\x03VRIWPD[\\x1d\\nUDQGRP\\x03DVVLJQPHQWV\\x03HVVHQWLDOO\\\\\\x03GRLQJ\\x03PRGHO\\x03DYHUDJLQJ\\x03EHWZHHQ\\x03\\x18\\x03\\x14\\x10OD\\\\HU\\x03UHOX\\x03QHWZRUNVVHDUFKBORVV\\x03DVVLJQPHQWV\\x03ORRN\\x03QRW\\x03VR\\x03EDG\\x03\\x10\\x10\\x03EXW\\x03WKH\\\\\\x03DSSHDU\\x03WR\\x03ILW\\x03WKH\\x03WUDLQLQJ\\x03VHW\\x03WRR\\x03PXFK\\x03SHUKDSV\"VHDUFKBORVV\\x03DVVLJQPHQWV\\x11\\x03\\x03WUDLQ\\x1e\\x03HYDOBWHVW\\x0f\\x03HYDOBWUDLQ\\nFigure 2: Mean gating output for the ﬁrst and second layers, both by translation and by class. Color\\nindicates gating weight. The distributions by translation show the mean gating assignment to each of\\nthe four experts for each of the 9\\x029possible translations. The distributions by class show the mean\\ngating assignment to each of the four experts (rows) for each of the ten classes (columns). Note\\nthe ﬁrst layer produces assignments exclusively by translation, while the second assigns experts by\\nclass. For comparison, we show assignments by class of a standard MoE trained on MNIST without\\njitters, using 5 experts \\x0220 hidden units.\\n5.2 Monophone Speech\\nTable 2 shows the error on the training and test sets. As was the case for MNIST, the mixture’s error\\non the training set falls between the two baselines. In this case, however, test set performance is\\nabout the same for both baselines as well as the mixture.\\nFig. 4 shows the 16 test examples with highest gating value for each expert combination (we show\\nonly 4 experts at the second layer due to space considerations). As before, ﬁrst-layer assignments\\nrun over the rows, while the second-layer runs over columns. While not as interpretable as for\\nMNIST, each expert combination appears to handle a distinct portion of the input. This is further\\nbolstered by Fig. 5, where we plot the average number of assignments to each expert combination.\\nHere, the choice of second-layer expert depends little on the choice of ﬁrst-layer expert.\\nTest Set Phone Error: Monophone Speech\\nModel Gate Hids Single Expert Mixed Experts Concat Layer2\\n4\\x02128\\x0016\\x02128 64\\x0064 0.55 0.55 0.56\\n4\\x02128(one layer) 64 0.58 0.55 0.55\\nTraining Set Phone Error: Monophone Speech\\nModel Gate Hids Single Expert Mixed Experts Concat Layer2\\n4\\x02128\\x0016\\x02128 64\\x0064 0.47 0.42 0.40\\n4\\x02128(one layer) 64 0.56 0.50 0.50\\nTable 2: Comparison of DMoE for monophone speech data. Here as well, we compare against\\nbaselines using only one second layer expert, or concatenating all second layer experts.\\n5\\nII\\x03QHWV\\x0f\\x03VLQJOH\\x03EORFN\\x1dVLQJOH\\x03OD\\\\HU\\x1dFigure 3: The nine test examples with highest gating value for each combination of experts, for the jittered\\nmnist dataset. First-layer experts are in rows, while second-layer are in columns.\\n6 Conclusion\\nThe Deep Mixture of Experts model we examine is a promising step towards developing large,\\nsparse models that compute only a subset of themselves for any given input. We see precisely the\\ngating assignments required to make effective use of all expert combinations: for jittered MNIST,\\na factorization into translation and class, and distinctive use of each combination for monophone\\nspeech data. However, we still use a continuous mixture of the experts’ outputs rather than restricting\\nto the top few — such an extension is necessary to fulﬁll our goal of using only a small part of the\\nmodel for each input. A method that accomplishes this for a single layer has been described by\\nCollobert et al. [4], which could possibly be adapted to our multilayer case; we hope to address this\\nin future work.\\nAcknowledgements\\nThe authors would like to thank Matthiew Zeiler for his contributions on enforcing balancing con-\\nstraints during training.\\n6\\n-RLQW\\x03$VVLJQPHQW\\x03([DPSOHV\\n/D\\\\HU\\x03\\x14/D\\\\HU\\x03\\x15\\x17\\x03H[SHUWVERWK\\x03OD\\\\HUVFigure 4: The 16 test examples with highest gating value for each combination of experts for the monophone\\nspeech data. First-layer experts are in rows, while second-layer are in columns. Each sample is represented by\\nits 40 frequency values (vertical axis) and 11 consecutive frames (horizontal axis). For this ﬁgure, we use four\\nexperts in each layer.\\nMonophone Speech: Conditional Assignments\\n([SHUWV\\x03$VVLJQPHQWV/D\\\\HU\\x03\\x143HU\\x03\\'DWDSRLQW/D\\\\HU\\x03\\x15%\\\\\\x03/DEHO\\n-RLQW\\x1d\\x03ZHOO\\x03PL[HG\\nFRORU\\x03VFDOH>\\x03\\x13\\x11\\x13\\x03\\x11\\x11\\x03\\x13\\x11\\x14\\x15\\x18\\x03@FRORU\\x03VFDOH>\\x03\\x13\\x11\\x13\\x03\\x11\\x11\\x03\\x14\\x11\\x13\\x03@\\nFigure 5: Joint assignment counts for the monophone speech dataset. Here we plot the average\\nproduct of ﬁrst and second layer gating weights for each expert combination. We normalize each\\nrow, to produce a conditional distribution: This shows the average gating assignments in the second\\nlayer given a ﬁrst layer assignment. Note the joint assignments are well mixed: Choice of second\\nlayer expert is not very dependent on the choice of ﬁrst layer expert. Colors range from dark blue\\n(0) to dark red (0.125).\\n7\\nReferences\\n[1] Y . Bengio. Deep learning of representations: Looking forward. CoRR , abs/1305.0445, 2013.\\n2\\n[2] Y . Bengio, N. L ´eonard, and A. C. Courville. Estimating or propagating gradients through\\nstochastic neurons for conditional computation. CoRR , abs/1308.3432, 2013. 2\\n[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. Flexible, high\\nperformance convolutional neural networks for image classiﬁcation. In IJCAI , 2011. 1\\n[4] R. Collobert, Y . Bengio, and S. Bengio. Scaling large learning problems with hard parallel\\nmixtures. International Journal on Pattern Recognition and Artiﬁcial Intelligence (IJPRAI) ,\\n17(3):349–365, 2003. 6\\n[5] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural net-\\nworks. In ICASSP , 2013. 1\\n[6] G. E. Hinton. Products of experts. ICANN , 1:1–6, 1999. 2\\n[7] R. A. Jacobs, M. I. Jordan, S. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.\\nNeural Computation , 3:1–12, 1991. 1, 2\\n[8] N. Jaitly, P. Nguyen, A. Senior, and V . Vanhoucke. Application of pretrained deep neural\\nnetworks to large vocabulary speech recognition. Interspeech , 2012. 4\\n[9] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural\\nComputation , 6:181–214, 1994. 2\\n[10] A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet classiﬁcation with deep convolutional\\nneural networks. In NIPS , 2012. 1\\n8',\n",
       "  'ref': '[1] Y . Bengio. Deep learning of representations: Looking forward. CoRR , abs/1305.0445, 2013.\\n2\\n[2] Y . Bengio, N. L ´eonard, and A. C. Courville. Estimating or propagating gradients through\\nstochastic neurons for conditional computation. CoRR , abs/1308.3432, 2013. 2\\n[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. Flexible, high\\nperformance convolutional neural networks for image classiﬁcation. In IJCAI , 2011. 1\\n[4] R. Collobert, Y . Bengio, and S. Bengio. Scaling large learning problems with hard parallel\\nmixtures. International Journal on Pattern Recognition and Artiﬁcial Intelligence (IJPRAI) ,\\n17(3):349–365, 2003. 6\\n[5] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural net-\\nworks. In ICASSP , 2013. 1\\n[6] G. E. Hinton. Products of experts. ICANN , 1:1–6, 1999. 2\\n[7] R. A. Jacobs, M. I. Jordan, S. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.\\nNeural Computation , 3:1–12, 1991. 1, 2\\n[8] N. Jaitly, P. Nguyen, A. Senior, and V . Vanhoucke. Application of pretrained deep neural\\nnetworks to large vocabulary speech recognition. Interspeech , 2012. 4\\n[9] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural\\nComputation , 6:181–214, 1994. 2\\n[10] A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet classiﬁcation with deep convolutional\\nneural networks. In NIPS , 2012. 1\\n8'},\n",
       " {'entry_id': 'http://arxiv.org/abs/2008.09662v1',\n",
       "  'title': 'Biased Mixtures Of Experts: Enabling Computer Vision Inference Under Data Transfer Limitations',\n",
       "  'abstract': 'We propose a novel mixture-of-experts class to optimize computer vision\\nmodels in accordance with data transfer limitations at test time. Our approach\\npostulates that the minimum acceptable amount of data allowing for\\nhighly-accurate results can vary for different input space partitions.\\nTherefore, we consider mixtures where experts require different amounts of\\ndata, and train a sparse gating function to divide the input space for each\\nexpert. By appropriate hyperparameter selection, our approach is able to bias\\nmixtures of experts towards selecting specific experts over others. In this\\nway, we show that the data transfer optimization between visual sensing and\\nprocessing can be solved as a convex optimization problem.To demonstrate the\\nrelation between data availability and performance, we evaluate biased mixtures\\non a range of mainstream computer vision problems, namely: (i) single shot\\ndetection, (ii) image super resolution, and (iii) realtime video action\\nclassification. For all cases, and when experts constitute modified baselines\\nto meet different limits on allowed data utility, biased mixtures significantly\\noutperform previous work optimized to meet the same constraints on available\\ndata.',\n",
       "  'authors': [arxiv.Result.Author('Alhabib Abbas'),\n",
       "   arxiv.Result.Author('Yiannis Andreopoulos')],\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2008.09662v1',\n",
       "  'doi': '10.1109/TIP.2020.3005508',\n",
       "  'updated': datetime.datetime(2020, 8, 21, 19, 38, 26, tzinfo=datetime.timezone.utc),\n",
       "  'published': datetime.datetime(2020, 8, 21, 19, 38, 26, tzinfo=datetime.timezone.utc),\n",
       "  'categories': ['cs.LG', 'eess.IV'],\n",
       "  'text': 'IEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 1\\nBiased Mixtures Of Experts: Enabling Computer\\nVision Inference Under Data Transfer Limitations\\nAlhabib Abbas and Yiannis Andreopoulos\\nAbstract —We propose a novel mixture-of-experts class to\\noptimize computer vision models in accordance with data trans-\\nfer limitations at test time. Our approach postulates that the\\nminimum acceptable amount of data allowing for highly-accurate\\nresults can vary for different input space partitions. Therefore,\\nwe consider mixtures where experts require different amounts of\\ndata, and train a sparse gating function to divide the input space\\nfor each expert. By appropriate hyperparameter selection, our\\napproach is able to bias mixtures of experts towards selecting\\nspeciﬁc experts over others. In this way, we show that the data\\ntransfer optimization between visual sensing and processing can\\nbe solved as a convex optimization problem. To demonstrate\\nthe relation between data availability and performance, we\\nevaluate biased mixtures on a range of mainstream computer\\nvision problems, namely: (i) single shot detection, (ii) image\\nsuper resolution, and (iii) realtime video action classiﬁcation.\\nFor all cases, and when experts constitute modiﬁed baselines\\nto meet different limits on allowed data utility, biased mixtures\\nsigniﬁcantly outperform previous work optimized to meet the\\nsame constraints on available data.\\nIndex Terms —mixtures of experts, constrained data transfer,\\nsingle shot object detection, single image super resolution, real-\\ntime action classiﬁcation.\\nI. I NTRODUCTION\\nWhen enough data is provided at test time, deep neural net-\\nworks perform well for a wide range of challenging computer\\nvision tasks. This is true especially for large models, as it is\\nnow well understood that the performance of neural networks\\nscales with the number of trainable weights and the dimension-\\nality of inputs processed during inference [19], [20]. However,\\nthe precondition of data availability at test time is only possible\\nwhen visual sensors and learned inference models coexist in\\nhardware, which excludes cases where data is collected from\\nsensors to be transferred and processed in remote environ-\\nments. To bridge the gap between the input requirements of\\nmodels that exist in such contexts, it is important to design\\nmodels that can perform well when available communication\\nresources are limited between the visual sensing and neural\\nnetwork processing parts of the system. For instance, cloud-\\nbased visual analysis, remote medical imaging, low-latency\\ngame streaming services, and drone or Internet-of-Things\\noriented computer vision [9] [29], [46], [55], have stringent\\nconstraints on the amount of data that can be provided between\\ndata-producing clients and data-consuming models on cloud\\nservers. In order to bring computer vision models to wider\\nThe authors are researchers in the Electronic and Electrical Engineering\\nDepartment of University College London, London, UK, Roberts Building,\\nWC1E 7JE (e-mail: falhabib.abbas.13, i.andreopoulos g@ucl.ac.uk). The au-\\nthors acknowledge support from the UK EPSRC grants EP/R025290/1 and\\nEP/P02243X/1.\\nFig. 1: Sample space of a classiﬁcation task using two features\\nf1(x)andf2(x)of arbitrary inputs x, where colours indicate\\ndifferent classes c1\\x00c6. The blue line shows an instance of\\na learnable input space partition E1(f1;f2), and the red line\\nshows a one-dimensional classiﬁcation boundary learnable by\\na designated expert E2(f1)with reduced data requirement\\nrelative toE1(f1;f2).\\npractical use, it is therefore imperative to provide a solution\\nto data availability constraints at test time.\\nSince deep learning models typically require a ﬁxed amount\\nof data for inference regardless of the speciﬁc nature of inputs\\nto process, this leads to unnecessary and often unachievable\\ndemands in the amount of required data trafﬁc for remote\\ninference. Although some work has been devoted to input\\ndimensionality reduction [18], [28], [50] and rate-constrained\\nmodel optimization for speciﬁc tasks [21], [55], to the best\\nof our knowledge, no task-agnostic method has been pro-\\nposed that explicitly addresses data scarcity at test time by\\nconsidering the variance between different domains in input\\nspace. The example of Figure 1 illustrates a classiﬁcation\\ntask where the acceptable data cost of inference can vary for\\ndifferent input space partitions. That is, two features f1and\\nf2can be used to classify the bottom-left examples in Figure\\n1, while one feature f1sufﬁces for distinguishing class c5\\nexamples from class c6examples on the top-right. Reducing\\nthe retained dimensions directly correlates with the data cost\\nof inference. To leverage inherent variances across different\\ninput space partitions, and by selecting among two experts\\nE1andE2which respectively require d1andd2bytes per\\ninput where d1>d2, decision boundaries can be determined\\nto appropriately pass more data for more difﬁcult inputs.\\nLearning decision boundaries similar to those of Figure 1\\ncan allow sensors to remotely communicate data as necessary,\\nsubject to the general position of an input within its respectivearXiv:2008.09662v1  [cs.LG]  21 Aug 2020\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 2\\nspace. This reduces the overall data cost of inference that is\\naccurate enough for the task at hand. Consequentially, this in\\nturn can relieve unnecessary load on communication resources\\nthat exist between sensors and remote machines used for\\nvisual inference. Our work proposes a solution to learning\\nsuch decision boundaries directly from data for any set of\\npretrained experts, and for any speciﬁed limit on data cost.\\nOur contributions are summarised below:\\n1) We introduce a novel class of mixtures-of-experts,\\nwherein some experts are favored to others by design.\\nWhen experts of different data requirements are in-\\ncluded, this allows mixtures to meet different constraints\\non allowed data utility.\\n2) We propose two methods to train biased mixtures such\\nthat input space is effectively partitioned for each expert\\nto realize data-efﬁcient mixtures.\\n3) We show that data transfer optimization between visual\\nsensing and processing can be formulated as a convex\\noptimization problem, and present an ablation study of\\nthe beneﬁt of biased mixtures under different contexts\\nof allowed limits on data utility.\\nThe expert utility biasing method proposed in this paper\\ncan be applied to reduce the data cost of any model wherein\\nthe size of inputs can be subsampled or reduced. To illustrate\\nthis, we train and validate on a variety of tasks spanning\\nmultiple domains. Speciﬁcally, we validate on the tasks of:\\nsingle shot object detection from the work of Wei et. al [25],\\nrealtime video action classiﬁcation from the work of Zhang et.\\nalin [53] and Chadha et. al [8], and image super resolution\\nfrom the work of Shi et. al [42] and Dong et. al [11]. The\\nremainder of this paper is organized as follows: In Section II,\\nwe give an overview of recent work on rate and complexity\\noptimization. Section III details the proposed biased expert\\nselection and describes its general architecture and how it is\\ntrained. In Section IV we evaluate the performance of the\\nproposed method on all tasks, and illustrate the beneﬁts that\\nbiased mixtures of experts can provide on multiple models\\nfor each task. Finally, Section V summarises our ﬁndings and\\noutlines possible directions for future work.\\nII. R ELATED WORK\\nWithin the ﬁeld of compact image representation, and in\\norder to communicate data-efﬁcient codes across networks\\nfor remote processing, directly engineered compression tech-\\nniques were extensively studied to culminate in existing image\\ncompression standards [32], [36]. More recently, learned meth-\\nods [33], [49], [50] have attracted attention as the next step\\ntowards more data-driven image compression. Salient among\\nrecent advances in this domain are variational autoencoders\\n[2], [31], [37] and adversarial models [10], [14], [35]. In\\norder to adapt learned codes to arithmetic coders, state-of-\\nthe-art proposals on learned compression [30], [34], [38], [49]\\nadditionally learn context models to predict posteriors of latent\\ncode components conditional on all preceding components.\\nSpeciﬁcally, and to move learned compression closer to re-\\nplacing established coders [32], [36], context models [30],\\n[38] use tractable masked convolutions to regulate entropies\\nof obtained image representations such that they can be codedmore effectively by subsequent entropy coders. In distributed\\nsystems of visual analysis, and in order to reduce throughput\\nrequirements on input, latent states of learned image recon-\\nstruction machines [2], [14], [31], [35] and entropy regulated\\ncompressors [30], [33], [38], [49] can be used instead of full-\\nlength inputs as representative signals to remote inference\\nmodels.\\nOther studies consider the regulation of input volumes\\nforcomplexity optimization, and propose modiﬁcations that\\nare applicable to a wide range of models. In this realm,\\nproposals such as static model pruning [15], [16], [19], reduce\\ncomplexity by modifying models in a persistent manner for all\\ninputs at test time. More recent proposals [3], [4], [23], [41]\\nshow how the test-time complexity of very large networks\\ncan be substantially reduced by conditioning computation to\\nthe content of feature maps at runtime, and do so by training\\nexternal agents to enable or disable different parts of models\\nsubject to the unique properties of each input. However, all of\\nthe aforementioned works optimize solely for complexity, and\\nalways consider the maximum amount of input to be available\\nat test time. Other proposals also studied speciﬁc vision\\ntasks in order to reduce the data requirement of deep neural\\nnetwork models. For example, this can be seen in previous\\nwork [8], [53], [55], where input volumes are reduced by\\ndistilling input sequences to their most useful elements before\\nrelaying to remote servers for semantic analysis. Other work\\n[22], [52] mainly focused on task-speciﬁc mappings of inputs\\nonto lower-dimensional space before training with more data-\\nefﬁcient models, and recent advances in domain adaptation\\nand transfer learning [26], [39], [48] can also be used to\\nlearn compressed codes tuned to particular models. However,\\nfor any speciﬁed source distribution, domain adaptation [26],\\n[39], [48] and other proposals mentioned above [8], [53], [55]\\nequally compact all sampled inputs to ﬁxed length codes, and\\nvarying degrees of entropy among input examples are ignored.\\nAs such, low-entropy inputs (which contain less information\\nrelative to others) are mapped to redundantly long code-\\nlengths, and subsequently incur unnecessary loads on data\\ntransfer assets and inference complexity. In this sense, while\\nthe aforementioned advances are important in determining\\nuseful transformations to ﬁxed-length codes, complementary\\ntechniques are necessary to determine required code lengths\\nprior to compression and inference.\\nIn our work, we consider the data cost optimization problem\\nin a task-agnostic manner, and determine required input vol-\\numes prior to visual inference . Speciﬁcally, we consider how\\ninput space partitions vary in the amount of data required per\\ninput in order to ensure good performance, and leverage this\\nvariance to train more data-efﬁcient mixtures of experts. To\\ndo so, we take inspiration from recent work [19], [23], [41]\\nto propose a mixture of experts where expert utility is biased\\ntowards speciﬁc experts. While meeting predeﬁned constraints\\non expert utility bias, we train a sparse gating function to\\nselect the most adequate expert to use from a set of experts of\\nvaried input requirements. Importantly, our method does not\\nmodify any pre-existing methods for complexity optimization\\nor task-speciﬁc data cost reduction. As such, our proposal can\\nbe applied in conjunction with recent proposals on learned\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 3\\nFig. 2: An illustration of how biased mixtures of experts can be applied for different computer vision tasks. ( \\x03) is a special\\noperator that transmits data to remote inference parts of the model whenever it receives a non-zero gate value. From left to\\nright: (a) single shot detection (SSD), (b) image super resolution, and (c) realtime action classiﬁcation.\\ncompression [30], [34], [38] and domain adaptation [26], [39],\\n[48] to reduce the data cost of visual inference. We show\\nthat our method can be augmented in accordance with any\\nset of pre-trained experts to partition input space such that\\nconstraints on data availability are met at test time, while\\nproviding the best possible accuracy of inference.\\nIII. B IASED EXPERT SELECTION\\nA. General Architecture Formulation\\nLetEdenote a mixture of Nexperts whereE=\\nfE1;E2;:::;E Ng, and each expert Enis a modiﬁed variant\\nof a task-performing baseline model. Per input x, a gating\\nfunction determines the contribution of the nthexpert as:\\nG(x;Wg)n=ef(x;Wg)n\\nPN\\nm6=nef(x;Wg)m(1)\\nwhereWgis a set of trainable weight parameters, mdenotes\\nremaining gate indices, and f(x;Wg)2RNis the output of\\na speciﬁed gating model (e.g, a multi-layer perceptron). The\\noutput yof the mixture is:\\ny=NX\\nn=1G(x;Wg)nEn(Pn(x)) (2)\\nwherePnis a preprocessing function to accommodate x\\nfor thenthexpert (e.g., Pnperforms subsampling if En\\ningests sub-sampled inputs). Mixtures-of-experts are typically\\ntrained using a task loss that calculates the error between\\na provisioned ground-truth and y. In our proposed Biased\\nMixtures-of-Experts (BMoE) paradigm, experts are activated\\nonly when needed, and activating some experts is more favor-\\nable to activating others. In addition, all experts are optimized\\nbefore training the mixture, and loss functions of yare back-\\npropagated through the gating function exclusively. In Figure\\n2 we illustrate some examples of how biased mixtures can be\\napplied to different tasks.To adjust mixtures for biased expert selection, we denote the\\ndesired amount of bias in expert selection by b, where each\\nof its components bnspeciﬁes per batch the ratio of input\\nexamples to pass to each nthexpert. Importantly, elements of\\nbdenote frequencies of use as ratios and cannot be assigned\\nnegative values (e.g., setting bn= 0:1to use expert En10% of\\nthe time), giving the properties 0\\x14bn\\x141andjjbjj1= 1. We\\nconsider two methods of training for biased expert selection:\\n(i)a soft regularization approach where a regularization term is\\nincluded in the total loss to encourage bias, and (ii)ﬁxing the\\naverage data cost per batch , by enforcing a constant number\\nof training examples to each expert in accordance with b\\nand training only with respect to the task loss. Both methods\\nencourage mixtures of experts to maximize performance while\\nmeeting the speciﬁed bias, and we describe in detail each\\nmethod in the following:\\nB. Soft Bias Regularization\\nWhen using soft bias regularization, the most suitable expert\\nto use is selected per input via a sparse gating function, and\\nall other experts are omitted. To do so, akin to [41] for each\\ninputxonly the expert associated with the highest gate value\\nis considered for inference, and we write the sparse gating\\nfunction as:\\nG(I;Wg)n= (f(x;Wg))n\\x01ef(x;Wg)n\\nPN\\nm6=nef(x;Wg)m(3)\\nwhere (f(x;Wg))is a non-linear operator which returns a\\none-hot vector indicating the top value in f(I;Wg). From (3)\\nwe also deﬁne the utility of each nthexpertunas its total\\ncontribution per batch XcomprisingMexamples:\\nun=1\\nMX\\nx2XG(x;Wg)n (4)\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 4\\nand we calculate the bias regularization loss lbiasas a function\\nofuand the speciﬁed bias vector b:\\nlbias=\\x00wbiaslog(1\\x001p\\n2jju\\x00bjj2) (5)\\nwherewbiasis a hyperparameter to control the amount of bias\\nto impose on the mixture. Since uandbdescribe frequencies\\nas ratios andjjujj1=jjbjj1= 1, the distancejju\\x00bjj2is\\nnormalized byp\\n2to ensure the expression within the log\\nfunction is always positive (p\\n2is the maximum possible\\neuclidian distance between vectors with an L1norm of one).\\nBy applying the modiﬁcations to the gating function in (3), and\\nincluding the bias regularization loss in (5) to the total loss,\\nthe mixture of experts is simultaneously trained to maximize\\ntask performance and meet the speciﬁed bias.\\nC. Batchwise Bias Enforcement\\nIn our second proposal, rather than encourage mixtures to\\nalign the utility of their experts with the speciﬁed bias, we\\nenforce bias per batch in accordance with b;and train the\\nmixture only with respect to its task loss. This in effect trains\\nmixtures to make better expert selections for each input, while\\nmeeting the bias constraint for every batch. Speciﬁcally, with\\na batch size of M, batches are segmented such that Mbn\\nexamples are passed to each nthexpert. To do so, starting\\nfrom (1), we consider G2RM\\x02Nas anMsized batch of\\ngate vectors G(x;Wg), and perform the procedure described\\nin Algorithm 1. For each nthexpert, we denote gate values\\nassigned to columns of input as G:;nand illustrate this in\\nFigure 3.\\nAlgorithm 1 Batchwise Bias Enforcement\\nInput: Soft gates batch G2RM\\x02N\\n1:forn= 1 ton=Ndo\\n2:K Mbn\\nCalculate number of inputs to pass to the nthexpert\\n3:T TopK (G:;n;K)\\nFind topKvalues corresponding to the nthexpert\\n4: fori= 1 toi=Mdo\\n5: ifTi6= 0 then\\n6:Gi;j 08j6=n\\nFor theithinput, set all gate values not corre-\\nsponding to nthexpert to 0\\n7: else\\n8:Gi;n 0\\nSet gate value corresponding to the ithinput and\\nnthexpert to 0\\n9: end if\\n10: end for\\n11:end for\\nD. Selecting Bias Values for Data Cost Optimization\\nSo far, we discussed how biased mixtures are trained to\\nmake informed expert selections when a bias vector bspeciﬁes\\nthe frequency of expert utility. Here we detail our method for\\nFig. 3: Batchwise bias enforcement example when N= 3,\\nM= 4 andb= [0:50;0:25;0:25]. Inputs are selected per\\nbatch by iteratively sorting and selecting the top Mbnhighest\\ngate values for each nthexpert. Gates subsequently set to zero\\nare highlighted in red, and top (Mbn)values are highlighted\\nin blue.\\nselecting useful biases that can optimize performance under\\ndifferent constraints on data utility. We consider the inference\\ndata cost vector d, where each of its components dnis the\\nsize of input volumes per example as seen by each expert\\n(i.e., the data cost associated with Pn(x)). When mixtures are\\nbiased, and an ample number of samples is considered, the\\naverage data cost is then expressed as \\x16d=bdT=PN\\nn=1bndn.\\nIn this way, the biasing vector bcan be tuned to allow\\nfor different average data costs of inference in the interval\\n[dmin;dmax], wheredmin anddmax are the minimum and\\nmaximum amounts of data that can be ingested by experts in\\nthe mixture.\\nImportantly, it can be seen that when N > 2there\\ncan be multiple instantiations of bthat produce the same\\naverage data cost \\x16d. Thus, when an average data cost target\\ndt2[dmin;dmax]is speciﬁed, it is necessary to deﬁne a\\nmethod by which to determine an appropriate bias vector bthat\\nis subsequently used in training biased mixtures. To address\\nthis, we consider pwherepnquantiﬁes the performance of\\neach optimized expert prior to inclusion in the mixture, and\\nselect bsuch that: (i)bsatisﬁes \\x16d=dt, and (ii)bmaximises\\nthe expected test performance as measured by bpT. That is,\\nwhen each component pndenotes an appropriate performance\\nmeasure for the nthexpert on a designated set of inputs\\nisolated from testing examples (e.g., pncan be accuracy for\\nclassiﬁcation tasks, or mean average precision for objection\\ndetection tasks), bpTis a measure of performance when\\nexamples are assigned to experts with respect to b. In doing\\nso, we reduce the problem of determining bfor a speciﬁed\\ndata costdtto a linear optimization problem that achieves\\nbdT=dt, while maximising bpT. Sincejjbjj1= 1 andbN\\ncan be expressed as bN= 1\\x00PN\\x001\\nn=1bn, by expanding and\\nsubstituting bNwe get the constraint:\\nb1d1+b2d2+:::+ (1\\x00N\\x001X\\nn=1bn)dN=dt (6)\\nand following that components of bmust be summable to\\nunity, we also get the additional (N\\x001)constraints:\\nb1\\x141;b2\\x141;:::;bN\\x001\\x141 (7)\\nwith the performance maximization objective:\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 5\\nmaxfb1p1+b2p2+:::+bNpNg (8)\\nNote that (6) and (7) deﬁne Nlinear constraints to max-\\nimize the objective (8) with Nbasic valuesfb1;b2;:::;b Ng.\\nFollowing the duality property of such convex problems [5],\\n[13], we can also formulate the dual (and equivalent) problem\\nthat ﬁnds bfor any speciﬁed performance target pt. That is,\\nappropriate biases can be found to meet ptwith the (N\\x001)\\nconstraints of (7) and the additional constraint on expected\\nperformance:\\nb1p1+b2p2+:::+ (1\\x00N\\x001X\\nn=1bn)pN=pt (9)\\nwith the data cost minimization objective:\\nminfb1d1+b2d2+:::+bNdNg (10)\\nThus, determining bis a convex problem that can be readily\\nsolved by any convex optimization technique [5], [6], [13],\\nsuch as the simplex method [5], [6]. That is, an appropriate\\nbiasing value bto use for training can be found for any\\nspeciﬁed target data cost dtby solving for bin (6)-(8), or\\nany target on expected performance ptby solving (7), (9),\\nand (10).\\nE. Final Observations\\nIn considering the performance of biased mixtures, the\\nquality of expert selections from Eis regulated by the com-\\nplexity of the gating function G(x;Wg); where increasing\\nthe complexity of G(x;Wg)can improve selections (e.g.,\\nby increasing the number of learnable weights), albeit with\\ndiminishing returns. In addition, and in the case of bias\\nenforcement, we intuitively expect the quality of selections to\\nbe directly correlated with batch sizes used for training. That\\nis, low batch size settings may not expose gating functions to\\na sufﬁcient amount of variance in inputs to make selections\\nof beneﬁt, and setting higher batch sizes is favorable.\\nImportantly, applications of biased mixtures allow gating\\nfunctionsG(x;Wg)to wholly observe inputs xprior to\\nselecting experts for data-economy. That is, biased mixtures\\ncan be distributed such that they allow for gating before\\npreprocessing to produce sampled inputs Pn(x);and before\\ninputs are subsequently sent to remote models for visual\\ninference (as illustrated in Figure 2). As a result, the constraint\\nfor gating functions is not input size, but the processing\\ncapability on-board visual sensors. We also note that, the\\nexpert selection methods detailed in Section III can be applied\\non mixtures comprising experts optimized via additional task-\\nspeciﬁc dimensionality reduction methods, and can also be\\napplied on experts that use different modalities to make their\\ninferences (as illustrated in (c) of Figure 2). Finally, while our\\nwork studies the problem of reducing data utility, bcan also\\nbe speciﬁed to prioritize any other expert property whenever\\nconstraints are properly quantiﬁed and made available to the\\nproposed gating architecture (e.g., to meet constraints on\\npower consumption or latency).IV. E VALUATION\\nA. Benchmarks and Evaluation Method\\nTo show how biased mixtures can optimize data costs\\nof inference for different problems, we evaluate on three\\ncomputer vision tasks: (i)object detection, (ii)image super\\nresolution, and (iii)realtime action classiﬁcation. In reporting\\nresults for all tasks, we compare our method against two\\nalternatives:\\n1)Previously Proposed Models : To benchmark our results\\nagainst relevant task-speciﬁc solutions, we consider the\\nperformance of constituent experts when optimized for\\ndifferent data cost constraints. In biased mixtures, this\\ncorresponds to specifying bas a one-hot vector, and\\nmeasures performance when the same amount of data\\nis used for all inputs during inference (e.g., when b=\\n[0;1;0]onlyE2is used for inference). We report this\\nto benchmark against previous work and to highlight\\nthe beneﬁt of uniquely dividing the input space for each\\nexpert.\\n2)Random Selection : Here, experts are randomly selected\\nfor inference at test time in order to satisfy the model\\nbiasing requirement b. This is to serve as the lower\\nbound of performance when biased mixtures are used\\nand the speciﬁed expert utility bias is met.\\nImportantly, when considering the problem of task-agnostic\\nmodel optimization under data cost constraints, there is no\\nprevious work similar to ours (see Section II). That is why,\\nwe benchmark against the maximum performance achievable\\nby recently proposed task-speciﬁc solutions when their input\\nvolumes are adjusted to meet different constraints on data cost.\\nThat is, biased mixtures consist of experts that also stand in as\\nexternal benchmarks . To highlight the latter, benchmark results\\nof constituent experts are indicated in comparative plots by\\nmarkers on dotted lines.\\nFor clarity, and to ensure consistency of representation\\nacross all tasks, we report the per input data cost of inference\\n\\x16das the average amount of data seen by the mixture after\\ninputs are fully decompressed. For each evaluated task we\\nspecify how the data cost for each expert dnis measured (i.e.,\\nthe data cost associated with Pn(x)). For a concise measure of\\nhow well models preform across different speciﬁed data cost\\nconstraints of dt2[dmin;dmax], and withptest(dt)denoting\\ntest performance when the target data cost is dt, we report the\\narea under curve when data cost is normalized as:\\n\\x1a=Z1\\n0ptest(dmin+t(dmax\\x00dmin))dt (11)\\nFor all mixtures, we specify the gating model (i.e.,\\nf(x;Wg)) as a single conv-pool layer followed by a fully\\nconnected network. To ensure that the model selection process\\nis of low complexity for all tasks, we use ReLU activated\\ndepthwise separable convolutions [43], and report the per input\\nnumber of multiply-accumulate gating operations Cg. We use\\ncross-validation to optimize the biasing weight wbiasand report\\nthe best performance when soft regularization is used. After\\nall experts included in the mixture are individually optimized,\\nbiased mixtures are trained by updating the weights of the\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 6\\ngating function exclusively, and the weights of experts are not\\nﬁne-tuned further. We have found that using higher batch sizes\\nis helpful when training biased mixtures, because it exposes\\nthe mixture to a more varied set of input examples to parti-\\ntion to each expert meaningfully. Therefore, to ensure gating\\nfunctions learn meaningful features for batch partitioning, for\\nall tasks we set the batch size to 128and the learning rate to\\n10\\x004.\\nB. Single-Shot Object Detection\\nWe test our method on single-shot detection (SSD) to reduce\\nthe data requirement for object detection while maintaining\\nhigh accuracy. Recent work [19], [20] [51] showed that SSD\\nmodels vary widely in performance and complexity when input\\nsizes are adjusted. When considering the varying degrees of\\ncomplexity of natural images, we expect that the minimum\\nrequired subsampling rate of inputs for accurate object detec-\\ntion should vary accordingly. To demonstrate this, we train a\\nbiased mixture of experts where each expert is optimized for a\\ndifferent image subsampling rate, and use the recent work of\\nLiuet. al [25] as a baseline for all experts (for an illustration,\\nsee (a) of Figure 2). When the resolution of inputs to each\\nexpert isRn\\x02Rnpixels, we measure the data cost associated\\nwithPn(x)as3\\x02Rn\\x02Rn\\x02K, where 3is the number of\\ncolor channels in RGB inputs, and Kis the number of bytes\\nneeded to store ﬂoating point decimals.\\nWe use VGG16 [12] and ResNet50 for feature extraction\\nand evaluate all models using 300 regional proposal boxes for\\nVGG16 [12], and 50 regional proposal boxes for ResNet50 .\\nFollowing recent work [20], [25], we train on COCO training\\ndata while excluding the 8k mini-eval images used in the 2012\\nchallenge [24], and report performance as the mean Average\\nPrecision (mAP) on COCO ( 07+12). We train mixtures for\\n20k steps to show our results when using soft regularization\\nand bias enforcement, and in Table III we detail the types and\\ncomplexities of all layers used in devising the gating model\\nf(x;Wg). Inputs to the gating model are pre-processed as\\n224\\x02224center crops of 300\\x02300images, and we ensure\\nthat the gating complexity of all mixtures remains at Cg<108\\nMult-Add operations.\\nTABLE I: Single shot detection comparison on COCO [24] of\\nbiased mixtures of SSD [25] experts against other benchmarks.\\nResolutionsfRngand data costsfdngare reported for all\\nexperts.\\nfRng=f100;150;300g(Pixels);fdng=f120;270;1080g(kB)\\nFeatureBiasing MethodmAP (dt)(%) whendt=\\x1aExtractor dmaxdmax\\n2dmax\\n3\\nVGG16Benchmark Experts\\n80.070.0 66.7 70.9\\nProposed bEnforcement 72.5 70.9 73.1\\nSoft Regularization [41] 67.1 65.0 68.9\\nRandom Selection 66.3 63.4 68.2\\nResNet50Benchmark Experts\\n75.765.1 61.3 66.1\\nProposed bEnforcement 67.8 65.9 68.3\\nSoft Regularization [41] 62.2 59.9 64.2\\nRandom Selection 61.9 57.4 63.3\\n——– ———-\\nFigure 4 shows the relationship between imposed bias, data\\ncost, and mAP when three VGG16 experts are used for single\\nshot detection, where the resolution of inputs to each expert isTABLE II: Relation between gating complexity, batch size,\\nand performance when bias enforcement is used.\\nCgM\\x1awhenfRng=\\n–f100;300g(Pixels)f100;150;300g(Pixels)\\n(Mult-Adds) VGG16 ResNet VGG16 ResNet50\\n23,048,57616 68.40 64.11 69.27 64.46\\n32 70.35 65.89 70.25 65.57\\n64 70.93 66.24 71.16 65.72\\n26,194,30416 70.85 66.92 71.82 67.04\\n32 71.49 67.25 72.50 67.41\\n64 71.84 67.59 72.97 68.04\\n38,700,21616 70.93 67.01 72.10 67.33\\n32 71.58 67.25 73.07 68.26\\n64 71.86 67.62 73.13 68.30\\n— —- ——- ———- ——\\nfRng=f100;150;300g. Notably, biased mixtures optimized\\nwith bias enforcement provide the slowest degradation in mAP\\nfor lower data costs, with diminishing gains when more data\\nis available at test time. Speciﬁcally, biasing via enforcement\\noutperforms individual experts by 7:5%when an average of\\n220 kilobytes per image is allowed, which is equal to the\\nperformance of individual experts at 490 kilobytes. That is,\\nwhen the minimum acceptable mAP is 70%, a reduction of\\n270 kilobytes in required data is achieved by our proposal\\n(which is equivalent to a saving of 55% in bitrate).\\nIn Table I we show the performance of biased mixtures\\nwhen applied to multiple models, and report \\x1aas a com-\\nprehensive measure of model performance across data costs.\\nWhen compared to random selection, we note that for both\\nResNet50 and VGG16 , imposing bias on mixtures provides\\nthe highest gain when lower values of data cost are considered\\n(e.g., when dt<dmax\\n3). Compared to soft regularization, and\\nfor all mixture conﬁgurations, we found that bias enforcement\\nis a much more effective method for training biased mixtures\\n(this is also true for all other tasks evaluated). We hypothesise\\nthis is because, when bias enforcement is used only the task\\nloss is back-propagated during training, which causes less\\ncompetition between losses and therefore less local minima\\nto exist in solution space.\\nTo further assess how biased mixtures learn useful bifurca-\\ntions of input space, Table ??details the performance of each\\nexpert on their assigned subset of inputs. Notably, Table V\\nhighlights how easier input examples are passed to the data-\\nefﬁcient expert E1, resulting in increased accuracies of E1\\ncompared to its baseline accuracy 57:91%, which is measured\\nover all test inputs of COCO [24]. Conversely, biased mix-\\ntures pass more difﬁcult examples to E2andE3, resulting\\nin lower accuracies over their assigned inputs compared to\\ntheir baseline accuracies. Interestingly, and especially for bias\\nenforcement, Table V also shows how improved accuracies of\\nE1(which correlate with how ”easy” its assigned inputs are to\\nclassify) are inversely proportional to the number of examples\\npassed to it, as reﬂected by b1(e.g., a difference of +12:72\\npercentile points in accuracy when b1= 0:8, compared to an\\nincrease of +18:31whenb1= 0:5).\\nIn Table II we study the effect of adjusting the gating\\ncomplexity Cg, batch size M, and number of experts Non\\nthe performance of biased mixtures when bias enforcement is\\nused. When we consider all mixtures, we ﬁnd that batch size\\nis critical to performance. This is because bias is enforced on\\na per batch basis, and to make meaningful decisions the gating\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 7\\nFig. 4: Single shot detection performance comparison of\\nbiased mixtures of VGG16 experts against other benchmarks\\nwhenfRng=f100;150;300g. The performance of individual\\nexperts is shown on the dotted line.\\nTABLE III: Layer complexities Cof the gating model\\nf(x;Wg)for biased mixtures evaluated on single shot de-\\ntection. Expert input resolutions are speciﬁed as fRng=\\nf100;150;300gandN= 3.\\nLayer Type Filter Shape Stride Input ShapeC\\n(Mult-Adds)\\nConvolutional 3\\x023\\x023\\x0264 2 224\\x02224\\x023 2;747;136\\nAvg. Pooling 7\\x027 5 111\\x02111\\x0264 —-\\nFlatten Op. \\x00\\x00 21\\x0221\\x0264 —-\\nFully Connected 28224\\x021024\\x00 1\\x0228224 28;901;376\\nFully Connected 1024\\x023\\x00 1\\x021024 3072\\nfunction needs to be exposed to an ample amount of variance\\nbetween examples. We also see that increasing the complexity\\nof gating does increase performance by helping partition the\\ninput space more effectively. However, this effect saturates at\\nCg\\x193:8\\x02107Mult-Add operations, which demonstrates\\nthat the optimal hyperplane to partition input space for N\\x143\\nexperts can be learned with low complexity.\\nBy comparing the left and right part of Table II, we see\\nthat adding more experts to the mixture provides a modest\\nincrease to performance. This is because having more experts\\nallows the mixture to further exploit the variance in different\\ninput sub-spaces (if any such variance exists). To see the\\nextent to which this is true, in Figure 5 we adjust the\\nlimits of allowed input resolutions to the mixture Rmin and\\nRmax, and report \\x1awhen considering different values of N.\\nImportantly, we see that when the difference between Rmin\\nandRmax is lower, using more experts yields less gain in\\nperformance, to the point where using more than three experts\\nfor(Rmin;Rmax) = (100;300) does not provide any beneﬁt.\\nThis is because, while setting high values of Nincreases the\\nnumber of intermediate resolutions between RminandRmax,\\nthe difference (Rmax\\x00Rmin)correlates with the amount of\\ndiscernable adequacy between experts, which in turn correlates\\nwith the beneﬁt of including more experts.\\nFig. 5:\\x1awhen bias enforcement is used and the number\\nof experts Nis conﬁgured. VGG16 is used for feature\\nextraction, and different colors indicate the resolution limits\\n(Rmin;Rmax)allowed to the mixture (where Ndetermines\\nthe number of intermediate input resolutions included).\\nC. Image Super-Resolution\\nWe test the applicability of biased mixtures on Single Image\\nSuper resolution (SISR), an image reconstruction task where\\nspatial features of high-resolution images are inferred from\\nlow-resolution input images. Several recent proposals have\\nshown good performance in terms of image reconstruction\\naccuracy and computational efﬁciency [11], [42], [47] [54].\\nHowever, current super resolution models do not take into\\naccount the variable amount of high-frequency edge content\\nbetween images. That is, when reconstructing images which\\ncontain many high frequency elements, SISR models are\\nlikely to beneﬁt from higher resolution input images, while\\nimages comprising predominately low-frequency content can\\nbe inferred just as well from lower resolution inputs. This\\nis true also when considering different parts of an image,\\nwhich usually vary in the breadth of their frequency elements.\\nTo demonstrate this, we train biased mixtures to determine\\nthe needed input resolution for good image reconstruction,\\nand in doing so, we show how different image parts can be\\nadaptively upsampled subject to their content. Such decisions\\nabout selected super-resolution experts can also be augmented\\nto existing media streaming standards (e.g., DASH/HLS in\\nHTTP [19]) for adaptive subsampling prior to transmission.\\nWe evaluate on the NTIRE17 challenge dataset DIV2K [1],\\nand use state-of-the-art proposals on super-resolution [11],\\n[42] as baselines for constituent experts of biased mixtures.\\nTo expose biased mixtures to the intra-image variance of\\nfrequency elements, images are divided using a ﬁxed grid into\\nparts of size 64\\x0264pixels, and super-resolution is performed\\non each part separately (for an illustration, see (b) of Figure 2).\\nBy inspecting the low-level semantics of each image part, the\\nmixture selects the most data efﬁcient expert for reconstruction\\nto preform an upscaling from the set fSng=f\\x024;\\x023;\\x022g.\\nFor each expert that upscales inputs with a factor of Snto\\nmatch the target resolution of 64\\x0264pixels, we measure\\nthe associated data cost as dn= (64=Sn)2\\x02K, whereKis\\nthe number of bytes needed to store ﬂoating point decimals.\\nTo expose gating to the high frequency components of input\\nimages, inputs to the gating model are not subsampled, and are\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 8\\nmaintained at the original resolution of resolution of 64\\x0264\\npixels. For all biased mixture results, mixtures are trained for\\n20epochs and we ensure the complexity of the gating function\\nis set toCg<107Mult-Add operations.\\nFig. 6: Super resolution performance comparison of biased\\nmixture of ESPCN [42] experts and other benchmarks when\\nfSng=f\\x024;\\x023;\\x022g.\\nTABLE IV: Image super resolution comparison on DIV2K\\n[1] of biased mixtures and other benchmarks. Upscale factors\\nfSngand data costsfdngare reported for all experts.\\nfSng=f\\x024;\\x023;\\x022g;fdng=f13:9;21:8;49:2g(kB)\\nModel Biasing MethodPSNR (dB) when dt=\\x1admaxdmax\\n2dmax\\n3\\nESPCN [42]Benchmark Experts [42]\\n33.330.4 28.4 30.7\\nProposed bEnforcement 30.7 28.8 31.0\\nSoft Regularization [41] 30.0 28.1 30.6\\nRandom Selection 29.8 28.0 30.5\\nF-SRCNN [11]Benchmark Experts [11]\\n32.829.8 28.0 30.3\\nProposed bEnforcement 30.1 28.3 30.5\\nSoft Regularization [41] 29.3 27.6 30.1\\nRandom Selection 29.2 27.5 30.0\\n——– ———-\\nIn Table IV we compare biased mixtures against other\\nbenchmarks when using ESPCN [42] and FRSCNN [11] as\\nbaselines, in Table VI we detail the performance of experts\\nover their assigned subsets of input, and in Figure 6 we\\nshow the relationship between average data cost and PSNR\\nwhen considering ESPCN [42]. Notably from Figure 6, when\\nbias enforcement is used and \\x16dis within the range of 18-22\\nkilobytes, biased mixtures outperform single experts with an\\naverage difference of 0:4dB. Over the same range of values of\\n\\x16d, and when compared to random selection, bias enforcement\\nprovides an average improvement of 0:7dB. This highlights\\nthe magnitude of intra-image high variance in required input\\nresolution for image reconstruction, which is not considered\\nby random selection and optimized experts. Overall, Figure 6\\nand Table IV show that biased mixtures outperform individual\\nexperts most when \\x16d <20kilobytes, with diminishing gains\\nin performance for higher values of \\x16d. Consistent with our\\nobservations on object detection, Table VI shows how easierinputs are passed to the data-efﬁcient super-resolution model\\nE1, thereby increasing its reconstruction accuracy, while more\\ndifﬁcult examples are passed to E2andE3resulting in a\\nmodest reduction of their PSNR performance.\\nFig. 7: Examples of expert assignments to different image\\nparts. Selected and non-selected experts are respectively high-\\nlighted by blue and red borders. Note the exploitable variance\\nin detail between images, which translates into the data cost\\nsavings reported in Table IV.\\nTABLE V: Layer complexities Cof the gating model\\nf(x;Wg)for biased mixtures evaluated on single image super-\\nresolution. Expert upscaling factors are speciﬁed as fSng=\\nf\\x024;\\x023;\\x022gandN= 3.\\nLayer Type Filter Shape Stride Input ShapeC\\n(Mult-Adds)\\nConvolutional 3\\x023\\x023\\x0264 2 64\\x0264\\x023 224;256\\nAvg. Pooling 3\\x023 2 21\\x0221\\x0264 —-\\nFlatten Op. \\x00\\x00 10\\x0210\\x0264 —-\\nFully Connected 6400\\x02512\\x00 1\\x026400 3;276;800\\nFully Connected 512\\x023\\x00 1\\x02512 1;536\\nIn Figure 7 we show examples of expert selections made\\nby the biased mixture to resolve different 64\\x0264inputs when\\nbias enforcement is used. The mixture learns to pass image\\nparts with high frequency components to the \\x022SISR model,\\nand passes other less demanding parts to the \\x024model (which\\nare blurrier, due to the lower frequency of their components).\\nD. Realtime Action Classiﬁcation\\nWe validate biased expert selection on realtime video action\\nclassiﬁcation in the compressed domain. While the best per-\\nforming action classiﬁcation models operate on uncompressed\\nvideo data, to reduce latency, the models proposed in recent\\nwork approximate a low-resolution optical ﬂow from codec\\nmotion vectors at high speeds for action classiﬁcation. The\\nclassiﬁers of use two-stream architectures to infer actions,\\nwhere spatial and temporal classiﬁers complement each other\\nby learning different sets of features from their respective\\ndomains. As such, for some action subsets, the use of only\\nthe spatio-temporal or spatial classiﬁer can sufﬁce in draw-\\ning accurate distinctions between actions, but combining the\\npredictions of both yields the highest accuracy.\\nDistinct from other compute-exhaustive models for action\\nclassiﬁcation, recent proposals on realtime video classiﬁcation\\nuse minimal volumes of data to ensure complexities and run-\\ntimes remain low. Moreover, and in order to bypass complex-\\nity overheads associated with dense optical ﬂow estimation,\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 9\\nthe proposals of produce spatio-temporal modes (which we\\nhereon refer to as temporal modes for brevity) directly from\\ncompressed bitstreams. To show how biased mixtures can be\\napplied in multi-modal settings, we expose temporal modes\\nof video to gating functions that appropriately select which\\nmodes are used for subsequent classiﬁcation. That is, prior\\nto sending inputs to remote realtime classiﬁcation models,\\nwe show how gating functions of biased mixtures can opt\\nto use modalities only when they are needed for accurate\\nclassiﬁcation (and we illustrate this in (c) of Figure 2). By\\ndoing so, we show how biased mixtures can learn to leverage\\nmodal variance to mitigate unnecessary trafﬁc between sensors\\nand remote classiﬁers.\\nWe evaluate on UCF-101 [45] and measure the cost asso-\\nciated with the spatial mode as F\\x02Hs\\x02Hs\\x02Ws\\x02K\\x023,\\nwhereFs= 2 is the number of RGB frames used, Hs= 360\\nandWs= 240 are the height and width of inputs, 3is the\\nnumber of color channels, and Kis the number of bytes to\\nstore ﬂoating point decimals. For the temporal model, we\\nmeasure the data cost as Ft\\x02Ht\\x02Wt\\x02K\\x022, where\\nFt= 150 is the number of frames used, Ht= 24 and\\nWt= 24 are the height and width of motion vector maps,\\nand2is the number of channels used to represent vertical\\nand horizontal motion. Importantly, we select spatial sampling\\nrates akin to those of [8] which sets Fs= 1,Ft\\x1510, and\\nthe proposal of [53] which sets Fs= 1,Ft\\x15100. This\\nis to meet complexity limits for realtime inference, where\\nthe benchmark models set modest sampling rates compared\\nto other exhaustive methods [7], which typically use dense\\noptical ﬂow approximations with Fs\\x1525andFt\\x15250.\\nMoreover, in implementing the benchmark model of Zhang\\net al. [53], we follow their method of upsampling 24\\x0224\\nmotion vector maps to 224\\x02224temporal mode inputs, and we\\nspeciﬁcally use a nearest-neighbours upsampling ﬁlter. Inputs\\nare upsampled after they are sent via the (\\x03)operator of Figure\\n2 (c), and therefore input shape parameters remain at Ht= 24\\nandWt= 24 when measuring data cost.\\nFor data-exhaustive action classiﬁcation, we use fusion\\nclassiﬁers which combine both modalities to predict actions\\nwith the highest possible accuracy. Fusion classiﬁers incur\\na data cost equal the sum of both modalities. We include\\nall modalities to train biased mixtures of experts, where\\nfMode ng=fTemporal;Spatial;Fusiong. Importantly, and to\\nallow for lower complexities of gating, inputs to the gating\\nmodel include only the temporal modes of videos, and spatial\\nmodes are not used. For all biased mixtures, we train for 80k\\nsteps and restrict the complexity of the gating function to\\nCg<108Mult-Add operations, where we detail the layer-\\nwise complexities of gating in Table VII.TABLE VI: Realtime action classiﬁcation on UCF-101 [45] of\\nbiased mixtures of experts and other benchmarks. Modalities\\nfMode ngand data costsfdngare reported for all experts.\\nModel Biasing MethodAccuracy (dt)(%) whendt=\\x1admaxdmax\\n2dmax\\n3\\nMV-3DCNN [8]Benchmark Experts [8]\\n88.079.0 77.9 80.9\\nProposed bEnforcement 82.0 80.4 83.5\\nSoft Regularization [41] 80.3 78.0 81.9\\nRandom Selection 78.8 77.3 81.3\\nEMV-CNN [53]Benchmark Experts [53]\\n85.676.6 75.5 78.7\\nProposed bEnforcement 80.2 79.2 81.3\\nSoft Regularization [41] 77.2 75.6 79.7\\nRandom Selection 75.7 74.9 79.0\\n——– ———-TABLE VII: Layer complexities Cof the gating model\\nf(x;Wg)for biased mixtures evaluated on realtime action\\nclassiﬁcation. Expert modalities are speciﬁed as fMode ng=\\nfTemporal;Spatial;FusiongandN= 3. Note that the gating\\nmodelf(x;Wg)ingests only temporal modalities of x.\\nLayer Type Filter Shape Stride Input ShapeC\\n(Mult-Adds)\\nConvolutional 3\\x023\\x02320\\x0264 2 24\\x0224\\x02320 3;363;840\\nFlatten Op. \\x00\\x00 11\\x0211\\x0264 —-\\nFully Connected 7744\\x021024\\x00 1\\x027744 7;929;856\\nFully Connected 1024\\x023\\x00 1\\x021024 3;072\\nIn Table VI we compare the performance of biased mixtures\\nagainst other benchmarks when using the spatial and temporal\\nclassiﬁers of [8] and [53] as baselines, and in Table VII we\\ndetail the performance of experts over their assigned input\\nsubsets as determined by G(x;Wg). From Table VI, we ﬁrst\\nnote that both biasing methods outperform random selection,\\nby up to 1%for soft regularization and up to 3:8%for bias\\nenforcement. This indicates that the biased mixture learns to\\ndiscern confusing classes for particular modalities to pass them\\nto others. Notably, when \\x16d=dmax\\n3= 860 kilobytes, bias\\nenforcement gives an accuracy 1:4%higher than that of the\\noptimized experts atdmax\\n2= 1290 kilobytes, which requires\\n430kilobytes more in data cost.\\nIn Figure 8 we show the relationship between \\x16dand action\\nclassiﬁcation accuracy for instances of bwhen biased mixtures\\nof MV-3DCNN [8] experts are used and the mode of each\\nexpert isfMode ng=fTemporal;Spatial;Fusiong. We ﬁrst\\nnote that, due to the low resolution of its inputs, the temporal\\nclassiﬁer requires the least amount of data and can predict\\nactions with an accuracy of 77:8%. By selecting among\\nthe three modes, both biasing methods outperform random\\nselection, with bias enforcement increasing accuracy by up\\nto3:4%for when \\x16d= 1032 kilobytes. Notably, and when\\nusing the temporal classiﬁer for 80% of videos at \\x16d= 1032\\nkilobytes (i.e., when b= [0:8;0:1;0:1]), bias enforcement is\\n1:6%more accurate than the spatial classiﬁer (which requires\\n811kilobytes more in data, equivalent to an increase of 78% in\\ndata cost). The latter shows the extent to which biased mixtures\\ncan improve performance by using modest amounts of data,\\neven compared to individual models that require substantially\\nmore in data cost.\\nTable VII shows how inputs are appropriately passed to\\nexperts for data-economic classiﬁcation. Speciﬁcally, it shows\\nhow biased mixtures learn to use the data-efﬁcient temporal\\nmodel for inputs that are easier to classify, where temporal\\nmodalities are likely to sufﬁce for accurate classiﬁcation.\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 10\\nFor example, this is evident when b1= 0:5andb1= 0:2,\\nwhere the temporal classiﬁer E1respectively gains +5:81\\nand+5:60percentile points in classifying its assigned inputs\\nwhen compared to its baseline accuracy measured over all\\ntest videos of UCF-101 [45]. On the other hand, Table\\nVII also shows how more difﬁcult inputs are passed to the\\nspatial and fusion classiﬁers, resulting in a modest loss of\\naccuracy when classifying their assigned inputs. Moreover,\\nTable VII highlights how bias enforcement is superior to\\nsoft regularization in assigning inputs to different modalities,\\nwhere this is evident across all values of b.\\nFig. 8: Realtime action classiﬁcation performance comparison\\nof biased mixtures of MV-3DCNN [8] experts, with expert\\nmodalitiesfMode ng=fTemporal,Spatial, Fusion g:\\nFig. 9: t-SNE [27] projections of 1024 UCF101 videos, where\\nfMode ng=fTemporal;Fusiongandb= [0:75;0:25]. In (a)\\ncolours indicate different classes, and in (b) mode assignments\\nare shown as 0or 1 for the temporal and fusion classiﬁers\\nrespectively. Zoom in to view in high-resolution .\\nTo visualize how different modalities are assigned to videos,\\nin Figure 9 we show two-dimensional t-SNE [27] projections\\nof1024 UCF101 examples as embedded by the last layer\\nof the temporal classiﬁer. We train via bias enforcement,\\nand for clarity of presentation, we use a biased mixture\\nof two modalities fMode ng=fTemporal;Fusiongwhere\\nb= [0:75;0:25]. In this way, we show the relation between\\ndifferent class labels and assigned modalities. In Figure 9\\nFig. 10: Percentile of videos assigned to the the fusion\\nclassiﬁer for the ﬁrst 30classes of UCF101 [45], where\\nfMode ng=fTemporal;Fusiongandb= [0:75;0:25].\\n(a) the middle region highlights instances of different classes\\nwhich are more entangled and therefore harder to classify. For\\na sample of instances, Figure 9 (b) shows modalities selected\\nby the biased mixture for action classiﬁcation. Notably, the\\nbiased mixture tends to select the data-exhaustive fusion mode\\nfor instances located in the entangled middle region , where\\ninputs are harder to classify (as indicated by label 1in (b)\\nof Figure 9), and temporal modes are predominantly used for\\nsufﬁciently isolated input clusters located outside the middle\\nregion (as indicated by label 0in (b) of Figure 9). That\\nis, Figure 9 shows how the biased mixture favors using the\\ntemporal classiﬁer for video clusters that are comparatively\\nisolated and easy to discern, while the fusion model is used\\nwhen videos are more entangled and harder to classify.\\nFor the same biased mixture that yields the t-SNE repre-\\nsentation of Figure 9, in Figure 10 we detail the classwise\\npercentile of videos assigned to the fusion classiﬁer. Evidently\\nfrom Figure 10, challenging inputs are typically sent to the\\ndata-exhaustive fusion mode when they contain: (i) signiﬁcant\\ncamera movement, leading to noise in underlying motion\\nﬂow (e.g., for 63% and57% of “Biking” and “Cliff Div-\\ning” instances, respectively), and (ii) relatively static scenes,\\nresulting in sparse motion vector maps (e.g., for 38% and\\n32% of “Apply Lipstick” and “Blow Dry Hair” instances,\\nrespectively). Hence, Figure 9 and Figure 10 show how\\nbiased expert mixtures can ﬁnd useful bifurcations of input\\nspace such that only necessary modalities are used for action\\nclassiﬁcation, and less data is used whenever possible.\\nV. C ONCLUSION\\nWe introduce biased expert utility in mixtures-of-experts\\nfor effective partitioning of input space to meet constraints\\non data availability at test time. We propose two methods\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 11\\nfor training biased mixtures, and evaluate their performance\\non multiple models for all investigated tasks. We show how\\nbiased mixtures are applicable to any situation wherein experts\\nvary in data requirement and performance, and demonstrate\\nthis on a wide range of computer vision tasks (we also make\\npublic a Tensorﬂow- 1:14implementation of biased mixtures\\ninhttps://github.com/UCL-Abbas/bmoe ). Our val-\\nidation shows that, especially for lower ranges of allowed\\ndata cost, biased mixtures signiﬁcantly outperform baseline\\nmodels optimized to meet the same constraints on available\\ndata. We also show how useful gating inferences that prioritise\\ndata economy can be realized with complexities that do not\\nexceed 108Mult-Add operations, which are feasible to run\\neven on embedded computation units (e.g., ARM Cortex-M7).\\nWithin contexts of distributed visual inference, and to meet\\ndifferent constraints on data transfer and bandwidth at test\\ntime, all of our observations and tests show the importance\\nof conditioning data utility for visual inference to the local\\nproximities and properties of inputs within their space. In\\nother words, the importance of doing so is applicable to all\\npresented vision tasks, and is likely to extend to other visual\\ninference tasks in order to mitigate unnecessary burdens on\\ncommunication resources and sensor hardware. We ﬁnally note\\nthat an important advantage of biased mixtures is the ﬂexibility\\nat which they can be applied, in that, biased mixtures do\\nnot modify their constituent experts, but rather augment their\\nfunction with an input preprocessing stage that allows for data-\\neconomic inference.\\nREFERENCES\\n[1] E. Agustsson and R. Timofte, “Ntire 2017 challenge on single image\\nsuper-resolution: Dataset and study,” in Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition Workshops , 2017,\\npp. 126–135.\\n[2] J. Ball ´e, V . Laparra, and E. P. Simoncelli, “End-to-end optimized image\\ncompression,” arXiv preprint arXiv:1611.01704 , 2016.\\n[3] E. Bengio, P.-L. Bacon, J. Pineau, and D. Precup, “Conditional\\ncomputation in neural networks for faster models,” arXiv preprint\\narXiv:1511.06297 , 2015.\\n[4] Y . Bengio, N. L ´eonard, and A. Courville, “Estimating or propagating\\ngradients through stochastic neurons for conditional computation,” arXiv\\npreprint arXiv:1308.3432 , 2013.\\n[5] K. H. Borgwardt, The Simplex Method: a probabilistic analysis .\\nSpringer Science & Business Media, 2012, vol. 1.\\n[6] S. Boyd and L. Vandenberghe, Convex optimization . Cambridge\\nuniversity press, 2004.\\n[7] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new\\nmodel and the kinetics dataset,” in Computer Vision and Pattern Recog-\\nnition (CVPR), 2017 IEEE Conference on . IEEE, 2017, pp. 4724–4733.\\n[8] A. Chadha, A. Abbas, and Y . Andreopoulos, “Video classiﬁcation with\\ncnns: Using the codec as a spatio-temporal activity sensor,” IEEE\\nTransactions on Circuits and Systems for Video Technology , vol. 29,\\nno. 2, pp. 475–485, 2017.\\n[9] S.-P. Chuah, N.-M. Cheung, and C. Yuen, “Layered coding for mobile\\ncloud gaming using scalable blinn-phong lighting,” IEEE Transactions\\non Image Processing , vol. 25, no. 7, pp. 3112–3125, 2016.\\n[10] E. L. Denton, S. Chintala, R. Fergus et al. , “Deep generative image\\nmodels using a laplacian pyramid of adversarial networks,” in Advances\\nin neural information processing systems , 2015, pp. 1486–1494.\\n[11] C. Dong, C. C. Loy, and X. Tang, “Accelerating the super-resolution\\nconvolutional neural network,” in European Conference on Computer\\nVision . Springer, 2016, pp. 391–407.\\n[12] C. Feichtenhofer, A. Pinz, and A. Zisserman, “Convolutional two-stream\\nnetwork fusion for video action recognition,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition , 2016, pp. 1933–\\n1941.[13] M. Fiedler, J. Nedoma, J. Ram ´ık, J. Rohn, and K. Zimmermann, Linear\\noptimization problems with inexact data . Springer Science & Business\\nMedia, 2006.\\n[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. Courville, and Y . Bengio, “Generative adversarial nets,” in\\nAdvances in neural information processing systems , 2014, pp. 2672–\\n2680.\\n[15] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing\\ndeep neural networks with pruning, trained quantization and huffman\\ncoding,” arXiv preprint arXiv:1510.00149 , 2015.\\n[16] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and con-\\nnections for efﬁcient neural network,” in Advances in neural information\\nprocessing systems , 2015, pp. 1135–1143.\\n[17] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in Proceedings of the IEEE conference on computer vision\\nand pattern recognition , 2016, pp. 770–778.\\n[18] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of\\ndata with neural networks,” science , vol. 313, no. 5786, pp. 504–507,\\n2006.\\n[19] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\\nT. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convo-\\nlutional neural networks for mobile vision applications,” arXiv preprint\\narXiv:1704.04861 , 2017.\\n[20] J. Huang, V . Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer,\\nZ. Wojna, Y . Song, S. Guadarrama et al. , “Speed/accuracy trade-offs for\\nmodern convolutional object detectors,” in IEEE CVPR , vol. 4, 2017.\\n[21] M. Jubran, A. Abbas, A. Chadha, and Y . Andreopoulos, “Rate-accuracy\\ntrade-off in video classiﬁcation with deep convolutional neural net-\\nworks,” IEEE Transactions on Circuits and Systems for Video Tech-\\nnology , 2018.\\n[22] Y . Li, D. Liu, H. Li, L. Li, Z. Li, and F. Wu, “Learning a convolutional\\nneural network for image compact-resolution,” IEEE Transactions on\\nImage Processing , vol. 28, no. 3, pp. 1092–1107, 2019.\\n[23] J. Lin, Y . Rao, J. Lu, and J. Zhou, “Runtime neural pruning,” in Advances\\nin Neural Information Processing Systems , 2017, pp. 2181–2191.\\n[24] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\\ncontext,” in European conference on computer vision . Springer, 2014,\\npp. 740–755.\\n[25] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and A. C.\\nBerg, “Ssd: Single shot multibox detector,” in European conference on\\ncomputer vision . Springer, 2016, pp. 21–37.\\n[26] M. Long, Z. Cao, J. Wang, and M. I. Jordan, “Conditional adversarial\\ndomain adaptation,” in Advances in Neural Information Processing\\nSystems , 2018, pp. 1640–1650.\\n[27] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” Journal\\nof machine learning research , vol. 9, no. Nov, pp. 2579–2605, 2008.\\n[28] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, “Adver-\\nsarial autoencoders,” arXiv preprint arXiv:1511.05644 , 2015.\\n[29] J. Martin, Y . Fu, N. Wourms, and T. Shaw, “Characterizing netﬂix band-\\nwidth consumption,” in 2013 IEEE 10th Consumer Communications and\\nNetworking Conference (CCNC) . IEEE, 2013, pp. 230–235.\\n[30] F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and L. V .\\nGool, “Practical full resolution learned lossless image compression,” in\\nProceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , 2019, pp. 10 629–10 638.\\n[31] L. Mescheder, S. Nowozin, and A. Geiger, “Adversarial variational\\nbayes: Unifying variational autoencoders and generative adversarial\\nnetworks,” in Proceedings of the 34th International Conference on\\nMachine Learning-Volume 70 . JMLR. org, 2017, pp. 2391–2400.\\n[32] J. Miano, Compressed image ﬁle formats: Jpeg, png, gif, xbm, bmp .\\nAddison-Wesley Professional, 1999.\\n[33] D. Minnen, J. Ball ´e, and G. D. Toderici, “Joint autoregressive and\\nhierarchical priors for learned image compression,” in Advances in\\nNeural Information Processing Systems , 2018, pp. 10 771–10 780.\\n[34] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu, “Pixel recurrent\\nneural networks,” arXiv preprint arXiv:1601.06759 , 2016.\\n[35] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation\\nlearning with deep convolutional generative adversarial networks,” arXiv\\npreprint arXiv:1511.06434 , 2015.\\n[36] G. Roelofs and R. Koman, PNG: the deﬁnitive guide . O’Reilly &\\nAssociates, Inc., 1999.\\n[37] J. T. Rolfe, “Discrete variational autoencoders,” arXiv preprint\\narXiv:1609.02200 , 2016.\\n[38] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma, “Pixelcnn++:\\nImproving the pixelcnn with discretized logistic mixture likelihood and\\nother modiﬁcations,” arXiv preprint arXiv:1701.05517 , 2017.\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 12\\n[39] S. Sankaranarayanan, Y . Balaji, C. D. Castillo, and R. Chellappa, “Gen-\\nerate to adapt: Aligning domains using generative adversarial networks,”\\ninProceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , 2018, pp. 8503–8512.\\n[40] L. Sevilla-Lara, Y . Liao, F. Guney, V . Jampani, A. Geiger, and M. J.\\nBlack, “On the integration of optical ﬂow and action recognition,” arXiv\\npreprint arXiv:1712.08416 , 2017.\\n[41] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\\nand J. Dean, “Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538 , 2017.\\n[42] W. Shi, J. Caballero, F. Husz ´ar, J. Totz, A. P. Aitken, R. Bishop,\\nD. Rueckert, and Z. Wang, “Real-time single image and video super-\\nresolution using an efﬁcient sub-pixel convolutional neural network,” in\\nProceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , 2016, pp. 1874–1883.\\n[43] L. Sifre and S. Mallat, “Rigid-motion scattering for texture classiﬁca-\\ntion,” arXiv preprint arXiv:1403.1687 , 2014.\\n[44] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.\\n[45] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101 human\\nactions classes from videos in the wild,” arXiv preprint arXiv:1212.0402 ,\\n2012.\\n[46] S. Srivastava and B. Lall, “Superresolution based medical image com-\\npression for mobile platforms,” in Workshop on Machine Learning for\\nHealthCare , 2015.\\n[47] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, and L. Zhang,\\n“Ntire 2017 challenge on single image super-resolution: Methods and\\nresults,” in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition Workshops , 2017, pp. 114–125.\\n[48] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discrim-\\ninative domain adaptation,” in Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition , 2017, pp. 7167–7176.\\n[49] A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves\\net al. , “Conditional image generation with pixelcnn decoders,” in Ad-\\nvances in neural information processing systems , 2016, pp. 4790–4798.\\n[50] W. Wang, Y . Huang, Y . Wang, and L. Wang, “Generalized autoencoder:\\nA neural network framework for dimensionality reduction,” in Proceed-\\nings of the IEEE conference on computer vision and pattern recognition\\nworkshops , 2014, pp. 490–497.\\n[51] W. Wang, J. Shen, and L. Shao, “Video salient object detection via\\nfully convolutional networks,” IEEE Transactions on Image Processing ,\\nvol. 27, no. 1, pp. 38–49, 2018.\\n[52] W. K. Wong, Z. Lai, J. Wen, X. Fang, and Y . Lu, “Low-rank embedding\\nfor robust image feature extraction,” IEEE Transactions on Image\\nProcessing , vol. 26, no. 6, pp. 2905–2917, 2017.\\n[53] B. Zhang, L. Wang, Z. Wang, Y . Qiao, and H. Wang, “Real-time\\naction recognition with deeply transferred motion vector cnns,” IEEE\\nTransactions on Image Processing , vol. 27, no. 5, pp. 2326–2339, 2018.\\n[54] Y . Zhang, Q. Fan, F. Bao, Y . Liu, and C. Zhang, “Single-image super-\\nresolution based on rational fractal interpolation,” IEEE Transactions on\\nImage Processing , vol. 27, no. 8, pp. 3782–3797, 2018.\\n[55] L. F. W. Z. Zhaoyang Zhang, Zhanghui Kuang, “Temporal sequence\\ndistillation: Towards few-frame action recognition in videos,” in Arxiv:\\n1808.05085 , 2018.',\n",
       "  'ref': 'IEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 1\\nBiased Mixtures Of Experts: Enabling Computer\\nVision Inference Under Data Transfer Limitations\\nAlhabib Abbas and Yiannis Andreopoulos\\nAbstract —We propose a novel mixture-of-experts class to\\noptimize computer vision models in accordance with data trans-\\nfer limitations at test time. Our approach postulates that the\\nminimum acceptable amount of data allowing for highly-accurate\\nresults can vary for different input space partitions. Therefore,\\nwe consider mixtures where experts require different amounts of\\ndata, and train a sparse gating function to divide the input space\\nfor each expert. By appropriate hyperparameter selection, our\\napproach is able to bias mixtures of experts towards selecting\\nspeciﬁc experts over others. In this way, we show that the data\\ntransfer optimization between visual sensing and processing can\\nbe solved as a convex optimization problem. To demonstrate\\nthe relation between data availability and performance, we\\nevaluate biased mixtures on a range of mainstream computer\\nvision problems, namely: (i) single shot detection, (ii) image\\nsuper resolution, and (iii) realtime video action classiﬁcation.\\nFor all cases, and when experts constitute modiﬁed baselines\\nto meet different limits on allowed data utility, biased mixtures\\nsigniﬁcantly outperform previous work optimized to meet the\\nsame constraints on available data.\\nIndex Terms —mixtures of experts, constrained data transfer,\\nsingle shot object detection, single image super resolution, real-\\ntime action classiﬁcation.\\nI. I NTRODUCTION\\nWhen enough data is provided at test time, deep neural net-\\nworks perform well for a wide range of challenging computer\\nvision tasks. This is true especially for large models, as it is\\nnow well understood that the performance of neural networks\\nscales with the number of trainable weights and the dimension-\\nality of inputs processed during inference [19], [20]. However,\\nthe precondition of data availability at test time is only possible\\nwhen visual sensors and learned inference models coexist in\\nhardware, which excludes cases where data is collected from\\nsensors to be transferred and processed in remote environ-\\nments. To bridge the gap between the input requirements of\\nmodels that exist in such contexts, it is important to design\\nmodels that can perform well when available communication\\nresources are limited between the visual sensing and neural\\nnetwork processing parts of the system. For instance, cloud-\\nbased visual analysis, remote medical imaging, low-latency\\ngame streaming services, and drone or Internet-of-Things\\noriented computer vision [9] [29], [46], [55], have stringent\\nconstraints on the amount of data that can be provided between\\ndata-producing clients and data-consuming models on cloud\\nservers. In order to bring computer vision models to wider\\nThe authors are researchers in the Electronic and Electrical Engineering\\nDepartment of University College London, London, UK, Roberts Building,\\nWC1E 7JE (e-mail: falhabib.abbas.13, i.andreopoulos g@ucl.ac.uk). The au-\\nthors acknowledge support from the UK EPSRC grants EP/R025290/1 and\\nEP/P02243X/1.\\nFig. 1: Sample space of a classiﬁcation task using two features\\nf1(x)andf2(x)of arbitrary inputs x, where colours indicate\\ndifferent classes c1\\x00c6. The blue line shows an instance of\\na learnable input space partition E1(f1;f2), and the red line\\nshows a one-dimensional classiﬁcation boundary learnable by\\na designated expert E2(f1)with reduced data requirement\\nrelative toE1(f1;f2).\\npractical use, it is therefore imperative to provide a solution\\nto data availability constraints at test time.\\nSince deep learning models typically require a ﬁxed amount\\nof data for inference regardless of the speciﬁc nature of inputs\\nto process, this leads to unnecessary and often unachievable\\ndemands in the amount of required data trafﬁc for remote\\ninference. Although some work has been devoted to input\\ndimensionality reduction [18], [28], [50] and rate-constrained\\nmodel optimization for speciﬁc tasks [21], [55], to the best\\nof our knowledge, no task-agnostic method has been pro-\\nposed that explicitly addresses data scarcity at test time by\\nconsidering the variance between different domains in input\\nspace. The example of Figure 1 illustrates a classiﬁcation\\ntask where the acceptable data cost of inference can vary for\\ndifferent input space partitions. That is, two features f1and\\nf2can be used to classify the bottom-left examples in Figure\\n1, while one feature f1sufﬁces for distinguishing class c5\\nexamples from class c6examples on the top-right. Reducing\\nthe retained dimensions directly correlates with the data cost\\nof inference. To leverage inherent variances across different\\ninput space partitions, and by selecting among two experts\\nE1andE2which respectively require d1andd2bytes per\\ninput where d1>d2, decision boundaries can be determined\\nto appropriately pass more data for more difﬁcult inputs.\\nLearning decision boundaries similar to those of Figure 1\\ncan allow sensors to remotely communicate data as necessary,\\nsubject to the general position of an input within its respectivearXiv:2008.09662v1  [cs.LG]  21 Aug 2020\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 2\\nspace. This reduces the overall data cost of inference that is\\naccurate enough for the task at hand. Consequentially, this in\\nturn can relieve unnecessary load on communication resources\\nthat exist between sensors and remote machines used for\\nvisual inference. Our work proposes a solution to learning\\nsuch decision boundaries directly from data for any set of\\npretrained experts, and for any speciﬁed limit on data cost.\\nOur contributions are summarised below:\\n1) We introduce a novel class of mixtures-of-experts,\\nwherein some experts are favored to others by design.\\nWhen experts of different data requirements are in-\\ncluded, this allows mixtures to meet different constraints\\non allowed data utility.\\n2) We propose two methods to train biased mixtures such\\nthat input space is effectively partitioned for each expert\\nto realize data-efﬁcient mixtures.\\n3) We show that data transfer optimization between visual\\nsensing and processing can be formulated as a convex\\noptimization problem, and present an ablation study of\\nthe beneﬁt of biased mixtures under different contexts\\nof allowed limits on data utility.\\nThe expert utility biasing method proposed in this paper\\ncan be applied to reduce the data cost of any model wherein\\nthe size of inputs can be subsampled or reduced. To illustrate\\nthis, we train and validate on a variety of tasks spanning\\nmultiple domains. Speciﬁcally, we validate on the tasks of:\\nsingle shot object detection from the work of Wei et. al [25],\\nrealtime video action classiﬁcation from the work of Zhang et.\\nalin [53] and Chadha et. al [8], and image super resolution\\nfrom the work of Shi et. al [42] and Dong et. al [11]. The\\nremainder of this paper is organized as follows: In Section II,\\nwe give an overview of recent work on rate and complexity\\noptimization. Section III details the proposed biased expert\\nselection and describes its general architecture and how it is\\ntrained. In Section IV we evaluate the performance of the\\nproposed method on all tasks, and illustrate the beneﬁts that\\nbiased mixtures of experts can provide on multiple models\\nfor each task. Finally, Section V summarises our ﬁndings and\\noutlines possible directions for future work.\\nII. R ELATED WORK\\nWithin the ﬁeld of compact image representation, and in\\norder to communicate data-efﬁcient codes across networks\\nfor remote processing, directly engineered compression tech-\\nniques were extensively studied to culminate in existing image\\ncompression standards [32], [36]. More recently, learned meth-\\nods [33], [49], [50] have attracted attention as the next step\\ntowards more data-driven image compression. Salient among\\nrecent advances in this domain are variational autoencoders\\n[2], [31], [37] and adversarial models [10], [14], [35]. In\\norder to adapt learned codes to arithmetic coders, state-of-\\nthe-art proposals on learned compression [30], [34], [38], [49]\\nadditionally learn context models to predict posteriors of latent\\ncode components conditional on all preceding components.\\nSpeciﬁcally, and to move learned compression closer to re-\\nplacing established coders [32], [36], context models [30],\\n[38] use tractable masked convolutions to regulate entropies\\nof obtained image representations such that they can be codedmore effectively by subsequent entropy coders. In distributed\\nsystems of visual analysis, and in order to reduce throughput\\nrequirements on input, latent states of learned image recon-\\nstruction machines [2], [14], [31], [35] and entropy regulated\\ncompressors [30], [33], [38], [49] can be used instead of full-\\nlength inputs as representative signals to remote inference\\nmodels.\\nOther studies consider the regulation of input volumes\\nforcomplexity optimization, and propose modiﬁcations that\\nare applicable to a wide range of models. In this realm,\\nproposals such as static model pruning [15], [16], [19], reduce\\ncomplexity by modifying models in a persistent manner for all\\ninputs at test time. More recent proposals [3], [4], [23], [41]\\nshow how the test-time complexity of very large networks\\ncan be substantially reduced by conditioning computation to\\nthe content of feature maps at runtime, and do so by training\\nexternal agents to enable or disable different parts of models\\nsubject to the unique properties of each input. However, all of\\nthe aforementioned works optimize solely for complexity, and\\nalways consider the maximum amount of input to be available\\nat test time. Other proposals also studied speciﬁc vision\\ntasks in order to reduce the data requirement of deep neural\\nnetwork models. For example, this can be seen in previous\\nwork [8], [53], [55], where input volumes are reduced by\\ndistilling input sequences to their most useful elements before\\nrelaying to remote servers for semantic analysis. Other work\\n[22], [52] mainly focused on task-speciﬁc mappings of inputs\\nonto lower-dimensional space before training with more data-\\nefﬁcient models, and recent advances in domain adaptation\\nand transfer learning [26], [39], [48] can also be used to\\nlearn compressed codes tuned to particular models. However,\\nfor any speciﬁed source distribution, domain adaptation [26],\\n[39], [48] and other proposals mentioned above [8], [53], [55]\\nequally compact all sampled inputs to ﬁxed length codes, and\\nvarying degrees of entropy among input examples are ignored.\\nAs such, low-entropy inputs (which contain less information\\nrelative to others) are mapped to redundantly long code-\\nlengths, and subsequently incur unnecessary loads on data\\ntransfer assets and inference complexity. In this sense, while\\nthe aforementioned advances are important in determining\\nuseful transformations to ﬁxed-length codes, complementary\\ntechniques are necessary to determine required code lengths\\nprior to compression and inference.\\nIn our work, we consider the data cost optimization problem\\nin a task-agnostic manner, and determine required input vol-\\numes prior to visual inference . Speciﬁcally, we consider how\\ninput space partitions vary in the amount of data required per\\ninput in order to ensure good performance, and leverage this\\nvariance to train more data-efﬁcient mixtures of experts. To\\ndo so, we take inspiration from recent work [19], [23], [41]\\nto propose a mixture of experts where expert utility is biased\\ntowards speciﬁc experts. While meeting predeﬁned constraints\\non expert utility bias, we train a sparse gating function to\\nselect the most adequate expert to use from a set of experts of\\nvaried input requirements. Importantly, our method does not\\nmodify any pre-existing methods for complexity optimization\\nor task-speciﬁc data cost reduction. As such, our proposal can\\nbe applied in conjunction with recent proposals on learned\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 3\\nFig. 2: An illustration of how biased mixtures of experts can be applied for different computer vision tasks. ( \\x03) is a special\\noperator that transmits data to remote inference parts of the model whenever it receives a non-zero gate value. From left to\\nright: (a) single shot detection (SSD), (b) image super resolution, and (c) realtime action classiﬁcation.\\ncompression [30], [34], [38] and domain adaptation [26], [39],\\n[48] to reduce the data cost of visual inference. We show\\nthat our method can be augmented in accordance with any\\nset of pre-trained experts to partition input space such that\\nconstraints on data availability are met at test time, while\\nproviding the best possible accuracy of inference.\\nIII. B IASED EXPERT SELECTION\\nA. General Architecture Formulation\\nLetEdenote a mixture of Nexperts whereE=\\nfE1;E2;:::;E Ng, and each expert Enis a modiﬁed variant\\nof a task-performing baseline model. Per input x, a gating\\nfunction determines the contribution of the nthexpert as:\\nG(x;Wg)n=ef(x;Wg)n\\nPN\\nm6=nef(x;Wg)m(1)\\nwhereWgis a set of trainable weight parameters, mdenotes\\nremaining gate indices, and f(x;Wg)2RNis the output of\\na speciﬁed gating model (e.g, a multi-layer perceptron). The\\noutput yof the mixture is:\\ny=NX\\nn=1G(x;Wg)nEn(Pn(x)) (2)\\nwherePnis a preprocessing function to accommodate x\\nfor thenthexpert (e.g., Pnperforms subsampling if En\\ningests sub-sampled inputs). Mixtures-of-experts are typically\\ntrained using a task loss that calculates the error between\\na provisioned ground-truth and y. In our proposed Biased\\nMixtures-of-Experts (BMoE) paradigm, experts are activated\\nonly when needed, and activating some experts is more favor-\\nable to activating others. In addition, all experts are optimized\\nbefore training the mixture, and loss functions of yare back-\\npropagated through the gating function exclusively. In Figure\\n2 we illustrate some examples of how biased mixtures can be\\napplied to different tasks.To adjust mixtures for biased expert selection, we denote the\\ndesired amount of bias in expert selection by b, where each\\nof its components bnspeciﬁes per batch the ratio of input\\nexamples to pass to each nthexpert. Importantly, elements of\\nbdenote frequencies of use as ratios and cannot be assigned\\nnegative values (e.g., setting bn= 0:1to use expert En10% of\\nthe time), giving the properties 0\\x14bn\\x141andjjbjj1= 1. We\\nconsider two methods of training for biased expert selection:\\n(i)a soft regularization approach where a regularization term is\\nincluded in the total loss to encourage bias, and (ii)ﬁxing the\\naverage data cost per batch , by enforcing a constant number\\nof training examples to each expert in accordance with b\\nand training only with respect to the task loss. Both methods\\nencourage mixtures of experts to maximize performance while\\nmeeting the speciﬁed bias, and we describe in detail each\\nmethod in the following:\\nB. Soft Bias Regularization\\nWhen using soft bias regularization, the most suitable expert\\nto use is selected per input via a sparse gating function, and\\nall other experts are omitted. To do so, akin to [41] for each\\ninputxonly the expert associated with the highest gate value\\nis considered for inference, and we write the sparse gating\\nfunction as:\\nG(I;Wg)n= (f(x;Wg))n\\x01ef(x;Wg)n\\nPN\\nm6=nef(x;Wg)m(3)\\nwhere (f(x;Wg))is a non-linear operator which returns a\\none-hot vector indicating the top value in f(I;Wg). From (3)\\nwe also deﬁne the utility of each nthexpertunas its total\\ncontribution per batch XcomprisingMexamples:\\nun=1\\nMX\\nx2XG(x;Wg)n (4)\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 4\\nand we calculate the bias regularization loss lbiasas a function\\nofuand the speciﬁed bias vector b:\\nlbias=\\x00wbiaslog(1\\x001p\\n2jju\\x00bjj2) (5)\\nwherewbiasis a hyperparameter to control the amount of bias\\nto impose on the mixture. Since uandbdescribe frequencies\\nas ratios andjjujj1=jjbjj1= 1, the distancejju\\x00bjj2is\\nnormalized byp\\n2to ensure the expression within the log\\nfunction is always positive (p\\n2is the maximum possible\\neuclidian distance between vectors with an L1norm of one).\\nBy applying the modiﬁcations to the gating function in (3), and\\nincluding the bias regularization loss in (5) to the total loss,\\nthe mixture of experts is simultaneously trained to maximize\\ntask performance and meet the speciﬁed bias.\\nC. Batchwise Bias Enforcement\\nIn our second proposal, rather than encourage mixtures to\\nalign the utility of their experts with the speciﬁed bias, we\\nenforce bias per batch in accordance with b;and train the\\nmixture only with respect to its task loss. This in effect trains\\nmixtures to make better expert selections for each input, while\\nmeeting the bias constraint for every batch. Speciﬁcally, with\\na batch size of M, batches are segmented such that Mbn\\nexamples are passed to each nthexpert. To do so, starting\\nfrom (1), we consider G2RM\\x02Nas anMsized batch of\\ngate vectors G(x;Wg), and perform the procedure described\\nin Algorithm 1. For each nthexpert, we denote gate values\\nassigned to columns of input as G:;nand illustrate this in\\nFigure 3.\\nAlgorithm 1 Batchwise Bias Enforcement\\nInput: Soft gates batch G2RM\\x02N\\n1:forn= 1 ton=Ndo\\n2:K Mbn\\nCalculate number of inputs to pass to the nthexpert\\n3:T TopK (G:;n;K)\\nFind topKvalues corresponding to the nthexpert\\n4: fori= 1 toi=Mdo\\n5: ifTi6= 0 then\\n6:Gi;j 08j6=n\\nFor theithinput, set all gate values not corre-\\nsponding to nthexpert to 0\\n7: else\\n8:Gi;n 0\\nSet gate value corresponding to the ithinput and\\nnthexpert to 0\\n9: end if\\n10: end for\\n11:end for\\nD. Selecting Bias Values for Data Cost Optimization\\nSo far, we discussed how biased mixtures are trained to\\nmake informed expert selections when a bias vector bspeciﬁes\\nthe frequency of expert utility. Here we detail our method for\\nFig. 3: Batchwise bias enforcement example when N= 3,\\nM= 4 andb= [0:50;0:25;0:25]. Inputs are selected per\\nbatch by iteratively sorting and selecting the top Mbnhighest\\ngate values for each nthexpert. Gates subsequently set to zero\\nare highlighted in red, and top (Mbn)values are highlighted\\nin blue.\\nselecting useful biases that can optimize performance under\\ndifferent constraints on data utility. We consider the inference\\ndata cost vector d, where each of its components dnis the\\nsize of input volumes per example as seen by each expert\\n(i.e., the data cost associated with Pn(x)). When mixtures are\\nbiased, and an ample number of samples is considered, the\\naverage data cost is then expressed as \\x16d=bdT=PN\\nn=1bndn.\\nIn this way, the biasing vector bcan be tuned to allow\\nfor different average data costs of inference in the interval\\n[dmin;dmax], wheredmin anddmax are the minimum and\\nmaximum amounts of data that can be ingested by experts in\\nthe mixture.\\nImportantly, it can be seen that when N > 2there\\ncan be multiple instantiations of bthat produce the same\\naverage data cost \\x16d. Thus, when an average data cost target\\ndt2[dmin;dmax]is speciﬁed, it is necessary to deﬁne a\\nmethod by which to determine an appropriate bias vector bthat\\nis subsequently used in training biased mixtures. To address\\nthis, we consider pwherepnquantiﬁes the performance of\\neach optimized expert prior to inclusion in the mixture, and\\nselect bsuch that: (i)bsatisﬁes \\x16d=dt, and (ii)bmaximises\\nthe expected test performance as measured by bpT. That is,\\nwhen each component pndenotes an appropriate performance\\nmeasure for the nthexpert on a designated set of inputs\\nisolated from testing examples (e.g., pncan be accuracy for\\nclassiﬁcation tasks, or mean average precision for objection\\ndetection tasks), bpTis a measure of performance when\\nexamples are assigned to experts with respect to b. In doing\\nso, we reduce the problem of determining bfor a speciﬁed\\ndata costdtto a linear optimization problem that achieves\\nbdT=dt, while maximising bpT. Sincejjbjj1= 1 andbN\\ncan be expressed as bN= 1\\x00PN\\x001\\nn=1bn, by expanding and\\nsubstituting bNwe get the constraint:\\nb1d1+b2d2+:::+ (1\\x00N\\x001X\\nn=1bn)dN=dt (6)\\nand following that components of bmust be summable to\\nunity, we also get the additional (N\\x001)constraints:\\nb1\\x141;b2\\x141;:::;bN\\x001\\x141 (7)\\nwith the performance maximization objective:\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 5\\nmaxfb1p1+b2p2+:::+bNpNg (8)\\nNote that (6) and (7) deﬁne Nlinear constraints to max-\\nimize the objective (8) with Nbasic valuesfb1;b2;:::;b Ng.\\nFollowing the duality property of such convex problems [5],\\n[13], we can also formulate the dual (and equivalent) problem\\nthat ﬁnds bfor any speciﬁed performance target pt. That is,\\nappropriate biases can be found to meet ptwith the (N\\x001)\\nconstraints of (7) and the additional constraint on expected\\nperformance:\\nb1p1+b2p2+:::+ (1\\x00N\\x001X\\nn=1bn)pN=pt (9)\\nwith the data cost minimization objective:\\nminfb1d1+b2d2+:::+bNdNg (10)\\nThus, determining bis a convex problem that can be readily\\nsolved by any convex optimization technique [5], [6], [13],\\nsuch as the simplex method [5], [6]. That is, an appropriate\\nbiasing value bto use for training can be found for any\\nspeciﬁed target data cost dtby solving for bin (6)-(8), or\\nany target on expected performance ptby solving (7), (9),\\nand (10).\\nE. Final Observations\\nIn considering the performance of biased mixtures, the\\nquality of expert selections from Eis regulated by the com-\\nplexity of the gating function G(x;Wg); where increasing\\nthe complexity of G(x;Wg)can improve selections (e.g.,\\nby increasing the number of learnable weights), albeit with\\ndiminishing returns. In addition, and in the case of bias\\nenforcement, we intuitively expect the quality of selections to\\nbe directly correlated with batch sizes used for training. That\\nis, low batch size settings may not expose gating functions to\\na sufﬁcient amount of variance in inputs to make selections\\nof beneﬁt, and setting higher batch sizes is favorable.\\nImportantly, applications of biased mixtures allow gating\\nfunctionsG(x;Wg)to wholly observe inputs xprior to\\nselecting experts for data-economy. That is, biased mixtures\\ncan be distributed such that they allow for gating before\\npreprocessing to produce sampled inputs Pn(x);and before\\ninputs are subsequently sent to remote models for visual\\ninference (as illustrated in Figure 2). As a result, the constraint\\nfor gating functions is not input size, but the processing\\ncapability on-board visual sensors. We also note that, the\\nexpert selection methods detailed in Section III can be applied\\non mixtures comprising experts optimized via additional task-\\nspeciﬁc dimensionality reduction methods, and can also be\\napplied on experts that use different modalities to make their\\ninferences (as illustrated in (c) of Figure 2). Finally, while our\\nwork studies the problem of reducing data utility, bcan also\\nbe speciﬁed to prioritize any other expert property whenever\\nconstraints are properly quantiﬁed and made available to the\\nproposed gating architecture (e.g., to meet constraints on\\npower consumption or latency).IV. E VALUATION\\nA. Benchmarks and Evaluation Method\\nTo show how biased mixtures can optimize data costs\\nof inference for different problems, we evaluate on three\\ncomputer vision tasks: (i)object detection, (ii)image super\\nresolution, and (iii)realtime action classiﬁcation. In reporting\\nresults for all tasks, we compare our method against two\\nalternatives:\\n1)Previously Proposed Models : To benchmark our results\\nagainst relevant task-speciﬁc solutions, we consider the\\nperformance of constituent experts when optimized for\\ndifferent data cost constraints. In biased mixtures, this\\ncorresponds to specifying bas a one-hot vector, and\\nmeasures performance when the same amount of data\\nis used for all inputs during inference (e.g., when b=\\n[0;1;0]onlyE2is used for inference). We report this\\nto benchmark against previous work and to highlight\\nthe beneﬁt of uniquely dividing the input space for each\\nexpert.\\n2)Random Selection : Here, experts are randomly selected\\nfor inference at test time in order to satisfy the model\\nbiasing requirement b. This is to serve as the lower\\nbound of performance when biased mixtures are used\\nand the speciﬁed expert utility bias is met.\\nImportantly, when considering the problem of task-agnostic\\nmodel optimization under data cost constraints, there is no\\nprevious work similar to ours (see Section II). That is why,\\nwe benchmark against the maximum performance achievable\\nby recently proposed task-speciﬁc solutions when their input\\nvolumes are adjusted to meet different constraints on data cost.\\nThat is, biased mixtures consist of experts that also stand in as\\nexternal benchmarks . To highlight the latter, benchmark results\\nof constituent experts are indicated in comparative plots by\\nmarkers on dotted lines.\\nFor clarity, and to ensure consistency of representation\\nacross all tasks, we report the per input data cost of inference\\n\\x16das the average amount of data seen by the mixture after\\ninputs are fully decompressed. For each evaluated task we\\nspecify how the data cost for each expert dnis measured (i.e.,\\nthe data cost associated with Pn(x)). For a concise measure of\\nhow well models preform across different speciﬁed data cost\\nconstraints of dt2[dmin;dmax], and withptest(dt)denoting\\ntest performance when the target data cost is dt, we report the\\narea under curve when data cost is normalized as:\\n\\x1a=Z1\\n0ptest(dmin+t(dmax\\x00dmin))dt (11)\\nFor all mixtures, we specify the gating model (i.e.,\\nf(x;Wg)) as a single conv-pool layer followed by a fully\\nconnected network. To ensure that the model selection process\\nis of low complexity for all tasks, we use ReLU activated\\ndepthwise separable convolutions [43], and report the per input\\nnumber of multiply-accumulate gating operations Cg. We use\\ncross-validation to optimize the biasing weight wbiasand report\\nthe best performance when soft regularization is used. After\\nall experts included in the mixture are individually optimized,\\nbiased mixtures are trained by updating the weights of the\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 6\\ngating function exclusively, and the weights of experts are not\\nﬁne-tuned further. We have found that using higher batch sizes\\nis helpful when training biased mixtures, because it exposes\\nthe mixture to a more varied set of input examples to parti-\\ntion to each expert meaningfully. Therefore, to ensure gating\\nfunctions learn meaningful features for batch partitioning, for\\nall tasks we set the batch size to 128and the learning rate to\\n10\\x004.\\nB. Single-Shot Object Detection\\nWe test our method on single-shot detection (SSD) to reduce\\nthe data requirement for object detection while maintaining\\nhigh accuracy. Recent work [19], [20] [51] showed that SSD\\nmodels vary widely in performance and complexity when input\\nsizes are adjusted. When considering the varying degrees of\\ncomplexity of natural images, we expect that the minimum\\nrequired subsampling rate of inputs for accurate object detec-\\ntion should vary accordingly. To demonstrate this, we train a\\nbiased mixture of experts where each expert is optimized for a\\ndifferent image subsampling rate, and use the recent work of\\nLiuet. al [25] as a baseline for all experts (for an illustration,\\nsee (a) of Figure 2). When the resolution of inputs to each\\nexpert isRn\\x02Rnpixels, we measure the data cost associated\\nwithPn(x)as3\\x02Rn\\x02Rn\\x02K, where 3is the number of\\ncolor channels in RGB inputs, and Kis the number of bytes\\nneeded to store ﬂoating point decimals.\\nWe use VGG16 [12] and ResNet50 for feature extraction\\nand evaluate all models using 300 regional proposal boxes for\\nVGG16 [12], and 50 regional proposal boxes for ResNet50 .\\nFollowing recent work [20], [25], we train on COCO training\\ndata while excluding the 8k mini-eval images used in the 2012\\nchallenge [24], and report performance as the mean Average\\nPrecision (mAP) on COCO ( 07+12). We train mixtures for\\n20k steps to show our results when using soft regularization\\nand bias enforcement, and in Table III we detail the types and\\ncomplexities of all layers used in devising the gating model\\nf(x;Wg). Inputs to the gating model are pre-processed as\\n224\\x02224center crops of 300\\x02300images, and we ensure\\nthat the gating complexity of all mixtures remains at Cg<108\\nMult-Add operations.\\nTABLE I: Single shot detection comparison on COCO [24] of\\nbiased mixtures of SSD [25] experts against other benchmarks.\\nResolutionsfRngand data costsfdngare reported for all\\nexperts.\\nfRng=f100;150;300g(Pixels);fdng=f120;270;1080g(kB)\\nFeatureBiasing MethodmAP (dt)(%) whendt=\\x1aExtractor dmaxdmax\\n2dmax\\n3\\nVGG16Benchmark Experts\\n80.070.0 66.7 70.9\\nProposed bEnforcement 72.5 70.9 73.1\\nSoft Regularization [41] 67.1 65.0 68.9\\nRandom Selection 66.3 63.4 68.2\\nResNet50Benchmark Experts\\n75.765.1 61.3 66.1\\nProposed bEnforcement 67.8 65.9 68.3\\nSoft Regularization [41] 62.2 59.9 64.2\\nRandom Selection 61.9 57.4 63.3\\n——– ———-\\nFigure 4 shows the relationship between imposed bias, data\\ncost, and mAP when three VGG16 experts are used for single\\nshot detection, where the resolution of inputs to each expert isTABLE II: Relation between gating complexity, batch size,\\nand performance when bias enforcement is used.\\nCgM\\x1awhenfRng=\\n–f100;300g(Pixels)f100;150;300g(Pixels)\\n(Mult-Adds) VGG16 ResNet VGG16 ResNet50\\n23,048,57616 68.40 64.11 69.27 64.46\\n32 70.35 65.89 70.25 65.57\\n64 70.93 66.24 71.16 65.72\\n26,194,30416 70.85 66.92 71.82 67.04\\n32 71.49 67.25 72.50 67.41\\n64 71.84 67.59 72.97 68.04\\n38,700,21616 70.93 67.01 72.10 67.33\\n32 71.58 67.25 73.07 68.26\\n64 71.86 67.62 73.13 68.30\\n— —- ——- ———- ——\\nfRng=f100;150;300g. Notably, biased mixtures optimized\\nwith bias enforcement provide the slowest degradation in mAP\\nfor lower data costs, with diminishing gains when more data\\nis available at test time. Speciﬁcally, biasing via enforcement\\noutperforms individual experts by 7:5%when an average of\\n220 kilobytes per image is allowed, which is equal to the\\nperformance of individual experts at 490 kilobytes. That is,\\nwhen the minimum acceptable mAP is 70%, a reduction of\\n270 kilobytes in required data is achieved by our proposal\\n(which is equivalent to a saving of 55% in bitrate).\\nIn Table I we show the performance of biased mixtures\\nwhen applied to multiple models, and report \\x1aas a com-\\nprehensive measure of model performance across data costs.\\nWhen compared to random selection, we note that for both\\nResNet50 and VGG16 , imposing bias on mixtures provides\\nthe highest gain when lower values of data cost are considered\\n(e.g., when dt<dmax\\n3). Compared to soft regularization, and\\nfor all mixture conﬁgurations, we found that bias enforcement\\nis a much more effective method for training biased mixtures\\n(this is also true for all other tasks evaluated). We hypothesise\\nthis is because, when bias enforcement is used only the task\\nloss is back-propagated during training, which causes less\\ncompetition between losses and therefore less local minima\\nto exist in solution space.\\nTo further assess how biased mixtures learn useful bifurca-\\ntions of input space, Table ??details the performance of each\\nexpert on their assigned subset of inputs. Notably, Table V\\nhighlights how easier input examples are passed to the data-\\nefﬁcient expert E1, resulting in increased accuracies of E1\\ncompared to its baseline accuracy 57:91%, which is measured\\nover all test inputs of COCO [24]. Conversely, biased mix-\\ntures pass more difﬁcult examples to E2andE3, resulting\\nin lower accuracies over their assigned inputs compared to\\ntheir baseline accuracies. Interestingly, and especially for bias\\nenforcement, Table V also shows how improved accuracies of\\nE1(which correlate with how ”easy” its assigned inputs are to\\nclassify) are inversely proportional to the number of examples\\npassed to it, as reﬂected by b1(e.g., a difference of +12:72\\npercentile points in accuracy when b1= 0:8, compared to an\\nincrease of +18:31whenb1= 0:5).\\nIn Table II we study the effect of adjusting the gating\\ncomplexity Cg, batch size M, and number of experts Non\\nthe performance of biased mixtures when bias enforcement is\\nused. When we consider all mixtures, we ﬁnd that batch size\\nis critical to performance. This is because bias is enforced on\\na per batch basis, and to make meaningful decisions the gating\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 7\\nFig. 4: Single shot detection performance comparison of\\nbiased mixtures of VGG16 experts against other benchmarks\\nwhenfRng=f100;150;300g. The performance of individual\\nexperts is shown on the dotted line.\\nTABLE III: Layer complexities Cof the gating model\\nf(x;Wg)for biased mixtures evaluated on single shot de-\\ntection. Expert input resolutions are speciﬁed as fRng=\\nf100;150;300gandN= 3.\\nLayer Type Filter Shape Stride Input ShapeC\\n(Mult-Adds)\\nConvolutional 3\\x023\\x023\\x0264 2 224\\x02224\\x023 2;747;136\\nAvg. Pooling 7\\x027 5 111\\x02111\\x0264 —-\\nFlatten Op. \\x00\\x00 21\\x0221\\x0264 —-\\nFully Connected 28224\\x021024\\x00 1\\x0228224 28;901;376\\nFully Connected 1024\\x023\\x00 1\\x021024 3072\\nfunction needs to be exposed to an ample amount of variance\\nbetween examples. We also see that increasing the complexity\\nof gating does increase performance by helping partition the\\ninput space more effectively. However, this effect saturates at\\nCg\\x193:8\\x02107Mult-Add operations, which demonstrates\\nthat the optimal hyperplane to partition input space for N\\x143\\nexperts can be learned with low complexity.\\nBy comparing the left and right part of Table II, we see\\nthat adding more experts to the mixture provides a modest\\nincrease to performance. This is because having more experts\\nallows the mixture to further exploit the variance in different\\ninput sub-spaces (if any such variance exists). To see the\\nextent to which this is true, in Figure 5 we adjust the\\nlimits of allowed input resolutions to the mixture Rmin and\\nRmax, and report \\x1awhen considering different values of N.\\nImportantly, we see that when the difference between Rmin\\nandRmax is lower, using more experts yields less gain in\\nperformance, to the point where using more than three experts\\nfor(Rmin;Rmax) = (100;300) does not provide any beneﬁt.\\nThis is because, while setting high values of Nincreases the\\nnumber of intermediate resolutions between RminandRmax,\\nthe difference (Rmax\\x00Rmin)correlates with the amount of\\ndiscernable adequacy between experts, which in turn correlates\\nwith the beneﬁt of including more experts.\\nFig. 5:\\x1awhen bias enforcement is used and the number\\nof experts Nis conﬁgured. VGG16 is used for feature\\nextraction, and different colors indicate the resolution limits\\n(Rmin;Rmax)allowed to the mixture (where Ndetermines\\nthe number of intermediate input resolutions included).\\nC. Image Super-Resolution\\nWe test the applicability of biased mixtures on Single Image\\nSuper resolution (SISR), an image reconstruction task where\\nspatial features of high-resolution images are inferred from\\nlow-resolution input images. Several recent proposals have\\nshown good performance in terms of image reconstruction\\naccuracy and computational efﬁciency [11], [42], [47] [54].\\nHowever, current super resolution models do not take into\\naccount the variable amount of high-frequency edge content\\nbetween images. That is, when reconstructing images which\\ncontain many high frequency elements, SISR models are\\nlikely to beneﬁt from higher resolution input images, while\\nimages comprising predominately low-frequency content can\\nbe inferred just as well from lower resolution inputs. This\\nis true also when considering different parts of an image,\\nwhich usually vary in the breadth of their frequency elements.\\nTo demonstrate this, we train biased mixtures to determine\\nthe needed input resolution for good image reconstruction,\\nand in doing so, we show how different image parts can be\\nadaptively upsampled subject to their content. Such decisions\\nabout selected super-resolution experts can also be augmented\\nto existing media streaming standards (e.g., DASH/HLS in\\nHTTP [19]) for adaptive subsampling prior to transmission.\\nWe evaluate on the NTIRE17 challenge dataset DIV2K [1],\\nand use state-of-the-art proposals on super-resolution [11],\\n[42] as baselines for constituent experts of biased mixtures.\\nTo expose biased mixtures to the intra-image variance of\\nfrequency elements, images are divided using a ﬁxed grid into\\nparts of size 64\\x0264pixels, and super-resolution is performed\\non each part separately (for an illustration, see (b) of Figure 2).\\nBy inspecting the low-level semantics of each image part, the\\nmixture selects the most data efﬁcient expert for reconstruction\\nto preform an upscaling from the set fSng=f\\x024;\\x023;\\x022g.\\nFor each expert that upscales inputs with a factor of Snto\\nmatch the target resolution of 64\\x0264pixels, we measure\\nthe associated data cost as dn= (64=Sn)2\\x02K, whereKis\\nthe number of bytes needed to store ﬂoating point decimals.\\nTo expose gating to the high frequency components of input\\nimages, inputs to the gating model are not subsampled, and are\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 8\\nmaintained at the original resolution of resolution of 64\\x0264\\npixels. For all biased mixture results, mixtures are trained for\\n20epochs and we ensure the complexity of the gating function\\nis set toCg<107Mult-Add operations.\\nFig. 6: Super resolution performance comparison of biased\\nmixture of ESPCN [42] experts and other benchmarks when\\nfSng=f\\x024;\\x023;\\x022g.\\nTABLE IV: Image super resolution comparison on DIV2K\\n[1] of biased mixtures and other benchmarks. Upscale factors\\nfSngand data costsfdngare reported for all experts.\\nfSng=f\\x024;\\x023;\\x022g;fdng=f13:9;21:8;49:2g(kB)\\nModel Biasing MethodPSNR (dB) when dt=\\x1admaxdmax\\n2dmax\\n3\\nESPCN [42]Benchmark Experts [42]\\n33.330.4 28.4 30.7\\nProposed bEnforcement 30.7 28.8 31.0\\nSoft Regularization [41] 30.0 28.1 30.6\\nRandom Selection 29.8 28.0 30.5\\nF-SRCNN [11]Benchmark Experts [11]\\n32.829.8 28.0 30.3\\nProposed bEnforcement 30.1 28.3 30.5\\nSoft Regularization [41] 29.3 27.6 30.1\\nRandom Selection 29.2 27.5 30.0\\n——– ———-\\nIn Table IV we compare biased mixtures against other\\nbenchmarks when using ESPCN [42] and FRSCNN [11] as\\nbaselines, in Table VI we detail the performance of experts\\nover their assigned subsets of input, and in Figure 6 we\\nshow the relationship between average data cost and PSNR\\nwhen considering ESPCN [42]. Notably from Figure 6, when\\nbias enforcement is used and \\x16dis within the range of 18-22\\nkilobytes, biased mixtures outperform single experts with an\\naverage difference of 0:4dB. Over the same range of values of\\n\\x16d, and when compared to random selection, bias enforcement\\nprovides an average improvement of 0:7dB. This highlights\\nthe magnitude of intra-image high variance in required input\\nresolution for image reconstruction, which is not considered\\nby random selection and optimized experts. Overall, Figure 6\\nand Table IV show that biased mixtures outperform individual\\nexperts most when \\x16d <20kilobytes, with diminishing gains\\nin performance for higher values of \\x16d. Consistent with our\\nobservations on object detection, Table VI shows how easierinputs are passed to the data-efﬁcient super-resolution model\\nE1, thereby increasing its reconstruction accuracy, while more\\ndifﬁcult examples are passed to E2andE3resulting in a\\nmodest reduction of their PSNR performance.\\nFig. 7: Examples of expert assignments to different image\\nparts. Selected and non-selected experts are respectively high-\\nlighted by blue and red borders. Note the exploitable variance\\nin detail between images, which translates into the data cost\\nsavings reported in Table IV.\\nTABLE V: Layer complexities Cof the gating model\\nf(x;Wg)for biased mixtures evaluated on single image super-\\nresolution. Expert upscaling factors are speciﬁed as fSng=\\nf\\x024;\\x023;\\x022gandN= 3.\\nLayer Type Filter Shape Stride Input ShapeC\\n(Mult-Adds)\\nConvolutional 3\\x023\\x023\\x0264 2 64\\x0264\\x023 224;256\\nAvg. Pooling 3\\x023 2 21\\x0221\\x0264 —-\\nFlatten Op. \\x00\\x00 10\\x0210\\x0264 —-\\nFully Connected 6400\\x02512\\x00 1\\x026400 3;276;800\\nFully Connected 512\\x023\\x00 1\\x02512 1;536\\nIn Figure 7 we show examples of expert selections made\\nby the biased mixture to resolve different 64\\x0264inputs when\\nbias enforcement is used. The mixture learns to pass image\\nparts with high frequency components to the \\x022SISR model,\\nand passes other less demanding parts to the \\x024model (which\\nare blurrier, due to the lower frequency of their components).\\nD. Realtime Action Classiﬁcation\\nWe validate biased expert selection on realtime video action\\nclassiﬁcation in the compressed domain. While the best per-\\nforming action classiﬁcation models operate on uncompressed\\nvideo data, to reduce latency, the models proposed in recent\\nwork approximate a low-resolution optical ﬂow from codec\\nmotion vectors at high speeds for action classiﬁcation. The\\nclassiﬁers of use two-stream architectures to infer actions,\\nwhere spatial and temporal classiﬁers complement each other\\nby learning different sets of features from their respective\\ndomains. As such, for some action subsets, the use of only\\nthe spatio-temporal or spatial classiﬁer can sufﬁce in draw-\\ning accurate distinctions between actions, but combining the\\npredictions of both yields the highest accuracy.\\nDistinct from other compute-exhaustive models for action\\nclassiﬁcation, recent proposals on realtime video classiﬁcation\\nuse minimal volumes of data to ensure complexities and run-\\ntimes remain low. Moreover, and in order to bypass complex-\\nity overheads associated with dense optical ﬂow estimation,\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 9\\nthe proposals of produce spatio-temporal modes (which we\\nhereon refer to as temporal modes for brevity) directly from\\ncompressed bitstreams. To show how biased mixtures can be\\napplied in multi-modal settings, we expose temporal modes\\nof video to gating functions that appropriately select which\\nmodes are used for subsequent classiﬁcation. That is, prior\\nto sending inputs to remote realtime classiﬁcation models,\\nwe show how gating functions of biased mixtures can opt\\nto use modalities only when they are needed for accurate\\nclassiﬁcation (and we illustrate this in (c) of Figure 2). By\\ndoing so, we show how biased mixtures can learn to leverage\\nmodal variance to mitigate unnecessary trafﬁc between sensors\\nand remote classiﬁers.\\nWe evaluate on UCF-101 [45] and measure the cost asso-\\nciated with the spatial mode as F\\x02Hs\\x02Hs\\x02Ws\\x02K\\x023,\\nwhereFs= 2 is the number of RGB frames used, Hs= 360\\nandWs= 240 are the height and width of inputs, 3is the\\nnumber of color channels, and Kis the number of bytes to\\nstore ﬂoating point decimals. For the temporal model, we\\nmeasure the data cost as Ft\\x02Ht\\x02Wt\\x02K\\x022, where\\nFt= 150 is the number of frames used, Ht= 24 and\\nWt= 24 are the height and width of motion vector maps,\\nand2is the number of channels used to represent vertical\\nand horizontal motion. Importantly, we select spatial sampling\\nrates akin to those of [8] which sets Fs= 1,Ft\\x1510, and\\nthe proposal of [53] which sets Fs= 1,Ft\\x15100. This\\nis to meet complexity limits for realtime inference, where\\nthe benchmark models set modest sampling rates compared\\nto other exhaustive methods [7], which typically use dense\\noptical ﬂow approximations with Fs\\x1525andFt\\x15250.\\nMoreover, in implementing the benchmark model of Zhang\\net al. [53], we follow their method of upsampling 24\\x0224\\nmotion vector maps to 224\\x02224temporal mode inputs, and we\\nspeciﬁcally use a nearest-neighbours upsampling ﬁlter. Inputs\\nare upsampled after they are sent via the (\\x03)operator of Figure\\n2 (c), and therefore input shape parameters remain at Ht= 24\\nandWt= 24 when measuring data cost.\\nFor data-exhaustive action classiﬁcation, we use fusion\\nclassiﬁers which combine both modalities to predict actions\\nwith the highest possible accuracy. Fusion classiﬁers incur\\na data cost equal the sum of both modalities. We include\\nall modalities to train biased mixtures of experts, where\\nfMode ng=fTemporal;Spatial;Fusiong. Importantly, and to\\nallow for lower complexities of gating, inputs to the gating\\nmodel include only the temporal modes of videos, and spatial\\nmodes are not used. For all biased mixtures, we train for 80k\\nsteps and restrict the complexity of the gating function to\\nCg<108Mult-Add operations, where we detail the layer-\\nwise complexities of gating in Table VII.TABLE VI: Realtime action classiﬁcation on UCF-101 [45] of\\nbiased mixtures of experts and other benchmarks. Modalities\\nfMode ngand data costsfdngare reported for all experts.\\nModel Biasing MethodAccuracy (dt)(%) whendt=\\x1admaxdmax\\n2dmax\\n3\\nMV-3DCNN [8]Benchmark Experts [8]\\n88.079.0 77.9 80.9\\nProposed bEnforcement 82.0 80.4 83.5\\nSoft Regularization [41] 80.3 78.0 81.9\\nRandom Selection 78.8 77.3 81.3\\nEMV-CNN [53]Benchmark Experts [53]\\n85.676.6 75.5 78.7\\nProposed bEnforcement 80.2 79.2 81.3\\nSoft Regularization [41] 77.2 75.6 79.7\\nRandom Selection 75.7 74.9 79.0\\n——– ———-TABLE VII: Layer complexities Cof the gating model\\nf(x;Wg)for biased mixtures evaluated on realtime action\\nclassiﬁcation. Expert modalities are speciﬁed as fMode ng=\\nfTemporal;Spatial;FusiongandN= 3. Note that the gating\\nmodelf(x;Wg)ingests only temporal modalities of x.\\nLayer Type Filter Shape Stride Input ShapeC\\n(Mult-Adds)\\nConvolutional 3\\x023\\x02320\\x0264 2 24\\x0224\\x02320 3;363;840\\nFlatten Op. \\x00\\x00 11\\x0211\\x0264 —-\\nFully Connected 7744\\x021024\\x00 1\\x027744 7;929;856\\nFully Connected 1024\\x023\\x00 1\\x021024 3;072\\nIn Table VI we compare the performance of biased mixtures\\nagainst other benchmarks when using the spatial and temporal\\nclassiﬁers of [8] and [53] as baselines, and in Table VII we\\ndetail the performance of experts over their assigned input\\nsubsets as determined by G(x;Wg). From Table VI, we ﬁrst\\nnote that both biasing methods outperform random selection,\\nby up to 1%for soft regularization and up to 3:8%for bias\\nenforcement. This indicates that the biased mixture learns to\\ndiscern confusing classes for particular modalities to pass them\\nto others. Notably, when \\x16d=dmax\\n3= 860 kilobytes, bias\\nenforcement gives an accuracy 1:4%higher than that of the\\noptimized experts atdmax\\n2= 1290 kilobytes, which requires\\n430kilobytes more in data cost.\\nIn Figure 8 we show the relationship between \\x16dand action\\nclassiﬁcation accuracy for instances of bwhen biased mixtures\\nof MV-3DCNN [8] experts are used and the mode of each\\nexpert isfMode ng=fTemporal;Spatial;Fusiong. We ﬁrst\\nnote that, due to the low resolution of its inputs, the temporal\\nclassiﬁer requires the least amount of data and can predict\\nactions with an accuracy of 77:8%. By selecting among\\nthe three modes, both biasing methods outperform random\\nselection, with bias enforcement increasing accuracy by up\\nto3:4%for when \\x16d= 1032 kilobytes. Notably, and when\\nusing the temporal classiﬁer for 80% of videos at \\x16d= 1032\\nkilobytes (i.e., when b= [0:8;0:1;0:1]), bias enforcement is\\n1:6%more accurate than the spatial classiﬁer (which requires\\n811kilobytes more in data, equivalent to an increase of 78% in\\ndata cost). The latter shows the extent to which biased mixtures\\ncan improve performance by using modest amounts of data,\\neven compared to individual models that require substantially\\nmore in data cost.\\nTable VII shows how inputs are appropriately passed to\\nexperts for data-economic classiﬁcation. Speciﬁcally, it shows\\nhow biased mixtures learn to use the data-efﬁcient temporal\\nmodel for inputs that are easier to classify, where temporal\\nmodalities are likely to sufﬁce for accurate classiﬁcation.\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 10\\nFor example, this is evident when b1= 0:5andb1= 0:2,\\nwhere the temporal classiﬁer E1respectively gains +5:81\\nand+5:60percentile points in classifying its assigned inputs\\nwhen compared to its baseline accuracy measured over all\\ntest videos of UCF-101 [45]. On the other hand, Table\\nVII also shows how more difﬁcult inputs are passed to the\\nspatial and fusion classiﬁers, resulting in a modest loss of\\naccuracy when classifying their assigned inputs. Moreover,\\nTable VII highlights how bias enforcement is superior to\\nsoft regularization in assigning inputs to different modalities,\\nwhere this is evident across all values of b.\\nFig. 8: Realtime action classiﬁcation performance comparison\\nof biased mixtures of MV-3DCNN [8] experts, with expert\\nmodalitiesfMode ng=fTemporal,Spatial, Fusion g:\\nFig. 9: t-SNE [27] projections of 1024 UCF101 videos, where\\nfMode ng=fTemporal;Fusiongandb= [0:75;0:25]. In (a)\\ncolours indicate different classes, and in (b) mode assignments\\nare shown as 0or 1 for the temporal and fusion classiﬁers\\nrespectively. Zoom in to view in high-resolution .\\nTo visualize how different modalities are assigned to videos,\\nin Figure 9 we show two-dimensional t-SNE [27] projections\\nof1024 UCF101 examples as embedded by the last layer\\nof the temporal classiﬁer. We train via bias enforcement,\\nand for clarity of presentation, we use a biased mixture\\nof two modalities fMode ng=fTemporal;Fusiongwhere\\nb= [0:75;0:25]. In this way, we show the relation between\\ndifferent class labels and assigned modalities. In Figure 9\\nFig. 10: Percentile of videos assigned to the the fusion\\nclassiﬁer for the ﬁrst 30classes of UCF101 [45], where\\nfMode ng=fTemporal;Fusiongandb= [0:75;0:25].\\n(a) the middle region highlights instances of different classes\\nwhich are more entangled and therefore harder to classify. For\\na sample of instances, Figure 9 (b) shows modalities selected\\nby the biased mixture for action classiﬁcation. Notably, the\\nbiased mixture tends to select the data-exhaustive fusion mode\\nfor instances located in the entangled middle region , where\\ninputs are harder to classify (as indicated by label 1in (b)\\nof Figure 9), and temporal modes are predominantly used for\\nsufﬁciently isolated input clusters located outside the middle\\nregion (as indicated by label 0in (b) of Figure 9). That\\nis, Figure 9 shows how the biased mixture favors using the\\ntemporal classiﬁer for video clusters that are comparatively\\nisolated and easy to discern, while the fusion model is used\\nwhen videos are more entangled and harder to classify.\\nFor the same biased mixture that yields the t-SNE repre-\\nsentation of Figure 9, in Figure 10 we detail the classwise\\npercentile of videos assigned to the fusion classiﬁer. Evidently\\nfrom Figure 10, challenging inputs are typically sent to the\\ndata-exhaustive fusion mode when they contain: (i) signiﬁcant\\ncamera movement, leading to noise in underlying motion\\nﬂow (e.g., for 63% and57% of “Biking” and “Cliff Div-\\ning” instances, respectively), and (ii) relatively static scenes,\\nresulting in sparse motion vector maps (e.g., for 38% and\\n32% of “Apply Lipstick” and “Blow Dry Hair” instances,\\nrespectively). Hence, Figure 9 and Figure 10 show how\\nbiased expert mixtures can ﬁnd useful bifurcations of input\\nspace such that only necessary modalities are used for action\\nclassiﬁcation, and less data is used whenever possible.\\nV. C ONCLUSION\\nWe introduce biased expert utility in mixtures-of-experts\\nfor effective partitioning of input space to meet constraints\\non data availability at test time. We propose two methods\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 11\\nfor training biased mixtures, and evaluate their performance\\non multiple models for all investigated tasks. We show how\\nbiased mixtures are applicable to any situation wherein experts\\nvary in data requirement and performance, and demonstrate\\nthis on a wide range of computer vision tasks (we also make\\npublic a Tensorﬂow- 1:14implementation of biased mixtures\\ninhttps://github.com/UCL-Abbas/bmoe ). Our val-\\nidation shows that, especially for lower ranges of allowed\\ndata cost, biased mixtures signiﬁcantly outperform baseline\\nmodels optimized to meet the same constraints on available\\ndata. We also show how useful gating inferences that prioritise\\ndata economy can be realized with complexities that do not\\nexceed 108Mult-Add operations, which are feasible to run\\neven on embedded computation units (e.g., ARM Cortex-M7).\\nWithin contexts of distributed visual inference, and to meet\\ndifferent constraints on data transfer and bandwidth at test\\ntime, all of our observations and tests show the importance\\nof conditioning data utility for visual inference to the local\\nproximities and properties of inputs within their space. In\\nother words, the importance of doing so is applicable to all\\npresented vision tasks, and is likely to extend to other visual\\ninference tasks in order to mitigate unnecessary burdens on\\ncommunication resources and sensor hardware. We ﬁnally note\\nthat an important advantage of biased mixtures is the ﬂexibility\\nat which they can be applied, in that, biased mixtures do\\nnot modify their constituent experts, but rather augment their\\nfunction with an input preprocessing stage that allows for data-\\neconomic inference.\\nREFERENCES\\n[1] E. Agustsson and R. Timofte, “Ntire 2017 challenge on single image\\nsuper-resolution: Dataset and study,” in Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition Workshops , 2017,\\npp. 126–135.\\n[2] J. Ball ´e, V . Laparra, and E. P. Simoncelli, “End-to-end optimized image\\ncompression,” arXiv preprint arXiv:1611.01704 , 2016.\\n[3] E. Bengio, P.-L. Bacon, J. Pineau, and D. Precup, “Conditional\\ncomputation in neural networks for faster models,” arXiv preprint\\narXiv:1511.06297 , 2015.\\n[4] Y . Bengio, N. L ´eonard, and A. Courville, “Estimating or propagating\\ngradients through stochastic neurons for conditional computation,” arXiv\\npreprint arXiv:1308.3432 , 2013.\\n[5] K. H. Borgwardt, The Simplex Method: a probabilistic analysis .\\nSpringer Science & Business Media, 2012, vol. 1.\\n[6] S. Boyd and L. Vandenberghe, Convex optimization . Cambridge\\nuniversity press, 2004.\\n[7] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new\\nmodel and the kinetics dataset,” in Computer Vision and Pattern Recog-\\nnition (CVPR), 2017 IEEE Conference on . IEEE, 2017, pp. 4724–4733.\\n[8] A. Chadha, A. Abbas, and Y . Andreopoulos, “Video classiﬁcation with\\ncnns: Using the codec as a spatio-temporal activity sensor,” IEEE\\nTransactions on Circuits and Systems for Video Technology , vol. 29,\\nno. 2, pp. 475–485, 2017.\\n[9] S.-P. Chuah, N.-M. Cheung, and C. Yuen, “Layered coding for mobile\\ncloud gaming using scalable blinn-phong lighting,” IEEE Transactions\\non Image Processing , vol. 25, no. 7, pp. 3112–3125, 2016.\\n[10] E. L. Denton, S. Chintala, R. Fergus et al. , “Deep generative image\\nmodels using a laplacian pyramid of adversarial networks,” in Advances\\nin neural information processing systems , 2015, pp. 1486–1494.\\n[11] C. Dong, C. C. Loy, and X. Tang, “Accelerating the super-resolution\\nconvolutional neural network,” in European Conference on Computer\\nVision . Springer, 2016, pp. 391–407.\\n[12] C. Feichtenhofer, A. Pinz, and A. Zisserman, “Convolutional two-stream\\nnetwork fusion for video action recognition,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition , 2016, pp. 1933–\\n1941.[13] M. Fiedler, J. Nedoma, J. Ram ´ık, J. Rohn, and K. Zimmermann, Linear\\noptimization problems with inexact data . Springer Science & Business\\nMedia, 2006.\\n[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. Courville, and Y . Bengio, “Generative adversarial nets,” in\\nAdvances in neural information processing systems , 2014, pp. 2672–\\n2680.\\n[15] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing\\ndeep neural networks with pruning, trained quantization and huffman\\ncoding,” arXiv preprint arXiv:1510.00149 , 2015.\\n[16] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and con-\\nnections for efﬁcient neural network,” in Advances in neural information\\nprocessing systems , 2015, pp. 1135–1143.\\n[17] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in Proceedings of the IEEE conference on computer vision\\nand pattern recognition , 2016, pp. 770–778.\\n[18] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of\\ndata with neural networks,” science , vol. 313, no. 5786, pp. 504–507,\\n2006.\\n[19] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\\nT. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convo-\\nlutional neural networks for mobile vision applications,” arXiv preprint\\narXiv:1704.04861 , 2017.\\n[20] J. Huang, V . Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer,\\nZ. Wojna, Y . Song, S. Guadarrama et al. , “Speed/accuracy trade-offs for\\nmodern convolutional object detectors,” in IEEE CVPR , vol. 4, 2017.\\n[21] M. Jubran, A. Abbas, A. Chadha, and Y . Andreopoulos, “Rate-accuracy\\ntrade-off in video classiﬁcation with deep convolutional neural net-\\nworks,” IEEE Transactions on Circuits and Systems for Video Tech-\\nnology , 2018.\\n[22] Y . Li, D. Liu, H. Li, L. Li, Z. Li, and F. Wu, “Learning a convolutional\\nneural network for image compact-resolution,” IEEE Transactions on\\nImage Processing , vol. 28, no. 3, pp. 1092–1107, 2019.\\n[23] J. Lin, Y . Rao, J. Lu, and J. Zhou, “Runtime neural pruning,” in Advances\\nin Neural Information Processing Systems , 2017, pp. 2181–2191.\\n[24] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\\ncontext,” in European conference on computer vision . Springer, 2014,\\npp. 740–755.\\n[25] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and A. C.\\nBerg, “Ssd: Single shot multibox detector,” in European conference on\\ncomputer vision . Springer, 2016, pp. 21–37.\\n[26] M. Long, Z. Cao, J. Wang, and M. I. Jordan, “Conditional adversarial\\ndomain adaptation,” in Advances in Neural Information Processing\\nSystems , 2018, pp. 1640–1650.\\n[27] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” Journal\\nof machine learning research , vol. 9, no. Nov, pp. 2579–2605, 2008.\\n[28] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, “Adver-\\nsarial autoencoders,” arXiv preprint arXiv:1511.05644 , 2015.\\n[29] J. Martin, Y . Fu, N. Wourms, and T. Shaw, “Characterizing netﬂix band-\\nwidth consumption,” in 2013 IEEE 10th Consumer Communications and\\nNetworking Conference (CCNC) . IEEE, 2013, pp. 230–235.\\n[30] F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and L. V .\\nGool, “Practical full resolution learned lossless image compression,” in\\nProceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , 2019, pp. 10 629–10 638.\\n[31] L. Mescheder, S. Nowozin, and A. Geiger, “Adversarial variational\\nbayes: Unifying variational autoencoders and generative adversarial\\nnetworks,” in Proceedings of the 34th International Conference on\\nMachine Learning-Volume 70 . JMLR. org, 2017, pp. 2391–2400.\\n[32] J. Miano, Compressed image ﬁle formats: Jpeg, png, gif, xbm, bmp .\\nAddison-Wesley Professional, 1999.\\n[33] D. Minnen, J. Ball ´e, and G. D. Toderici, “Joint autoregressive and\\nhierarchical priors for learned image compression,” in Advances in\\nNeural Information Processing Systems , 2018, pp. 10 771–10 780.\\n[34] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu, “Pixel recurrent\\nneural networks,” arXiv preprint arXiv:1601.06759 , 2016.\\n[35] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation\\nlearning with deep convolutional generative adversarial networks,” arXiv\\npreprint arXiv:1511.06434 , 2015.\\n[36] G. Roelofs and R. Koman, PNG: the deﬁnitive guide . O’Reilly &\\nAssociates, Inc., 1999.\\n[37] J. T. Rolfe, “Discrete variational autoencoders,” arXiv preprint\\narXiv:1609.02200 , 2016.\\n[38] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma, “Pixelcnn++:\\nImproving the pixelcnn with discretized logistic mixture likelihood and\\nother modiﬁcations,” arXiv preprint arXiv:1701.05517 , 2017.\\nIEEE TRANSACTIONS ON IMAGE PROCESSING, TO APPEAR 12\\n[39] S. Sankaranarayanan, Y . Balaji, C. D. Castillo, and R. Chellappa, “Gen-\\nerate to adapt: Aligning domains using generative adversarial networks,”\\ninProceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , 2018, pp. 8503–8512.\\n[40] L. Sevilla-Lara, Y . Liao, F. Guney, V . Jampani, A. Geiger, and M. J.\\nBlack, “On the integration of optical ﬂow and action recognition,” arXiv\\npreprint arXiv:1712.08416 , 2017.\\n[41] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\\nand J. Dean, “Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538 , 2017.\\n[42] W. Shi, J. Caballero, F. Husz ´ar, J. Totz, A. P. Aitken, R. Bishop,\\nD. Rueckert, and Z. Wang, “Real-time single image and video super-\\nresolution using an efﬁcient sub-pixel convolutional neural network,” in\\nProceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , 2016, pp. 1874–1883.\\n[43] L. Sifre and S. Mallat, “Rigid-motion scattering for texture classiﬁca-\\ntion,” arXiv preprint arXiv:1403.1687 , 2014.\\n[44] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.\\n[45] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101 human\\nactions classes from videos in the wild,” arXiv preprint arXiv:1212.0402 ,\\n2012.\\n[46] S. Srivastava and B. Lall, “Superresolution based medical image com-\\npression for mobile platforms,” in Workshop on Machine Learning for\\nHealthCare , 2015.\\n[47] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, and L. Zhang,\\n“Ntire 2017 challenge on single image super-resolution: Methods and\\nresults,” in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition Workshops , 2017, pp. 114–125.\\n[48] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discrim-\\ninative domain adaptation,” in Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition , 2017, pp. 7167–7176.\\n[49] A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves\\net al. , “Conditional image generation with pixelcnn decoders,” in Ad-\\nvances in neural information processing systems , 2016, pp. 4790–4798.\\n[50] W. Wang, Y . Huang, Y . Wang, and L. Wang, “Generalized autoencoder:\\nA neural network framework for dimensionality reduction,” in Proceed-\\nings of the IEEE conference on computer vision and pattern recognition\\nworkshops , 2014, pp. 490–497.\\n[51] W. Wang, J. Shen, and L. Shao, “Video salient object detection via\\nfully convolutional networks,” IEEE Transactions on Image Processing ,\\nvol. 27, no. 1, pp. 38–49, 2018.\\n[52] W. K. Wong, Z. Lai, J. Wen, X. Fang, and Y . Lu, “Low-rank embedding\\nfor robust image feature extraction,” IEEE Transactions on Image\\nProcessing , vol. 26, no. 6, pp. 2905–2917, 2017.\\n[53] B. Zhang, L. Wang, Z. Wang, Y . Qiao, and H. Wang, “Real-time\\naction recognition with deeply transferred motion vector cnns,” IEEE\\nTransactions on Image Processing , vol. 27, no. 5, pp. 2326–2339, 2018.\\n[54] Y . Zhang, Q. Fan, F. Bao, Y . Liu, and C. Zhang, “Single-image super-\\nresolution based on rational fractal interpolation,” IEEE Transactions on\\nImage Processing , vol. 27, no. 8, pp. 3782–3797, 2018.\\n[55] L. F. W. Z. Zhaoyang Zhang, Zhanghui Kuang, “Temporal sequence\\ndistillation: Towards few-frame action recognition in videos,” in Arxiv:\\n1808.05085 , 2018.'},\n",
       " {'entry_id': 'http://arxiv.org/abs/2302.02043v1',\n",
       "  'title': 'mixdistreg: An R Package for Fitting Mixture of Experts Distributional Regression with Adaptive First-order Methods',\n",
       "  'abstract': \"This paper presents a high-level description of the R software package\\nmixdistreg to fit mixture of experts distributional regression models. The\\nproposed framework is implemented in R using the deepregression software\\ntemplate, which is based on TensorFlow and follows the neural structured\\nadditive learning principle. The software comprises various approaches as\\nspecial cases, including mixture density networks and mixture regression\\napproaches. Various code examples are given to demonstrate the package's\\nfunctionality.\",\n",
       "  'authors': [arxiv.Result.Author('David Rügamer')],\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2302.02043v1',\n",
       "  'doi': None,\n",
       "  'updated': datetime.datetime(2023, 2, 4, 0, 38, 3, tzinfo=datetime.timezone.utc),\n",
       "  'published': datetime.datetime(2023, 2, 4, 0, 38, 3, tzinfo=datetime.timezone.utc),\n",
       "  'categories': ['stat.CO'],\n",
       "  'text': 'mixdistreg: An R Package for Fitting\\nMixture of Experts Distributional Regression\\nwith Adaptive First-order Methods\\nDavid R\\x7f ugamer\\nDepartment of Statistics, TU Dortmund\\nDepartment of Statistics, LMU Munich\\nMunich Center for Machine Learning\\nFebruary 7, 2023\\nAbstract\\nThis paper presents a high-level description of the R software package mixdistreg to \\x0ct\\nmixture of experts distributional regression models. The proposed framework is implemented\\nin R using the deepregression software template, which is based on TensorFlow and follows the\\nneural structured additive learning principle. The software comprises various approaches as\\nspecial cases, including mixture density networks and mixture regression approaches. Various\\ncode examples are given to demonstrate the package\\'s functionality.\\n1 Summary\\nMixture models are a common choice when data stems from di\\x0berent sub-populations, but only\\nthe pooled data with unknown membership is observed. Models are typically estimated using the\\nEM algorithm. As the log-likelihood of mixture models is, in general, not convex [14], existing op-\\ntimization techniques are vulnerable to ending up in local optima [5]. In contrast, mixture density\\nnetworks [MDN; 4] that can also be considered as a type of mixture regression models, are success-\\nfully optimized with \\x0crst-order (gradient descent) methods. [21] therefore proposed the mixture of\\nexperts distributional regression, a combination of interpretable mixtures of distributional regres-\\nsion models and MDNs to facilitate robust estimation and extend mixtures of regression models to\\nthe distribution regression case. The framework in [21] is based on a neural network formulation\\nimplemented in mixdistreg (https://github.com/neural-structured-additive-learning/\\nmixdistreg ), which is presented in this paper.\\n1arXiv:2302.02043v1  [stat.CO]  4 Feb 2023\\n2 Statement of need\\nCommon EM optimization routines are limited in their \\rexibility to specify mixtures of (many)\\npotentially di\\x0berent distributions, cannot cope with large amounts of data and, in particular, are\\nnot robust in high dimensions. First-order methods used in deep learning are applied on mini-\\nbatches of data allowing large data set applications and can be used in a generic fashion for all\\nmodel classes. In order to obtain a scalable framework, we implement the proposed framework by\\n[21] in R [15] using the software template deepregression [19] which relies on TensorFlow [1].\\nThe template described in [19] provides the basis for many other neural network-based modeling\\napproaches referred to as neural structured additive learning (NSAL) . Examples include neural-\\nbased and autoregressive transformation models [2, 8, 9, 17], survival regression in neural networks\\n[3, 10, 11], distributional regression [20] or scalable factor models and factorizations [18, 16]. The\\nNSAL principle is also followed by mixdistreg which allows combining it with other neural-based\\napproaches straightforwardly. The software further comprises many di\\x0berent other approaches\\nas a special case, including neural density networks [13], MDNs, and various mixture regression\\napproaches (with penalized smooth e\\x0bects) as proposed in [12, 7, 22].\\n3 Implementation Details\\nIn the following, we brie\\ry describe the main function mixdistreg of the eponymous package.\\nGiven a realization yof the outcome of interest Yand featuresx2Rp, the package models\\nthe following density:\\nfYjx(yjx) =MX\\nm=1\\x19m(x)fm(yj\\x12m(x)); (1)\\nwherefmare density functions of (potentially di\\x0berent) distributions Fm;m= 1;:::;M with\\nparameters\\x12m= (\\x12m;1;:::;\\x12m;km)>, and\\x19m2[0;1] are mixture weights that sum to 1.\\nKey Features: mixdistreg allows, among other things, for\\n•mixtures of the same parametric distributions ( Fm\\x11F\\x038m2f1;:::;Mg);\\n•mixtures of di\\x0berent parametric distributions with the same domain ( Fm6=Fnfor some\\nm;n2f1;:::;Mg;m6=n);\\n•mixture components ( fm) to be chosen from a variety of distributions [see 19];\\n•de\\x0cning distribution parameters via an additive predictor \\x11and a link function g(i.e.,\\ng\\x001(\\x12(x)) =\\x11(x) and additive structure \\x11(x) =PJ\\nj=1\\x11j(x))\\n•individual additive predictors for di\\x0berent mixture components m(i.e.,g\\x001(\\x12m(x)) =\\x11m(x));\\n•individual additive predictors and link functions for di\\x0berent distribution parameters k\\nwithin one mixture component m(i.e.,g\\x001\\nm;k(\\x12m;k(x)) =\\x11m;k(x));\\n2\\n•a separate model de\\x0cnition that relates the categorical distribution to features of all sorts\\n(i.e.,g\\x001\\n\\x19(\\x19m(x)) =PJ\\x19\\nj=1\\x11m;j(x));\\n•mixtures with one-point degenerate distributions (i.e., ( fm(y) = 1y=a;a2R) as, e.g., used\\nin zero-in\\rated models [see, e.g., 6].\\nA formula interface for the di\\x0berent (or same) mixture components follows the intuitive S-like\\nformula interface to de\\x0cne the additive predictor functions \\x11. These can be de\\x0cned \\rexibly,\\nincluding linear e\\x0bects (e.g., \\x11j(x) =x>\\x0c), smooth terms, L1-penalized sparse e\\x0bects, (deep)\\nneural network components or a combination thereof [for details see 19].\\nConvenience Functions: Several convenience functions exist, wrapping this main function:\\n•sammer : a simpler interface for samemixture(distributional) regression\\n•inflareg : a simpler interface for in\\ratedregression models forming a mixture of one or\\nmore degenerate distribution(s) and another parametric distribution;\\n•zinreg, oinreg, zoinreg : convenience functions, in turn, wrapping inflareg to allow for\\na simple model de\\x0cnition of zero-in\\rated, one-in\\rated and zero-and-one-in\\rated regression\\nmodels.\\nNext to these modeling functions, the package provides a function genmixdist maker to allow\\nfor more complex user-de\\x0cned mixtures (e.g, mixtures of various parametric distributions and one-\\npoint mass distributions), and methods for plotting ( plot ), obtaining model coe\\x0ecients ( coef ),\\ncalculating posterior probabilities for all clusters ( getpis) and extracting statistics of the mixture\\ncomponents ( getstats mixcomps ).\\n4 Examples\\nThe following examples demonstrate the interface of mixdistreg .\\n4.1 Mixture of Linear Regressions\\nWe start with a simple special case, the mixture of linear regressions, and compare the results\\nwith flexmix [12], a well-established package for mixtures of regression models in R.\\nlibrary(flexmix)\\nlibrary(dplyr)\\nlibrary(mixdistreg)\\n# Load the data\\nset.seed(42)\\nNPreg <- ExNPreg(n = 1000)\\nnr_comps <- 2\\nNPreg$xsq <- NPreg$x^2\\n3\\n# Fit a mixture of regression models with mixtools\\nset.seed(42)\\nfm_mod <- flexmix(yn ~ x + xsq,\\ndata = NPreg,\\nk = nr_comps)\\n# Fitted values\\npred_fm <- fm_mod %>% predict()\\n# Fit a mixture of normal regression with mixdistreg\\ndr_mod <- sammer(y = NPreg$yn,\\nfamily = \"normal\",\\nnr_comps = nr_comps,\\nlist_of_formulas = list(mean = ~ 1 + x + xsq,\\nscale = ~1),\\ndata = NPreg,\\noptimizer = optimizer_rmsprop(learning_rate = 0.01),\\ntf_seed = 42\\n)\\n# Train network\\ndr_mod %>% fit(epochs = 5000L,\\nvalidation_split = 0.1,\\npatience = 100L,\\nearly_stopping = TRUE,\\nverbose = FALSE)\\n# Fitted means of normal distributions\\npred_dr <- dr_mod %>% get_stats_mixcomps(what = \"means\")\\n# Compare\\nplot(NPreg$yn ~ NPreg$x)\\nfor(i in 1:nr_comps) f\\npoints(pred_fm[[i]] ~ NPreg$x, col=\"blue\", pch=\"x\")\\npoints(pred_dr[,i] ~ NPreg$x, col=\"red\", pch=\"x\")\\ng\\nlegend(\"bottomright\", pch=\"x\", col=c(\"blue\", \"red\"),\\nlegend = c(\"flexmix\", \"mixdistreg\"))\\nThe results of the above code are shown in Figure 1.\\n4.2 Mixture of Di\\x0berent Regressions\\nThe previous data could alternatively also be \\x0ctted with a mixture of di\\x0berent distributions. We\\nhere choose the normal and Laplace distribution.\\n4\\n0 2 4 6 8 1001020304050\\nNPreg$xNPreg$ynxxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxxx\\nxx\\nxxxxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxx xx\\nx xx\\nxxx\\nxxxxx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nx\\nxxxx\\nxx\\nxxx\\nxx\\nxx x\\nxx\\nx\\nxxxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nxxxxx\\nxx\\nxxxxx\\nxx x\\nxxx xx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxx\\nxx\\nx x\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxxxxxxx\\nxxx\\nxx\\nxxxx xx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxxx x\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx x\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxxx\\nx\\nxx\\nxx\\nx xx\\nxxxxx\\nxxxxxxxx\\nxx\\nxx\\nxx\\nxx\\nxxxxxx\\nxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nxxx\\nx\\nxxxxxx\\nx\\nxxx\\nxxx\\nxx xx\\nx\\nxx x\\nxxxx\\nx\\nxxxxxx\\nxxxx\\nxx\\nx\\nxx\\nx\\nxx\\nxx\\nx\\nxxx xx\\nxx\\nx\\nx\\nxx\\nxxxx\\nxxx\\nxxx\\nx\\nxxx x\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nxxx xxxxx\\nx x\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx\\nxxxxxx\\nxxx\\nxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxx x\\nxxx xx\\nxxxx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nx\\nxxxx\\nx\\nx xx\\nx\\nxx\\nxx xxx\\nx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nx\\nxx xxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxxx x\\nxxxx\\nx xx\\nxx\\nx\\nx xxx\\nxxxx\\nx\\nxxx\\nxx\\nxxx\\nxx\\nxx\\nxxxxx\\nxxx\\nx\\nxxxx\\nx\\nxxxxx\\nxx\\nx\\nxxxx\\nxx x\\nxx\\nxxxx\\nxxx\\nxxxx\\nx\\nxx\\nxx x\\nxx\\nxx\\nxxx\\nxxx\\nxxxxx\\nx\\nxx\\nx\\nx\\nxx\\nxx xx x\\nxxx\\nx\\nxx\\nxxxxxxx\\nxx\\nxx\\nx\\nxxxxx\\nx xx\\nxxx\\nxxx\\nxx\\nxxxx\\nxxxx\\nxxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxxxx x x\\nxxx xxx xx\\nx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nx\\nxxx\\nxxx x x\\nxx\\nxx\\nxxxx\\nxxx x\\nxx\\nxxx x\\nxxxx\\nxx\\nxx\\nxxx\\nxxxxx\\nxx\\nxxx\\nx\\nxxx\\nxx\\nx x\\nxx\\nxx x\\nxx xx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nxxx\\nxxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nxxxx\\nx\\nx\\nxxxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxxxxx xx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxx\\nxxxx\\nx xx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxxxxx\\nxx\\nxxxxxxxx xx\\nxxxx\\nx\\nxx\\nxx\\nxx\\nx\\nx xx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxxxx\\nxx\\nxxxx x\\nxxx x\\nxx\\nxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxxx\\nxxx\\nxxxx x\\nx\\nxx\\nx\\nx xxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxxx\\nx xx\\nxxx\\nxxx\\nxx\\nx\\nxx x\\nxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxxx\\nxx x\\nx\\nxxx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxxxx\\nxxx\\nx xxxxx\\nxx\\nxx xx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nx x\\nxxx\\nxxxxxxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxxx\\nxxx\\nx xx\\nx\\nxx x\\nxxxxx\\nxxx\\nx\\nxx\\nxx\\nxxxxxx\\nxx\\nxxxx\\nxxxx\\nxx x\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nxx\\nxxxx\\nxxx\\nxx xxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx x\\nx\\nx\\nxxx\\nxxxx\\nxxxx x\\nxxxxx\\nxx\\nxx x\\nxxx x\\nxx\\nx\\nxx\\nxx\\nx\\nxxxx xxxx\\nxx\\nxx\\nxx\\nxxxx x\\nxx\\nxx\\nx\\nxxx\\nxxx\\nxx x x\\nxx x\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxxxx xx\\nxxxx\\nx\\nxxx xx\\nxx\\nx\\nxx\\nxx\\nx\\nxx\\nxxxxxx\\nxxx\\nxxxx\\nx xx\\nxxx\\nxxx\\nx\\nx\\nxxx\\nxx\\nxx\\nxx\\nx\\nxxxxxx\\nxx\\nx\\nx xxxxxx\\nx\\nxx x\\nxx\\nxx xx\\nxx\\nxxxx\\nxx\\nxx\\nxxxx\\nxx x\\nx\\nxxxx\\nx\\nxx\\nxxx\\nx xx\\nxxx\\nxxxx\\nx\\nxxxx x\\nxx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxxxxx\\nxxx\\nxxx\\nxx\\nxxxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxxxxxx\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nx\\nxx\\nx\\nx xxx\\nxxx\\nx\\nxxxx x\\nx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxxxx\\nxxxx\\nxx\\nxx x\\nx\\nxx\\nxx\\nx\\nx xx\\nxx\\nxx\\nxx xx\\nx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxxx\\nxx\\nxxx\\nxxx\\nx xxxxxx\\nxxx\\nx\\nxxx\\nxxx\\nxxxxx\\nxxxx\\nx\\nxx\\nxxxxxxx\\nxxxx\\nxxx\\nx\\nxx\\nxxx xxx\\nxx\\nxxx\\nx\\nxxxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nx\\nxxxxx\\nxxx\\nxxx xx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxxx\\nxxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxxxxx\\nxx\\nxxxx\\nx\\nxx x\\nx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxxxx\\nx\\nxxxxx x\\nx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxxx\\nxx\\nxxxxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxx xx\\nx xx\\nxxx\\nxxxxx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nx\\nxxxx\\nxx\\nxxx\\nxx\\nxx x\\nxx\\nx\\nxxxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nxxxxx\\nxx\\nxxxxx\\nxx x\\nxxx xx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxxxxxxx\\nxxx\\nxx\\nxxxx xx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxxx x\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx xx\\nxxx x\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxxx\\nx\\nxx\\nxx\\nx xx\\nxxxxx\\nxxxxxxxx\\nxx\\nxx\\nxx\\nxx\\nxxxxxx\\nxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nxxx\\nx\\nxxxxxx\\nx\\nxxx\\nxxx\\nxx xx\\nx\\nxx x\\nxxxx\\nx\\nxxxxxx\\nxxxx\\nxx\\nx\\nxx\\nx\\nxx\\nxx\\nx\\nxxx xx\\nxx\\nx\\nx\\nxx\\nxxxx\\nxxx\\nxxx\\nx\\nxxx x\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nxxx xxxxx\\nx x\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx\\nxxxxxx\\nxxx\\nxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxx x\\nxxx xx\\nxxxx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nx\\nxxxx\\nx\\nx xx\\nx\\nxx\\nxx xxx\\nx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nx\\nxx xxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxxx x\\nxxxx\\nx xx\\nxx\\nx\\nx xxx\\nxxxx\\nx\\nxxx\\nxx\\nxxx\\nxx\\nxx\\nxxxxx\\nxxx\\nxxxxx\\nx\\nxxxxx\\nxx\\nx\\nxxxx\\nxx x\\nxx\\nxxxx\\nxxx\\nxxxx\\nx\\nxx\\nxx x\\nxx\\nxx\\nxxx\\nxxx\\nxxxxx\\nx\\nxx\\nx\\nx\\nxx\\nxx xx x\\nxxx\\nx\\nxx\\nxxxxxxx\\nxx\\nxx\\nx\\nxxxxx\\nx xx\\nxxx\\nxxx\\nxx\\nxxxx\\nxxxx\\nxxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxxxx x x\\nxxx xxx xx\\nx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nx\\nxxx\\nxxx x x\\nxx\\nxx\\nxxxx\\nxxx x\\nxx\\nxxx x\\nxxxx\\nxx\\nxx\\nxxx\\nxxxxx\\nxx\\nxxx\\nx\\nxxx\\nxx\\nx x\\nxx\\nxx x\\nxx xx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nxxx\\nxxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nxxxx\\nx\\nx\\nxxxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxxxxx xx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxx\\nxxxx\\nx xx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxxxxx\\nxx\\nxxxxxxxx xx\\nxxxx\\nx\\nxx\\nxx\\nxx\\nx\\nx xx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxxxx\\nxx\\nxxxx x\\nxxx x\\nxx\\nxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxxx\\nxxx\\nxxxx x\\nx\\nxx\\nx\\nx xxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxxx\\nx xx\\nxxx\\nxxx\\nxx\\nx\\nxx x\\nxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxxx\\nxx x\\nx\\nxxx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxxxx\\nxxx\\nx xxxxx\\nxx\\nxx xx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nx x\\nxxx\\nxxxxxxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxxx\\nxxx\\nx xx\\nx\\nxx x\\nxxxxx\\nxxx\\nx\\nxx\\nxx\\nxxxxxx\\nxx\\nxxxx\\nxxxx\\nxx x\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nxx\\nxxxx\\nxxx\\nxx xxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx x\\nx\\nx\\nxxx\\nxxxx\\nxxxx x\\nxxxxx\\nxx\\nxx x\\nxxx x\\nxx\\nx\\nxx\\nxx\\nx\\nxxxx xxxx\\nxx\\nxx\\nxx\\nxxxx x\\nxx\\nxx\\nx\\nxxx\\nxxx\\nxx x x\\nxx x\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxxxx xx\\nxxxx\\nx\\nxxx xx\\nxx\\nx\\nxx\\nxx\\nx\\nxx\\nxxxxxx\\nxxx\\nxxxx\\nx xx\\nxxx\\nxxx\\nx\\nx\\nxxx\\nxx\\nxx\\nxx\\nx\\nxxxxxx\\nxx\\nx\\nx xxxxxx\\nx\\nxx x\\nxx\\nxx xx\\nxx\\nxxxx\\nxx\\nxx\\nxxxx\\nxx x\\nx\\nxxxx\\nx\\nxx\\nxxx\\nx xx\\nxxx\\nxxxx\\nx\\nxxxx x\\nxx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxxxxx\\nxxx\\nxxx\\nxx\\nxxxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxxxxxx\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nx\\nxx\\nx\\nx xxx\\nxxx\\nx\\nxxxx x\\nx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxxxx\\nxxxx\\nxx\\nxx x\\nx\\nxx\\nxx\\nx\\nx xx\\nxx\\nxx\\nxx xx\\nx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxxx\\nxx\\nxxx\\nxxx\\nx xxxxxx\\nxxx\\nx\\nxxx\\nxxx\\nxxxxx\\nxxxx\\nx\\nxx\\nxxxxxxx\\nxxxx\\nxxx\\nx\\nxx\\nxxx xxx\\nxxxxx\\nx\\nxxxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nx\\nxxxxx\\nxxx\\nxxx xx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxxx\\nxxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxxxxx\\nxx\\nxxxx\\nx\\nxx x\\nx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxxxx\\nx\\nxxxxx x\\nx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxxxx\\nxxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxxx\\nxx\\nx\\nxxxxx\\nx\\nxx\\nxxx\\nx\\nx\\nxxxx\\nxx\\nxxx\\nxx\\nxxxx\\nx\\nx\\nxxxx\\nxxxx\\nx\\nxxx\\nxx\\nxxxxx\\nxx\\nx\\nxxx\\nx\\nxxx\\nxxxx\\nxxxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nx\\nx\\nxxxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nxxxx\\nx\\nx\\nxxxx\\nx\\nxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxx\\nx\\nxxx\\nxx\\nx\\nx\\nxxx\\nxx\\nxx\\nx\\nxx\\nx\\nx\\nxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nxx\\nxxxxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nxxx\\nxxxx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nxxxx\\nxx\\nxxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxxxx\\nxx\\nx\\nxxx\\nx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nx\\nx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nx\\nx\\nxxxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxxxx\\nx\\nxxxx\\nx\\nx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nxxxxx\\nx\\nxx\\nxxx\\nx\\nxxx\\nx\\nxxx\\nxxxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nx\\nx\\nxxxxx\\nxx\\nx\\nxx\\nx\\nxx\\nxxx\\nx\\nxxxxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxx\\nx\\nx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxxxx\\nxx\\nxxxx\\nxx\\nxx\\nx\\nxxxxx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxxxxxx\\nxxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nx\\nxxxxx\\nxxx\\nx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxxxxxxxx\\nxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxxxxxx\\nxxx\\nx\\nx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxxx\\nx\\nxx\\nxxxxx\\nxxxx\\nx\\nxxx\\nx\\nx\\nxx\\nxxxx\\nxxxx\\nx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxxx\\nx\\nx\\nxxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxxxx\\nxxx\\nx\\nx\\nxxxx\\nxxx\\nxx\\nxxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxxxx\\nxxxxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nx\\nxx\\nxxxx\\nxxxx\\nxx\\nx\\nxx\\nxx\\nx\\nx\\nxx\\nxx\\nx\\nxxxx\\nxxx\\nx\\nxxx\\nxxx\\nxxxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxxxx\\nxxx\\nx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxxx\\nxxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nxxxx\\nxxx\\nx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nx\\nxx\\nxx\\nxxx\\nxxx\\nxxxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxxx\\nx\\nxxxx\\nxxxxx\\nxx\\nxxx\\nxx\\nxxxxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxx\\nxx\\nx\\nxx\\nx\\nxxxxx\\nx\\nxxx\\nxxxxx\\nxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxxx\\nxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nx\\nxxxxx\\nxx\\nxx\\nxx\\nxxxxx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nx\\nx\\nxx\\nxx\\nx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx\\nxx\\nx\\nx\\nx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxx\\nxxxx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nxxxx\\nx\\nxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx\\nx\\nxxxxxxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxxxxxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxxx\\nxxx\\nx\\nx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxxxx\\nxxx\\nx\\nx\\nxx\\nxx\\nx\\nxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxxxxx\\nx\\nxxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxxxx\\nxxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxxx\\nxx\\nx\\nxxxxx\\nx\\nxx\\nxxx\\nx\\nx\\nxxxx\\nxx\\nxxx\\nxx\\nxxxx\\nx\\nx\\nxxxx\\nxxxx\\nx\\nxxx\\nxx\\nxxxxx\\nxx\\nx\\nxxx\\nx\\nxxx\\nxxxx\\nxxxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nx\\nx\\nxxxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nxxxxx\\nx\\nxxxx\\nx\\nxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxx\\nx\\nxxx\\nxx\\nx\\nx\\nxxx\\nxx\\nxx\\nx\\nxx\\nx\\nx\\nxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nxx\\nxxxxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nxxx\\nxxxx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nxxxx\\nxx\\nxxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxxxx\\nxx\\nx\\nxxx\\nx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nx\\nx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nx\\nx\\nxxxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxxxx\\nx\\nxxxx\\nx\\nx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nxxxxx\\nx\\nxx\\nxxx\\nx\\nxxx\\nx\\nxxx\\nxxxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nx\\nx\\nxxxxx\\nxx\\nx\\nxx\\nx\\nxx\\nxxx\\nx\\nxxxxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxx\\nx\\nx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxxxx\\nxx\\nxxxx\\nxx\\nxx\\nx\\nxxxxx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxxxxxx\\nxxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nx\\nxxxxx\\nxxx\\nx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxxxxxxxx\\nxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxxxxxx\\nxxx\\nx\\nx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxxxx\\nxx\\nxxxxx\\nxxxx\\nx\\nxxx\\nx\\nx\\nxx\\nxxxx\\nxxxx\\nx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxxx\\nx\\nx\\nxxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxxxx\\nxxx\\nx\\nx\\nxxxx\\nxxx\\nxx\\nxxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxxxx\\nxxxxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nx\\nxx\\nxxxx\\nxxxx\\nxx\\nx\\nxx\\nxx\\nx\\nx\\nxx\\nxx\\nx\\nxxxx\\nxxxx\\nxxx\\nxxx\\nxxxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxxxx\\nxxx\\nx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxxx\\nxxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nxxxx\\nxxx\\nx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nx\\nxx\\nxx\\nxxx\\nxxx\\nxxxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxxxx\\nxxx\\nx\\nxxxx\\nxxxxx\\nxx\\nxxx\\nxx\\nxxxxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxx\\nxx\\nx\\nxx\\nx\\nxxxxx\\nx\\nxxx\\nxxxxx\\nxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxxxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nx\\nxxxxx\\nxx\\nxx\\nxx\\nxxxxx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nx\\nx\\nxx\\nxx\\nx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx\\nxx\\nx\\nx\\nx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxx\\nxxxx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nxxxx\\nx\\nxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx\\nx\\nxxxxxxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxxxxxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxxx\\nxxx\\nx\\nx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxxxx\\nxxx\\nx\\nx\\nxx\\nxx\\nx\\nxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxxxxx\\nx\\nxxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxxx\\nxxx\\nx\\nxxxx\\nx\\nxflexmix\\nmixdistregFigure 1: Comparison of flexmix (blue) and mixdistreg (red) results on the NPreg data showing\\na mixture of two trends in the variable x, which are found by both methods.\\n# Define a different distribution\\ndr_mod2 <- mixdistreg(y = NPreg$yn + 1,\\nfamilies = c(\"normal\", \"laplace\"),\\nnr_comps = nr_comps,\\nlist_of_formulas = list(\\n# parameters for normal\\nmean = ~ 1 + x + xsq,\\nscale = ~1,\\n# parameters for laplace\\nlocation = ~ 1 + x + xsq,\\nscale = ~1\\n),\\ndata = NPreg,\\noptimizer = optimizer_rmsprop(\\nlearning_rate = 0.01),\\ntf_seed = 42\\n)\\n# Train network\\ndr_mod2 %>% fit(epochs = 5000L,\\nvalidation_split = 0.1,\\npatience = 100L,\\n5\\nearly_stopping = TRUE,\\nverbose = FALSE)\\n# Get estimated means distributions\\npred_dr2 <- dr_mod2 %>% get_stats_mixcomps(what = \"means\")\\nplot(NPreg$yn ~ NPreg$x)\\nfor(i in 1:nr_comps) f\\npoints(pred_dr2[,i] ~ NPreg$x, col=c(\"blue\",\"red\")[i], pch=\"x\")\\ng\\nlegend(\"bottomright\", pch=\"x\", col=c(\"blue\", \"red\"),\\nlegend = c(\"normal\", \"laplace\"))\\nThe results of the above code are shown in Figure 2.\\n0 2 4 6 8 1001020304050\\nNPreg$xNPreg$ynxxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxxx\\nxx\\nxxxxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxx xx\\nx xx\\nxxx\\nxxxxx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nx\\nxxxx\\nxx\\nxxx\\nxx\\nxx x\\nxx\\nx\\nxxxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nxxxxx\\nxx\\nxxxxx\\nxx x\\nxxx xx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxx\\nxx\\nx x\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxxxxxxx\\nxxx\\nxx\\nxxxx xx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxxx x\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx x\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxxx\\nx\\nxx\\nxx\\nx xx\\nxxxxx\\nxxxxxxxx\\nxx\\nxx\\nxx\\nxx\\nxxxxxx\\nxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nxxx\\nx\\nxxxxxx\\nx\\nxxx\\nxxx\\nxx xx\\nx\\nxx x\\nxxxx\\nx\\nxxxxxx\\nxxxx\\nxx\\nx\\nxx\\nx\\nxx\\nxx\\nx\\nxxx xx\\nxx\\nx\\nx\\nxx\\nxxxx\\nxxx\\nxxx\\nx\\nxxx x\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nxxx xxxxx\\nx x\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx\\nxxxxxx\\nxxx\\nxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxx x\\nxxx xx\\nxxxx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nx\\nxxxx\\nx\\nx xx\\nx\\nxx\\nxx xxxx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nx\\nxx xxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxxx x\\nxxxx\\nx xx\\nxx\\nx\\nx xxx\\nxxxx\\nx\\nxxx\\nxx\\nxxx\\nxx\\nxx\\nxxxxx\\nxxx\\nx\\nxxxx\\nx\\nxxxxx\\nxx\\nx\\nxxxx\\nxx x\\nxx\\nxxxx\\nxxx\\nxxxx\\nx\\nxx\\nxx x\\nxx\\nxx\\nxxx\\nxxx\\nxxxxx\\nx\\nxx\\nx\\nx\\nxx\\nxx xx x\\nxxx\\nx\\nxx\\nxxxxxxx\\nxx\\nxx\\nx\\nxxxxx\\nx xx\\nxxx\\nxxx\\nxx\\nxxxx\\nxxxx\\nxxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxxxx x x\\nxxx xxx xx\\nx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nx\\nxxx\\nxxx x x\\nxx\\nxx\\nxxxx\\nxxx x\\nxx\\nxxx x\\nxxxx\\nxx\\nxx\\nxxx\\nxxxxx\\nxx\\nxxx\\nx\\nxxx\\nxx\\nx x\\nxx\\nxx x\\nxx xx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nxxx\\nxxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nxxxx\\nx\\nx\\nxxxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxxxxx xx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxx\\nxxxx\\nx xx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxxxxx\\nxx\\nxxxxxxxx xx\\nxxxx\\nx\\nxx\\nxx\\nxx\\nx\\nx xx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxxxx\\nxx\\nxxxx x\\nxxx x\\nxx\\nxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxxx\\nxxx\\nxxxx x\\nx\\nxx\\nx\\nx xxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxxx\\nx xx\\nxxx\\nxxx\\nxx\\nx\\nxx x\\nxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxxx\\nxx x\\nx\\nxxx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxxxx\\nxxx\\nx xxxxx\\nxx\\nxx xx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nx x\\nxxx\\nxxxxxxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxxx\\nxxx\\nx xx\\nx\\nxx x\\nxxxxx\\nxxx\\nx\\nxx\\nxx\\nxxxxxx\\nxx\\nxxxx\\nxxxx\\nxx x\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nxx\\nxxxx\\nxxx\\nxx xxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx x\\nx\\nx\\nxxx\\nxxxx\\nxxxx x\\nxxxxx\\nxx\\nxx x\\nxxx x\\nxx\\nx\\nxx\\nxx\\nx\\nxxxx xxxx\\nxx\\nxx\\nxx\\nxxxx x\\nxx\\nxx\\nx\\nxxx\\nxxx\\nxx x x\\nxx x\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxxxx xx\\nxxxx\\nx\\nxxx xx\\nxx\\nx\\nxx\\nxx\\nx\\nxx\\nxxxxxx\\nxxx\\nxxxx\\nx xx\\nxxx\\nxxx\\nx\\nx\\nxxx\\nxx\\nxx\\nxx\\nx\\nxxxxxx\\nxx\\nx\\nx xxxxxx\\nx\\nxx x\\nxx\\nxx xx\\nxx\\nxxxx\\nxx\\nxx\\nxxxx\\nxx x\\nx\\nxxxx\\nx\\nxx\\nxxx\\nx xx\\nxxx\\nxxxx\\nx\\nxxxx x\\nxx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxxxxx\\nxxx\\nxxx\\nxx\\nxxxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxxxxxx\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nx\\nxx\\nx\\nx xxx\\nxxx\\nx\\nxxxx x\\nx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxxxx\\nxxxx\\nxx\\nxx x\\nx\\nxx\\nxx\\nx\\nx xx\\nxx\\nxx\\nxx xx\\nx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxxx\\nxx\\nxxx\\nxxx\\nx xxxxxx\\nxxx\\nx\\nxxx\\nxxx\\nxxxxx\\nxxxx\\nx\\nxx\\nxxxxxxx\\nxxxx\\nxxx\\nx\\nxx\\nxxx xxx\\nxxxxx\\nx\\nxxxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nx\\nxxxxx\\nxxx\\nxxx xx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxxx\\nxxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxxxxx\\nxx\\nxxxx\\nx\\nxx x\\nx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxxxx\\nx\\nxxxxx x\\nx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxxxx\\nxxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxxx\\nxx\\nx\\nxxxxx\\nx\\nxx\\nxxx\\nx\\nx\\nxxxx\\nxx\\nxxx\\nxx\\nxxxx\\nx\\nx\\nxxxx\\nxxxx\\nx\\nxxx\\nxx\\nxxxxx\\nxx\\nx\\nxxx\\nx\\nxxx\\nxxxx\\nxxxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nx\\nx\\nxxxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nxxxxx\\nx\\nxxxx\\nx\\nxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxx\\nx\\nxxx\\nxx\\nx\\nx\\nxxx\\nxx\\nxx\\nx\\nxx\\nx\\nx\\nxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nxx\\nxxxxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nxxx\\nxxxx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nxxxx\\nxx\\nxxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxxxx\\nxx\\nx\\nxxx\\nx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nx\\nx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nx\\nx\\nxxxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxxxx\\nx\\nxxxx\\nx\\nx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nxxxxx\\nx\\nxx\\nxxx\\nx\\nxxx\\nx\\nxxx\\nxxxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nx\\nx\\nxxxxx\\nxx\\nx\\nxx\\nx\\nxx\\nxxx\\nx\\nxxxxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxx\\nx\\nx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxxxx\\nxx\\nxxxx\\nxx\\nxx\\nx\\nxxxxx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxxxxxx\\nxxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nx\\nxxxxx\\nxxx\\nx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxxxxxxxx\\nxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxxxxxx\\nxxx\\nx\\nx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxxxx\\nxx\\nxxxxx\\nxxxx\\nx\\nxxx\\nx\\nx\\nxx\\nxxxx\\nxxxx\\nx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxxx\\nx\\nx\\nxxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxxxx\\nxxx\\nx\\nx\\nxxxx\\nxxx\\nxx\\nxxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxxxx\\nxxxxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nx\\nxx\\nxxxx\\nxxxx\\nxx\\nx\\nxx\\nxx\\nx\\nx\\nxx\\nxx\\nx\\nxxxx\\nxxxx\\nxxx\\nxxx\\nxxxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxxxx\\nxxx\\nx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxxx\\nxxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nxxxx\\nxxx\\nx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nx\\nxx\\nxx\\nxxx\\nxxx\\nxxxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxxxx\\nxxx\\nx\\nxxxx\\nxxxxx\\nxx\\nxxx\\nxx\\nxxxxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxx\\nxx\\nx\\nxx\\nx\\nxxxxx\\nx\\nxxx\\nxxxxx\\nxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxxxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nx\\nxxxxx\\nxx\\nxx\\nxx\\nxxxxx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nx\\nx\\nxx\\nxx\\nx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx\\nxx\\nx\\nx\\nx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxx\\nxxxx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nxxxx\\nx\\nxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx\\nx\\nxxxxxxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxxxxxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxxx\\nxxx\\nx\\nx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxxxx\\nxxx\\nx\\nx\\nxx\\nxx\\nx\\nxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxxxxx\\nx\\nxxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxxx\\nxxx\\nx\\nxxxx\\nx\\nxnormal\\nlaplace\\nFigure 2: The estimated means of the two di\\x0berent mixture components (normal and laplace\\ndistribution).\\n4.3 Mixture of Experts Distributional Regression\\nNext, we change the above data-generation process to include a heterogeneous variance di\\x0berent\\nin both clusters. We then adapt the model to a mixture of experts distributional regression.\\n6\\n# Generate response differently\\nset.seed(32)\\nn <- 1000\\nNPreg$yn <-\\nc(5 * NPreg$x[1:n] + 3 * rnorm(n, 0, exp(-1+NPreg$x/5)),\\n40 - (NPreg$x[(n + 1):(2 * n)] - 5)^2 + 3 * rnorm(n))\\n# Define a mixture of distributional regressions\\ndr_mod3 <- sammer(y = NPreg$yn,\\nfamily = \"normal\",\\nnr_comps = nr_comps,\\nlist_of_formulas = list(mean = ~ 1 + x + xsq,\\nscale = ~1 + x),\\ndata = NPreg,\\noptimizer = optimizer_rmsprop(learning_rate = 0.01),\\ntf_seed = 42\\n)\\n# Train network\\ndr_mod3 %>% fit(epochs = 5000L,\\nvalidation_split = 0.1,\\npatience = 100L,\\nearly_stopping = TRUE,\\nverbose = FALSE)\\n# Get estimated mean and standard deviations\\npred_dr3 <- dr_mod3 %>% get_stats_mixcomps(what = \"means\")\\nstddev_dr3 <- dr_mod3 %>% get_stats_mixcomps(what = \"stddev\")\\nplot(NPreg$yn ~ NPreg$x)\\nfor(i in 1:nr_comps) f\\npoints(pred_dr3[,i]~NPreg$x, col=c(\"blue\",\"red\")[i], pch=\"x\")\\npoints(pred_dr3[,i]+2*stddev_dr3[,i]~NPreg$x, col=c(\"blue\",\"red\")[i], pch=\"-\")\\npoints(pred_dr3[,i]-2*stddev_dr3[,i]~NPreg$x, col=c(\"blue\",\"red\")[i], pch=\"-\")\\ng\\nlegend(\"bottomright\", pch=\"x\", col=c(\"blue\", \"red\"),\\nlegend = c(\"Comp. 1\", \"Comp. 2\"))\\nThe results of the above code are shown in Figure 3.\\n4.4 Semi-structured Mixture Density Networks\\nIn contrast to the previous examples, we now construct a mixture model that learns one of the\\nmixtures using an (unstructured) deep neural network in the style of a mixture density network.\\n7\\n0 2 4 6 8 10010203040506070\\nNPreg$xNPreg$ynxxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxxx\\nxx\\nxxxxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxx xx\\nx xx\\nxxx\\nxxxxx\\nx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxx\\nx\\nxxxx\\nxx\\nxxx\\nxx\\nxx x\\nxx\\nx\\nxxxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nxxxxx\\nxx\\nxxxxx\\nxx x\\nxxx xx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxxxxxx\\nxxx\\nxx\\nxxxx xx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxxx x\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx xx\\nxxx x\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxxxx\\nxx\\nxx\\nx xx\\nxx xxx\\nx xxxxxxx\\nxx\\nxx\\nxx\\nxx\\nxxxxxx\\nxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nxxx\\nx\\nxxxxxx\\nx\\nxxx\\nxxx\\nxx xx\\nx\\nxx x\\nxxxxx\\nxxxxxx\\nxxxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nx\\nxxx xxxx\\nx\\nx\\nxx\\nxxxx\\nxxx\\nxxx\\nx\\nxxx x\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nxxx xxxxx\\nx x\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx\\nxxxxxxxxx\\nxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxx x\\nxxx xx\\nxxxx\\nxx\\nxxxx\\nxxx\\nxxx\\nx\\nxxxx\\nx\\nx xx\\nxxx\\nxx xxxx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nx\\nxx xxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxxx x\\nxxxxx xx\\nxx\\nx\\nxxxx\\nxxxx\\nx\\nxxx\\nxx\\nxxx\\nx x\\nxx\\nxxxxx\\nxxx\\nxxxxx\\nx\\nxxxxxx x\\nx\\nxxxxxx x\\nxx\\nxxxx\\nxxx\\nxxxx\\nx\\nxx\\nxx x\\nxx\\nxx\\nxxx\\nxxx\\nxxxxx\\nx\\nxx\\nx\\nx\\nxx\\nxx xx x\\nxxx\\nx\\nxx\\nxxxxxxx\\nxx\\nxx\\nx\\nxxxxx\\nx xx\\nxxx\\nxxx\\nxx\\nxxxx\\nxxxx\\nxxx\\nxxxxx\\nxx\\nxx\\nxxx\\nxxxx x x\\nxxx xxx xx\\nx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nx\\nxxx\\nxxx x x\\nxx\\nxx\\nxxxx\\nxxx x\\nxx\\nxxx x\\nxxxx\\nxx\\nxx\\nxxxxxxxx\\nxx\\nxxx\\nx\\nxxx\\nxx\\nx x\\nxx\\nxx x\\nxx xx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nxxx\\nxxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nxxxx\\nx\\nx\\nxxxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxxxx x xx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxx\\nxxxx\\nx xx\\nxxx\\nxxxx\\nx\\nxx\\nxxxx\\nxx\\nxx\\nxxx\\nxxxxxx xx\\nxxxxx\\nxx\\nxxxxxxxx xx\\nxxxxx\\nxx\\nxx\\nxx\\nx\\nx xx\\nxx\\nxxxxxx\\nxx\\nxx\\nxxxx\\nxx\\nxxxx x\\nxxx x\\nxxxx\\nx\\nxxx\\nxxx\\nxxx\\nxx\\nxxxx\\nxxx\\nxxxx x\\nx\\nxx\\nx\\nx xx x\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxx\\nxxx\\nx\\nxxxx\\nxxxx\\nxxx\\nx\\nxx\\nxxx\\nxxx\\nx xx\\nxxx\\nxxx\\nxx\\nx\\nxx x\\nxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxxx\\nxx x\\nx\\nxxx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxxxx\\nxxx\\nx xxxxx\\nxx\\nxx xx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxxxxx\\nxxx x\\nxxx\\nxxxxxxx\\nx x\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxxx\\nxxx\\nx xx\\nx\\nxx x\\nxxxxx\\nxxx\\nx\\nxx\\nxx\\nxxxxxx\\nxx\\nxxxx\\nxxxxxx x\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx x x x\\nx\\nxx\\nxxxx\\nxxx\\nxx xxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx x\\nx\\nx\\nxxx\\nxxxx\\nx xxx x\\nxxxxx\\nxx\\nxx x\\nxxx x\\nxx\\nx\\nxx\\nxx\\nx\\nxxxx xxxx\\nxx\\nxx\\nxx\\nxxxx xxx\\nxx\\nx\\nxxx\\nxxx\\nxx x x\\nxx x\\nxxx x\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxxxx xx\\nxxxx\\nx\\nxxx xx\\nxx\\nx\\nxx\\nxx\\nx\\nxx\\nxxxxxx\\nxxx\\nxxxxx xx\\nxxx\\nxxx\\nx\\nx\\nxxx\\nxx\\nxx\\nxx\\nx\\nxxxx xx\\nxx\\nx\\nx xxxxxx\\nx\\nxx x\\nxx\\nx x xx\\nxx\\nxxxxxx\\nxx\\nxxxx\\nxx x\\nx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx x\\nxxxx\\nx\\nxxxx x\\nxx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxxx x\\nx\\nxxx\\nxx\\nxxxx\\nxxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxxxxx\\nxxx\\nxxx\\nxx\\nxxxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nxx\\nx xxx xx\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nx\\nxx\\nx\\nx xxx\\nxxx\\nx\\nxxxx x\\nx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxxxx\\nxx xx\\nxx\\nxx x\\nx\\nxx\\nxx\\nx\\nx xx\\nxx\\nxx\\nxx xx\\nx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nx xxxxxx\\nxxx\\nx\\nxxx\\nxxx\\nxxxxx\\nxxxx\\nx\\nxx\\nxxxxxxx\\nxxxx\\nxxx\\nx\\nxx\\nxxx xxx\\nxxxxx\\nx\\nxxxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nx\\nxxxxx\\nxxx\\nxxx xx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxxx\\nxxx\\nxxx\\nxx\\nxxxx\\nx\\nxxxxx\\nx xxxxx\\nx\\nxx x\\nx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nx xxx\\nxxx\\nxxxx\\nx\\nxxxxx x\\nx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nx−−−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−\\n−−−−\\n−−\\n−−−− −\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−− −−\\n−−−\\n−−−\\n−−−−−\\n−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−−−\\n−−\\n−− −\\n−−\\n−\\n−−− −\\n−−\\n−−−−−\\n−−−\\n−−−\\n−−−−−\\n−−\\n−−−−−\\n−− −\\n−−− −−\\n−−\\n−\\n−−−\\n−−−\\n−−−\\n−\\n−−−\\n−\\n−−\\n−−\\n− −\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−−−−−−\\n−−−\\n−−\\n−−−− −−\\n−−\\n−−\\n− −\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−− −\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−−−\\n−−− −\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−−−\\n−−\\n−−\\n− −−\\n−−−−−\\n−−−−−−−−\\n−−\\n−−\\n−−\\n−−\\n−−−−−−\\n−−−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−−\\n−−\\n−−−\\n−\\n−−−−−−−\\n−−−\\n−−−\\n−− −−\\n−\\n−− −\\n−−−−\\n−\\n−−−−−−\\n−−−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−\\n−−− −−−−\\n−\\n−\\n−−\\n−−−−\\n−−−\\n−−−\\n−\\n−−− −\\n− −\\n−−\\n−−\\n−−\\n−−\\n−−−−−\\n−−−\\n−−−\\n−−− −−−−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−−−−−−−\\n−−−\\n−−\\n−−− −\\n−−\\n−−\\n−−\\n−−− −\\n−−−−−\\n−−−−\\n−−\\n−−−−\\n−−−\\n−−−\\n−\\n−−−−\\n−\\n− −−\\n−\\n−−\\n−− −−−−\\n−−−−\\n−\\n−−−−−\\n−−−\\n−\\n−−−−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−− −−\\n−−− −\\n−−−−\\n− −−\\n−−\\n−\\n− −−−\\n−−−−\\n−\\n−−−\\n−−\\n−−−\\n−−\\n−−\\n−−−−−\\n−−−\\n−−−−−\\n−\\n−−−−−\\n−−\\n−\\n−−−−−− −\\n−−\\n−−−−\\n−−−\\n−−−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−−−\\n−−−\\n−−−−−\\n−\\n−−\\n−\\n−\\n−−\\n−− −− −\\n−−−\\n−\\n−−\\n−−−−−−−\\n−−\\n−−\\n−−−−−−\\n− −−\\n−−−\\n−−−\\n−−\\n−−−−\\n−−−−\\n−−−\\n−−−−−\\n−−\\n−−\\n−−−\\n−−−− − −\\n−−− −−− −−\\n−\\n−−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−\\n−\\n−−−\\n−−− − −\\n−−\\n−−\\n−−−−\\n−−− −\\n−−\\n−−− −\\n−−−−\\n−−\\n−−\\n−−−\\n−−−−−\\n−−\\n−−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−− −\\n−−−−\\n−−\\n−−−\\n−− −−\\n−−\\n−\\n−−−\\n−−−\\n−−−\\n− −−\\n−−−−\\n−−\\n−−\\n−\\n−− −\\n−−\\n−\\n−−−−\\n−\\n−\\n−−−−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−−−−− −−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−−\\n−−\\n−\\n−−\\n−−−−\\n− −−\\n−−−\\n−−−−\\n−\\n−−\\n−−−−\\n−−\\n−−\\n−−−\\n−−−−−− −−\\n−−−−−\\n−−\\n−−−−− −−− −−\\n−−−−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−−−−−−\\n−−\\n−−\\n−−−−\\n−−\\n−−−− −\\n−−− −\\n−−\\n−−\\n−\\n−−−\\n−−−\\n−\\n−−\\n−−\\n−−−−\\n−−−\\n−−−− −\\n−\\n−−\\n−\\n−−−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−− −−\\n−−\\n−−−\\n−−−−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−−−\\n− −−\\n−−−\\n−−−\\n−−\\n−\\n−− −\\n−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−−\\n−− −\\n−\\n−−−\\n−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−−−\\n−−−\\n− −−−−−\\n−−\\n−− −−\\n−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−−−−\\n−−\\n− −\\n−−−\\n−−−−−−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−\\n− −−\\n−\\n−− −\\n−−−−−\\n−−−\\n−\\n−−\\n−−\\n− −−−−−\\n−−\\n−−−−\\n−−−−−− −\\n−−\\n−−\\n−−−−\\n−−\\n−−−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−− −−−\\n−\\n−−\\n−−−−\\n−−−\\n−− −−−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−− −\\n−\\n−\\n−−−\\n−−−−\\n−−−− −\\n−−−−−\\n−−\\n−− −\\n−−− −\\n−−\\n−\\n−−\\n−−\\n−\\n−−−− −−−−\\n−−\\n−−\\n−−\\n− −−− −−−\\n−−\\n−\\n−−−\\n−− −\\n−− − −\\n−− −\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−−− −−\\n−−−−\\n−\\n−− − −−\\n−−\\n−\\n−−\\n−−\\n−\\n−−\\n−−−−−−\\n−−−\\n−−−−\\n− −−\\n−−−\\n−−−\\n−−\\n−−−\\n−−\\n− −−−\\n−\\n−−−−−−\\n−−\\n−\\n− −−−−−−\\n−\\n−− −\\n−−\\n−− −−\\n−−\\n−−−−−−\\n−−\\n−−−−\\n−− −\\n−\\n−−−−\\n−\\n−−\\n−−−\\n− −−\\n−−−\\n−−−−\\n−\\n−−−− −\\n−−−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−−−\\n−\\n−−−\\n−−\\n−−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−− −−−\\n−−−\\n−−−\\n−−\\n−−−−−−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−−−− −−\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−\\n−\\n−−\\n−\\n− −−−\\n−−−\\n−\\n−−−− −\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−−\\n−−−−\\n−−\\n−− −\\n−−−\\n−−\\n−\\n− −−\\n−−\\n−−\\n−− −−\\n−\\n−−−−\\n−−−−\\n−−\\n−−\\n−−\\n−−−−\\n−−\\n−−−−−\\n−−−\\n−−−\\n− −−−−−−\\n−−−\\n−\\n−−−\\n−−−\\n−−−−−\\n−−− −\\n−\\n−−\\n−−−−−−−\\n−−−−\\n−−−\\n−−−\\n−−− − −−\\n−−−−−\\n−\\n−−− −\\n−−−−\\n−−\\n−−−\\n−\\n−−\\n−\\n− −−−−\\n−−−\\n−−− −−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n− −−−\\n−\\n−−−\\n−\\n−−−−\\n−−−\\n−−−\\n−−\\n−\\n−−−\\n−\\n−−−−−\\n−−\\n−−−−\\n−\\n−− −\\n−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−−−\\n−\\n−−−−− −\\n−\\n−− −\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−\\n−−−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−\\n−−−−\\n−−\\n−− −−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−− −−\\n− −−\\n−−−\\n−−−−−\\n−\\n−−−\\n−−−−\\n−−−\\n− −−\\n−−\\n−\\n−−−−\\n−−\\n−−−\\n−−\\n−− −\\n−−\\n−\\n−−−−\\n−−\\n−−−−−\\n−−−\\n−−−\\n−−−−−\\n−−\\n−−−−−\\n−−−\\n−−− −−\\n−−\\n−\\n−−−\\n−−−\\n−−−\\n−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−− −\\n−−−\\n−\\n−\\n−−−\\n−−−\\n−−\\n−−\\n−\\n−−−−−−−\\n−−−\\n−−\\n−−−− −−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−− −\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n− −−\\n−−− −\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−−−\\n−−\\n−−\\n− −−\\n−− −−−\\n− −−−−−−−\\n−−\\n−−\\n−−\\n−−\\n−−− −−−\\n−−−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−−\\n−−\\n−−−\\n−\\n−−−−−−\\n−\\n−−−\\n−−−\\n−−−−\\n−\\n−− −\\n−−−−−\\n−−−−−\\n−\\n−−−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−\\n−−− −−−−\\n−\\n−\\n−−\\n−−−−\\n−−−\\n−−−\\n−\\n−−− −\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−− −−\\n−−−\\n−−−\\n−−− −−−−−\\n− −\\n−−\\n−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−−− −−−−\\n−−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−− −\\n−−− −−\\n−\\n− −−\\n−−\\n−− −−\\n−−−\\n−−−\\n−\\n−−−−\\n−\\n− −−\\n−−−\\n−− −−−−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−\\n−− −− −\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−− −−\\n−−− −\\n−−−−−−−\\n−−\\n−\\n−−−−\\n−−−−\\n−\\n−−−\\n−−\\n−−−\\n− −\\n−−\\n−−−−−\\n−−−\\n−−−−−\\n−\\n−−−−−− −\\n−\\n− −−−\\n−− −\\n−−\\n−−−−\\n−−−\\n−−−−\\n−\\n−−\\n−− −\\n−−\\n−−\\n−−−\\n−−−\\n−−−−−\\n−\\n−−\\n−−\\n−−\\n−−−− −\\n−−−\\n−\\n−−\\n−−−−−−−\\n−−\\n−−\\n−\\n−−−−−\\n−−−\\n−−−\\n−−−\\n−−\\n−−−−\\n−−−−\\n−−−\\n−−−−−\\n−−\\n−−\\n−−−\\n−−−− − −\\n−−−−−− −−\\n−\\n−−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−\\n−\\n−−−\\n−−− − −\\n−−\\n−−\\n−−−−\\n−−− −\\n−−\\n−−− −\\n−−− −\\n−−\\n−−\\n−−\\n−−−−−−\\n−−\\n−−−\\n−\\n−− −\\n−−\\n− −\\n−−\\n−− −\\n−− −−\\n−−\\n−−−\\n−−−−\\n−−\\n−\\n−−−\\n−−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−\\n−−−−\\n−\\n−\\n−−−−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−−−− − −−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−−\\n−−\\n−\\n−−\\n−−−−\\n−−−\\n−−−\\n−−−−\\n−\\n−−\\n− −−−\\n−−\\n−−\\n−−−\\n−−−− −− −−\\n−−−−−\\n−−\\n−−\\n−−−−−− −−\\n−−−−−\\n−−\\n−−\\n−−\\n−\\n− −−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−−\\n−−\\n−−−− −\\n−−−−\\n−−−−\\n−\\n−−−\\n−−−\\n−−−\\n−−\\n−−−−\\n−−−\\n− −−− −\\n−\\n−−\\n−\\n− −− −\\n−−−\\n−−−\\n−−\\n−− −\\n−−−\\n−−− −−\\n−−\\n−−−\\n−\\n−−−−\\n−−−−\\n−−−\\n−\\n−−\\n−−−\\n−−−\\n−−−\\n−−−\\n−− −\\n−−\\n−\\n−− −\\n−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−−\\n−− −\\n−\\n−−−\\n−\\n−−−\\n−−\\n−−−\\n−−\\n−−−−\\n−−−\\n−−−−−−\\n−−\\n−− −−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−−−−\\n−−− −\\n− −−\\n−−−−−−−\\n− −\\n−−−\\n−−\\n−\\n− −−\\n−−\\n−−\\n−−\\n−−−−\\n−−−\\n− −−\\n−\\n−− −\\n−−−−−\\n−−−\\n−\\n−−\\n−−\\n−−−−−−\\n−−\\n−−−−\\n−−−−\\n−− −\\n−−\\n−−\\n−−\\n−−−−\\n−−−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−− − −\\n−\\n−−\\n−− −−\\n−−−\\n−−−−−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−− −\\n−\\n−\\n−−−\\n−−−−\\n− −−− −\\n−\\n−−−−\\n−−\\n−− −\\n−−− −\\n−−\\n−\\n−−\\n−−\\n−\\n−− −− −−−−\\n−−\\n−−\\n−−−−−− −\\n−−\\n−−\\n−\\n−−−\\n−−−\\n−− − −\\n−−−\\n−−− −\\n−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−−− −−\\n−−−−\\n−\\n−−−−−\\n−−\\n−\\n−−\\n−−\\n−\\n−−\\n−−−−−−\\n−−−\\n−−−−− −−\\n−−−\\n−−−\\n−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−\\n−−−− −−\\n− −\\n−\\n− −\\n−−−−−\\n−\\n−− −\\n−−\\n− −−−\\n−−\\n−−−−−−\\n−−\\n−−−−\\n−− −\\n−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−− −\\n−−−−\\n−\\n−−−− −\\n−−\\n−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−− −\\n−\\n−−−\\n−−\\n−−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−−−−−\\n−−−\\n−−−\\n−−\\n−−−−−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−−\\n−−\\n− −−−−−\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−−\\n−−\\n−\\n−−\\n−\\n−−−−\\n−− −\\n−\\n−−−− −\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−−\\n−− −−\\n−−\\n−− −\\n−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−− −−\\n−\\n−−\\n−−\\n−− −−\\n−−\\n−−\\n−−\\n−−−−\\n−−\\n−−−−−\\n−−−\\n−−−\\n−−−−−−−\\n−−−\\n−\\n−−−\\n−− −\\n−−−−−\\n−−−−\\n−\\n−−\\n−−−−−−−\\n−−−−\\n−−−\\n−\\n−−\\n−−−−−−\\n−−−−−\\n−\\n−−−−\\n− −−−\\n−−\\n−−−\\n−\\n−−\\n−\\n−−−−−\\n−−−\\n−−−−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−−\\n−\\n−−−\\n−\\n−−−−\\n−−−\\n−−−\\n−−\\n−−−−\\n−\\n−−−−−\\n− −−−−−\\n−\\n−− −\\n−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−\\n− − −−\\n−−−\\n−−−−\\n−\\n−−−−− −\\n−\\n−−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−xx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxxxx\\nxxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxxx\\nxx\\nx\\nxxxxx\\nx\\nxx\\nxxx\\nx\\nx\\nxxxx\\nxx\\nxxx\\nxx\\nxxxx\\nx\\nx\\nxxxx\\nxxxx\\nx\\nxxx\\nxx\\nxxxxx\\nxx\\nx\\nxxx\\nx\\nxxx\\nxxxx\\nxxxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nx\\nx\\nxxxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nxxxxx\\nxxxxx\\nx\\nxx\\nxx\\nxxxxx\\nxxxx\\nxx\\nx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxx\\nx\\nxxx\\nxx\\nx\\nx\\nxxx\\nxx\\nxx\\nx\\nxx\\nx\\nx\\nxx\\nxxxx\\nxxxx\\nx\\nxxx\\nxx\\nxxxxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nxxx\\nxxxx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nxxxx\\nxx\\nxxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxxxx\\nxx\\nxxxx\\nx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nx\\nx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxxxx\\nx\\nxxxx\\nx\\nx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nxxxxx\\nx\\nxx\\nxxx\\nx\\nxxx\\nx\\nxxx\\nxxxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nx\\nx\\nxxxxx\\nxx\\nx\\nxx\\nx\\nxx\\nxxx\\nx\\nxxxxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxx\\nx\\nx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxxxx\\nxx\\nxxxx\\nxx\\nxx\\nx\\nxxxxx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxxxxxx\\nxxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nx\\nxxxxx\\nxxx\\nx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxxxxxxxx\\nxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxxxxx\\nxxx\\nx\\nx\\nx\\nxx\\nxx\\nxxxx\\nxxxx\\nxxxx\\nxx\\nxxxxx\\nxxxx\\nx\\nxxx\\nx\\nx\\nxx\\nxxxx\\nxxxx\\nx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxxx\\nx\\nx\\nxxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxxxxx\\nxxx\\nx\\nx\\nxxxx\\nxxx\\nxx\\nxxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxxxx\\nxxxxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nx\\nxx\\nxxxx\\nxxxx\\nxx\\nx\\nxx\\nxx\\nx\\nx\\nxx\\nxx\\nx\\nxxxx\\nxxxx\\nxxx\\nxxx\\nxxxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxxxx\\nxxx\\nx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxxx\\nxxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxx\\nx\\nxxx\\nxxxx\\nxxx\\nx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nx\\nxx\\nxx\\nxxx\\nxxx\\nxxxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxxxx\\nxxx\\nx\\nxxxx\\nxxxxx\\nxx\\nxxx\\nxx\\nxxxxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxx\\nxx\\nx\\nxx\\nx\\nxxxxx\\nx\\nxxx\\nxxxxx\\nxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxxxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nx\\nxxxxx\\nxx\\nxx\\nxx\\nxxxxx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nx\\nx\\nxx\\nxx\\nx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxxxxx\\nxxxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxx\\nx\\nx\\nx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxxx\\nxxxx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nxxxx\\nx\\nxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx\\nx\\nxxxxxxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxxxxxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxxx\\nxxx\\nx\\nx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxxxx\\nxxx\\nx\\nx\\nxx\\nxx\\nx\\nxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxxxxx\\nx\\nxxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxxx\\nxxx\\nx\\nxxxx−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−−\\n−−\\n−\\n−−−−−\\n−\\n−−\\n−−−\\n−\\n−\\n−−−−\\n−−\\n−−−\\n−−\\n−−−−\\n−\\n−\\n−−−−\\n−−−−\\n−\\n−−−\\n−−\\n−−−−−\\n−−\\n−\\n−−−\\n−\\n−−−\\n−−−−\\n−−−−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−\\n−\\n−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−−−−\\n−\\n−\\n−−−−\\n−\\n−−\\n−−\\n−−−−−\\n−−−−\\n−−\\n−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−\\n−−−\\n−\\n−−−\\n−−\\n−\\n−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−\\n−\\n−−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−−\\n−−−−−\\n−−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−\\n−\\n−−−\\n−−−−\\n−\\n−−−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−−−−\\n−−\\n−−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−\\n−−−\\n−\\n−\\n−−−\\n−−−\\n−\\n−−−\\n−\\n−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−\\n−\\n−−−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−−−\\n−\\n−−−−\\n−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−\\n−−−−−\\n−\\n−−−−\\n−\\n−\\n−−\\n−−−\\n−\\n−−−\\n−\\n−−−\\n−−−−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−\\n−\\n−−−−−\\n−−\\n−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−−−−−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−\\n−\\n−\\n−−−\\n−−−\\n−\\n−−−\\n−\\n−−−−\\n−−\\n−−−−\\n−−\\n−−\\n−\\n−−−−−\\n−−−\\n−\\n−−\\n−−−\\n−−−−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−−\\n−−−−\\n−−−−−−\\n−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−\\n−\\n−−−−−\\n−−−\\n−\\n−\\n−−−\\n−−\\n−\\n−−\\n−\\n−−\\n−\\n−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−−−−−−−\\n−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−\\n−−−−−−\\n−−−\\n−\\n−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−\\n−\\n−−\\n−−−−−\\n−−−\\n−\\n−\\n−−−\\n−\\n−\\n−−\\n−−−−\\n−−−−\\n−\\n−\\n−−−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−\\n−\\n−\\n−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−\\n−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−−−−\\n−−−\\n−\\n−\\n−−−−\\n−−−\\n−−\\n−−−−\\n−−\\n−\\n−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−\\n−−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−−−\\n−−−−−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−\\n−−−−−\\n−\\n−\\n−−\\n−−−−\\n−−−−\\n−−\\n−\\n−−\\n−−\\n−\\n−\\n−−\\n−−\\n−\\n−−−−\\n−−−\\n−\\n−−−\\n−−−\\n−−−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−\\n−−−\\n−−−−\\n−−−\\n−\\n−−\\n−\\n−−−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−−−−\\n−−−\\n−\\n−\\n−−−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−\\n−\\n−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−−\\n−\\n−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−\\n−−\\n−−−−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−−\\n−\\n−−\\n−−−−\\n−−−\\n−\\n−−−−\\n−−−−−\\n−−\\n−−−\\n−−\\n−−−−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−\\n−−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−−\\n−−−\\n−−\\n−\\n−−\\n−\\n−−−−−\\n−\\n−−−\\n−−−−−\\n−−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−\\n−\\n−−\\n−−\\n−−\\n−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−\\n−−−−−\\n−−\\n−−\\n−−\\n−−−−−\\n−\\n−−−−\\n−−−\\n−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−\\n−\\n−−\\n−−\\n−\\n−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−\\n−−−−\\n−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−−\\n−−\\n−\\n−\\n−\\n−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−−\\n−\\n−\\n−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−−−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−\\n−−−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−\\n−−−−\\n−−\\n−−\\n−−−−\\n−\\n−−−−−\\n−\\n−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−\\n−−−−−−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−−\\n−−−\\n−\\n−−\\n−−−−−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−−\\n−−−\\n−\\n−\\n−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−\\n−\\n−−\\n−−\\n−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−−−−−\\n−\\n−−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−−\\n−−−\\n−\\n−−−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−−−\\n−−−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−−\\n−−−\\n−−−−−−\\n−−\\n−−−−\\n−\\n−−−−\\n−−\\n−−−\\n−−\\n−−−−\\n−\\n−\\n−−−−\\n−−−−\\n−\\n−−−−−\\n−−−−−\\n−−\\n−\\n−−−−\\n−−−\\n−−−−\\n−−−−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−−\\n−\\n−−−−−−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−−−−−\\n−−−−−−\\n−−\\n−−\\n−−−−−\\n−−−−\\n−−\\n−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−\\n−−−\\n−\\n−−−\\n−−\\n−\\n−\\n−−−\\n−−\\n−−−\\n−−\\n−\\n−\\n−−\\n−−−−\\n−−−−\\n−\\n−−−\\n−−\\n−−−−−\\n−−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−\\n−\\n−−−\\n−−−−\\n−\\n−−−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−−−−\\n−−−−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−−−−\\n−\\n−\\n−−−\\n−−−\\n−\\n−−−\\n−\\n−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−−−\\n−\\n−−−−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−\\n−−−−−\\n−\\n−−−−−\\n−\\n−−\\n−−−\\n−\\n−−−\\n−\\n−−−\\n−−−−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−\\n−\\n−−−−−\\n−−\\n−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−−−−−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−\\n−\\n−\\n−−−\\n−−−\\n−\\n−−−−\\n−−−−\\n−−\\n−−−−\\n−−−−\\n−\\n−−−−−\\n−−−−\\n−−\\n−−−\\n−−−−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−−\\n−−−−\\n−−−−−−\\n−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−\\n−\\n−−−−−\\n−−−−\\n−\\n−−−\\n−−\\n−\\n−−−\\n−−−\\n−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−−−−−−−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−\\n−−−−\\n−−−−−−\\n−−−−\\n−\\n−\\n−−\\n−−\\n−−−−\\n−−−−\\n−−−−\\n−−\\n−−−−−\\n−−−−\\n−\\n−−−\\n−\\n−\\n−−\\n−−−−\\n−−−−\\n−\\n−\\n−−−−\\n−−−\\n−\\n−−\\n−−\\n−−−−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−\\n−\\n−\\n−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−−−−−\\n−−\\n−−−\\n−−−\\n−−−\\n−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−−−−−−−\\n−\\n−\\n−−−−\\n−−−\\n−−\\n−−−−−−−\\n−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−\\n−−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−−−\\n−−−−−\\n−−\\n−−−\\n−\\n−−\\n−−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−\\n−−−−−\\n−\\n−\\n−−\\n−−−−\\n−−−−\\n−−\\n−\\n−−\\n−−\\n−\\n−\\n−−\\n−−\\n−\\n−−−−\\n−−−−\\n−−−\\n−−−\\n−−−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−\\n−−−\\n−−−−\\n−−−\\n−\\n−−−\\n−−−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−−−\\n−−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−−−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−−−−\\n−−−−\\n−\\n−−−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−−\\n−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−−\\n−\\n−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−\\n−−\\n−−−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−−\\n−\\n−−\\n−−−−\\n−−−\\n−\\n−−−−\\n−−−−−\\n−−\\n−−−\\n−−\\n−−−−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−−\\n−−−\\n−−\\n−\\n−−\\n−\\n−−−−−\\n−\\n−−−\\n−−−−−\\n−−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−−−−−−\\n−\\n−\\n−−\\n−−\\n−−\\n−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−\\n−−−−−\\n−−\\n−−\\n−−\\n−−−−−−\\n−−−−\\n−−−\\n−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−\\n−\\n−−\\n−−−\\n−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−−−−\\n−−−−−−\\n−−−\\n−−\\n−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−\\n−\\n−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−−−−−\\n−\\n−−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−−−\\n−−−−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−\\n−−−−\\n−−\\n−−−−\\n−−\\n−−−−\\n−−−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−\\n−−−−\\n−−\\n−−\\n−−−−−\\n−−−−−−\\n−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−\\n−−−−−−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−−−\\n−−−\\n−\\n−−\\n−−−−−−−\\n−\\n−−\\n−−\\n−−−\\n−−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−−\\n−−−−\\n−\\n−\\n−−−−\\n−−−−−\\n−−\\n−−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−−−\\n−−−\\n−\\n−\\n−−\\n−−\\n−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−−−−−\\n−\\n−−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−−−−−\\n−−−\\n−\\n−−−−\\nx\\nxComp. 1\\nComp. 2Figure 3: The estimated means and \\x06two times the estimated standard deviations of the two\\nnormal distributions \\x0ctted on a heterogeneous version of the NPreg data where one component\\nhas an increased variance for larger xvalues.\\nThe second distribution is again learned with a structured predictor. We use the data from the\\nprevious subsection to demonstrate this.\\n# Deep network\\ndeep_net <- keras_model_sequential() %>%\\nlayer_dense(units = 64, activation = \"relu\", use_bias = FALSE) %>%\\nlayer_dense(units = 64, activation = \"relu\", use_bias = FALSE) %>%\\nlayer_dense(units = 32, activation = \"relu\", use_bias = FALSE)\\n# Deep network head for the mean\\ndeep_mean <- function(x) x %>%\\ndeep_net %>%\\nlayer_dense(units = 1)\\n# Deep network head for the standard deviation\\ndeep_std <- function(x) x %>%\\ndeep_net %>%\\nlayer_dense(units = 1, activation = \"softplus\")\\n# Semi-structured mixture density network\\ndr_mod4 <- mixdistreg(y = NPreg$yn,\\nfamilies = c(\"normal\", \"normal\"),\\n8\\nnr_comps = nr_comps,\\nlist_of_formulas = list(\\n# structured model part\\nmean1 = ~ 1 + x + xsq,\\nscale1 = ~1 + x,\\n# unstructured deep network\\nmean1 = ~ 1 + dm(x),\\nscale1 = ~1 + ds(x)\\n),\\nlist_of_deep_models = list(dm = deep_mean,\\nds = deep_std),\\ndata = NPreg,\\noptimizer = optimizer_adam(learning_rate = 0.001),\\ntf_seed = 42\\n)\\n# Train network\\ndr_mod4 %>% fit(epochs = 5000L,\\nvalidation_split = 0.1,\\npatience = 500L,\\nearly_stopping = TRUE,\\nverbose = FALSE)\\n# Extract means and standard deviations\\npred_dr4 <- dr_mod4 %>% get_stats_mixcomps(what = \"means\")\\nstddev_dr4 <- dr_mod4 %>% get_stats_mixcomps(what = \"stddev\")\\nplot(NPreg$yn ~ NPreg$x)\\nfor(i in 1:nr_comps) f\\npoints(pred_dr4[,i]~NPreg$x, col=c(\"blue\",\"red\")[i], pch=\"x\")\\npoints(pred_dr4[,i]+2*stddev_dr4[,i]~NPreg$x, col=c(\"blue\",\"red\")[i], pch=\"-\")\\npoints(pred_dr4[,i]-2*stddev_dr4[,i]~NPreg$x, col=c(\"blue\",\"red\")[i], pch=\"-\")\\ng\\nlegend(\"bottomright\", pch=\"x\", col=c(\"blue\", \"red\"),\\nlegend = c(\"Comp. 1\", \"Comp. 2\"))\\nThe results of the above code are shown in Figure 4.\\n4.5 Zero-and-one In\\rated Regression\\nAs a last example, we generate zero-in\\rated data and show how to use the package\\'s wrapper\\nfunction zinreg to estimate a mixture of a point distribution at zero and a normal distribution.\\n9\\n0 2 4 6 8 10010203040506070\\nNPreg$xNPreg$ynxxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxxx\\nxx\\nxxxxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxx xx\\nx xx\\nxxx\\nxxxxx\\nx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxx\\nx\\nxxxx\\nxx\\nxxx\\nxx\\nxx x\\nxx\\nx\\nxxxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nxxxxx\\nxx\\nxxxxx\\nxx x\\nxxx xx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxxxxxx\\nxxx\\nxx\\nxxxx xx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxxx x\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx xx\\nxxx x\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxxxx\\nxx\\nxx\\nx xx\\nxx xxx\\nx xxxxxxx\\nxx\\nxx\\nxx\\nxx\\nxxxxxx\\nxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nxxx\\nx\\nxxxxxx\\nx\\nxxx\\nxxx\\nxx xx\\nx\\nxx x\\nxxxxx\\nxxxxxx\\nxxxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nx\\nxxx xxxx\\nx\\nx\\nxx\\nxxxx\\nxxx\\nxxx\\nx\\nxxx x\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nxxx xxxxx\\nx x\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx\\nxxxxxxxxx\\nxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxx x\\nxxx xx\\nxxxx\\nxx\\nxxxx\\nxxx\\nxxx\\nx\\nxxxx\\nx\\nx xx\\nxxx\\nxx xxxx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nx\\nxx xxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxxx x\\nxxxxx xx\\nxx\\nx\\nxxxx\\nxxxx\\nx\\nxxx\\nxx\\nxxx\\nx x\\nxx\\nxxxxx\\nxxx\\nxxxxx\\nx\\nxxxxxx x\\nx\\nxxxxxx x\\nxx\\nxxxx\\nxxx\\nxxxx\\nx\\nxx\\nxx x\\nxx\\nxx\\nxxx\\nxxx\\nxxxxx\\nx\\nxx\\nx\\nx\\nxx\\nxx xx x\\nxxx\\nx\\nxx\\nxxxxxxx\\nxx\\nxx\\nx\\nxxxxx\\nx xx\\nxxx\\nxxx\\nxx\\nxxxx\\nxxxx\\nxxx\\nxxxxx\\nxx\\nxx\\nxxx\\nxxxx x x\\nxxx xxx xx\\nx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nx\\nxxx\\nxxx x x\\nxx\\nxx\\nxxxx\\nxxx x\\nxx\\nxxx x\\nxxxx\\nxx\\nxx\\nxxxxxxxx\\nxx\\nxxx\\nx\\nxxx\\nxx\\nx x\\nxx\\nxx x\\nxx xx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxxx\\nxxx\\nxxx\\nxxx\\nxxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nxxxx\\nx\\nx\\nxxxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxxxx x xx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nx\\nxx\\nxxxx\\nx xx\\nxxx\\nxxxx\\nx\\nxx\\nxxxx\\nxx\\nxx\\nxxx\\nxxxxxx xx\\nxxxxx\\nxx\\nxxxxxxxx xx\\nxxxxx\\nxx\\nxx\\nxx\\nx\\nx xx\\nxx\\nxxxxxx\\nxx\\nxx\\nxxxx\\nxx\\nxxxx x\\nxxx x\\nxxxx\\nx\\nxxx\\nxxx\\nxxx\\nxx\\nxxxx\\nxxx\\nxxxx x\\nx\\nxx\\nx\\nx xx x\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxxx xx\\nxx\\nxxx\\nx\\nxxxx\\nxxxx\\nxxx\\nx\\nxx\\nxxx\\nxxx\\nx xx\\nxxx\\nxxx\\nxx\\nx\\nxx x\\nxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxxx\\nxx x\\nx\\nxxx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxxxx\\nxxx\\nx xxxxx\\nxx\\nxx xx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxxxxxx\\nxxx x\\nxxx\\nxxxxxxx\\nx x\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxxx\\nxxx\\nx xx\\nx\\nxx x\\nxxxxx\\nxxx\\nx\\nxx\\nxx\\nxxxxxx\\nxx\\nxxxx\\nxxxxxx x\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx x x x\\nx\\nxx\\nxxxx\\nxxx\\nxx xxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx x\\nx\\nx\\nxxx\\nxxxx\\nx xxx x\\nxxxxx\\nxx\\nxx x\\nxxx x\\nxx\\nx\\nxx\\nxx\\nx\\nxxxx xxxx\\nxx\\nxx\\nxx\\nxxxx xxx\\nxx\\nx\\nxxx\\nxxx\\nxx x x\\nxx x\\nxxx x\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxxxx xx\\nxxxx\\nx\\nxxx xx\\nxx\\nx\\nxx\\nxx\\nx\\nxx\\nxxxxxx\\nxxx\\nxxxx\\nx xx\\nxxx\\nxxx\\nx\\nx\\nxxx\\nxx\\nxx\\nxx\\nx\\nxxxx xx\\nxx\\nx\\nx xxxxxx\\nx\\nxx x\\nxx\\nx x xx\\nxx\\nxxxxxx\\nxx\\nxxxx\\nxx x\\nx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxxx\\nxxxx\\nx\\nxxxx x\\nxx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxxx x\\nx\\nxxx\\nxx\\nxxxx\\nxxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxxxxx\\nxxx\\nxxx\\nxx\\nxxxxxxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nxx\\nx xxx xx\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nx\\nxx\\nx\\nx xxx\\nxxx\\nx\\nxxxx x\\nx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxxxx\\nxx xx\\nxx\\nxx x\\nx\\nxx\\nxx\\nx\\nx xx\\nxx\\nxx\\nxx xx\\nx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxxxxx\\nxxx\\nxxx\\nx xxxxxx\\nxxx\\nx\\nxxx\\nxxx\\nxxxxx\\nxxxx\\nx\\nxx\\nxxxxxxx\\nxxxx\\nxxx\\nx\\nxx\\nxxx xxx\\nxxxxx\\nx\\nxxxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nx\\nxxxxx\\nxxx\\nxxx xx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxxx\\nxxx\\nxxx\\nxx\\nxxxx\\nx\\nxxxxx\\nx xxxxx\\nx\\nxx x\\nx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nx xxx\\nxxx\\nxxxx\\nx\\nxxxxx x\\nx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nx−−−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−\\n−−−−\\n−−\\n−−−− −\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−− −−\\n− −−\\n−−−\\n−−−−−\\n−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−−−\\n−−\\n−− −\\n−−\\n−\\n−−− −\\n−−\\n−−−−−\\n−−−\\n−−−\\n−−−−−\\n−−\\n−−−−−\\n−− −\\n−−− −−\\n−−\\n−\\n−−−\\n−−−\\n−−−\\n−\\n−−−\\n−\\n−−\\n−−\\n− −\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−−−−−−\\n−−−\\n−−\\n−−−− −−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−− −\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−−−\\n−−− −\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−−−\\n−−\\n−−\\n− −−\\n−−−−−\\n−−−−−−−−\\n−−\\n−−\\n−−\\n−−\\n−−−−−−\\n−−−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−−\\n−−\\n−−−\\n−\\n−−−−−−\\n−\\n−−−\\n−−−\\n−− −−\\n−\\n−− −\\n−−−−\\n−\\n−−−−−−\\n−−−−\\n−−\\n−\\n−−\\n−\\n−−\\n−−\\n−\\n−−− −−−−\\n−\\n−\\n−−\\n−−−−\\n−−−\\n−−−\\n−\\n−−− −\\n− −\\n−−\\n−−\\n−−\\n−−\\n−−−−−\\n−−−\\n−−−\\n−−− −−−−−\\n− −\\n−−\\n−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−−−−−−−\\n−−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−− −\\n−−− −−\\n−−−−\\n−−\\n−−−−\\n−−−\\n−−−\\n−\\n−−−−\\n−\\n− −−\\n−\\n−−\\n−− −−−−\\n−−−−\\n−\\n−−−−−\\n−−−\\n−\\n−− −−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−− −−\\n−−− −\\n−−−−\\n− −−\\n−−\\n−\\n− −−−\\n−−−−\\n−\\n−−−\\n−−\\n−−−\\n− −\\n−−\\n−−−−−\\n−−−\\n−−−−−\\n−\\n−−−−−\\n− −\\n−\\n−−−−−− −\\n−−\\n−−−−\\n−−−\\n−−−−\\n−\\n−−\\n−− −\\n−−\\n−−\\n−−−\\n−−−\\n−−−−−\\n−\\n−−\\n−\\n−\\n−−\\n−− −− −\\n−−−\\n−\\n−−\\n−−−−−−−\\n−−\\n−−\\n−−−−−−\\n− −−\\n−−−\\n−−−\\n−−\\n−−−−\\n−−−−\\n−−−\\n−−−−−\\n−−\\n−−\\n−−−\\n−−−− − −\\n−−− −−− −−\\n−\\n−−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−\\n−\\n−−−\\n−−− − −\\n−−\\n−−\\n−−−−\\n−−− −\\n−−\\n−−− −\\n−−−−\\n−−\\n−−\\n−−−\\n−−−−−\\n−−\\n−−−\\n−\\n−−−\\n−−\\n− −\\n−−\\n−− −\\n−− −−\\n−−\\n−−−\\n−−−−\\n−−\\n−\\n−−−\\n−−−\\n−−−\\n− −−\\n−−−−\\n−−\\n−−\\n−\\n−− −\\n−−\\n−\\n−−−−\\n−\\n−\\n−−−−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−−−−− −−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−−\\n−−\\n−\\n−−\\n−−−−\\n− −−\\n−−−\\n−−−−\\n−\\n−−\\n−−−−\\n−−\\n−−\\n−−−\\n−−−−−− −−\\n−−−−−\\n−−\\n−−−−−−−− −−\\n−−−−−\\n−−\\n−−\\n−−\\n−\\n− −−\\n−−\\n−−−−−−\\n−−\\n−−\\n−−−−\\n−−\\n−−−− −\\n−−− −\\n−−\\n−−\\n−\\n−−−\\n−−−\\n−−−\\n−−\\n−−−−\\n−−−\\n−−−− −\\n−\\n−−\\n−\\n− −−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−− −−\\n−−\\n−−−\\n−\\n−−−−\\n−−−−\\n−−−\\n−\\n−−\\n−−−\\n−−−\\n− −−\\n−−−\\n−−−\\n−−\\n−\\n−− −\\n−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−−\\n−− −\\n−\\n−−−\\n−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−−−\\n−−−\\n− −−−−−\\n−−\\n−− −−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−−−−\\n−−− −\\n−−−\\n−−−−−−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−\\n− −−\\n−\\n−− −\\n−−−−−\\n−−−\\n−\\n−−\\n−−\\n− −−−−−\\n−−\\n−−−−\\n−−−−−− −\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−− − −−\\n−\\n−−\\n−−−−\\n−−−\\n−− −−−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−− −\\n−\\n−\\n−−−\\n−−−−\\n−−−− −\\n−−−−−\\n−−\\n−− −\\n−−− −\\n−−\\n−\\n−−\\n−−\\n−\\n−−−− −−−−\\n−−\\n−−\\n−−\\n−−−− −−−\\n−−\\n−\\n−−−\\n−−−\\n−− − −\\n−− −\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−−− −−\\n−−−−\\n−\\n−− − −−\\n−−\\n−\\n−−\\n−−\\n−\\n−−\\n−−−−−−\\n−−−\\n−−−−\\n− −−\\n−−−\\n−−−\\n−−\\n−−−\\n−−\\n− −\\n−−\\n−\\n−−−−−−\\n−−\\n−\\n− −−−−−−\\n−\\n−− −\\n−−\\n−− −−\\n−−\\n−−−−−−\\n−−\\n−−−−\\n−− −\\n−\\n−−−−\\n−\\n−−\\n−−−\\n− −−\\n−−−\\n−−−−\\n−\\n−−−− −\\n−−\\n−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−−−\\n−\\n−−−\\n−−\\n−−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−−−−−\\n−−−\\n−−−\\n−−\\n−−−−−−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−−−− −−\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−\\n−\\n−−\\n−\\n− −−−\\n−−−\\n−\\n−−−− −\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−−\\n−− −−\\n−−\\n−− −\\n−\\n−−\\n−−\\n−\\n− −−\\n−−\\n−−\\n−− −−\\n−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−−−\\n−−\\n−−−−−\\n−−−\\n−−−\\n− −−−−−−\\n−−−\\n−\\n−−−\\n−−−\\n−−−−−\\n−−− −\\n−\\n−−\\n−−−−−−−\\n−−−−\\n−−−\\n−−−\\n−−− − −−\\n−−−−−\\n−\\n−−−−\\n−−−−\\n−−\\n−−−\\n−\\n−−\\n−\\n− −−−−\\n−−−\\n−−− −−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−−\\n−\\n−−−\\n−\\n−−−−\\n−−−\\n−−−\\n−−\\n−\\n−−−\\n−\\n−−−−−\\n−−−−−−\\n−\\n−− −\\n−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−−−\\n−−−\\n−−−−\\n−\\n−−−−− −\\n−\\n−− −\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−\\n−−−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−\\n−−−−\\n−−\\n−−−−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−− −−\\n− −−\\n−−−\\n−−−−−\\n−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−\\n−\\n−−−−\\n−−\\n−−−\\n−−\\n−− −\\n−−\\n−\\n−−−−\\n−−\\n−−−−−\\n−−−\\n−−−\\n−−−−−\\n−−\\n−−−−−\\n−− −\\n−−− −−\\n−−\\n−\\n−−−\\n−−−\\n−−−\\n−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−− −\\n−−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−\\n−−−−−−−\\n−−−\\n−−\\n−−−− −−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−− −\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n− −−\\n−−− −\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−−−\\n−−\\n−−\\n− −−\\n−− −−−\\n− −−−−−−−\\n−−\\n−−\\n−−\\n−−\\n−−−−−−\\n−−−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−−\\n−−\\n−−−\\n−\\n−−−−−−\\n−\\n−−−\\n−−−\\n−−−−\\n−\\n−− −\\n−−−−−\\n−−−−−−\\n−−−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−\\n−−− −−−−\\n−\\n−\\n−−\\n−−−−\\n−−−\\n−−−\\n−\\n−−− −\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−− −−\\n−−−\\n−−−\\n−−− −−−−−\\n− −\\n−−\\n−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−−−−−−−\\n−−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−− −\\n−−− −−\\n−− −−\\n−−\\n−−−−\\n−−−\\n−−−\\n−\\n−−−−\\n−\\n− −−\\n−−−\\n−− −−−−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−\\n−− −−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−− −−\\n−−− −\\n−−−−− −−\\n−−\\n−\\n−−−−\\n−−−−\\n−\\n−−−\\n−−\\n−−−\\n− −\\n−−\\n−−−−−\\n−−−\\n−−−−−\\n−\\n−−−−−− −\\n−\\n− −−−\\n−− −\\n−−\\n−−−−\\n−−−\\n−−−−\\n−\\n−−\\n−− −\\n−−\\n−−\\n−−−\\n−−−\\n−−−−−\\n−\\n−−\\n−\\n−\\n−−\\n−−−− −\\n−−−\\n−\\n−−\\n−−−−−−−\\n−−\\n−−\\n−\\n−−−−−\\n− −−\\n−−−\\n−−−\\n−−\\n−−−−\\n−−−−\\n−−−\\n−−−−−\\n−−\\n−−\\n−−−\\n−−−− − −\\n−−−−−− −−\\n−\\n−−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−\\n−\\n−−−\\n−−− − −\\n−−\\n−−\\n−−−−\\n−−− −\\n−−\\n−−− −\\n−−− −\\n−−\\n−−\\n−−\\n−−−−−−\\n−−\\n−−−\\n−\\n−− −\\n−−\\n− −\\n−−\\n−− −\\n−− −−\\n−−\\n−−−\\n−−−−\\n−−\\n−\\n−−−\\n−−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−\\n−−−−\\n−\\n−\\n−−−−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−−−− − −−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−−\\n−−\\n−\\n−−\\n−−−−\\n− −−\\n−−−\\n−−−−\\n−\\n−−\\n− −−−\\n−−\\n−−\\n−−−\\n−−−−−− −−\\n−−−−−\\n−−\\n−−\\n−−−−−− −−\\n−−−−−\\n−−\\n−−\\n−−\\n−\\n− −−\\n−−\\n−−−−−−\\n−−\\n−−\\n−−−−\\n−−\\n−−−− −\\n−−− −\\n−−−−\\n−\\n−−−\\n−−−\\n−−−\\n−−\\n−−−−\\n−−−\\n−−−− −\\n−\\n−−\\n−\\n− −− −\\n−−−\\n−−−\\n−−\\n−− −\\n−−−\\n−−− −−\\n−−\\n−−−\\n−\\n−−−−\\n−−−−\\n−−−\\n−\\n−−\\n−−−\\n−−−\\n− −−\\n−−−\\n−−−\\n−−\\n−\\n−− −\\n−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−−\\n−− −\\n−\\n−−−\\n−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−−−\\n−−−\\n−−−−−−\\n−−\\n−− −−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−−−−\\n−−− −\\n−−−\\n−−−−−−−\\n− −\\n−−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−\\n− −−\\n−\\n−− −\\n−−−−−\\n−−−\\n−\\n−−\\n−−\\n−−−−−−\\n−−\\n−−−−\\n−−−−\\n−− −\\n−−\\n−−\\n−−\\n−−−−\\n−−−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−− − −\\n−\\n−−\\n−− −−\\n−−−\\n−− −−−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−− −\\n−\\n−\\n−−−\\n−−−−\\n− −−− −\\n−−−−−\\n−−\\n−− −\\n−−− −\\n−−\\n−\\n−−\\n−−\\n−\\n−−−− −−−−\\n−−\\n−−\\n−−\\n−−−− −−−\\n−−\\n−\\n−−−\\n−−−\\n−− − −\\n−−−\\n−−− −\\n−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−−− −−\\n−−−−\\n−\\n−−− −−\\n−−\\n−\\n−−\\n−−\\n−\\n−−\\n−−−−−−\\n−−−\\n−−−−− −−\\n−−−\\n−−−\\n−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−\\n−−−− −−\\n−−\\n−\\n− −−−−−−\\n−\\n−− −\\n−−\\n− − −−\\n−−\\n−−−−−−\\n−−\\n−−−−\\n−− −\\n−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−− −\\n−−−−\\n−\\n−−−− −\\n−−\\n−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−− −\\n−\\n−−−\\n−−\\n−−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−−−−−\\n−−−\\n−−−\\n−−\\n−−−−−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−−\\n−−\\n− −−−−−\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−−\\n−−\\n−\\n−−\\n−\\n− −−−\\n−−−\\n−\\n−−−− −\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−−\\n−− −−\\n−−\\n−− −\\n−\\n−−\\n−−\\n−\\n− −−\\n−−\\n−−\\n−− −−\\n−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−−−\\n−−\\n−−−−−\\n−−−\\n−−−\\n− −−−−−−\\n−−−\\n−\\n−−−\\n−− −\\n−−−−−\\n−−−−\\n−\\n−−\\n−−−−−−−\\n−−−−\\n−−−\\n−\\n−−\\n−−−−−−\\n−−−−−\\n−\\n−−−−\\n− −−−\\n−−\\n−−−\\n−\\n−−\\n−\\n−−−−−\\n−−−\\n−−− −−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−−\\n−\\n−−−\\n−\\n−−−−\\n−−−\\n−−−\\n−−\\n−−−−\\n−\\n−−−−−\\n− −−−−−\\n−\\n−− −\\n−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−\\n− −−−\\n−−−\\n−−−−\\n−\\n−−−−− −\\n−\\n−−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−xx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxxxx\\nxxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxxx\\nxx\\nx\\nxxxxx\\nx\\nxx\\nxxx\\nx\\nx\\nxxxx\\nxx\\nxxx\\nxx\\nxxxx\\nx\\nx\\nxxxx\\nxxxx\\nx\\nxxx\\nxx\\nxxxxx\\nxx\\nx\\nxxx\\nx\\nxxx\\nxxxx\\nxxxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nx\\nx\\nxxxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nxxxxx\\nxxxxx\\nx\\nxx\\nxx\\nxxxxx\\nxxxx\\nxx\\nx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nx\\nxxx\\nx\\nxxx\\nxx\\nx\\nx\\nxxx\\nxx\\nxx\\nx\\nxx\\nx\\nx\\nxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nxx\\nxxxxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nxxx\\nxxxx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nxxxx\\nxx\\nxxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxxxx\\nxx\\nxxxx\\nx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nx\\nx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nxxxx\\nx\\nxxxx\\nx\\nx\\nxx\\nxx\\nxxx\\nx\\nxx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nxxxxx\\nx\\nxx\\nxxx\\nx\\nxxx\\nx\\nxxx\\nxxxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nx\\nx\\nxxxxx\\nxx\\nx\\nxx\\nx\\nxx\\nxxx\\nx\\nxxxxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxx\\nx\\nx\\nx\\nxxx\\nxxx\\nx\\nxxx\\nx\\nxxxx\\nxx\\nxxxx\\nxx\\nxx\\nx\\nxxxxx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxxxxxx\\nxxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nx\\nxxxxx\\nxxx\\nx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxxxxxxxx\\nxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxxxxx\\nxxx\\nx\\nx\\nx\\nxx\\nxx\\nxxxx\\nxxxx\\nxxxx\\nxx\\nxxxxx\\nxxxx\\nx\\nxxx\\nx\\nx\\nxx\\nxxxx\\nxxxx\\nx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxxx\\nx\\nx\\nxxx\\nxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxxxxx\\nxxx\\nx\\nx\\nxxxx\\nxxx\\nxx\\nxxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxxxxx\\nxxxxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nx\\nxxxxx\\nx\\nx\\nxx\\nxxxx\\nxxxx\\nxx\\nx\\nxx\\nxx\\nx\\nx\\nxx\\nxx\\nx\\nxxxx\\nxxxx\\nxxx\\nxxx\\nxxxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nx\\nxxx\\nxxxx\\nxxx\\nx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nx\\nxxx\\nxx\\nxx\\nxxx\\nxxx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nx\\nxxxx\\nx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nx\\nxx\\nx\\nxxx\\nxxxx\\nxxx\\nx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nx\\nxx\\nxx\\nxxx\\nxxx\\nxxxx\\nxx\\nx\\nxx\\nxxxx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxxxx\\nxxx\\nx\\nxxxx\\nxxxxx\\nxx\\nxxx\\nxx\\nxxxxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxxx\\nxx\\nxx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxx\\nxx\\nx\\nxx\\nx\\nxxxxx\\nx\\nxxx\\nxxxxx\\nxxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nxx\\nxxx\\nxxxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nx\\nx\\nxxx\\nxx\\nxx\\nxx\\nxxx\\nx\\nxxx\\nx\\nxxxxx\\nxx\\nxx\\nxx\\nxxxxx\\nx\\nxxxx\\nxxx\\nx\\nxx\\nx\\nxxx\\nxxxx\\nxx\\nx\\nx\\nxx\\nxx\\nx\\nx\\nxx\\nx\\nxxx\\nx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nxxxx\\nxx\\nxxx\\nxx\\nxx\\nxxxx\\nxxx\\nxxx\\nxxx\\nxx\\nx\\nx\\nx\\nxx\\nxxx\\nxxx\\nxxx\\nx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nx\\nx\\nx\\nxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxxx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxxx\\nxxxx\\nxxxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nx\\nxxxx\\nxx\\nxxxx\\nxx\\nxx\\nxx\\nxxxx\\nx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxx\\nxxx\\nxx\\nxxx\\nxxx\\nx\\nxxxx\\nxx\\nxx\\nxxxx\\nx\\nxxxxx\\nx\\nx\\nxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx\\nx\\nxxxxxxx\\nxxx\\nxx\\nx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxxx\\nxx\\nxxx\\nx\\nxx\\nxxxxxxx\\nx\\nxx\\nxx\\nxx\\nx\\nxxx\\nx\\nxxx\\nxx\\nxx\\nxxx\\nxx\\nx\\nxx\\nxxxx\\nxxx\\nx\\nx\\nx\\nxx\\nxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxx\\nx\\nxx\\nxx\\nxxxx\\nxxx\\nx\\nx\\nxx\\nxx\\nx\\nxx\\nxx\\nx\\nxx\\nxxx\\nx\\nxx\\nxxx\\nxx\\nxx\\nxxxxx\\nx\\nxxx\\nxxx\\nxxxx\\nxxx\\nxxx\\nxx\\nxx\\nxx\\nxx\\nxx\\nxxxx\\nxx\\nxx\\nxxx\\nxxx\\nx\\nxxxx−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−−\\n−−\\n−\\n−−−−−\\n−\\n−−\\n−−−\\n−\\n−\\n−−−−\\n−−\\n−−−\\n−−\\n−−−−\\n−\\n−\\n−−−−\\n−−−−\\n−\\n−−−\\n−−\\n−−−−−\\n−−\\n−\\n−−−\\n−\\n−−−\\n−−−−\\n−−−−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−\\n−\\n−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−−\\n−\\n−−\\n−−−−\\n−\\n−\\n−−−−\\n−\\n−−\\n−−\\n−−\\n−−−\\n−−−−\\n−−\\n−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−\\n−−−\\n−\\n−−−\\n−−\\n−\\n−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−\\n−\\n−−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−−\\n−−−−−\\n−−−\\n−\\n−−\\n−−\\n−\\n−−\\n−\\n−−\\n−−−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−\\n−\\n−\\n−−−\\n−−−−\\n−\\n−−−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−−−−\\n−−\\n−−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−\\n−−−\\n−\\n−\\n−−−\\n−−−\\n−\\n−−−\\n−\\n−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−\\n−\\n−−−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−−−\\n−\\n−−−−\\n−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−\\n−−−−−\\n−\\n−\\n−−−\\n−\\n−\\n−−\\n−−−\\n−\\n−−−\\n−\\n−−−\\n−−−−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−\\n−\\n−−−−−\\n−−\\n−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−−−−−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−\\n−\\n−\\n−−−\\n−−−\\n−\\n−−−\\n−\\n−−−−\\n−−\\n−−−−\\n−−\\n−−\\n−\\n−−−−−\\n−−−\\n−\\n−−\\n−−−\\n−−−−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−\\n−\\n−−−−−\\n−−−\\n−\\n−\\n−−−\\n−−\\n−\\n−−\\n−\\n−−\\n−\\n−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−−−−−−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−\\n−−−−−−\\n−−−\\n−\\n−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−−\\n−\\n−−\\n−−−−−\\n−−−\\n−\\n−\\n−−−\\n−\\n−\\n−−\\n−−−−\\n−−−−\\n−\\n−\\n−−−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−\\n−\\n−\\n−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−\\n−\\n−−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−−−\\n−\\n−−−\\n−\\n−\\n−−−−\\n−−−\\n−−\\n−−−−\\n−−\\n−\\n−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−\\n−−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−−−\\n−−−−−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−\\n−−−−−\\n−\\n−\\n−−\\n−−−−\\n−−−−\\n−−\\n−\\n−−\\n−−\\n−\\n−\\n−−\\n−−\\n−\\n−−−−\\n−−−\\n−\\n−−−\\n−−−\\n−−−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−\\n−−\\n−\\n−−−−\\n−−−\\n−\\n−−\\n−\\n−−−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−−−−\\n−−−\\n−\\n−\\n−−−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−\\n−\\n−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−−\\n−\\n−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−\\n−−\\n−−−−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−−\\n−\\n−−\\n−−−\\n−\\n−−−\\n−\\n−−−−\\n−−−−−\\n−−\\n−−−\\n−−\\n−−−−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−\\n−−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−−\\n−−−\\n−−\\n−\\n−−\\n−\\n−−−−−\\n−\\n−−−\\n−−−−−\\n−−−\\n−−−\\n−−−\\n−−\\n−\\n−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−\\n−\\n−−\\n−−\\n−−\\n−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−\\n−−−−−\\n−−\\n−−\\n−−\\n−−−−−\\n−\\n−−−−\\n−−−\\n−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−\\n−\\n−−\\n−−\\n−\\n−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−\\n−−−−\\n−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−−\\n−−\\n−\\n−\\n−\\n−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−−\\n−\\n−\\n−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−−−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−\\n−−−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−−−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−\\n−−−−\\n−−\\n−−\\n−−−−\\n−\\n−−−−−\\n−\\n−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−\\n−−−−−−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−\\n−\\n−−−\\n−−\\n−−−\\n−\\n−−\\n−−−−−−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−−\\n−−−\\n−\\n−\\n−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−\\n−\\n−−\\n−−\\n−\\n−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−−−−−\\n−\\n−−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−−\\n−−−\\n−\\n−−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−−−\\n−−−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−−−−−\\n−−−−\\n−−−\\n−−−−− −\\n−−\\n−−−−\\n−\\n−−−−\\n−−\\n−−−\\n−−\\n−−−−\\n−\\n−\\n−−−−\\n−−−− −\\n−−−−−\\n−−−−− −−\\n−\\n−−−−\\n−−−\\n−−−−\\n−−−−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−−−\\n−−−−−−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−−− −−\\n−−−− − −\\n−−\\n−−\\n−−−−−\\n−−−−\\n−−\\n−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−\\n−−−\\n−\\n−−−\\n−−\\n−\\n−\\n−−−\\n−−\\n−− −\\n−−\\n−\\n−\\n−−\\n−−−−\\n−−−−\\n−\\n−−−\\n−−\\n−−−−−\\n−−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−−\\n−−−\\n−\\n−−−\\n−−−−\\n−\\n−−−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−−−−\\n−−−− −\\n−−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−−−−−\\n−\\n−\\n−−−\\n−−−\\n−\\n−−−\\n−\\n−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−−−−\\n−\\n−−−− −\\n−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−\\n−−−−\\n−−\\n−−\\n−−−\\n−−−−−\\n−\\n−−−−−\\n−\\n−−\\n−−−\\n−\\n−−−\\n−\\n−−−\\n−−−−\\n−−\\n−−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−\\n−\\n−−−−−\\n−−\\n−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−−−− −−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−\\n−−−\\n−− −\\n−\\n−−−−\\n−−−−\\n−−\\n−−−−\\n−− −−\\n−\\n−−−−−\\n−−−−\\n−−\\n−−−\\n−−−−\\n−−−−\\n−−−−\\n−−\\n−−\\n−− −\\n−−\\n−\\n−−−\\n−−\\n−−−−\\n−−−−−−\\n−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−\\n−\\n−−−−−\\n−−−−\\n−\\n−−−\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−\\n−−\\n−− −\\n−−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−−−−−−− − −\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−\\n−−−−\\n−−−−−−\\n−−−−\\n−\\n−\\n−−\\n−−\\n−−−−\\n−−−−\\n−−−−\\n−−\\n−−−−−\\n−−−−\\n−\\n−− −\\n−\\n−\\n−−\\n−−−−\\n−−−− −\\n−\\n−−−−\\n−−−\\n−\\n−−\\n−−\\n−−− − −\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−−−\\n−−−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−−\\n−−−−\\n−\\n−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−− −−− −\\n−−\\n−−−\\n−−−\\n−−−\\n−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−−−− −− −\\n−\\n−\\n−− −−\\n−−−\\n−−\\n−−−−−− −\\n−\\n−−\\n−−\\n−−\\n−−−\\n−\\n−−\\n−\\n−−−\\n−−\\n−−−−−−\\n−−\\n−−\\n−−\\n−−−\\n−−−−−\\n−−−−−\\n−−\\n−−−\\n−\\n−−\\n−−− −\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−−−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−\\n−−−−−−\\n−\\n−−\\n−−−−\\n−−− −\\n−−\\n−\\n−−\\n−−−−\\n−−\\n−−\\n−\\n−−−−\\n−−−−\\n−−−\\n−−−\\n−−−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−−\\n−−\\n−\\n−−−\\n−−−−\\n−−−\\n−\\n−− −\\n−−−−\\n−−−−\\n−\\n−−−\\n−−\\n−−\\n−−−\\n−−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−− −\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−\\n−\\n−−−−\\n−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−−−−\\n−−\\n−\\n−−−\\n−−−\\n−−−−\\n−−− −\\n−\\n−−−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−− −\\n−\\n−−\\n−−\\n−−−−\\n−−\\n−−\\n−−\\n−− −\\n−−−−\\n−−−−\\n−\\n−−\\n−−\\n−−−−−−\\n−−−−\\n−−\\n−\\n−−\\n−−−−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−−\\n−−−\\n−−\\n−−\\n−− −\\n−\\n−−−−−−\\n−\\n−−\\n−−−−\\n−−−\\n−\\n−−−−\\n−−−−−\\n−−\\n−−−\\n−−\\n−−−−−\\n−−\\n−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−−−\\n−−\\n−−\\n−−\\n−−\\n−\\n−−\\n−−\\n−−\\n−\\n−−−−\\n−−\\n−−\\n−−\\n−−−\\n−−−\\n−−− −\\n−−\\n−\\n−−−\\n−−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−−\\n−−−\\n−−\\n−\\n−−\\n−\\n−−−−−\\n−\\n−−−\\n−−−−−\\n−−−\\n−−−\\n−−−\\n−−−\\n−\\n−−\\n−−\\n−−−\\n−−−−−−−\\n−\\n−\\n−−\\n−−\\n−−−\\n−\\n−−−\\n−−\\n−−\\n−−\\n−−− −\\n−−−\\n−\\n−−−−−\\n−−\\n−−\\n−−\\n−−−− − −\\n−−−−\\n−−−−\\n−−\\n−−−−\\n−−−−\\n−− −\\n−\\n−−\\n−− −\\n−\\n−−\\n−\\n−−−\\n−\\n−−\\n−−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−−−−−\\n−−−−−−\\n−−−\\n−−\\n−−\\n−−−− −− −−−−\\n−−−\\n−−−\\n−\\n−\\n−−\\n−−−\\n−−−\\n−−−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−−−\\n−−\\n−\\n−−−\\n−− −−\\n−\\n−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−−− −−\\n−\\n−−−−\\n−−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−−−\\n−−−−\\n−−−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−−−\\n−\\n−−−−\\n−−\\n−−−−\\n−−\\n−−−−\\n−−−−\\n−\\n−−\\n−−−\\n−−\\n−\\n−−\\n−−−\\n−−−\\n−−\\n−− −\\n−−−−\\n−−−−\\n−−\\n−−\\n−−−−−\\n−−−− −−\\n−\\n−−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−\\n−−−−−−−\\n−−−\\n−−\\n−\\n−−\\n−−\\n−−−\\n−−\\n−\\n−−−−−\\n−−−−\\n−−\\n−−−−−−−\\n−\\n−−\\n−−\\n−−−−−−−\\n−−−\\n−−\\n−−\\n−−−−−\\n−\\n−−\\n−−−−\\n−−−−\\n−\\n−\\n−−−−\\n−−−−−\\n−−\\n−−− −\\n−\\n−−\\n−−\\n−− −\\n−−\\n−−\\n−−−−\\n−−−\\n−\\n−\\n−−\\n−−−\\n−−\\n−−−\\n−−\\n−−−\\n−\\n−−\\n−−−\\n−−\\n−−\\n−−−− −−\\n−−−\\n−−−\\n−−−−\\n−−−\\n−−−\\n−−\\n−−\\n−−−−−−\\n−−−−\\n−−−−−− −\\n−− −\\n−\\n−−−−\\nx\\nxComp. 1\\nComp. 2Figure 4: The estimated means and \\x06two times the estimated standard deviations of the two\\nnormal distributions \\x0ctted on a heterogeneous version of the NPreg data where one component is\\nlearned using a deep neural network.\\n# zero-inflated\\nset.seed(32)\\nn <- 1000\\nNPreg$yn <-\\nc(5 * NPreg$x[1:n] + 3 * rnorm(n, 0, exp(-1+NPreg$x/5)),\\nrep(0, n))\\n# Zero-inflated regression with normal distribution\\ndr_mod4 <- zinreg(y = NPreg$yn,\\nfamily = \"normal\",\\nlist_of_formulas = list(mean = ~ 1 + x + xsq,\\nscale = ~1 + x),\\ndata = NPreg,\\noptimizer = optimizer_rmsprop(learning_rate = 0.01),\\ntf_seed = 42\\n)\\n# Train network\\ndr_mod4 %>% fit(epochs = 5000L,\\nvalidation_split = 0.1,\\npatience = 100L,\\n10\\nearly_stopping = TRUE,\\nverbose = FALSE)\\n# Check estimated probabilities\\n(dr_mod4 %>% get_pis())[1,]\\n## [1] 0.5379843 0.4620157\\nResults show that the probability for zero-in\\ration is learned almost correctly. Using the function\\ninflareg , the previous code could be adapted to arbitrary value-in\\rated distributions, also for\\nmore than one value (e.g., for zero-one-in\\ration).\\nReferences\\n[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J.,\\nDevin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser,\\nL., Kudlur, M., Levenberg, J., Man\\x13 e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens,\\nJ., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vi\\x13 egas, F., Vinyals, O.,\\nWarden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on\\nheterogeneous systems (2015), https://www.tensorflow.org/\\n[2] Baumann, P.F.M., Hothorn, T., R\\x7f ugamer, D.: Deep conditional transformation models. In: Machine Learning\\nand Knowledge Discovery in Databases (ECML-PKDD). pp. 3{18. Springer International Publishing (2021)\\n[3] Bender, A., R\\x7f ugamer, D., Scheipl, F., Bischl, B.: A general machine learning framework for survival anal-\\nysis. In: Machine Learning and Knowledge Discovery in Databases (ECML-PKDD). pp. 158{173. Springer\\nInternational Publishing (2021)\\n[4] Bishop, C.M.: Mixture Density Networks. Aston University (1994)\\n[5] Chaganty, A.T., Liang, P.: Spectral experts for estimating mixtures of linear regressions. In: International\\nConference on Machine Learning. pp. 1040{1048 (2013)\\n[6] Fritz, C., Dorigatti, E., R\\x7f ugamer, D.: Combining graph neural networks and spatio-temporal disease models\\nto predict covid-19 cases in germany. Scienti\\x0cc Reports 12, 2045{2322 (2022)\\n[7] Gr\\x7f un, B., Leisch, F.: Fitting \\x0cnite mixtures of generalized linear regressions in R. Computational Statistics\\n& Data Analysis 51(11), 5247{5252 (2007)\\n[8] Kook, L., Baumann, P.F., D\\x7f urr, O., Sick, B., R\\x7f ugamer, D.: Estimating conditional distributions with neural\\nnetworks using r package deeptrafo. arXiv preprint arXiv:2211.13665 (2022)\\n[9] Kook, L., Herzog, L., Hothorn, T., D\\x7f urr, O., Sick, B.: Deep and interpretable regression models for ordinal\\noutcomes. Pattern Recognition 122, 108263 (2022)\\n[10] Kopper, P., P\\x7f olsterl, S., Wachinger, C., Bischl, B., Bender, A., R\\x7f ugamer, D.: Semi-structured deep piece-\\nwise exponential models. In: Proceedings of AAAI Spring Symposium on Survival Prediction { Algorithms,\\nChallenges, and Applications, PMLR. pp. 40{53 (2021)\\n[11] Kopper, P., Wiegrebe, S., Bischl, B., Bender, A., R\\x7f ugamer, D.: Deeppamm: Deep piecewise exponential\\nadditive mixed models for complex hazard structures in survival analysis. In: Advances in Knowledge Discovery\\nand Data Mining (PAKDD). pp. 249{261. Springer International Publishing (2022)\\n[12] Leisch, F.: FlexMix: A general framework for \\x0cnite mixture models and latent class regression in R. Journal\\nof Statistical Software 11(8), 1{18 (2004). https://doi.org/10.18637/jss.v011.i08\\n[13] Magdon-Ismail, M., Atiya, A.: Neural networks for density estimation. In: Advances in Neural Information\\nProcessing Systems. vol. 11 (1998)\\n[14] Murphy, K.P.: Machine Learning: A Probabilistic Perspective. MIT Press (2012)\\n[15] R Core Team: R: A Language and Environment for Statistical Computing. R Foundation for Statistical\\nComputing, Vienna, Austria (2022), https://www.R-project.org/\\n[16] R\\x7f ugamer, D.: Additive higher-order factorization machines. arXiv preprint arXiv:2205.14515 (2022)\\n11\\n[17] R\\x7f ugamer, D., Baumann, P., Kneib, T., Hothorn, T.: Probabilistic time series forecasts with autoregressive\\ntransformation models. Statistics & Computing (2023), accepted.\\n[18] R\\x7f ugamer, D., Bender, A., Wiegrebe, S., Racek, D., Bischl, B., M\\x7f uller, C., Stachl, C.: Factorized struc-\\ntured regression for large-scale varying coe\\x0ecient models. In: Machine Learning and Knowledge Discovery in\\nDatabases (ECML-PKDD). Springer International Publishing (2022), accepted\\n[19] R\\x7f ugamer, D., Kolb, C., Fritz, C., P\\x0csterer, F., Kopper, P., Bischl, B., Shen, R., Bukas, C., de Andrade e\\nSousa, L.B., Thalmeier, D., Baumann, P., Kook, L., Klein, N., M\\x7f uller, C.L.: deepregression: A \\rexible neural\\nnetwork framework for semi-structured deep distributional regression. Journal of Statistical Software (2022),\\naccepted\\n[20] R\\x7f ugamer, D., Kolb, C., Klein, N.: Semi-structured distributional regression. The American Statistician 0(ja),\\n1{25 (2023). https://doi.org/10.1080/00031305.2022.2164054\\n[21] R\\x7f ugamer, D., P\\x0csterer, F., Bischl, B., Gr\\x7f un, B.: Mixture of Experts Distributional Regression: Implementa-\\ntion Using Robust Estimation with Adaptive First-order Methods (2022)\\n[22] Stasinopoulos, D.M., Rigby, R.A.: Generalized additive models for location scale and shape (gamlss) in R.\\nJournal of Statistical Software 23(7), 1{46 (2007)\\n12',\n",
       "  'ref': '[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J.,\\nDevin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser,\\nL., Kudlur, M., Levenberg, J., Man\\x13 e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens,\\nJ., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vi\\x13 egas, F., Vinyals, O.,\\nWarden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on\\nheterogeneous systems (2015), https://www.tensorflow.org/\\n[2] Baumann, P.F.M., Hothorn, T., R\\x7f ugamer, D.: Deep conditional transformation models. In: Machine Learning\\nand Knowledge Discovery in Databases (ECML-PKDD). pp. 3{18. Springer International Publishing (2021)\\n[3] Bender, A., R\\x7f ugamer, D., Scheipl, F., Bischl, B.: A general machine learning framework for survival anal-\\nysis. In: Machine Learning and Knowledge Discovery in Databases (ECML-PKDD). pp. 158{173. Springer\\nInternational Publishing (2021)\\n[4] Bishop, C.M.: Mixture Density Networks. Aston University (1994)\\n[5] Chaganty, A.T., Liang, P.: Spectral experts for estimating mixtures of linear regressions. In: International\\nConference on Machine Learning. pp. 1040{1048 (2013)\\n[6] Fritz, C., Dorigatti, E., R\\x7f ugamer, D.: Combining graph neural networks and spatio-temporal disease models\\nto predict covid-19 cases in germany. Scienti\\x0cc Reports 12, 2045{2322 (2022)\\n[7] Gr\\x7f un, B., Leisch, F.: Fitting \\x0cnite mixtures of generalized linear regressions in R. Computational Statistics\\n& Data Analysis 51(11), 5247{5252 (2007)\\n[8] Kook, L., Baumann, P.F., D\\x7f urr, O., Sick, B., R\\x7f ugamer, D.: Estimating conditional distributions with neural\\nnetworks using r package deeptrafo. arXiv preprint arXiv:2211.13665 (2022)\\n[9] Kook, L., Herzog, L., Hothorn, T., D\\x7f urr, O., Sick, B.: Deep and interpretable regression models for ordinal\\noutcomes. Pattern Recognition 122, 108263 (2022)\\n[10] Kopper, P., P\\x7f olsterl, S., Wachinger, C., Bischl, B., Bender, A., R\\x7f ugamer, D.: Semi-structured deep piece-\\nwise exponential models. In: Proceedings of AAAI Spring Symposium on Survival Prediction { Algorithms,\\nChallenges, and Applications, PMLR. pp. 40{53 (2021)\\n[11] Kopper, P., Wiegrebe, S., Bischl, B., Bender, A., R\\x7f ugamer, D.: Deeppamm: Deep piecewise exponential\\nadditive mixed models for complex hazard structures in survival analysis. In: Advances in Knowledge Discovery\\nand Data Mining (PAKDD). pp. 249{261. Springer International Publishing (2022)\\n[12] Leisch, F.: FlexMix: A general framework for \\x0cnite mixture models and latent class regression in R. Journal\\nof Statistical Software 11(8), 1{18 (2004). https://doi.org/10.18637/jss.v011.i08\\n[13] Magdon-Ismail, M., Atiya, A.: Neural networks for density estimation. In: Advances in Neural Information\\nProcessing Systems. vol. 11 (1998)\\n[14] Murphy, K.P.: Machine Learning: A Probabilistic Perspective. MIT Press (2012)\\n[15] R Core Team: R: A Language and Environment for Statistical Computing. R Foundation for Statistical\\nComputing, Vienna, Austria (2022), https://www.R-project.org/\\n[16] R\\x7f ugamer, D.: Additive higher-order factorization machines. arXiv preprint arXiv:2205.14515 (2022)\\n11\\n[17] R\\x7f ugamer, D., Baumann, P., Kneib, T., Hothorn, T.: Probabilistic time series forecasts with autoregressive\\ntransformation models. Statistics & Computing (2023), accepted.\\n[18] R\\x7f ugamer, D., Bender, A., Wiegrebe, S., Racek, D., Bischl, B., M\\x7f uller, C., Stachl, C.: Factorized struc-\\ntured regression for large-scale varying coe\\x0ecient models. In: Machine Learning and Knowledge Discovery in\\nDatabases (ECML-PKDD). Springer International Publishing (2022), accepted\\n[19] R\\x7f ugamer, D., Kolb, C., Fritz, C., P\\x0csterer, F., Kopper, P., Bischl, B., Shen, R., Bukas, C., de Andrade e\\nSousa, L.B., Thalmeier, D., Baumann, P., Kook, L., Klein, N., M\\x7f uller, C.L.: deepregression: A \\rexible neural\\nnetwork framework for semi-structured deep distributional regression. Journal of Statistical Software (2022),\\naccepted\\n[20] R\\x7f ugamer, D., Kolb, C., Klein, N.: Semi-structured distributional regression. The American Statistician 0(ja),\\n1{25 (2023). https://doi.org/10.1080/00031305.2022.2164054\\n[21] R\\x7f ugamer, D., P\\x0csterer, F., Bischl, B., Gr\\x7f un, B.: Mixture of Experts Distributional Regression: Implementa-\\ntion Using Robust Estimation with Adaptive First-order Methods (2022)\\n[22] Stasinopoulos, D.M., Rigby, R.A.: Generalized additive models for location scale and shape (gamlss) in R.\\nJournal of Statistical Software 23(7), 1{46 (2007)\\n12'},\n",
       " {'entry_id': 'http://arxiv.org/abs/2207.09094v1',\n",
       "  'title': 'MoEC: Mixture of Expert Clusters',\n",
       "  'abstract': 'Sparsely Mixture of Experts (MoE) has received great interest due to its\\npromising scaling capability with affordable computational overhead. MoE\\nconverts dense layers into sparse experts, and utilizes a gated routing network\\nto make experts conditionally activated. However, as the number of experts\\ngrows, MoE with outrageous parameters suffers from overfitting and sparse data\\nallocation. Such problems are especially severe on tasks with limited data,\\nthus hindering the progress for MoE models to improve performance by scaling\\nup. In this work, we propose Mixture of Expert Clusters - a general approach to\\nenable expert layers to learn more diverse and appropriate knowledge by\\nimposing variance-based constraints on the routing stage. We further propose a\\ncluster-level expert dropout strategy specifically designed for the expert\\ncluster structure. Our experiments reveal that MoEC could improve performance\\non machine translation and natural language understanding tasks, and raise the\\nperformance upper bound for scaling up experts under limited data. We also\\nverify that MoEC plays a positive role in mitigating overfitting and sparse\\ndata allocation.',\n",
       "  'authors': [arxiv.Result.Author('Yuan Xie'),\n",
       "   arxiv.Result.Author('Shaohan Huang'),\n",
       "   arxiv.Result.Author('Tianyu Chen'),\n",
       "   arxiv.Result.Author('Furu Wei')],\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2207.09094v1',\n",
       "  'doi': None,\n",
       "  'updated': datetime.datetime(2022, 7, 19, 6, 9, 55, tzinfo=datetime.timezone.utc),\n",
       "  'published': datetime.datetime(2022, 7, 19, 6, 9, 55, tzinfo=datetime.timezone.utc),\n",
       "  'categories': ['cs.CL', 'cs.LG'],\n",
       "  'text': 'MoEC: Mixture of Expert Clusters\\nYuan Xie, Shaohan Huang, Tianyu Chen, Furu Wei\\nMicrosoft Research Asia, China\\nfv-yuanxie, shaohanh, v-tianyuchen, fuwei g@microsoft.com\\nAbstract\\nSparsely Mixture of Experts (MoE) has received great interest\\ndue to its promising scaling capability with affordable com-\\nputational overhead. MoE converts dense layers into sparse\\nexperts, and utilizes a gated routing network to make experts\\nconditionally activated. However, as the number of experts\\ngrows, MoE with outrageous parameters suffers from overﬁt-\\nting and sparse data allocation. Such problems are especially\\nsevere on tasks with limited data, thus hindering the progress\\nfor MoE models to improve performance by scaling up. In\\nthis work, we propose Mixture of Expert Clusters — a general\\napproach to enable expert layers to learn more diverse and ap-\\npropriate knowledge by imposing variance-based constraints\\non the routing stage. We further propose a cluster-level ex-\\npert dropout strategy speciﬁcally designed for the expert clus-\\nter structure. Our experiments reveal that MoEC could im-\\nprove performance on machine translation and natural lan-\\nguage understanding tasks, and raise the performance upper\\nbound for scaling up experts under limited data. We also ver-\\nify that MoEC plays a positive role in mitigating overﬁtting\\nand sparse data allocation.\\nIntroduction\\nScaling up the model capacity has shown to be promising\\nto achieve better performance on a variety of tasks, includ-\\ning natural language understanding (Brown et al. 2020; Raf-\\nfel et al. 2019) and visual representation learning (Dosovit-\\nskiy et al. 2020; Bao, Dong, and Wei 2021). The continued\\ngrowth in model size and parameters brings higher compu-\\ntational cost, while large dense models have almost hit the\\nboundary of hardware capacity. In pursuit of better compu-\\ntational efﬁciency, sparse Mixture-of-Experts (MoE) is pro-\\nposed as an efﬁcient alternative to dense models (Lepikhin\\net al. 2020; Fedus, Zoph, and Shazeer 2021; Riquelme et al.\\n2021; Lewis et al. 2021). For the sparsely-gated MoE trans-\\nformers, the feed-forward network (FFN) sub-layer will be\\nreplaced by a set of experts with independent parameters.\\nThe sparsity of MoE is brought by experts and the gated\\nrouting network. The gated routing network will calculate\\nthe routing score between input tokens and each expert and\\nactivate experts with top-k routing scores. Most experts will\\nnot be activated, thus forming a sparse structure. Since the\\nCopyright © 2022, Association for the Advancement of Artiﬁcial\\nIntelligence (www.aaai.org). All rights reserved.computation cost is only proportional to the activated top-k\\nsub-network, sparsely activated MoE models could scale up\\nmodel parameters without signiﬁcantly increasing computa-\\ntional cost. With affordable computational overhead, MoE\\nmodels could achieve better performance than dense models\\non various tasks such as neural machine translation (Lewis\\net al. 2019; Conneau and Lample 2019; Lepikhin et al.\\n2020),image recognition (Riquelme et al. 2021) and speech\\nrecognition (Kumatani et al. 2021).\\nRecent studies have reached a consensus that more ex-\\nperts mean more parameters and large model capacity,\\nwhich always bring improvements. However, some studies\\nshow that more trainable parameters and sparse conditional\\ncomputation may introduce overﬁtting (Xue et al. 2021; Lou\\net al. 2021; Xue et al. 2022), especially for downstream tasks\\nwith limited data. As depicted in Figure 1, as the number of\\nexperts grows, overﬁtting gradually becomes apparent in the\\nmachine translation task. Moreover, we ﬁnd that enlarging\\nthe size of the MoE will not always lead to improvement.\\nThere seems to exist a performance upper bound of scaling\\nup experts with limited data.\\nMoreover, we ﬁnd an unreasonable phenomenon in Fig-\\nure 1: 64-expert MoE with more parameters and larger\\nmodel capacity has higher training loss than 32-expert MoE.\\nIt implies that large-scale MoE not only suffers from overﬁt-\\nting, but also has other hidden problems that affect training.\\nAccording to our analysis, the probability of each expert get-\\nting a token reduces proportionally as the number of experts\\ngrows. With the same data, each expert will get less diverse\\nsamples. It may affect the sufﬁcient learning of expert lay-\\ners. Insufﬁcient data to match more parameters is also a ma-\\njor cause of overﬁtting. Therefore, we want to explore ways\\nin which experts could get diverse samples and learn abun-\\ndant knowledge, thereby alleviating overﬁtting and sparse\\ndata allocation.\\nIn this work, we propose Mixture of Expert Clusters\\n(MoEC), a general optimizing strategy for MoE models.\\nWe close the routing probability among neighbor experts\\nto form the clustered expert structure. The inductive bias\\nexpects that the similarity of intra-cluster experts is high\\nwhile the similarity of inter-cluster experts is low. Experts\\nwithin a cluster are prone to tokens with similar hidden\\nstates and could “share” similar tokens. Moreover, we pro-\\npose a cluster-level expert dropout strategy for the expertarXiv:2207.09094v1  [cs.CL]  19 Jul 2022\\nFigure 1: A simple demonstration of loss curves of MoE\\nmodels on WMT-14 English-to-German translation task. We\\nshow the loss curve of MoE baseline models with different\\nexperts. The value in the box represents the minimum loss.\\ncluster structure. Several experts in the cluster will be ran-\\ndomly dropped, the dropped experts will not participate in\\nthe routing stage. The activated experts will be selected from\\nthe remaining experts in the cluster. Implementing dropout\\nwithin clusters will make tokens always dispatched to suit-\\nable experts, no matter how random the dropout is.\\nWe evaluate our MoEC on machine translation and natu-\\nral language understanding tasks. Experiment results show\\nthat MoEC outperforms dense models and baseline MoE\\nmodels. It indicates that MoEC retains the advantages of the\\nsparse structure of MoE, and alleviates overﬁtting and sparse\\ndata allocation problems.\\nOur contributions are summarized as follows:\\n• We point out the overﬁtting and sparse data allocation\\nproblems for large-scale MoE models, and experts get-\\nting less diverse samples could be the common cause of\\nboth problems.\\n• We propose to build expert clusters by variance-based\\nconstraints, which allows experts to get a more diverse\\nset of similar tokens. We also implement cluster-level ex-\\npert dropout as a regularization method.\\n• We conduct experiments on translation and natural lan-\\nguage understanding tasks. MoEC could improve perfor-\\nmance and alleviate problems caused by scaling up ex-\\nperts.\\n• We ﬁnd that there exists a performance upper bound for\\nscaling up MoE models with limited data, and MoEC\\ncould raise the upper bound.\\nRelated Work\\nIn the context of modern deep learning architectures, scal-\\ning up transformers using sparse Mixture of Experts (MoE)\\nis proven to be effective to achieve state-of-the-art perfor-\\nmance on various NLP and CV tasks (Shazeer et al. 2017;\\nLepikhin et al. 2020; Riquelme et al. 2021; Fedus, Zoph, andShazeer 2021). Compared with dense transformers, an MoE\\nmodel contains several experts (feed-forward networks), and\\na router to select top-k experts for input tokens. It increases\\nthe model capacity by such conditional computation while\\nmaintaining computational efﬁciency. To future explore the\\npotential of MoE, some studies focus on router assignment\\nalgorithm (Lewis et al. 2021; Roller et al. 2021; Dai et al.\\n2022). Besides, some work focus on optimizing training\\nmethods for MoE models. Dua et al. (2021) applied a tem-\\nperature heating mechanism for sparse MoE models on the\\ntranslation task. Chi et al. (2022) proposed a dimension re-\\nduction to estimate the routing scores between tokens and\\nexperts on a low-dimensional hyper-sphere. Our work is also\\nproposed to optimize the MoE model. Instead of changing\\nthe model structure and routing strategy, MoEC establishes\\nexpert clusters, which allows experts to be assigned a more\\ndiverse set of similar tokens.\\nAlthough MoE models have achieved promising results,\\nthey are proven to have overﬁtting problems (Fedus, Zoph,\\nand Shazeer 2021; Wu et al. 2022; Xue et al. 2022) on down-\\nstream tasks with limited data. To mitigate overﬁtting, some\\nworks use knowledge distillation to distill MoE models into\\nsmall-sized MoE models or dense models (Xue et al. 2022;\\nDai et al. 2022). Another approach is to apply the dropout\\nstrategy during training. Fedus, Zoph, and Shazeer (2021)\\nset a small dropout rate at non-expert layers and a larger\\ndropout rate at expert layers. Liu et al. (2022) propose gat-\\ning dropout, which allows some tokens to ignore the gated\\nrouting network and stay at their local machines to reduce\\ncross-machine communication. In our work, we propose the\\ncluster-level expert dropout. Randomly selected experts in\\nthe cluster will be dropped so that they will not participate\\nin the routing stage.\\nPreliminary\\nTo build MoE transformers, it is a common practice to re-\\nplace feed-forward network (FFN) sub-layers with a set of\\nexperts. The experts share the same structure with the FFN\\nlayer in the dense transformer model. We denote the hidden\\nrepresentation of input token xash, and the embedding for\\nthei-th expert as ei. The router computes the routing score\\nsi=hTeito compare the similarity between hand the set\\nof expertsE. Then, the router utilizes a gating function \\x0b(\\x01)\\nto compute the gated value of expert i.\\n\\x0bi=8\\n>>><\\n>>>:exp(si)PE\\nj=1exp(sj);softmaxgating\\n1\\n1 +exp(\\x00si);sigmoidgating(1)\\nThe gating function \\x0birepresents the probability of dis-\\npatching input token to expert i. The top-k gated-value is\\nused for dispatching the token xaccording to \\x0bi. The corre-\\nsponding k expert networks are conditionally activated. We\\ndenote the set of selected top-k indices as K.\\ny=X\\ni2K\\x0bi\\x01Ei(x) (2)\\nFigure 2: Illustration of a conventional MoE layer and our\\nproposed MoEC layer. The similarity between hidden states\\nHiis represented by the color.\\nwhereEi(x)is thei-th expert network, which is a feed-\\nforward network. The output of the gated routing network is\\nthe linearly weighted combination of each expert’s compu-\\ntation on the token by the gate value.\\nMethod\\nIn this work, our goal is to give experts access to more di-\\nverse training samples, thus learning abundant knowledge\\nand mitigating overﬁtting and sparse data allocation. We\\nclose the routing probability among neighbor experts to\\nform the clustered expert structure. We apply the variance-\\nbased clustering loss to implement constraints. Then, we fur-\\nther propose a cluster-level expert dropout strategy. In our\\nwork, we use top-1 gating. Only the expert with the largest\\nrouting score is activated. And we choose softmax as our ac-\\ntivation function. Experts in a cluster will be distributed on\\nthe same device to reduce communication costs.\\nMixture of Expert Clusters\\nWe illustrated our MoEC (Mixture of Expert Clusters) in\\nFigure 2. For conventional MoE, the routing probability of\\ntokens will not be constrained. The router will always dis-\\npatch input tokens to their best-matched experts, while other\\nsimilar tokens have little chance of being selected. When\\nscaling up the number of experts, the sparse data distribution\\nwill cause each expert to get less diverse tokens. The expert\\nlayer could not get adequately trained. Also, the amount of\\ndata is insufﬁcient to match the growing number of parame-\\nters, which is also the main reason for overﬁtting. In order to\\nsolve the problems of conventional MoE, our MoEC allows\\neach expert to get more rich and diverse tokens. We impose\\nvariance-based constraints on the routing stage, aiming to\\nmake neighbor experts have similar routing probabilities for\\ninput tokens, thus forming expert clusters prone to tokens\\nwith similar hidden states. In MoEC, experts will get a more\\ndiverse set of similar input tokens by “sharing” input tokens\\nwith other experts in the cluster.\\nCompared with previous work related to MoE, our train-\\ning objective added an extra term - cluster loss. The overall\\ntraining objective is to minimize:L=Ltask+Lbalance +Lcluster (3)\\nLtaskis determined by the speciﬁc task. In our work, we\\nemploy the label smoothed cross-entropy loss for neural ma-\\nchine translation, masked language modeling loss for pre-\\ntraining language model, and negative log-likelihood loss\\n(NLL loss) or mean-squared loss (MSE loss) for GLUE\\ntasks. In the following, we will introduce Lbalance and\\nLcluster .\\nLoad Balancing Loss. During training, there exists a load\\nimbalance issue between experts (Shazeer et al. 2017; Lep-\\nikhin et al. 2020): Most tokens are dispatched to a small\\nnumber of experts, while many other experts do not get suf-\\nﬁciently trained at all. Besides, imbalanced assignments will\\nresult in a high computational bottleneck in the MoE layer\\nand thus limit the computational efﬁciency. We follow the\\nwork in (Fedus, Zoph, and Shazeer 2021) and add the bal-\\nance loss to the training objective to encourage a balanced\\nload across experts. Given Nexperts indexed by i=1 toN,\\nthe balance loss is computed as follows:\\nLbalance =\\x0bN\\x01NX\\ni=1fi\\x01pi (4)\\nwherefiis the fraction of tokens dispatching to expert i.\\nWe denote the number of tokens dispatched to the i-th expert\\nasCounti. Given a batch BwithTtokens,fi=Counti=T.\\npiis the fraction of the routing probability allocated for ex-\\npertiin the batch B. It is calculated by averaging the prob-\\nability of routing token xto expertiin the batch B.\\npi=1\\nTX\\nx2B\\x0bi(x) (5)\\nwhere\\x0bi(x)is the gating function depicted in Equation 1,\\nwhich represents the probability of dispatching token xto\\nexperti. The balance loss in Equation 4 encourages uniform\\nrouting since it would be minimized under a uniform distri-\\nbution. To control the impact of balance loss in the training\\nprocess, a hyper-parameter \\x0bis applied as a multiplicative\\ncoefﬁcient for the loss. Throughout this work, we use an\\n\\x0b= 10\\x002which was sufﬁciently large to ensure load bal-\\nancing while small enough not to overwhelm the primary\\ncross-entropy objective.\\nClustering Loss.\\nIn our work, we ﬁnd the sparse allocation of data severely\\nhinders the adequate training of MoE layers and exacerbates\\noverﬁtting. In order to allow experts to get rich and diverse\\ntokens to mitigate the impact of sparse allocation, we design\\nthe clustering loss. This loss is designed to constrain certain\\nadjacent experts so that they will share similar routing prob-\\nabilities to tokens, thus forming a cluster-like distribution.\\nFor input tokens originally dispatched to the best-matched\\nexperts, clustering loss will give them more opportunities to\\naccess other experts in the cluster. As a result, experts will\\nbe assigned a more diverse set of similar tokens, thus allevi-\\nating the problem of sparse allocation.\\nIn MoE models with Nexperts, the clustering loss will\\nguide experts to form mclusters (mshould be divisible\\nbyN), and each cluster contains L=N\\nmexperts. We use\\nEj\\nito represent the j-th expert in the i-th cluster, while pj\\ni\\nrepresents the routing probability allocated for Ej\\ni(i=\\n0;1;:::;m\\x001;j= 0;1;:::;L\\x001). According to the size\\nand number of clusters, p0\\ni;p1\\ni;:::;pL\\x001\\niwill compose a one-\\ndimensional matrix ~Pi2RLto represent the routing proba-\\nbilities of the Lexperts in the i-th cluster, and we denote the\\nmean value of them as pi. We deﬁne the clustering loss as\\nfollows:\\nLclustering =\\x0cN\\x01Cintra\\x01Cinter\\n=\\x0cN\\x01Pm\\x001\\ni=0\\x0e(~Pi)\\nm\\x01e\\x00\\x16maxfpig\\x00max2fpig\\nmaxfpig\\n(6)\\nAs can be seen from Equation 6, clustering loss is mainly\\ncomposed of two parts: the variance-based intra-cluster con-\\nstraint Cintra and the difference-based inter-cluster con-\\nstraint Cinter .\\n\\x0e(~Pi) =(p0\\ni\\x00pi)2+(p1\\ni\\x00pi)2+:::+(pL\\x001\\ni\\x00pi)2]\\nLrepresents the\\nvariance of the routing probability in the i-th cluster. We\\ncompute the mean variance of mclusters as the intra-cluster\\nconstraint Cintra , which will be minimized when the routing\\nprobabilities of experts within the same cluster are balanced.\\nBesides, we use Cinter to measure the probability dif-\\nference between the dispatched cluster and the sub-optimal\\ncluster. maxf\\x01gmeans the max value of pi(i=0,1,...,m-1)\\nandmax 2f\\x01gmeans the second max value. Cinter will be\\nminimized when the probability of a token being dispatched\\nto a suboptimal cluster is low. \\x16is the coefﬁcient used to\\ncontrol the value of Cinter . When we set \\x16= 0, the proba-\\nbility difference between clusters will not be considered. We\\ncould also set \\x16to a non-zero value to activate Cinter . We\\nwill conduct in-depth experiments and analysis on it in the\\nExperiments chapter.\\nTo minimize clustering loss, the probability distribution\\nwithin the cluster should be uniform, and the probability dif-\\nference between the clusters should be more apparent (op-\\ntional). In the initial training steps, the variance among ex-\\nperts will be very high, so the clustering loss will dominate\\nthe optimization and guide the rapid formation of expert\\nclusters. When the intra-cluster variance is stable, the clus-\\ntering loss will become relatively small to maintain the ex-\\npert clusters. Similar to the practice in balance loss, a hyper-\\nparameter\\x0cis applied. The value of the \\x0cshould be rel-\\natively small, because a large \\x0cmeans a strong clustering\\nconstraint, thus making experts in the cluster too similar. It\\nwill cause these experts to lose their characteristics, and the\\ncontributions of multiple similar experts are only approxi-\\nmately equal to one expert. In our work, we set the value\\nof\\x0cas10\\x002by default. Experiments on the selection of \\x0c\\nvalues could be found in Appendix A.\\nCluster-level expert dropout\\nWhen applying large-scale MoE models on tasks with lim-\\nited data, over-ﬁtting issues naturally arise. Previous MoE-\\nrelated work (Raffel et al. 2019; Fedus, Zoph, and Shazeer\\n2021) used dropout (Srivastava et al. 2014) at each layer to\\nFigure 3: Illustration of global-level expert dropout and\\ncluster-level expert dropout. The similarity between hidden\\nstatesHiis represented by the color.\\nprevent overﬁtting. Here, cluster-level expert dropout acts as\\na regularization technique completely different from tradi-\\ntional dropout. It does not drop parameters, but drops some\\nexperts in the cluster, which makes the dispatching of tokens\\nmore random.\\nImplementation in clusters. First, our cluster-level ex-\\npert dropout works at the routing stage, so it will only be\\nimplemented at expert layers. For experts in a cluster, we\\nrandomly drop some of them by deleting the expert ids from\\nthe candidate expert list when calculating the routing prob-\\nability. Thus, the corresponding experts will be ignored in\\nthe routing stage. Assume the dropout rate as \\r, only the re-\\nmainingN(1\\x00\\r)experts will participate in the calculation\\nof routing probability during training. The dimension of the\\nmatrixPwill decrease from RNtoRN\\x01(1\\x00\\r). All clusters\\nimplement the dropout simultaneously. It allows tokens to\\nhave more opportunities to be dispatched to other experts in\\nthe same cluster, instead of being repeatedly dispatched to\\nthe expert with the highest probability. From another per-\\nspective, each expert will receive more diverse tokens with-\\nout adding training data.\\nCluster-level expert dropout vs Traditional expert\\ndropout.\\nTraditional expert dropout is recommended in Fedus,\\nZoph, and Shazeer (2021). It is a dropout technique (Srivas-\\ntava et al. 2014) to regularize MoE models, which acts on the\\nfeed-forward layer to reduce overﬁtting caused by too many\\nparameters. By setting a relatively small dropout rate at non-\\nexpert layers (0.1), expert dropout increases the dropout rate\\nby an explicit amount at the interim feed-forward computa-\\ntion at each expert layer (0.4). Our expert dropout acts com-\\npletely different from it. We perform random dropout on the\\ncandidate list of experts during the routing stage. It does not\\nreduce the number of parameters during training but allo-\\ncates tokens more diversely and ﬂexibly. While traditional\\nexpert dropout is usually used for ﬁne-tuning on downstream\\ntasks, our cluster-level expert dropout is a general regular-\\nization mechanism with strong generality. In addition, our\\ndropout can be applied together with Fedus’ expert dropout,\\nand they can work together to improve the performance of\\nMoE.\\nWhy cluster-level is better?\\nIt is natural to think that expert dropout could be imple-\\nmented at the global level, which provides more opportu-\\nnities for tokens to access other sub-optimal experts. But\\nfor global-level expert dropout, as shown in Figure 3, if a\\nrandom dropout happens to drop suitable experts, tokens\\nmay be dispatched to less relevant experts. Inappropriate dis-\\npatching may negatively impact the learning of experts.\\nIn MoEC, We address this problem by exploiting the\\ncluster-like structure and design a cluster-level expert\\ndropout. Cluster-level dropout could give tokens the option\\nto be randomly re-dispatched while conﬁning the routing re-\\nsults to a more reasonable range. No matter how random the\\ndropout is implemented, tokens will always be dispatched to\\nexperts with similar routing probability.\\nExperiments\\nWe name our model MoEC (Mixture of Expert Clusters),\\nand evaluate the performance on bilingual machine transla-\\ntion and natural language understanding tasks. We use the\\nX-MoE model from Chi et al. (2022) as our backbone ar-\\nchitecture, which has shown better performance than prior\\nMoE models such as Switch Transformers (Fedus, Zoph,\\nand Shazeer 2021) on widely-used cross-lingual understand-\\ning benchmarks.\\nEvaluation Dataset\\nWMT 2014 English-to-German Ninth Workshop on Sta-\\ntistical Machine Translation (WMT 2014) releases a collec-\\ntion of datasets used in shared tasks including machine trans-\\nlation. We add additional news-commentary-v12 data from\\nWMT-17 for training and validation. The total training data\\ncontains 3.96M English-to-German sentence pairs.\\nGLUE General Language Understanding Evaluation (Wang\\net al. 2018) benchmark is a collection of tools for evaluating\\nthe performance of models across a diverse set of existing\\nNLU tasks, including MNLI (Williams, Nangia, and Bow-\\nman 2017), CoLA (Warstadt, Singh, and Bowman 2019),\\nSST-2 (Socher et al. 2013), QQP, QNLI (Rajpurkar et al.\\n2016), MRPC (Dolan and Brockett 2005) and STS-B (Cer\\net al. 2017). We do not perform experiments on RTE be-\\ncause previous work (Chen et al. 2022) demonstrated that\\nMoE is not suitable for this task. It is worth mentioning that\\nwe will pre-train our model on the BooksCorpus (Zhu et al.\\n2015) and English Wikipedia corpus (Foundation) for 120k\\nsteps before ﬁne-tuning on GLUE tasks.\\nExperiments Setup\\nModel Architecture For our MoEC and all baseline mod-\\nels, we follow the recommended settings in (Vaswani et al.\\n2017) and use Transformer-big as the uniﬁed backbone ar-\\nchitecture on WMT 2014 English-German translation task.\\nFor GLUE tasks, we use Transformer-base as the backbone\\narchitecture.\\nFor MoE layers, we apply the 64-expert MoE model with\\n3 FFN sub-layers in the 3rd encoder block and 3rd decoder\\nblock. A more detailed model hyper-parameters could be\\nfound in Appendix B.Baselines\\nWe conduct two baselines in our experiments. The ﬁrst is\\ndense transformer (Vaswani et al. 2017). For another, we\\nfollow the work in (Chi et al. 2022) and apply X-MoE as\\nourMoE baseline . It could serve as a strong baseline that\\nshows better performance than Switch Transformer (Fedus,\\nZoph, and Shazeer 2021) on widely-used cross-lingual un-\\nderstanding benchmarks. The MoE baseline estimates rout-\\ning scores between tokens and experts on a low-dimensional\\nhypersphere and adds a learnable temperature scalar in the\\ngating function. For a fair comparison, the two baseline\\nmethods are built with the same setting as MoEC, which\\ncould be found in Appendix B.\\nMoEC Hyper-parameters For MoEC, several unique\\nhyper-parameters are introduced. For clustering loss, we set\\n\\x0cto10\\x002according to the experiment results (see Appendix\\nA) and set\\x16= 0by default. For cluster size (the number of\\nexperts in a cluster) and expert dropout rate, we will have\\ndetailed related experiments in the following sections.\\nTraining Hyper-parameters For a fair comparison, the\\ndense model, MoE baseline model, and MoEC model share\\nthe same training hyper-parameters. All models are trained\\nwith the Adam optimizer (Kingma and Ba 2014) ( \\x0c1=\\n0:9;\\x0c2= 0:98). The learning rate is set 5e\\x004with 4000\\nwarm-up steps and inverse square root scheduler (Raffel\\net al. 2019). Batch size, training steps, and dropout rate are\\nset by different tasks, which are recorded in Appendix C.\\nExperiments results\\nWe train dense models, baseline MoE and MoEC models\\non several widely-used evaluation tasks, and the results are\\nshown in Table 1. Compared with dense models, MoE mod-\\nels exhibit signiﬁcant performance improvements, which\\nbeneﬁt from the large model capacity. Besides, MoEC could\\nbring notable improvement over the MoE baseline without\\napplying the dropout strategy to experts. On WMT-14, it\\ngives a 1.62 BLUE score boost. The advantage could be at-\\ntributed to the clustered distribution of experts, which en-\\ndows experts with more diverse and appropriate training\\nsamples. Moreover, with the application of the cluster-level\\nexpert dropout strategy, the performance of MoEC will be\\nfurther improved.\\nAs shown in Figure 4, the MoE baseline severely suf-\\nfers from overﬁtting on WMT-14, while our MoEC shows\\nexcellent ability to mitigate overﬁtting. The overﬁtting phe-\\nnomenon on the validation set is almost eliminated, and the\\nvalidation loss is relatively lower. It shows that when our\\nMoEC solves the sparse allocation of data, each expert could\\nget more abundant and diverse training samples. In this way,\\nthe training data of each expert is kept sufﬁcient, thereby\\nalleviating the phenomenon of overﬁtting. Furthermore, we\\nfound that MoEC converges slightly slower. It is due to the\\nfact that each expert needs to learn from more diverse train-\\ning samples, which takes more steps to allow the expert to\\nget sufﬁciently trained.\\nDetailed analysis of expert clusters\\nNext, we conduct a detailed analysis of expert clusters. Fig-\\nure 5 shows the fraction of tokens dispatched to cluster 0\\nTable 1: The performance on machine translation and GLUE tasks for baselines and MoEC models. WMT-14 is measured\\non the test set, while GLUE tasks are measured on the development sets. We report the average results by a set of seeds (see\\nAppendix C). All experiments are conducted with 64 experts.\\nNMT GLUE Tasks\\nWMT14 En-De MNLI CoLA SST-2 QQP QNLI MRPC STS-B GLUE Avg\\nDense 27.10 85.97 57.10 92.87 91.20 92.23 87.50 89.18 85.16\\nMoE Baseline 30.59 87.27 75.60 93.30 91.37 92.33 86.30 88.28 87.78\\nMoEC (w/o expert dropout) 32.21 87.37 75.93 93.43 91.45 92.40 88.07 89.11 88.25\\nMoEC 32.50 87.37 76.80 93.37 91.40 92.45 88.23 89.24 88.41\\nFigure 4: Loss curves on the WMT-14 validation set. All\\nexperiments are conducted with 64 experts for a fair com-\\nparison. The validation loss that rises with increasing train-\\ning steps indicates the overﬁtting phenomenon. Our MoEC\\nshows excellent ability to mitigate overﬁtting.\\n(expert 0\\x183) during training and inference. During training,\\nthe experts in cluster 0 get similar input tokens, which are\\naffected by balance loss and clustering loss. During infer-\\nence, the routing probabilities of experts in the cluster vary,\\nwhich indicates that they still retain their own characteris-\\ntics. They learn more ﬁne-grained knowledge, which is the\\nadvantage of multiple similar experts compared to a single\\nexpert. For WMT14, the BLUE score of MoE with 16 ex-\\nperts is 30.49, while the BLUE score of MoE with 16 clus-\\nters (cluster size=4) is 32.16. It shows that multiple similar\\nexperts have an obvious advantage over a single expert.\\nThe cluster size also has a critical impact on the learning\\nof MoEC, so we conduct experiments on different cluster\\nsizes. As depicted in Table 2, the best performance is ob-\\ntained when cluster size = 8. Compared to the MoE base-\\nline with 64 experts, expert clusters could bring about a 1.62\\nBLUE scores improvement. When the cluster size is rel-\\natively small, the data shared among experts will be less,\\nand the improvement brought by MoEC will not be fully\\nexploited. As a special case, when cluster size=1, a single\\nexpert could not be called a cluster, and MoEC is equiva-\\nlent to MoE baseline. When the cluster size is large, the data\\nshared among experts will increase, but the similarity and\\ncorrelation of these data will become lower, which will lead\\nto an adverse impact on the “professionalism” of each ex-\\npert. When we expand the cluster size to 16, the performance\\nFigure 5: Fraction of tokens dispatched to Expert 0 \\x183 (i.e.fi\\nmentioned above) of 64-expert MoEC (cluster size = 4) dur-\\ning training and inference. The graph on the left represents\\nthe fraction of tokens dispatched to cluster 0 during training,\\nwhile the right shows the fraction of tokens dispatched to\\ncluster 0 during inference.\\nof MoEC is even lower than that of the MoE baseline, which\\nmeans that an excessively large cluster size will suppress the\\nadvantages of MoE structure and hurt the performance.\\nTable 2: The performance of MoEC with different cluster\\nsizes on WMT-14. All experiments were conducted with 64\\nexperts. For a fair comparison, all methods do not employ\\nthe dropout on experts.\\nCluster size Number of clusters BLEU\\n1 64 30.59\\n4 16 32.16\\n8 8 32.21\\n16 4 29.98\\nExpert dropout: Cluster-level vs global-level\\nIn Table 3, we experiment on WMT-14 with the cluster-level\\nexpert dropout rate. We ﬁnd that cluster-level dropout could\\nenhance the generalization performance of MoEC. Such a\\nregularization method could bring a 0.29 BLUE scores im-\\nprovement for MoEC. Experimental results show that 0.5 is\\na good choice for the dropout rate. Besides, it is obvious that\\nglobal-level expert dropout will hurt the performance.\\nTable 3: Cluster-level vs global-level expert dropout on\\nWMT-14. All experiments are conducted on the 64-expert\\nMoEC and cluster size = 8. Under this setting, the BLUE\\nscore of MoEC without expert dropout is 32.21.\\nDropout rate cluster-level global-level\\n0 32.21 32.21\\n0.25 32.32 31.88\\n0.5 32.50 31.53\\n0.75 32.02 29.73\\nFor cluster-level expert dropout, when dropping the best-\\nmatched expert for input tokens, the routing decision will\\nstill be made among the rest experts in the cluster. Regard-\\nless of how the dropped experts are selected, there will al-\\nways be experts left in each cluster. It ensures that suitable\\nexperts are always available. But for the global-level one,\\ndue to the random distribution of experts, if all matched\\nexperts are dropped, the token will be routed to an inap-\\npropriate expert. It could cause experts to be distracted by\\nlow-relevant data, thus negatively impacting the learning of\\nknowledge. Take Figure 3 as a simple example (with set-\\nting the dropout rate to 0.5). For global-level expert dropout,\\nwhen both expert1 and expert2 are dropped, then Hnwill\\nonly be dispatched to expert3 or expert4. This inappropriate\\nallocation could hurt the performance of the model.\\nRole of the inter-cluster constraint coefﬁcient Cinter\\nWe further explore whether the inter-cluster constraint co-\\nefﬁcient Cinter (in Equation 6) will help improve perfor-\\nmance. As depicted in Figure 6, when dropout=0.75 or clus-\\nter size=4, setting \\x16to 1 will get better results. In other cases,\\nit is better not to apply inter-cluster constraints by setting \\x16\\nto 0.\\nWhen there are sufﬁcient experts in the cluster, it is better\\nnot to use the inter-cluster constraint by setting \\x16to 0. Intra-\\ncluster constraints have already made other experts in the\\ncluster have higher routing probabilities, while inter-cluster\\nconstraints will further widen the routing probability gap be-\\ntween clusters. This will cause the entropy of the routing\\nprobability distribution to be too small, which is not con-\\nducive to the learning of the gated network.\\nWe ﬁnd that the inter-cluster constraint will beneﬁt MoEC\\nwhen the cluster size is small or the expert dropout rate is\\nhigh. In this case, the number of experts in the cluster is\\nsmall, and the intra-cluster constraint alone is not enough to\\nform a globally reasonable routing probability distribution,\\nso the assistance of constraints between clusters is needed.\\nRaising the upper bound of MoE\\nIn general, a higher number of experts means higher model\\ncapacity and better performance. However, for tasks with\\nlimited data, there exists a performance upper bound on scal-\\ning up MoE models. We take a deep dive into the ability of\\nMoEC to raise the upper bound. As shown in Table 4, for the\\nMoE baseline, expert = 32 is the upper bound, which means\\nthat continuing to increase the number of experts will not\\nFigure 6: Two sets of experiments on the inter-cluster con-\\nstraint coefﬁcient Cinter . All experiments are performed on\\nWMT14 En-De. The ﬁgure on the left is about experiments\\nwith different expert dropout rates (cluster size=8), and The\\nﬁgure on the right is about experiments with different cluster\\nsizes (without expert dropout).\\nTable 4: Results of scaling up MoEC.\\nExpert num MoE baseline MoEC Beneﬁts\\n16 30.49 30.50 +0.01\\n32 30.81 30.84 +0.03\\n64 30.59 32.50 +1.91\\n128 30.21 32.40 +2.19\\nbring any gain to the model. Our MoEC not only has a per-\\nformance advantage over the MoE baseline with the same\\nnumber of experts, but also improves the upper bound from\\n32 to 64.\\nWith the increase of experts, our MoEC could bring more\\ngains. It is because MoEC could fully show its promis-\\ning ability to solve severe overﬁtting and sparse allocation\\nproblems. With the mitigation of the above two problems,\\nthe superiority of the large-scale MoE model will be bet-\\nter exerted, thereby achieving the improvement of the upper\\nbound of MoE models. With the help of MoEC, we could\\ntry to build sparse models with more experts.\\nConclusion\\nIn our work, we point out the overﬁtting and the sparse data\\nallocation problems of large-scale MoE models and pro-\\npose a novel training strategy - MoEC to convert experts\\ninto clusters. Each expert could get more abundant and di-\\nverse training samples. In this way, the training data of each\\nexpert is kept sufﬁcient, thereby alleviating overﬁtting. We\\nalso propose the cluster-level expert dropout to realize regu-\\nlarization. We conduct experiments on machine translation\\nand natural language understanding tasks. Experiment re-\\nsults show MoEC could improve performance and allevi-\\nate problems caused by scaling up experts without changing\\nthe model structure and routing strategy. The superiority of\\nthe large-scale MoE model will be better exerted by MoEC,\\nthereby raising the upper bound of MoE models. With the\\nhelp of MoEC, we could try to build sparse models with\\nmore experts.\\nReferences\\nBao, H.; Dong, L.; and Wei, F. 2021. Beit: Bert pre-training\\nof image transformers. arXiv preprint arXiv:2106.08254 .\\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\\nA.; et al. 2020. Language models are few-shot learners. Ad-\\nvances in neural information processing systems , 33: 1877–\\n1901.\\nCer, D.; Diab, M.; Agirre, E.; Lopez-Gazpio, I.; and Specia,\\nL. 2017. Semeval-2017 task 1: Semantic textual similarity-\\nmultilingual and cross-lingual focused evaluation. arXiv\\npreprint arXiv:1708.00055 .\\nChen, T.; Huang, S.; Xie, Y .; Jiao, B.; Jiang, D.; Zhou,\\nH.; Li, J.; and Wei, F. 2022. Task-Speciﬁc Expert\\nPruning for Sparse Mixture-of-Experts. arXiv preprint\\narXiv:2206.00277 .\\nChi, Z.; Dong, L.; Huang, S.; Dai, D.; Ma, S.; Patra, B.;\\nSinghal, S.; Bajaj, P.; Song, X.; and Wei, F. 2022. On\\nthe Representation Collapse of Sparse Mixture of Experts.\\narXiv preprint arXiv:2204.09179 .\\nConneau, A.; and Lample, G. 2019. Cross-lingual language\\nmodel pretraining. Advances in neural information process-\\ning systems , 32.\\nDai, D.; Dong, L.; Ma, S.; Zheng, B.; Sui, Z.; Chang, B.;\\nand Wei, F. 2022. StableMoE: Stable routing strategy for\\nmixture of experts. arXiv preprint arXiv:2204.08396 .\\nDolan, B.; and Brockett, C. 2005. Automatically construct-\\ning a corpus of sentential paraphrases. In Third International\\nWorkshop on Paraphrasing (IWP2005) .\\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\\nwords: Transformers for image recognition at scale. arXiv\\npreprint arXiv:2010.11929 .\\nDua, D.; Bhosale, S.; Goswami, V .; Cross, J.; Lewis, M.;\\nand Fan, A. 2021. Tricks for Training Sparse Translation\\nModels. arXiv preprint arXiv:2110.08246 .\\nFedus, W.; Zoph, B.; and Shazeer, N. 2021. Switch trans-\\nformers: Scaling to trillion parameter models with simple\\nand efﬁcient sparsity. arXiv preprint arXiv:2101.03961 .\\nFoundation, W. ???? Wikimedia Downloads.\\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\\nstochastic optimization. arXiv preprint arXiv:1412.6980 .\\nKumatani, K.; Gmyr, R.; Salinas, F. C.; Liu, L.; Zuo, W.;\\nPatel, D.; Sun, E.; and Shi, Y . 2021. Building a great multi-\\nlingual teacher with sparsely-gated mixture of experts for\\nspeech recognition. arXiv preprint arXiv:2112.05820 .\\nLepikhin, D.; Lee, H.; Xu, Y .; Chen, D.; Firat, O.; Huang,\\nY .; Krikun, M.; Shazeer, N.; and Chen, Z. 2020. Gshard:\\nScaling giant models with conditional computation and au-\\ntomatic sharding. arXiv preprint arXiv:2006.16668 .\\nLewis, M.; Bhosale, S.; Dettmers, T.; Goyal, N.; and Zettle-\\nmoyer, L. 2021. Base layers: Simplifying training of large,\\nsparse models. In International Conference on Machine\\nLearning , 6265–6274. PMLR.Lewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\\n2019. Bart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and comprehen-\\nsion. arXiv preprint arXiv:1910.13461 .\\nLiu, R.; Kim, Y . J.; Muzio, A.; Mozafari, B.; and Awadalla,\\nH. H. 2022. Gating Dropout: Communication-efﬁcient\\nRegularization for Sparsely Activated Transformers. arXiv\\npreprint arXiv:2205.14336 .\\nLou, Y .; Xue, F.; Zheng, Z.; and You, Y . 2021. Sparse-mlp: A\\nfully-mlp architecture with conditional computation. arXiv\\npreprint arXiv:2109.02008 .\\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Explor-\\ning the limits of transfer learning with a uniﬁed text-to-text\\ntransformer. arXiv preprint arXiv:1910.10683 .\\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\\nSquad: 100,000+ questions for machine comprehension of\\ntext. arXiv preprint arXiv:1606.05250 .\\nRiquelme, C.; Puigcerver, J.; Mustafa, B.; Neumann, M.; Je-\\nnatton, R.; Susano Pinto, A.; Keysers, D.; and Houlsby, N.\\n2021. Scaling vision with sparse mixture of experts. Ad-\\nvances in Neural Information Processing Systems , 34.\\nRoller, S.; Sukhbaatar, S.; Weston, J.; et al. 2021. Hash lay-\\ners for large sparse models. Advances in Neural Information\\nProcessing Systems , 34.\\nShazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.;\\nHinton, G.; and Dean, J. 2017. Outrageously large neu-\\nral networks: The sparsely-gated mixture-of-experts layer.\\narXiv preprint arXiv:1701.06538 .\\nSocher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,\\nC. D.; Ng, A. Y .; and Potts, C. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment treebank. In\\nProceedings of the 2013 conference on empirical methods in\\nnatural language processing , 1631–1642.\\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\\nSalakhutdinov, R. 2014. Dropout: a simple way to prevent\\nneural networks from overﬁtting. The journal of machine\\nlearning research , 15(1): 1929–1958.\\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\\ntention is all you need. Advances in neural information pro-\\ncessing systems , 30.\\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\\nBowman, S. R. 2018. GLUE: A multi-task benchmark and\\nanalysis platform for natural language understanding. arXiv\\npreprint arXiv:1804.07461 .\\nWarstadt, A.; Singh, A.; and Bowman, S. R. 2019. Neural\\nnetwork acceptability judgments. Transactions of the Asso-\\nciation for Computational Linguistics , 7: 625–641.\\nWilliams, A.; Nangia, N.; and Bowman, S. R. 2017. A\\nbroad-coverage challenge corpus for sentence understand-\\ning through inference. arXiv preprint arXiv:1704.05426 .\\nWu, L.; Liu, M.; Chen, Y .; Chen, D.; Dai, X.; and Yuan,\\nL. 2022. Residual Mixture of Experts. arXiv preprint\\narXiv:2204.09636 .\\nXue, F.; He, X.; Ren, X.; Lou, Y .; and You, Y . 2022. One\\nStudent Knows All Experts Know: From Sparse to Dense.\\narXiv preprint arXiv:2201.10890 .\\nXue, F.; Shi, Z.; Wei, F.; Lou, Y .; Liu, Y .; and You,\\nY . 2021. Go wider instead of deeper. arXiv preprint\\narXiv:2107.11817 .\\nZhu, Y .; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun,\\nR.; Torralba, A.; and Fidler, S. 2015. Aligning books and\\nmovies: Towards story-like visual explanations by watching\\nmovies and reading books. In Proceedings of the IEEE in-\\nternational conference on computer vision , 19–27.\\nAppendix\\nA Selection of the value of \\x0c\\nTable 5: The performance of MoEC with different \\x0ccoef-\\nﬁcients on WMT-14. All experiments are conducted with 64\\nexperts. The cluster sizes=8, and expert dropout rate=0.25.\\nValue of\\x0cMoEC\\n1e-3 32.21\\n5e-3 32.17\\n1e-2 32.32\\n5e-2 31.21\\nTable 5 presents the experiments on selecting the best\\nvalue of\\x0c. MoEC works best when \\x0cis set to 1e-2. And\\nwhen the beta value is too large, the performance of MoEC\\ndrops signiﬁcantly, which conﬁrms our analysis in the main\\ntext. Based on the results, we uniformly set the value of \\x0cas\\n10\\x002as a default in all experiments above.\\nB Architecture parameters\\nTable 6 presents the architecture parameters for different\\ntasks.\\nTable 6: Architecture parameters for all tasks\\n- WMT-14 En-De Pre-train&GLUE\\nTransformer blocks 12 12\\nAttention heads 16 12\\nEncoder/Decoder embedding 1024 768\\nFFN embedding 4096 3072\\nExperts [16,32,64,128] [16,32,64,128]\\nRouting dimension [8,16,32,64] [8,16,32,64]\\nMoE layers 2 1\\nSub-layers 3 3\\nC Training hyper-parameters\\nTable 7 presents the training hyper-parameters for WMT-\\n14 and pre-training. Table 8 presents the training hyper-\\nparameters on downstream GLUE tasks.Table 7: Training hyper-parameters for all tasks\\n- WMT-14 En-De Pre-train\\nOptimizer Adam Adam\\nAdam\\x0f 1e-6 1e-6\\nAdam\\x0c (0.9,0.98) (0.9,0.98)\\nTraining Steps 32k 125k\\nBatch size 8k 2k\\nMaximum learning rate 5e-4 5e-4\\nLearning Rate Scheduler inverse sqrt inverse sqrt\\nWarmup steps 4k 4k\\nWeight decay 0 0.01\\nDropout 0.3 0.1\\nAttention dropout 0.1 0\\nGradient Clip Norm 0.1 0.1\\nLabel smoothing 0.1 -\\nCapacity factor 2 2\\nMoE dropout 0.4 0\\nMoE activation dropout 0.1 0\\nbalancing coefﬁcient \\x0b 0.01 0.01\\nTable 8: Training hyper-parameters for GLUE.\\nHyper-parameters MNLI SST-2 QQP QNLI CoLA STS-B MRPC RTE\\nBatch Size 32 32 32 32 32 32 32 32\\nEpochs [3,5] [3,5] [3,5] [3,5] [3,5,10] [10,15,20] [5,10,15,20] [3,5,10]\\nLearning rate [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5\\nWarm up 16 16 16 16 16 16 16 16\\nSeed [1,2,3] [1,2,3] [1,2,3] [1,2,3] [1,2,3] [2,42,123] [2,42,123] [1,2,3]',\n",
       "  'ref': 'Bao, H.; Dong, L.; and Wei, F. 2021. Beit: Bert pre-training\\nof image transformers. arXiv preprint arXiv:2106.08254 .\\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\\nA.; et al. 2020. Language models are few-shot learners. Ad-\\nvances in neural information processing systems , 33: 1877–\\n1901.\\nCer, D.; Diab, M.; Agirre, E.; Lopez-Gazpio, I.; and Specia,\\nL. 2017. Semeval-2017 task 1: Semantic textual similarity-\\nmultilingual and cross-lingual focused evaluation. arXiv\\npreprint arXiv:1708.00055 .\\nChen, T.; Huang, S.; Xie, Y .; Jiao, B.; Jiang, D.; Zhou,\\nH.; Li, J.; and Wei, F. 2022. Task-Speciﬁc Expert\\nPruning for Sparse Mixture-of-Experts. arXiv preprint\\narXiv:2206.00277 .\\nChi, Z.; Dong, L.; Huang, S.; Dai, D.; Ma, S.; Patra, B.;\\nSinghal, S.; Bajaj, P.; Song, X.; and Wei, F. 2022. On\\nthe Representation Collapse of Sparse Mixture of Experts.\\narXiv preprint arXiv:2204.09179 .\\nConneau, A.; and Lample, G. 2019. Cross-lingual language\\nmodel pretraining. Advances in neural information process-\\ning systems , 32.\\nDai, D.; Dong, L.; Ma, S.; Zheng, B.; Sui, Z.; Chang, B.;\\nand Wei, F. 2022. StableMoE: Stable routing strategy for\\nmixture of experts. arXiv preprint arXiv:2204.08396 .\\nDolan, B.; and Brockett, C. 2005. Automatically construct-\\ning a corpus of sentential paraphrases. In Third International\\nWorkshop on Paraphrasing (IWP2005) .\\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\\nwords: Transformers for image recognition at scale. arXiv\\npreprint arXiv:2010.11929 .\\nDua, D.; Bhosale, S.; Goswami, V .; Cross, J.; Lewis, M.;\\nand Fan, A. 2021. Tricks for Training Sparse Translation\\nModels. arXiv preprint arXiv:2110.08246 .\\nFedus, W.; Zoph, B.; and Shazeer, N. 2021. Switch trans-\\nformers: Scaling to trillion parameter models with simple\\nand efﬁcient sparsity. arXiv preprint arXiv:2101.03961 .\\nFoundation, W. ???? Wikimedia Downloads.\\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\\nstochastic optimization. arXiv preprint arXiv:1412.6980 .\\nKumatani, K.; Gmyr, R.; Salinas, F. C.; Liu, L.; Zuo, W.;\\nPatel, D.; Sun, E.; and Shi, Y . 2021. Building a great multi-\\nlingual teacher with sparsely-gated mixture of experts for\\nspeech recognition. arXiv preprint arXiv:2112.05820 .\\nLepikhin, D.; Lee, H.; Xu, Y .; Chen, D.; Firat, O.; Huang,\\nY .; Krikun, M.; Shazeer, N.; and Chen, Z. 2020. Gshard:\\nScaling giant models with conditional computation and au-\\ntomatic sharding. arXiv preprint arXiv:2006.16668 .\\nLewis, M.; Bhosale, S.; Dettmers, T.; Goyal, N.; and Zettle-\\nmoyer, L. 2021. Base layers: Simplifying training of large,\\nsparse models. In International Conference on Machine\\nLearning , 6265–6274. PMLR.Lewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\\n2019. Bart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and comprehen-\\nsion. arXiv preprint arXiv:1910.13461 .\\nLiu, R.; Kim, Y . J.; Muzio, A.; Mozafari, B.; and Awadalla,\\nH. H. 2022. Gating Dropout: Communication-efﬁcient\\nRegularization for Sparsely Activated Transformers. arXiv\\npreprint arXiv:2205.14336 .\\nLou, Y .; Xue, F.; Zheng, Z.; and You, Y . 2021. Sparse-mlp: A\\nfully-mlp architecture with conditional computation. arXiv\\npreprint arXiv:2109.02008 .\\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Explor-\\ning the limits of transfer learning with a uniﬁed text-to-text\\ntransformer. arXiv preprint arXiv:1910.10683 .\\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\\nSquad: 100,000+ questions for machine comprehension of\\ntext. arXiv preprint arXiv:1606.05250 .\\nRiquelme, C.; Puigcerver, J.; Mustafa, B.; Neumann, M.; Je-\\nnatton, R.; Susano Pinto, A.; Keysers, D.; and Houlsby, N.\\n2021. Scaling vision with sparse mixture of experts. Ad-\\nvances in Neural Information Processing Systems , 34.\\nRoller, S.; Sukhbaatar, S.; Weston, J.; et al. 2021. Hash lay-\\ners for large sparse models. Advances in Neural Information\\nProcessing Systems , 34.\\nShazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.;\\nHinton, G.; and Dean, J. 2017. Outrageously large neu-\\nral networks: The sparsely-gated mixture-of-experts layer.\\narXiv preprint arXiv:1701.06538 .\\nSocher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,\\nC. D.; Ng, A. Y .; and Potts, C. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment treebank. In\\nProceedings of the 2013 conference on empirical methods in\\nnatural language processing , 1631–1642.\\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\\nSalakhutdinov, R. 2014. Dropout: a simple way to prevent\\nneural networks from overﬁtting. The journal of machine\\nlearning research , 15(1): 1929–1958.\\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\\ntention is all you need. Advances in neural information pro-\\ncessing systems , 30.\\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\\nBowman, S. R. 2018. GLUE: A multi-task benchmark and\\nanalysis platform for natural language understanding. arXiv\\npreprint arXiv:1804.07461 .\\nWarstadt, A.; Singh, A.; and Bowman, S. R. 2019. Neural\\nnetwork acceptability judgments. Transactions of the Asso-\\nciation for Computational Linguistics , 7: 625–641.\\nWilliams, A.; Nangia, N.; and Bowman, S. R. 2017. A\\nbroad-coverage challenge corpus for sentence understand-\\ning through inference. arXiv preprint arXiv:1704.05426 .\\nWu, L.; Liu, M.; Chen, Y .; Chen, D.; Dai, X.; and Yuan,\\nL. 2022. Residual Mixture of Experts. arXiv preprint\\narXiv:2204.09636 .\\nXue, F.; He, X.; Ren, X.; Lou, Y .; and You, Y . 2022. One\\nStudent Knows All Experts Know: From Sparse to Dense.\\narXiv preprint arXiv:2201.10890 .\\nXue, F.; Shi, Z.; Wei, F.; Lou, Y .; Liu, Y .; and You,\\nY . 2021. Go wider instead of deeper. arXiv preprint\\narXiv:2107.11817 .\\nZhu, Y .; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun,\\nR.; Torralba, A.; and Fidler, S. 2015. Aligning books and\\nmovies: Towards story-like visual explanations by watching\\nmovies and reading books. In Proceedings of the IEEE in-\\nternational conference on computer vision , 19–27.\\nAppendix\\nA Selection of the value of \\x0c\\nTable 5: The performance of MoEC with different \\x0ccoef-\\nﬁcients on WMT-14. All experiments are conducted with 64\\nexperts. The cluster sizes=8, and expert dropout rate=0.25.\\nValue of\\x0cMoEC\\n1e-3 32.21\\n5e-3 32.17\\n1e-2 32.32\\n5e-2 31.21\\nTable 5 presents the experiments on selecting the best\\nvalue of\\x0c. MoEC works best when \\x0cis set to 1e-2. And\\nwhen the beta value is too large, the performance of MoEC\\ndrops signiﬁcantly, which conﬁrms our analysis in the main\\ntext. Based on the results, we uniformly set the value of \\x0cas\\n10\\x002as a default in all experiments above.\\nB Architecture parameters\\nTable 6 presents the architecture parameters for different\\ntasks.\\nTable 6: Architecture parameters for all tasks\\n- WMT-14 En-De Pre-train&GLUE\\nTransformer blocks 12 12\\nAttention heads 16 12\\nEncoder/Decoder embedding 1024 768\\nFFN embedding 4096 3072\\nExperts [16,32,64,128] [16,32,64,128]\\nRouting dimension [8,16,32,64] [8,16,32,64]\\nMoE layers 2 1\\nSub-layers 3 3\\nC Training hyper-parameters\\nTable 7 presents the training hyper-parameters for WMT-\\n14 and pre-training. Table 8 presents the training hyper-\\nparameters on downstream GLUE tasks.Table 7: Training hyper-parameters for all tasks\\n- WMT-14 En-De Pre-train\\nOptimizer Adam Adam\\nAdam\\x0f 1e-6 1e-6\\nAdam\\x0c (0.9,0.98) (0.9,0.98)\\nTraining Steps 32k 125k\\nBatch size 8k 2k\\nMaximum learning rate 5e-4 5e-4\\nLearning Rate Scheduler inverse sqrt inverse sqrt\\nWarmup steps 4k 4k\\nWeight decay 0 0.01\\nDropout 0.3 0.1\\nAttention dropout 0.1 0\\nGradient Clip Norm 0.1 0.1\\nLabel smoothing 0.1 -\\nCapacity factor 2 2\\nMoE dropout 0.4 0\\nMoE activation dropout 0.1 0\\nbalancing coefﬁcient \\x0b 0.01 0.01\\nTable 8: Training hyper-parameters for GLUE.\\nHyper-parameters MNLI SST-2 QQP QNLI CoLA STS-B MRPC RTE\\nBatch Size 32 32 32 32 32 32 32 32\\nEpochs [3,5] [3,5] [3,5] [3,5] [3,5,10] [10,15,20] [5,10,15,20] [3,5,10]\\nLearning rate [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5\\nWarm up 16 16 16 16 16 16 16 16\\nSeed [1,2,3] [1,2,3] [1,2,3] [1,2,3] [1,2,3] [2,42,123] [2,42,123] [1,2,3]'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
