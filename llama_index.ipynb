{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "import arxiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    }
   ],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "\n",
    "client = GrobidClient(config_path='grobid_client_python/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.process_batch(\n",
    "    input_path='pdfs',\n",
    "    input_files = ['./pdfs/2010.11929v2.pdf', './pdfs/1706.03762v7.pdf', './pdfs/2305.02582v2.pdf'],\n",
    "    output='output',\n",
    "    n=10,\n",
    "    generateIDs=False,\n",
    "    consolidate_header=True,\n",
    "    consolidate_citations=False,\n",
    "    include_raw_citations=False,\n",
    "    include_raw_affiliations=False,\n",
    "    tei_coordinates=False,\n",
    "    segment_sentences=False,\n",
    "    force=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "COLLECTION_NAME = \"arxiv\"\n",
    "PERSIST_DIR = \"arxiv_vdb\"\n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "try:\n",
    "    chromadb.PersistentClient('arxiv_vdb').delete_collection('arxiv')\n",
    "except:\n",
    "    pass\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "vectordb = chroma.Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x7fe0339a4220>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-13 20:10:19 - Loaded .env file\n",
      "2024-02-13 20:10:19 - file_cache is only supported with oauth2client<4.0.0\n",
      "2024-02-13 20:10:20 - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=&id_list=1706.03762%2C2402.02592%2C2010.11929&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2024-02-13 20:10:21 - Got first page: 3 of 3 total results\n",
      "GROBID server is up and running\n",
      "2024-02-13 20:10:27 - Created a chunk of size 578, which is longer than the specified 512\n",
      "2024-02-13 20:10:27 - Created a chunk of size 578, which is longer than the specified 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raihan/projects/arxiv-chatbot/venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-13 20:10:29 - Created a chunk of size 537, which is longer than the specified 512\n",
      "2024-02-13 20:10:29 - Created a chunk of size 551, which is longer than the specified 512\n",
      "2024-02-13 20:10:29 - Created a chunk of size 1151, which is longer than the specified 512\n",
      "2024-02-13 20:10:29 - Created a chunk of size 1253, which is longer than the specified 512\n",
      "2024-02-13 20:10:29 - Created a chunk of size 537, which is longer than the specified 512\n",
      "2024-02-13 20:10:29 - Created a chunk of size 551, which is longer than the specified 512\n",
      "2024-02-13 20:10:29 - Created a chunk of size 1151, which is longer than the specified 512\n",
      "2024-02-13 20:10:29 - Created a chunk of size 1253, which is longer than the specified 512\n",
      "2024-02-13 20:10:30 - Created a chunk of size 544, which is longer than the specified 512\n",
      "2024-02-13 20:10:30 - Created a chunk of size 606, which is longer than the specified 512\n",
      "2024-02-13 20:10:30 - Created a chunk of size 534, which is longer than the specified 512\n",
      "2024-02-13 20:10:30 - Created a chunk of size 544, which is longer than the specified 512\n",
      "2024-02-13 20:10:30 - Created a chunk of size 606, which is longer than the specified 512\n",
      "2024-02-13 20:10:30 - Created a chunk of size 534, which is longer than the specified 512\n",
      "2024-02-13 20:10:34 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from arxiv_bot.search import IndexNewArxivPapers\n",
    "\n",
    "IndexNewArxivPapers(\n",
    "    vectordb,\n",
    "    n_search_results=3,\n",
    "    pdf_parser=\"PyMuPDF\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap=10\n",
    "    \n",
    "    )._run('transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['68b5f542-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f543-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f544-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f545-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f546-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f547-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f548-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f549-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f54a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f54b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f54c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f54d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f54e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f54f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f550-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f551-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f552-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f553-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f554-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f555-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f556-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f557-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f558-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f559-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f55a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f55b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f55c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f55d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f55e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f55f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f560-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f561-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f562-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f563-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f564-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f565-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f566-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f567-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f568-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f569-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f56a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f56b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f56c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f56d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f56e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f56f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f570-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f571-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f572-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f573-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f574-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f575-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f576-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f577-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f578-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f579-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f57a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f57b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f57c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f57d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f57e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f57f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f580-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f581-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f582-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f583-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f584-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f585-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f586-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f587-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f588-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f589-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f58a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f58b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f58c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f58d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f58e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f58f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f590-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f591-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f592-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f593-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f594-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f595-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f596-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f597-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f598-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f599-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f59a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f59b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f59c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f59d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f59e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f59f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5a0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5a1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5a2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5a3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5a4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5a5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5a6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5a7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5a8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5a9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5aa-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ab-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ac-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ad-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ae-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5af-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5b0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5b1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5b2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5b3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5b4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5b5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5b6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5b7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5b8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5b9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ba-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5bb-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5bc-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5bd-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5be-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5bf-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5c0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5c1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5c2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5c3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5c4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5c5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5c6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5c7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5c8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5c9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ca-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5cb-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5cc-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5cd-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ce-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5cf-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5d0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5d1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5d2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5d3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5d4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5d5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5d6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5d7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5d8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5d9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5da-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5db-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5dc-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5dd-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5de-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5df-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5e0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5e1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5e2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5e3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5e4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5e5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5e6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5e7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5e8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5e9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ea-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5eb-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ec-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ed-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ee-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ef-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5f0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5f1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5f2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5f3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5f4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5f5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5f6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5f7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5f8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5f9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5fa-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5fb-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5fc-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5fd-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5fe-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f5ff-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f600-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f601-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f602-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f603-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f604-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f605-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f606-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f607-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f608-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f609-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f60a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f60b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f60c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f60d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f60e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f60f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f610-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f611-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f612-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f613-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f614-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f615-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f616-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f617-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f618-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f619-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f61a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f61b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f61c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f61d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f61e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f61f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f620-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f621-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f622-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f623-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f624-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f625-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f626-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f627-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f628-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f629-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f62a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f62b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f62c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f62d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f62e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f62f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f630-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f631-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f632-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f633-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f634-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f635-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f636-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f637-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f638-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f639-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f63a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f63b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f63c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f63d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f63e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f63f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f640-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f641-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f642-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f643-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f644-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f645-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f646-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f647-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f648-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f649-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f64a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f64b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f64c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f64d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f64e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f64f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f650-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f651-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f652-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f653-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f654-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f655-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f656-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f657-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f658-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f659-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f65a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f65b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f65c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f65d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f65e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f65f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f660-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f661-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f662-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f663-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f664-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f665-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f666-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f667-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f668-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f669-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f66a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f66b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f66c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f66d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f66e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f66f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f670-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f671-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f672-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f673-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f674-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f675-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f676-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f677-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f678-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f679-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f67a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f67b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f67c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f67d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f67e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f67f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f680-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f681-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f682-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f683-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f684-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f685-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f686-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f687-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f688-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f689-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f68a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f68b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f68c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f68d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f68e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f68f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f690-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f691-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f692-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f693-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f694-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f695-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f696-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f697-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f698-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f699-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f69a-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f69b-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f69c-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f69d-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f69e-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f69f-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6a0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6a1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6a2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6a3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6a4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6a5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6a6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6a7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6a8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6a9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6aa-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ab-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ac-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ad-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ae-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6af-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6b0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6b1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6b2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6b3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6b4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6b5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6b6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6b7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6b8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6b9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ba-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6bb-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6bc-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6bd-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6be-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6bf-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6c0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6c1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6c2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6c3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6c4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6c5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6c6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6c7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6c8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6c9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ca-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6cb-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6cc-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6cd-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ce-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6cf-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6d0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6d1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6d2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6d3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6d4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6d5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6d6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6d7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6d8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6d9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6da-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6db-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6dc-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6dd-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6de-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6df-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6e0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6e1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6e2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6e3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6e4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6e5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6e6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6e7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6e8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6e9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ea-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6eb-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ec-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ed-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ee-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ef-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6f0-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6f1-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6f2-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6f3-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6f4-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6f5-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6f6-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6f7-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6f8-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6f9-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6fa-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6fb-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6fc-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6fd-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6fe-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f6ff-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f700-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f701-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f702-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f703-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f704-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f705-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f706-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f707-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f708-ca8a-11ee-bbf2-395025af1bc9',\n",
       "  '68b5f709-ca8a-11ee-bbf2-395025af1bc9'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.',\n",
       "   'authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       "   'chunk_id': '1706.03762v7-125',\n",
       "   'date': '2017-06-12',\n",
       "   'paper_id': '1706.03762v7',\n",
       "   'title': 'Attention Is All You Need'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'Deep learning for time series forecasting has traditionally operated within a\\none-model-per-dataset framework, limiting its potential to leverage the\\ngame-changing impact of large pre-trained models. The concept of universal\\nforecasting, emerging from pre-training on a vast collection of time series\\ndatasets, envisions a single Large Time Series Model capable of addressing\\ndiverse downstream forecasting tasks. However, constructing such a model poses\\nunique challenges specific to time series data: i) cross-frequency learning,\\nii) accommodating an arbitrary number of variates for multivariate time series,\\nand iii) addressing the varying distributional properties inherent in\\nlarge-scale data. To address these challenges, we present novel enhancements to\\nthe conventional time series Transformer architecture, resulting in our\\nproposed Masked Encoder-based Universal Time Series Forecasting Transformer\\n(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive\\n(LOTSA) featuring over 27B observations across nine domains, Moirai achieves\\ncompetitive or superior performance as a zero-shot forecaster when compared to\\nfull-shot models. Code, model weights, and data will be released.',\n",
       "   'authors': 'Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo',\n",
       "   'chunk_id': '2402.02592v1-177',\n",
       "   'date': '2024-02-04',\n",
       "   'paper_id': '2402.02592v1',\n",
       "   'title': 'Unified Training of Universal Time Series Forecasting Transformers'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'},\n",
       "  {'abstract': 'While the Transformer architecture has become the de-facto standard for\\nnatural language processing tasks, its applications to computer vision remain\\nlimited. In vision, attention is either applied in conjunction with\\nconvolutional networks, or used to replace certain components of convolutional\\nnetworks while keeping their overall structure in place. We show that this\\nreliance on CNNs is not necessary and a pure transformer applied directly to\\nsequences of image patches can perform very well on image classification tasks.\\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\\nTransformer (ViT) attains excellent results compared to state-of-the-art\\nconvolutional networks while requiring substantially fewer computational\\nresources to train.',\n",
       "   'authors': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',\n",
       "   'chunk_id': '2010.11929v2-151',\n",
       "   'date': '2020-10-22',\n",
       "   'paper_id': '2010.11929v2',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'}],\n",
       " 'documents': ['Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5].\\n\\nNumerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures',\n",
       "  '[38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences.\\n\\nAligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples.',\n",
       "  'Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation\\n\\n[32], while also improving model performance in case of the latter.\\n\\nThe fundamental\\nconstraint of sequential computation, however, remains.\\n\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences',\n",
       "  '[2, 19].\\n\\nIn all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\n\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\n\\n\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.',\n",
       "  '2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet\\n\\n[18] and ConvS2S\\n\\n[9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions.\\n\\nIn these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.',\n",
       "  'This makes\\nit more difficult to learn dependencies between distant positions [12].\\n\\nIn the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\n\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence.',\n",
       "  'Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].',\n",
       "  'To the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.\\n\\nIn the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure\\n\\n[5, 2, 35].',\n",
       "  'Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn).\\n\\nGiven z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time.\\n\\nAt each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n\\n\\n2\\nFigure 1: The Transformer - model architecture.',\n",
       "  'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers.\\n\\nEach layer has two\\nsub-layers.\\n\\nThe first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network.',\n",
       "  'We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1].\\n\\nThat is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself.\\n\\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\n\\n\\nThe decoder is also composed of a stack of N = 6 identical layers.',\n",
       "  'In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack.\\n\\nSimilar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization.\\n\\nWe also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions.',\n",
       "  'This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors.\\n\\nThe output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention.',\n",
       "  '(right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n\\n\\n3.2.1\\nScaled Dot-Product Attention\\n\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).\\n\\nThe input consists of\\nqueries and keys of dimension dk, and values of dimension dv.',\n",
       "  'We compute the dot products of the\\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the\\nvalues.\\n\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q.\\n\\nThe keys and values are also packed together into matrices K and V .\\n\\nWe compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\ndk\\n)\\n\\nV\\n(1)\\nThe two most commonly used attention functions are additive attention',\n",
       "  '[2], and dot-product (multi-\\nplicative) attention.\\n\\nDot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\ndk .\\n\\nAdditive attention computes the compatibility function using a feed-forward network with\\na single hidden layer.\\n\\nWhile the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.',\n",
       "  'While for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk\\n\\n[3].\\n\\nWe suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4.\\n\\nTo counteract this effect, we scale the dot products by\\n1\\ndk .',\n",
       "  '3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.',\n",
       "  'On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1.\\n\\nThen their dot product, q  k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values.',\n",
       "  'These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions.\\n\\nWith a single attention head, averaging inhibits this.\\n\\n\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )',\n",
       "  'Where the projections are parameter matrices W Q\\ni\\n Rdmodeldk, W K\\ni\\n Rdmodeldk, W V\\ni\\n Rdmodeldv\\nand W O  Rhdvdmodel.\\n\\n\\nIn this work we employ h = 8 parallel attention layers, or heads.\\n\\nFor each of these we use\\ndk = dv = dmodel/h = 64.\\n\\nDue to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n\\n\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n',\n",
       "  'In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder.\\n\\nThis allows every\\nposition in the decoder to attend over all positions in the input sequence.\\n\\nThis mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\n\\n\\n\\nThe encoder contains self-attention layers.',\n",
       "  'In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder.\\n\\nEach position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\n\\n\\n\\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position.',\n",
       "  'We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property.\\n\\nWe implement this\\ninside of scaled dot-product attention by masking out (setting to ) all values in the input\\nof the softmax which correspond to illegal connections.\\n\\nSee Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks',\n",
       "  'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically.\\n\\nThis\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x)\\n\\n= max(0, xW1 + b1)W2 + b2\\n(2)\\n\\n\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer.',\n",
       "  'Another way of describing this is as two convolutions with kernel size 1.\\n\\n\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n\\n\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel.\\n\\nWe also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities.',\n",
       "  'In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30].\\n\\nIn the embedding layers, we multiply those weights by dmodel.\\n\\n\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types.\\n\\nn is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.',\n",
       "  'Layer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2  d)\\nO(1)\\nO(1)\\n\\n\\nRecurrent\\nO(n  d2)\\nO(n)\\nO(n)\\n\\n\\nConvolutional\\nO(k  n  d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r  n  d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence.',\n",
       "  'To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks.\\n\\nThe positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed.\\n\\nThere are many choices of positional encodings,\\nlearned and fixed [9].\\n\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension.',\n",
       "  'That is, each dimension of the positional encoding\\ncorresponds to a sinusoid.\\n\\nThe wavelengths form a geometric progression from 2 to 10000  2.\\n\\nWe\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.',\n",
       "  'We also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)).\\n\\nWe chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.',\n",
       "  '4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi  Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder.\\n\\nMotivating our use of self-attention we\\nconsider three desiderata.\\n\\n\\nOne is the total computational complexity per layer.',\n",
       "  'Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\n\\n\\nThe third is the path length between long-range dependencies in the network.\\n\\nLearning long-range\\ndependencies is a key challenge in many sequence transduction tasks.\\n\\nOne key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network.',\n",
       "  'The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies\\n\\n[12].\\n\\nHence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\n\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations.',\n",
       "  'In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair\\n\\n[31] representations.',\n",
       "  'To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position.\\n\\nThis would increase the maximum\\npath length to O(n/r).\\n\\nWe plan to investigate this approach further in future work.\\n\\n\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions.',\n",
       "  'Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n))\\n\\nin the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network.\\n\\nConvolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k  n  d + n  d2).',\n",
       "  'Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\n\\n\\nAs side benefit, self-attention could yield more interpretable models.\\n\\nWe inspect attention distributions\\nfrom our models and present and discuss examples in the appendix.',\n",
       "  'Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n\\n\\n5\\nTraining\\nThis section describes the training regime for our models.\\n\\n\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs.\\n\\nSentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens.',\n",
       "  'For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38].\\n\\nSentence pairs were batched together by approximate sequence length.\\n\\nEach training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\n\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs.',\n",
       "  'For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds.\\n\\nWe\\ntrained the base models for a total of 100,000 steps or 12 hours.\\n\\nFor our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds.\\n\\nThe big models were trained for 300,000 steps\\n(3.5 days).\\n\\n\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with 1 = 0.9, 2 = 0.98 and  = 109.',\n",
       "  'We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d0.5\\nmodel  min(step_num0.5, step_num  warmup_steps1.5)\\n(3)\\n\\n\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number.',\n",
       "  'We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\n\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet\\n\\n[18]\\n23.75\\nDeep-Att + PosUnk\\n\\n[39]\\n39.2\\n1.0  1020\\nGNMT + RL',\n",
       "  '[38]\\n24.6\\n39.92\\n2.3  1019\\n1.4  1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6  1018\\n1.5  1020\\nMoE\\n\\n[32]\\n26.03\\n40.56\\n2.0  1019\\n1.2  1020\\nDeep-Att + PosUnk Ensemble\\n\\n[39]\\n40.4\\n8.0  1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8  1020\\n1.1  1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7  1019\\n1.2  1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3  1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3  1019\\nResidual Dropout\\n\\n\\nWe apply dropout\\n\\n[33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized.',\n",
       "  'In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks.\\n\\nFor the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ls = 0.1\\n\\n[36].\\n\\nThis\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.',\n",
       "  '6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4.\\n\\nThe configuration of this model is\\nlisted in the bottom line of Table 3.\\n\\nTraining took 3.5 days on 8 P100 GPUs.',\n",
       "  'Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\n\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model.\\n\\nThe Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.',\n",
       "  'For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals.\\n\\nFor the big models, we averaged the last 20 checkpoints.\\n\\nWe\\nused beam search with a beam size of 4 and length penalty\\n\\n = 0.6\\n\\n[38].\\n\\nThese hyperparameters\\nwere chosen after experimentation on the development set.\\n\\nWe set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].',\n",
       "  'Table 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature.',\n",
       "  'We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture.',\n",
       "  'Unlisted values are identical to those of the base\\nmodel.\\n\\nAll metrics are on the English-to-German translation development set, newstest2013.\\n\\nListed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\n\\n\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8',\n",
       "  '32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013.\\n\\nWe used beam search as described in the previous section, but no\\ncheckpoint averaging.\\n\\nWe present these results in Table 3.',\n",
       "  'In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2.\\n\\nWhile single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.',\n",
       "  'This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial.\\n\\nWe further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting.\\n\\nIn row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.',\n",
       "  '6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing.\\n\\nThis task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input.\\n\\nFurthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].',\n",
       "  'We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences.\\n\\nWe also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37].\\n\\nWe used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.',\n",
       "  'We performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model.\\n\\nDuring inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al.\\n\\n(2014)',\n",
       "  '(2014)\\n\\n[37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006)\\n\\n[29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013)\\n\\n[40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016)\\n\\n[8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013)\\n\\n[40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009)\\n\\n[14]\\nsemi-supervised\\n91.3\\nMcClosky et al.\\n\\n(2006)\\n\\n[26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014)',\n",
       "  '[37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015)\\n\\n[23]\\nmulti-task\\n93.0\\nDyer et al. (2016)\\n\\n[8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300.\\n\\nWe used a beam size of 21 and  = 0.3\\nfor both WSJ only and the semi-supervised setting.',\n",
       "  'Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar\\n\\n[8].\\n\\n\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser\\n\\n[29] even when training only on the WSJ training set of 40K sentences.',\n",
       "  '7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\n\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.\\n\\nOn both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art.',\n",
       "  'In the former task our best\\nmodel outperforms even all previously reported ensembles.\\n\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks.\\n\\nWe\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video.\\n\\nMaking generation less sequential is another research goals of ours.',\n",
       "  'The code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\n\\n\\nAcknowledgements\\n\\n\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n\\n\\n\\n\\nRecurrent neural networks, long short-term memory\\n\\n[13] and gated recurrent',\n",
       "  '[7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5].\\n\\nNumerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures\\n\\n[38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences.',\n",
       "  'Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples.\\n\\nRecent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation',\n",
       "  '[32], while also improving model performance in case of the latter.\\n\\nThe fundamental\\nconstraint of sequential computation, however, remains.\\n\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences\\n\\n[2, 19].\\n\\nIn all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.',\n",
       "  'In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\n\\n\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet',\n",
       "  '[18] and ConvS2S\\n\\n[9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions.\\n\\nIn these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\\n\\nThis makes\\nit more difficult to learn dependencies between distant positions [12].',\n",
       "  'In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\n\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence.',\n",
       "  'Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].',\n",
       "  'To the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.\\n\\nIn the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure\\n\\n[5, 2, 35].',\n",
       "  'Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn).\\n\\nGiven z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time.\\n\\nAt each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n\\n\\n2\\nFigure 1: The Transformer - model architecture.',\n",
       "  'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers.\\n\\nEach layer has two\\nsub-layers.\\n\\nThe first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network.',\n",
       "  'We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1].\\n\\nThat is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself.\\n\\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\n\\n\\nThe decoder is also composed of a stack of N = 6 identical layers.',\n",
       "  'In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack.\\n\\nSimilar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization.\\n\\nWe also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions.',\n",
       "  'This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors.\\n\\nThe output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention.',\n",
       "  '(right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n\\n\\n3.2.1\\nScaled Dot-Product Attention\\n\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).\\n\\nThe input consists of\\nqueries and keys of dimension dk, and values of dimension dv.',\n",
       "  'We compute the dot products of the\\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the\\nvalues.\\n\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q.\\n\\nThe keys and values are also packed together into matrices K and V .\\n\\nWe compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\ndk\\n)\\n\\nV\\n(1)\\nThe two most commonly used attention functions are additive attention',\n",
       "  '[2], and dot-product (multi-\\nplicative) attention.\\n\\nDot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\ndk .\\n\\nAdditive attention computes the compatibility function using a feed-forward network with\\na single hidden layer.\\n\\nWhile the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.',\n",
       "  'While for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk\\n\\n[3].\\n\\nWe suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4.\\n\\nTo counteract this effect, we scale the dot products by\\n1\\ndk .',\n",
       "  '3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.',\n",
       "  'On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1.\\n\\nThen their dot product, q  k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values.',\n",
       "  'These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions.\\n\\nWith a single attention head, averaging inhibits this.\\n\\n\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )',\n",
       "  'Where the projections are parameter matrices W Q\\ni\\n Rdmodeldk, W K\\ni\\n Rdmodeldk, W V\\ni\\n Rdmodeldv\\nand W O  Rhdvdmodel.\\n\\n\\nIn this work we employ h = 8 parallel attention layers, or heads.\\n\\nFor each of these we use\\ndk = dv = dmodel/h = 64.\\n\\nDue to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n\\n\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n',\n",
       "  'In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder.\\n\\nThis allows every\\nposition in the decoder to attend over all positions in the input sequence.\\n\\nThis mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\n\\n\\n\\nThe encoder contains self-attention layers.',\n",
       "  'In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder.\\n\\nEach position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\n\\n\\n\\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position.',\n",
       "  'We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property.\\n\\nWe implement this\\ninside of scaled dot-product attention by masking out (setting to ) all values in the input\\nof the softmax which correspond to illegal connections.\\n\\nSee Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks',\n",
       "  'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically.\\n\\nThis\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x)\\n\\n= max(0, xW1 + b1)W2 + b2\\n(2)\\n\\n\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer.',\n",
       "  'Another way of describing this is as two convolutions with kernel size 1.\\n\\n\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n\\n\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel.\\n\\nWe also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities.',\n",
       "  'In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30].\\n\\nIn the embedding layers, we multiply those weights by dmodel.\\n\\n\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types.\\n\\nn is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.',\n",
       "  'Layer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2  d)\\nO(1)\\nO(1)\\n\\n\\nRecurrent\\nO(n  d2)\\nO(n)\\nO(n)\\n\\n\\nConvolutional\\nO(k  n  d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r  n  d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence.',\n",
       "  'To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks.\\n\\nThe positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed.\\n\\nThere are many choices of positional encodings,\\nlearned and fixed [9].\\n\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension.',\n",
       "  'That is, each dimension of the positional encoding\\ncorresponds to a sinusoid.\\n\\nThe wavelengths form a geometric progression from 2 to 10000  2.\\n\\nWe\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.',\n",
       "  'We also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)).\\n\\nWe chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.',\n",
       "  '4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi  Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder.\\n\\nMotivating our use of self-attention we\\nconsider three desiderata.\\n\\n\\nOne is the total computational complexity per layer.',\n",
       "  'Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\n\\n\\nThe third is the path length between long-range dependencies in the network.\\n\\nLearning long-range\\ndependencies is a key challenge in many sequence transduction tasks.\\n\\nOne key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network.',\n",
       "  'The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies\\n\\n[12].\\n\\nHence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\n\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations.',\n",
       "  'In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair\\n\\n[31] representations.',\n",
       "  'To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position.\\n\\nThis would increase the maximum\\npath length to O(n/r).\\n\\nWe plan to investigate this approach further in future work.\\n\\n\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions.',\n",
       "  'Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n))\\n\\nin the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network.\\n\\nConvolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k  n  d + n  d2).',\n",
       "  'Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\n\\n\\nAs side benefit, self-attention could yield more interpretable models.\\n\\nWe inspect attention distributions\\nfrom our models and present and discuss examples in the appendix.',\n",
       "  'Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n\\n\\n5\\nTraining\\nThis section describes the training regime for our models.\\n\\n\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs.\\n\\nSentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens.',\n",
       "  'For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38].\\n\\nSentence pairs were batched together by approximate sequence length.\\n\\nEach training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\n\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs.',\n",
       "  'For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds.\\n\\nWe\\ntrained the base models for a total of 100,000 steps or 12 hours.\\n\\nFor our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds.\\n\\nThe big models were trained for 300,000 steps\\n(3.5 days).\\n\\n\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with 1 = 0.9, 2 = 0.98 and  = 109.',\n",
       "  'We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d0.5\\nmodel  min(step_num0.5, step_num  warmup_steps1.5)\\n(3)\\n\\n\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number.',\n",
       "  'We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\n\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet\\n\\n[18]\\n23.75\\nDeep-Att + PosUnk\\n\\n[39]\\n39.2\\n1.0  1020\\nGNMT + RL',\n",
       "  '[38]\\n24.6\\n39.92\\n2.3  1019\\n1.4  1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6  1018\\n1.5  1020\\nMoE\\n\\n[32]\\n26.03\\n40.56\\n2.0  1019\\n1.2  1020\\nDeep-Att + PosUnk Ensemble\\n\\n[39]\\n40.4\\n8.0  1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8  1020\\n1.1  1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7  1019\\n1.2  1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3  1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3  1019\\nResidual Dropout\\n\\n\\nWe apply dropout\\n\\n[33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized.',\n",
       "  'In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks.\\n\\nFor the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ls = 0.1\\n\\n[36].\\n\\nThis\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.',\n",
       "  '6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4.\\n\\nThe configuration of this model is\\nlisted in the bottom line of Table 3.\\n\\nTraining took 3.5 days on 8 P100 GPUs.',\n",
       "  'Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\n\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model.\\n\\nThe Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.',\n",
       "  'For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals.\\n\\nFor the big models, we averaged the last 20 checkpoints.\\n\\nWe\\nused beam search with a beam size of 4 and length penalty\\n\\n = 0.6\\n\\n[38].\\n\\nThese hyperparameters\\nwere chosen after experimentation on the development set.\\n\\nWe set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].',\n",
       "  'Table 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature.',\n",
       "  'We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture.',\n",
       "  'Unlisted values are identical to those of the base\\nmodel.\\n\\nAll metrics are on the English-to-German translation development set, newstest2013.\\n\\nListed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\n\\n\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8',\n",
       "  '32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013.\\n\\nWe used beam search as described in the previous section, but no\\ncheckpoint averaging.\\n\\nWe present these results in Table 3.',\n",
       "  'In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2.\\n\\nWhile single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.',\n",
       "  'This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial.\\n\\nWe further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting.\\n\\nIn row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.',\n",
       "  '6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing.\\n\\nThis task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input.\\n\\nFurthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].',\n",
       "  'We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences.\\n\\nWe also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37].\\n\\nWe used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.',\n",
       "  'We performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model.\\n\\nDuring inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al.\\n\\n(2014)',\n",
       "  '(2014)\\n\\n[37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006)\\n\\n[29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013)\\n\\n[40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016)\\n\\n[8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013)\\n\\n[40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009)\\n\\n[14]\\nsemi-supervised\\n91.3\\nMcClosky et al.\\n\\n(2006)\\n\\n[26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014)',\n",
       "  '[37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015)\\n\\n[23]\\nmulti-task\\n93.0\\nDyer et al. (2016)\\n\\n[8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300.\\n\\nWe used a beam size of 21 and  = 0.3\\nfor both WSJ only and the semi-supervised setting.',\n",
       "  'Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar\\n\\n[8].\\n\\n\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser\\n\\n[29] even when training only on the WSJ training set of 40K sentences.',\n",
       "  '7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\n\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.\\n\\nOn both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art.',\n",
       "  'In the former task our best\\nmodel outperforms even all previously reported ensembles.\\n\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks.\\n\\nWe\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video.\\n\\nMaking generation less sequential is another research goals of ours.',\n",
       "  'The code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\n\\n\\nAcknowledgements\\n\\n\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.',\n",
       "  'In the era of foundation models (FMs) (Bommasani et al.,\\n2021), the landscape of deep learning for time series fore-\\ncasting is experiencing a revolution.\\n\\nIn contrast to FMs capa-\\nble of tackling a multitude of downstream tasks, the current\\ndeep forecasting paradigm, involving training a model on\\na single dataset with a fixed context and prediction length,\\nappears increasingly antiquated, lacking the capacity to gen-\\neralize or adapt to diverse scenarios or datasets.',\n",
       "  'Given\\nthe unreasonable effectiveness of large pre-trained models\\n1Salesforce AI Research 2School of Computing and Infor-\\nmation Systems, Singapore Management University.\\n\\nCorrespon-\\ndence to: Gerald Woo <gwoo@salesforce.com>, Chenghao Liu\\n<chenghao.liu@salesforce.com>.\\nPreprint.\\n\\n\\nUniversal Forecaster\\n1) Multiple Frequencies\\n2) Any-variate Forecasting\\n3) Varying Distributions\\nMultiple Domains\\nFigure 1.',\n",
       "  'A universal forecaster is a large pre-trained model capa-\\nble of handling any time series forecasting problem.\\n\\nIt is trained\\non a large-scale time series dataset spanning multiple domains.\\n\\n\\nCompared to the existing paradigm, universal forecasting faces the\\nthree key issues of i) multiple frequencies, ii) any-variate forecast-\\ning, and iii) varying distributions.\\nin improving performance and data efficiency via transfer\\nlearning in modalities like vision and language (Dosovitskiy\\net al., 2020;',\n",
       "  'Brown et al., 2020), we are starting to see a\\npush to transition away from the existing paradigm, towards\\na universal forecasting paradigm (see Figure 1), where a sin-\\ngle large pre-trained model is able to handle any time series\\nforecasting problem.\\n\\nHowever, the road to building a univer-\\nsal time series forecasting model is mired with challenges.\\n\\n\\nUnlike the modalities of vision and language which have the\\nunified formats of images and text respectively, time series\\ndata is highly heterogeneous.',\n",
       "  'Firstly, the frequency (e.g.\\nminutely, hourly, daily sampling rates) of time series plays\\nan important role in determining the patterns present in the\\ntime series.\\n\\nCross-frequency learning has been shown to be\\na challenging task due to negative interference (Van Ness\\net al., 2023), with existing work simply avoiding this\\nproblem for multi-frequency datasets by learning one model\\nper frequency (Oreshkin et al., 2020).',\n",
       "  'Secondly, time series\\ndata are heterogeneous in terms of dimensionality, whereby\\nmultivariate time series can have different number of\\nvariates.\\n\\nFurthermore, each variate measures a semantically\\ndifferent quantity across datasets.\\n\\nWhile considering each\\nvariate of a multivariate time series independently (Nie et al.,\\n2023; Ekambaram et al., 2023) can sidestep this problem,\\nwe expect a universal model to be sufficiently flexible to\\n1\\narXiv:2402.02592v1',\n",
       "  '[cs.LG]  4 Feb 2024\\nUnified Training of Universal Time Series Forecasting Transformers\\nTable 1.\\n\\nComparison between pre-trained forecasting models.\\n\\n\\nindicates a work in progress, where authors have indicated their\\nintention, and ? indicates unknown intention, as of writing.\\n\\n\\nAny-variate\\n(Zero-shot)\\nProbabilistic\\nForecasting\\nFlexible\\nDistribution\\nPre-training Data (Size)\\nOpen-source\\nMOIRAI\\n\\n\\n\\nLOTSA (> 27B)',\n",
       "  '\\nTimeGPT-1\\n\\n\\n\\nUnknown (100B)\\n\\nForecastPFN\\n\\n\\n-\\nSynthetic Data (60M)\\n\\nLag-Llama\\n\\n\\n\\nMonash (< 1B)\\n\\nPreDcT\\n\\n\\n-\\nGoogle Trends + Others (1B)\\n?\\nTTM\\n\\n\\n-\\nMonash (< 1B)\\n\\nLLMTime\\n\\n\\n\\nWeb-scale Text\\n\\nconsider multivariate interactions and take into account\\nexogenous covariates.',\n",
       "  'Thirdly, probabilistic forecasting is a\\ncritical feature often required by practitioners, yet, different\\ndatasets have differing support and distributional properties\\n for example, using a symmetric distribution (e.g. Normal,\\nStudent-T) as the predictive distribution is not suitable\\nfor positive time series  making standard approaches of\\npre-defining a simple parametric distribution (Salinas et al.,\\n2020) to be insufficiently flexible to capture a wide variety\\nof datasets.',\n",
       "  'Lastly, a large pre-trained model capable\\nof universal forecasting requires a large-scale dataset\\nfrom diverse domains.\\n\\nExisting time series datasets are\\ninsufficiently large to support the training of such models.',\n",
       "  'Starting from a masked encoder architecture which has\\nbeen shown to be a strong candidate architecture for scaling\\nup pre-trained time series forecasting models (Woo et al.,\\n2023), we alleviate the above issues by introducing novel\\nmodifications which allows the architecture to handle\\nthe heterogeneity of arbitrary time series data.\\n\\n\\nFirstly,\\nwe propose to learn multiple input and output projection\\nlayers to handle the differing patterns from time series of\\nvarying frequencies.',\n",
       "  'Using patch-based projections with\\nlarger patch sizes for high-frequency data and vice versa,\\nprojection layers are specialized to learn the patterns of that\\nfrequency.',\n",
       "  'Secondly, we address the problem of varying\\ndimensionality with our proposed Any-variate Attention,\\nwhich simultaneously considers both time and variate axes\\nas a single sequence, leveraging Rotary Position Embed-\\ndings (RoPE) (Su et al., 2024), and learned binary attention\\nbiases (Yang et al., 2022b) to encode time and variate axes\\nrespectively.\\n\\nImportantly, Any-variate Attention allows the\\nmodel to take as input arbitrary number of variates.',\n",
       "  'Thirdly,\\nwe overcome the issue of requiring flexible predictive\\ndistributions with a mixture of parametric distributions.\\n\\n\\nFurthermore, optimizing the negative log-likelihood of a\\nflexible distribution has the added benefit of being competi-\\ntive with target metric optimization (Awasthi et al., 2022), a\\npowerful feature for pre-training universal forecasters, given\\nthat it can be evaluated with any target metric subsequently.',\n",
       "  'To power the training of our Large Time Series Model\\n(LTM), we introduce the Large-scale Open Time Series\\nArchive (LOTSA), the largest collection of open time series\\ndatasets with 27B observations across nine domains.\\n\\nWe\\noptimize the negative log-likelihood of the mixture distri-\\nbution, and randomly sample context and prediction lengths\\nduring training, allowing for flexible downstream usage\\nof the pre-trained model.',\n",
       "  'We train our proposed method,\\nMasked EncOder-based UnIveRsAl TIme Series Forecast-\\ning Transformer (MOIRAI1), in three sizes  MOIRAISmall,\\nMOIRAIBase, and MOIRAILarge, with 14m, 91m, and\\n311m parameters respectively.\\n\\nWe perform experimental\\nevaluations on both in and out-of-distribution settings, and\\nshow that MOIRAI consistently achieves competitive or\\nsuperior performance compared to state-of-the-art full-shot\\nbaselines.\\n\\nOur contributions are summarized as follows:\\n\\n\\n1.',\n",
       "  '1.\\n\\nWe introduce a novel Transformer architecture to\\nsupport the requirements of universal forecasting.\\n\\n\\nCrucially, the components we propose extend beyond\\nmasked encoders and are versatile, applicable to a\\nbroad range of Transformer variants.\\n\\n\\n2.\\n\\nWe introduce LOTSA, a new large-scale collection of\\nopen time series datasets to empower pre-training of\\nLTMs.\\n\\nLOTSA, the model weights, and our library\\nfor unified training of universal time series models,\\nUNI2TS, will be fully open sourced.',\n",
       "  '3. Trained on LOTSA data, MOIRAI achieves competitive\\nor superior performance as a zero-shot forecaster when\\ncompared to full-shot models.\\n\\n\\n2.\\n\\nRelated Work\\nPre-training for Zero-shot Forecasting\\nTable 1 pro-\\nvides a summary of the key differences between recent\\npre-trained models with zero-shot forecasting capabilities,\\nwhich is a recently emerging field.',\n",
       "  'TimeGPT-1 (Garza\\n& Mergenthaler-Canseco, 2023) first presented a closed-\\nsource model, offering zero-shot forecasting capabilities\\nas well as supporting fine-tuning through an API, currently\\nonly available to their beta users.\\n\\nForecastPFN (Dooley\\net al., 2023) proposes to pre-train on synthetic time series,\\nwhich can be subsequently be leveraged as a zero-shot\\nforecaster, albeit specialized for data or time limited\\nsettings.',\n",
       "  'Lag-llama (Rasul et al., 2023) works towards a\\nfoundation model for time series forecasting, leveraging\\nthe LLaMA (Touvron et al., 2023) architecture design with\\nlagged time series features, and also presents neural scaling\\nlaws for time series forecasting.\\n\\nPreDcT\\n\\n(Das et al., 2023b)\\nis a patch-based decoder-only foundation model for time\\nseries forecasting, introducing a larger output patch size for\\nfaster decoding.',\n",
       "  'They collect a private dataset from Google\\nTrends to pre-train their model in combination with open-\\ndata.\\n\\nTiny Time Mixers (TTMs) (Ekambaram et al., 2024)\\n1In ancient Greek religion and mythology, the Moirai, often\\nknown in English as the Fates, were the personifications of destiny.\\n\\n\\n(Wikipedia contributors, 2024)\\n2\\nUnified Training of Universal Time Series Forecasting Transformers\\nis a concurrent work leveraging lightweight mixer-style\\narchitecture.',\n",
       "  'They perform data augmentation by downsam-\\npling high-frequency time series, and support multivariate\\ndownstream tasks by fine-tuning an exogenous mixer.\\n\\n\\nleverage Large Language Models (LLMs), pre-trained on\\nweb-scale text data, have been leveraged for zero-shot fore-\\ncasting.\\n\\nSpecifically, LLMTime (Gruver et al., 2023) treats\\ntime series as strings, applying careful pre-processing based\\non the specific LLMs tokenizer, showing that LLMs have\\nthe inherent capability to perform zero-shot forecasting.',\n",
       "  'Pre-train\\n\\n+\\n\\nFine-tune for Time Series Forecasting\\nPre-training with subsequent fine-tuning on downstream\\nforecasting tasks has predated the recent zero-shot fore-\\ncasting efforts.',\n",
       "  'Denoising autoencoders (Zerveas et al.,\\n2021) and contrastive learning (Yue et al., 2022; Woo et al.,\\n2022) have been shown to be effective pretext tasks for\\ntime series forecasting, but have largely been applied to\\nthe existing paradigm of pre-training and fine-tuning on\\nthe same dataset, without exploring their generalization\\ncapabilities.',\n",
       "  'More recently, Dong et al. (2023) explored\\ncombining both reconstruction and contrastive based\\npre-training approaches, and performed initial explorations\\ninto cross-dataset transfer.\\n\\nThe topic has been well explored,\\nand we refer readers to more comprehensive surveys (Zhang\\net al., 2023; Ma et al., 2023).\\n\\nReprogramming is a recent\\ndirection which involves fine-tuning the model weights\\nof an LLM which has been pre-trained on text data, for\\ndownstream tasks for other modalities.',\n",
       "  'Zhou et al. (2023);\\nJin et al. (2023) introduce modules and fine-tuning methods\\nto adapt LLMs for time series tasks including forecasting.\\n\\n\\n3. Method\\nProblem\\nFormulation\\nConsider\\na\\ndataset\\nof\\nN\\ntime series D\\n=\\n{(Y (i), Z(i))}N\\ni=1, where Y (i)\\n=\\n(y(i)\\n1 , y(i)\\n2 , . . .\\n\\n, y(i)\\nTi )  RdyiTi is a target time series of\\ndyi variates and Ti time steps.\\n\\nEach time series is associated\\nwith a set of covariates Z(i) = (z(i)\\n1 , z(i)\\n2 , . . .\\n\\n, z(i)\\n\\n\\nTi ) \\nRdziTi.',\n",
       "  'The goal is to forecast the predictive distribution\\np(Yt:t+h|) by predicting distribution parameters  via\\na learned model f : (Ytl:t, Ztl:t+h) 7\\n which\\nmaximizes the log-likelihood:\\nmax\\n\\nE\\n(Y,Z)p(D)\\n(t,l,h)p(T |D)\\nlog p(Yt:t+h| ),\\ns.t.  = f(Ytl:t, Ztl:t+h),\\n(1)\\nwhere p(D) is the data distribution which samples for a time\\nseries, (Y , Z), and p(T |D) is the task distribution which\\ndefines the lookback window, Ytl:t = (ytl, . . .',\n",
       "  ', yt1)\\nwith\\ncontext\\nlength\\nl\\nand\\nforecast\\nhorizon,\\nYt:t+h = (yt, . .\\n\\n. , yt+h1) with prediction length h.\\n3.1.\\n\\nArchitecture\\nIllustrated in Figure 2, MOIRAI follows a (non-overlapping)\\npatch-based approach to modeling time series with a masked\\nencoder architecture.\\n\\nOne of our proposed modifications\\nto extend the architecture to the any-variate setting is to\\nflatten multivariate time series, considering all variates as\\na single sequence.',\n",
       "  'Patches are subsequently projected into\\nvector representations via a multi patch size input projection\\nlayer.\\n\\nThe [mask] signifies a learnable embedding which\\nreplaces patches falling within the forecast horizon.\\n\\nThe out-\\nput tokens are then decoded via the multi patch size output\\nprojection into the parameters of the mixture distribution.',\n",
       "  'While not visualized, (non-learnable) instance normalization\\n(Kim et al., 2022) is applied to inputs/outputs, aligning with\\nthe current standard practice for deep forecasting models.\\n\\n\\nThe core Transformer module is an encoder-only Trans-\\nformer architecture, leveraging various improvements as\\nproposed by recent state-of-the-art LLM architectures.',\n",
       "  'We\\nuse pre-normalization (Xiong et al., 2020) and replace all\\nLayerNorms with RMSNorm (Zhang & Sennrich, 2019),\\nand also apply query-key normalization (Henry et al., 2020).\\n\\n\\nThe non-linearity in FFN layers are replaced with SwiGLU\\n(Shazeer, 2020), adjusting the hidden dimension to have\\nequal number of parameters as the original FFN layer.\\n\\nWe\\nomit biases in all layers of the Transformer module.\\n\\n\\n3.1.1.\\n\\nMULTI PATCH SIZE PROJECTION LAYERS',\n",
       "  'In the context of universal forecasting, a single model should\\npossess the capability to handle time series spanning a wide\\nrange of frequencies.\\n\\nExisting patch-based architectures\\nrely on a single patch size hyperparameter, a legacy feature\\nfrom the prevailing one-model-per-dataset paradigm.',\n",
       "  'In-\\nstead, we aim for a more flexible strategy: opting for a larger\\npatch size to handle high-frequency data, thereby lower the\\nburden of the quadratic computation cost of attention while\\nmaintaining a long context length.\\n\\nSimultaneously, we\\nadvocate for a smaller patch size for low-frequency data to\\ntransfer computation to the Transformer layers, rather than\\nrelying solely on simple linear embedding layers.',\n",
       "  'To im-\\nplement this approach, we propose learning multiple input\\nand output embedding layers, each associated with varying\\npatch sizes.\\n\\nThe selection of the appropriate patch size for\\na given time series frequency relies on pre-defined settings\\n(see Appendix B.1).\\n\\nNote that we only learn one set of\\nprojection weights per patch size, which is shared amongst\\nfrequencies if there is an overlap based on the settings.\\n\\n\\n3.1.2.',\n",
       "  '3.1.2.\\n\\nANY-VARIATE ATTENTION\\nUniversal forecasters must be equipped to handle arbitrary\\nmultivariate time series.\\n\\nExisting time series Transform-\\ners often rely on an independent variate assumption or are\\nlimited to a single dimensionality due to embedding layers\\n3\\nUnified Training of Universal Time Series Forecasting Transformers\\nVariate 0\\nVariate 1\\nVariate 2\\nPatch Size 8\\nPatch Size 16\\nPatch Size 32\\nPatch Size 64\\nPatch Size 128\\nMulti Patch Size\\nInput Projection\\n[mask]',\n",
       "  '[mask]\\nPatch\\nEmbedding\\n2\\n2\\n2\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n2\\n0\\n1\\n2\\n0\\n1\\n2\\nVariate ID\\nTime ID\\nPatch Size 8\\nPatch Size 16\\nPatch Size 32\\nPatch Size 64\\nPatch Size 128\\nMulti Patch Size\\nOutput Projection\\nTransformer (Full Self-Attention)\\nMixture\\nDistribution\\nFigure 2.\\n\\nOverall architecture of MOIRAI.\\n\\nVisualized is a 3-variate time series, where variates 0 and 1 are target variables (i.e. to be\\nforecasted, and variate 2 is a dynamic covariate (values in forecast horizon known).',\n",
       "  'Based on a patch size of 64, each variate is patchified\\ninto 3 tokens.\\n\\nThe patch embeddings along with sequence and variate id are fed into the Transformer.\\n\\nThe shaded patches represent the\\nforecast horizon to be forecasted, whose corresponding output representations are mapped into the mixture distribution parameters.\\n\\n\\nmapping Rdy  Rdh, where Rdh is the hidden dimension.',\n",
       "  'We overcome this limitation as shown in Figure 2, by flat-\\ntening a multivariate time series to consider all variates as a\\nsingle sequence.\\n\\nThis introduces a new requirement of hav-\\ning variate encodings to enable the model to disambiguate\\nbetween different variates in the sequence.\\n\\nFurthermore, we\\nneed to ensure that permutation equivariance w.r.t. variate\\nordering, and permutation invariance w.r.t.\\n\\nvariate indices\\nare respected.',\n",
       "  'Conventional approaches like sinusoidal or\\nlearned embeddings do not meet these requirements, and\\nare unable to handle an arbitrary number of variates.\\n\\nTo\\naddress this, we propose Any-variate Attention, leveraging\\nbinary attention biases to encode variate indices.',\n",
       "  'Dropping layer and attention head indices, and scaling factor\\nfor brevity, the attention score between the (i, m)-th query\\nwhere i represents the time index and m represents the\\nvariate index, and the (j, n)-th key, Aij,mn  R, is given by:\\nEij,mn = (W Qxi,m)T\\n\\nRij(W Kxj,n)\\n+ u(1)  1{m=n} + u(2)  1{m=n},\\n(2)\\nAij,mn =\\nexp{Eij,mn}\\nP\\nk,o exp{Eik,mo},\\n(3)\\nwhere W Qxi,m, W Kxj,n  Rdh are the respective query\\nand key vectors,\\n\\nRij \\n\\nRdhdh is the rotary matrix (Su\\net al., 2024), u(1), u(2)',\n",
       "  ' R are learnable scalars for each\\nhead in each layer, and 1{cond} =\\n\\x08 1, if cond\\n0, otherwise is the indica-\\ntor function.\\n\\nThe binary attention bias component allows\\nfor disambiguation between variates via attention scores,\\nfulfills the criteria of permutation equivariance/invariance\\nw.r.t.\\n\\nvariate ordering/indices, and can extend to arbitrary\\nnumber of variates.\\n\\n\\n3.1.3.',\n",
       "  '3.1.3.\\n\\nMIXTURE DISTRIBUTION\\nTo achieve the goal of having a flexible distribution, yet\\nensuring that operations of sampling and evaluating the\\nloss function remains simple, we propose to use a mixture\\nof parametric distributions.\\n\\nA mixture distribution of c\\ncomponents has p.d.f.:\\np(Yt:t+h| ) =\\nc\\nX\\ni=1\\nwipi(Yt:t+h| i),\\n(4)\\nwhere \\n=\\n{w1, 1, . . .\\n\\n, wc, c}, and pi is the i-th\\ncomponents p.d.f.',\n",
       "  'While the choice of mixture components\\nis flexible and implementing any combination of parametric\\ndistributions is straightforward, we specifically propose\\nto use the following mixture components: i) a Students\\nt-distribution which has shown to be a robust option for\\ngeneral time series, ii) a negative binomial distribution\\nfor positive count data, iii) a log-normal distribution to\\nmodel right-skewed data commonly across economic and\\nand natural phenomena, and iv) a low variance normal\\ndistribution for high confidence predictions.',\n",
       "  'Further details\\ncan be found in Appendix B.2.\\n\\n\\n3.2.\\n\\nUnified Training\\n3.2.1.\\n\\nLOTSA DATA\\nExisting work has predominantly relied on three primary\\nsources of data  the Monash Time Series Forecasting\\nArchive (Godahewa et al., 2021), datasets provided by the\\nGluonTS library (Alexandrov et al., 2020), and datasets\\nfrom the popular long sequence forecasting benchmark\\n(Lai et al., 2018; Wu et al., 2021).\\n\\nWhile Monash and\\n4\\nUnified Training of Universal Time Series Forecasting Transformers\\nTable 2.',\n",
       "  'Key statistics of LOTSA by domain.\\n\\n\\nEnergy\\nTransport\\nClimate\\nCloudOps\\nWeb\\nSales\\nNature\\nEcon/Fin\\nHealthcare\\n# Datasets\\n30\\n23\\n6\\n3\\n3\\n6\\n5\\n23\\n6\\n# Obs.\\n\\n\\n16,358,600,896\\n4,900,453,419\\n4,188,011,890\\n1,518,268,292\\n428,082,373\\n197,984,339\\n28,547,647\\n24,919,596\\n1,594,281\\n%\\n59.17%\\n17.73%\\n15.15%\\n5.49%\\n1.55%\\n0.72%\\n0.09%\\n0.10%\\n0.01%\\nTable 3.\\n\\nKey statistics of LOTSA by frequency.\\n\\n\\nYearly\\nQuarterly\\nMonthly\\nWeekly\\nDaily\\n(Multi) Hourly\\n(Multi) Minute-level\\n(Multi) Second-level\\n# Datasets\\n4\\n5\\n10\\n7\\n21\\n31\\n25\\n2\\n# Obs.',\n",
       "  '873,297\\n2,312,027\\n11,040,648\\n18,481,871\\n709,017,118\\n19,875,993,973\\n7,013,949,430\\n14,794,369\\n%\\n0.003%\\n0.008%\\n0.040%\\n0.067%\\n2.565%\\n71.893%\\n25.370%\\n0.054%\\nGluonTS comprise of datasets from diverse domains, they\\nare constrained in size, with approximately 1B observations\\ncombined.\\n\\nIn comparison, LLMs are trained on trillions\\nof tokens.\\n\\nDas et al. (2023b) builds a private dataset based\\non Google Trends, but lacks diversity and is similarly sized\\nat 1B observations.',\n",
       "  'The effectiveness of FMs heavily stem from large-scale pre-\\ntraining data.\\n\\nGiven that existing data sources fall short of\\nsupporting such a paradigm, attempting to train an LTM\\non them may result in misleading conclusions.\\n\\nThus, we\\ntackle this issue head-on by building a large-scale archive\\nof open time series datasets by collating publicly available\\nsources of time series datasets.\\n\\nThis effort aims to cover\\na broad spectrum of domains, consolidating datasets from\\ndiverse sources with varying formats.',\n",
       "  'We design a uni-\\nfied storage format using Arrow (Richardson et al., 2023)\\nwhich is ready for deep learning pipelines.\\n\\nThe result-\\ning collection, LOTSA, spans nine domains, with a total\\nof 27, 646, 462, 733 observations, with key statistics in Ta-\\nbles 2 and 3, and in-depth details in Appendix A.\\n3.2.2.\\n\\nPRE-TRAINING\\nAs introduced in Equation (1), our pre-training task is for-\\nmulated to optimize the mixture distribution log-likelihood.',\n",
       "  'The design of both the data distribution and task distribu-\\ntion are two critical aspects of the pre-training pipeline.\\n\\n\\nThis design imparts versatile capabilities to our LTM, en-\\nabling it to adapt to a range of downstream tasks.\\n\\nThis\\nflexibility stands in contrast to the prevailing deep forecast-\\ning paradigm, where models are typically specialized for\\nspecific datasets and settings.\\n\\n\\nData Distribution\\nThe data distribution, (Y, Z)  p(D),\\ndefines how time series are sampled from the dataset.',\n",
       "  'Trained on LOTSA, which is a dataset of datasets, we in-\\ntroduce the notion of sub-datasets, by decomposing the\\ndata distribution into a sub-dataset distribution, and a time\\nseries distribution conditioned on a sub-dataset, p(D) =\\np(Y, Z|D)p(D).\\n\\nThus, we first sample a sub-dataset from\\np(D), and given that sub-dataset, we sample a time series.\\n\\n\\nTable 4.\\n\\nDetails of MOIRAI model sizes.',\n",
       "  'Layers\\ndmodel\\ndff\\nHeads\\ndkv\\nParams\\nMOIRAISmall\\n6\\n384\\n1536\\n6\\n64\\n14m\\nMOIRAIBase\\n12\\n768\\n3072\\n12\\n64\\n91m\\nMOIRAILarge\\n24\\n1024\\n4096\\n16\\n64\\n311m\\n\\n\\nFor K sub-datasets, where Dk represents the set of indices\\nof time series belonging to sub-dataset k, the structure of\\np(Y (i), Z(i)|Dk) =\\nTi1{iDk}\\nP\\njDk Tj , proportionate to the num-\\nber of observations, is straightforward.',\n",
       "  'However, due to data imbalance across domains and fre-\\nquency, we avoid sampling sub-datasets proportionately,\\nand instead cap the contribution of each sub-dataset at\\n = 0.001, before re-normalizing: p(Dk) =\\nk\\nPK\\ni=1 i ,\\nwhere k = min(|Dk|,)\\n\\n\\nPK\\ni |Di| , and |Dk| = P\\niDk Ti.\\n\\n\\nTask Distribution\\nDifferent from the existing deep fore-\\ncasting paradigm, we aim to train a model with forecasting\\ncapabilities over varying context and prediction lengths.',\n",
       "  'Rather than defining a fixed context and prediction length,\\nwe sample from a task distribution, (t, l, h)  p(T |D)\\nwhich defines the lookback window and forecasting horizon,\\ngiven a time series.\\n\\nIn practice, rather than sampling t, l, h,\\ngiven a time series, we crop a uniformly sampled window,\\nwhose length is uniformly sampled from a range.\\n\\nThis range\\nis defined by a minimum sequence length per variate of 2,\\nand a total maximum sequence length of 512.',\n",
       "  'The window\\nis then split into lookback and horizon segments, where\\nthe prediction length is uniformly sampled as a proportion\\n(within the range [0.15, 0.5]) of the window.\\n\\nWe further\\naugment training by i) uniformly subsampling multivariate\\ntime series in the variate dimension, and ii) constructing\\nmultivariate time series from sub-datasets with univariate\\ntime series, by randomly concatenating them.',\n",
       "  'The number\\nof variates is sampled from a beta-binomial distribution\\nwith parameters n = 128, a = 2, b = 5 which supports a\\nmaximum of 128 variates, with mean\\n\\n 37 for efficiency.\\n\\n\\n5\\nUnified Training of Universal Time Series Forecasting Transformers\\nSES\\nNaive\\nTheta\\nARIMA\\nETS\\nPR\\nN-BEATS\\nTransformer\\nCatBoost\\nDeepAR\\nTBATS\\nWaveNet\\nFFNN\\nMoiraiSmall\\nMoiraiBase\\nMoiraiLarge\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nNormalized MAE\\nFigure 3.\\n\\nAggregate results of the Monash Time Series Forecasting\\nBenchmark.',\n",
       "  'The normalized MAE is reported, which normalizes\\nthe MAE of each dataset by the naive forecasts MAE, and aggre-\\ngated by taking the geometric mean across datasets.\\n\\n\\nTraining\\nWe train MOIRAI in three sizes  small, base,\\nand large, with key parameter details listed in Table 4.\\n\\n\\nThe small model is trained for 100, 000 steps, while\\nbase and large models are trained for 1, 000, 000 steps\\nwith a batch size of 256.',\n",
       "  'For optimization, we use the\\nAdamW optimizer with the following hyperparameters,\\nlr = 1e-3, weight decay = 1e-1, 1 = 0.9, 2 = 0.98.\\n\\nWe\\nalso apply a learning rate scheduler with linear warmup\\nfor the first 10, 000 steps, and cosine annealing thereafter.\\n\\n\\nModels are trained on NVIDIA A100-40G GPUs with\\nTF32 precision.',\n",
       "  'We implement sequence packing (Raffel\\net al., 2020) to avoid large amounts of padding due to\\nthe disparity of sequence lengths in the new setting with\\nvarying context, prediction, and variate lengths, thereby\\nincreasing the effective batch size.\\n\\n\\n4.\\n\\nExperiments\\n4.1.\\n\\nIn-distribution Forecasting\\nWe first perform an in-distribution evaluation using the\\nMonash benchmark, which aim to measure generaliza-\\ntion capability across diverse domains.',\n",
       "  'Described in Ap-\\npendix A, LOTSA includes the Monash Time Series Fore-\\ncasting Archive as a source of data.\\n\\nFor a large portion of\\nthese datasets, we only include the train set, holding out\\nthe test set which we now use for in-distribution evaluation.',\n",
       "  'In this evaluation, we consider a standard setting with a\\ncontext length of 1000, and a patch size of 32 for all fre-\\nquencies, except for quarterly data with a patch size of 8.\\nFigure 3 summarizes the results based on the normalized\\nmean absolute error (MAE), in comparison with the base-\\nlines presented in the Monash benchmark.\\n\\nIt is worth noting\\nthat each baseline in the Monash benchmark is typically\\ntrained individually per dataset or per time series within a\\ndataset.',\n",
       "  'In contrast, MOIRAI stands out by being a single\\nmodel evaluated across various datasets.\\n\\nFull results as well\\nas a comparison with LLMTime (Gruver et al., 2023) can\\nbe found in Appendix D.1.\\n\\n\\nWe observe that MOIRAI outperforms all baselines from the\\nMonash benchmark regardless of model size, displaying the\\nstrong in-distribution and cross-domain capabilities arising\\nfrom our unified training methodology.',\n",
       "  'We highlight that\\neach instance of MOIRAI is a single model evaluated across\\ndatasets, compared to baselines for which one model is\\ntrained per dataset.\\n\\n\\n4.2.\\n\\nOut-of-distribution / Zero-shot Forecasting\\nNext, we perform an out-of-distribution evaluation on un-\\nseen target datasets.\\n\\nHere, MOIRAI is a zero-shot fore-\\ncaster compared with state-of-the-art full-shot baselines\\nwhich have been trained on the individual target datasets.',\n",
       "  'While the ideal scenario would be to include other univer-\\nsal forecasters, this proves to be a challenging task.\\n\\nAs a\\nnascent field, most universal forecasters currently do not yet\\nhave open weights avaiable for evaluation.\\n\\nFurthermore, the\\nproblem of comparing zero-shot methods is exacerbated by\\nnot having a standard held-out test split, making it challeng-\\ning to collate a set of datasets which all the models have not\\nbeen trained on.',\n",
       "  'Thus, we establish the strong zero-shot ca-\\npabilities of MOIRAI by displaying competitive or stronger\\nresults compared with SOTA full-shot methods  datasets\\nused in the following have not been included in LOTSA.\\n\\n\\nProbabilistic Forecasting\\nWe evaluate on seven datasets\\nacross energy, transport, climate, and sales domains,\\nfollowing a rolling evaluation setup with stride equal to\\nprediction length.\\n\\nPrediction lengths and number of rolling\\nevaluations are defined for each dataset based on frequency.',\n",
       "  'We report the Continuous Ranked Probability Score (CRPS)\\nand Mean Scaled Interval Score (MSIS) metrics (definitions\\nin Appendix C), comparing against four full-shot baselines \\nDeepAR (Salinas et al., 2020), PatchTST (Nie et al., 2023),\\nand TiDE (Das et al., 2023a) with Students t-distribution\\nprediction heads, and TFT based on quantile prediction\\n(Lim et al., 2021), all implemented with the GluonTS\\nlibrary (Alexandrov et al., 2020), as well as simple\\nbaselines AutoARIMA (Garza et al., 2022) and Seasonal\\nNaive (Hyndman & Athanasopoulos, 2018).',\n",
       "  'For each\\ndataset and baseline, we perform hyperparameter tuning\\non a validation CRPS, and report results averaged over\\nfive training runs with different seeds.\\n\\nFor MOIRAI, we\\nperform inference time tuning, selecting context length\\nfrom {1000, 2000, 3000, 4000, 5000} and patch sizes based\\non frequency, on the validation CRPS.\\n\\nFull details of the\\nevaluation setting can be found in Appendix C.\\nTable 5 reports the CRPS and MSIS, with full results\\nincluding deterministic metrics in Appendix D.2.',\n",
       "  'We\\nobserve that MOIRAIBase and MOIRAILarge consistently\\nachieve strong zero-shot performance, obtaining either best\\nor second best results for all datasets except Walmart and\\nIstanbul Traffic.\\n\\nEven for these datasets, performance is\\nstill close to the best performance, despite being a single\\nzero-shot model compared to baselines which have been\\ntuned and trained on the train sets.\\n\\n\\n6\\nUnified Training of Universal Time Series Forecasting Transformers\\nTable 5.\\n\\nProbabilistic forecasting results.',\n",
       "  'Best results are highlighted in bold, and second best results are underlined.\\n\\nBaseline results are\\naggregated over five training runs with different seeds, reporting the mean and standard deviation.',\n",
       "  'Zero-shot\\nFull-shot\\nBaseline\\nMOIRAISmall\\nMOIRAIBase\\nMOIRAILarge\\nPatchTST\\nTiDE\\nTFT\\nDeepAR\\nAutoARIMA\\nSeasonal Naive\\nElectricity\\nCRPS\\n0.072\\n0.055\\n0.050\\n0.0520.00\\n0.0480.00\\n0.0500.00\\n0.0650.01\\n0.327\\n0.070\\nMSIS\\n7.999\\n6.172\\n5.875\\n5.7440.12\\n5.6720.08\\n6.2780.24\\n6.8930.82\\n29.412\\n35.251\\nSolar\\nCRPS\\n0.471\\n0.419\\n0.406\\n0.5180.09\\n0.4200.00\\n0.4460.03\\n0.4310.01\\n1.055\\n0.512\\nMSIS\\n8.425\\n7.011\\n6.250\\n8.4471.59\\n13.7540.32\\n8.0573.51\\n11.1810.67\\n25.849\\n48.130\\nWalmart\\nCRPS\\n0.103\\n0.093\\n0.098\\n0.0820.01\\n0.0770.00\\n0.0870.00\\n0.1210.00\\n0.124\\n0.151\\nMSIS\\n9.371\\n8.421\\n8.520\\n6.0050.21\\n6.2580.12\\n8.7180.10\\n12.5020.03\\n9.888\\n49.458\\nWeather\\nCRPS\\n0.049\\n0.041\\n0.051\\n0.0590.01\\n0.0540.00\\n0.0430.00\\n0.1320.11\\n0.252\\n0.068\\nMSIS\\n5.236\\n5.136\\n4.962\\n7.7590.49\\n8.0951.74\\n7.7910.44\\n21.65117.34\\n19.805\\n31.293\\nIstanbul Traffic\\nCRPS\\n0.173\\n0.116\\n0.112\\n0.1120.00\\n0.1100.01\\n0.1100.01\\n0.1080.00\\n0.589\\n0.257\\nMSIS\\n5.937\\n4.461\\n4.277\\n3.8130.09\\n4.7520.17\\n4.0570.44\\n4.0940.31\\n16.317\\n45.473\\nTurkey Power\\nCRPS\\n0.048\\n0.040\\n0.036\\n0.0540.01\\n0.0460.01\\n0.0390.00\\n0.0660.02\\n0.116\\n0.085\\nMSIS\\n7.127\\n6.766\\n6.341\\n8.9780.51\\n8.5790.52\\n7.9430.31\\n13.5201.17\\n14.863\\n36.256\\nTable 6.',\n",
       "  'Long sequence forecasting results.\\n\\nResults are averaged across prediction lengths {96, 192, 336, 720}.\\n\\nBest results are highlighted\\nin bold, and second best results are underlined.\\n\\nFull-shot results are obtained from Liu et al. (2023b).',\n",
       "  'Zero-shot\\nFull-shot\\nMOIRAISmall\\nMOIRAIBase\\nMOIRAILarge\\niTransformer\\nTimesNet\\nPatchTST\\nCrossformer\\nTiDE\\nDLinear\\nSCINet\\nFEDformer\\nETTh1\\nMSE\\n0.400\\n0.434\\n0.510\\n0.454\\n0.458\\n0.469\\n0.529\\n0.541\\n0.456\\n0.747\\n0.44\\nMAE\\n0.424\\n0.438\\n0.469\\n0.448\\n0.450\\n0.455\\n0.522\\n0.507\\n0.452\\n0.647\\n0.46\\nETTh2\\nMSE\\n0.341\\n0.345\\n0.354\\n0.383\\n0.414\\n0.387\\n0.942\\n0.611\\n0.559\\n0.954\\n0.437\\nMAE\\n0.379\\n0.382\\n0.376\\n0.407\\n0.497\\n0.407\\n0.684\\n0.550\\n0.515\\n0.723\\n0.449\\nETTm1\\nMSE\\n0.448\\n0.381\\n0.390\\n0.407\\n0.400\\n0.387\\n0.513\\n0.419\\n0.403\\n0.486\\n0.448\\nMAE\\n0.409\\n0.388\\n0.389\\n0.410\\n0.406\\n0.400\\n0.495\\n0.419\\n0.407\\n0.481\\n0.452\\nETTm2\\nMSE\\n0.300\\n0.272\\n0.276\\n0.288\\n0.291\\n0.281\\n0.757\\n0.358\\n0.35\\n0.571\\n0.305\\nMAE\\n0.341\\n0.321\\n0.320\\n0.332\\n0.333\\n0.326\\n0.611\\n0.404\\n0.401\\n0.537\\n0.349\\nElectricity\\nMSE\\n0.233\\n0.188\\n0.188\\n0.178\\n0.193\\n0.216\\n0.244\\n0.252\\n0.212\\n0.268\\n0.214\\nMAE\\n0.320\\n0.274\\n0.273\\n0.270\\n0.295\\n0.304\\n0.334\\n0.344\\n0.3\\n0.365\\n0.327\\nWeather\\nMSE\\n0.242\\n0.238\\n0.259\\n0.258\\n0.259\\n0.259\\n0.259\\n0.271\\n0.265\\n0.292\\n0.309\\nMAE\\n0.267\\n0.261\\n0.275\\n0.278\\n0.287\\n0.281\\n0.315\\n0.320\\n0.317\\n0.363\\n0.36\\nLong Sequence Forecasting\\nWe evaluate on a subset\\nof the popular long sequence forecasting benchmark (Wu\\net al., 2021), omitting datasets which have datasets from\\nthe same source present in our pre-training data and cannot\\nbe considered zero-shot.',\n",
       "  'We report the Mean Squared Er-\\nror (MSE) and MAE, comparing against six state-of-the-art\\nbaselines, iTransformer (Liu et al., 2023b), TimesNet (Wu\\net al., 2023), PatchTST, Crossformer (Zhang & Yan, 2023),\\nTiDE, DLinear (Zeng et al., 2023), SCINet (Liu et al., 2022),\\nand FEDformer (Zhou et al., 2022b).\\n\\nPoint forecasts are ob-\\ntained from MOIRAI by taking the median from the samples\\nof the predictive distribution.',\n",
       "  'Tuning for MOIRAI was based\\non the average validation MSE across prediction lengths,\\nfurther including the options between channel indepedent\\nand channel mixing strategies (Nie et al., 2023) for the low\\ndimension datasets (ETT and Weather).\\n\\n\\nTable 6 reports the average performance across prediction\\nlengths, with full results in Appendix D.3.\\n\\nWe observe\\nthat MOIRAI achieves strong results compared to full-shot\\nbaselines.',\n",
       "  'While MOIRAIBase consistently achieves strong\\nperformance across datasets with either best or second-best\\nperformance, the large model is less consistent, with\\nslightly weaker but competitive results.\\n\\nThe relationship\\nbetween performance and model size is tenuous in this\\nsetting, however, this does not constitute strong evidence\\nagainst the potential of scaling, since these results are\\nTable 7.\\n\\nAblation study on Monash benchmark.',\n",
       "  'The aggregated\\nnormalized MAE, similarly calculated as in Figure 3 is reported.\\n\\n\\nNormalized MAE\\nMOIRAISmall\\n0.655\\nw/o patch size constraints\\n0.720\\nw/o multi patch size\\n1.156\\n\\n\\nw/o Any-variate Attention\\n0.904\\nw/o mixture distribution\\n0.740\\nw/o LOTSA\\n0.809\\n\\n\\nw/o packing\\n0.785\\nbased on models trained on a fixed dataset size and settings.\\n\\n\\nRather, this calls for more comprehensive neural scaling\\nlaws (Kaplan et al., 2020) for LTMs, to build a stronger\\nunderstanding of their scaling behavior.\\n\\n\\n4.3.',\n",
       "  '4.3.\\n\\nAblation Study\\nArchitecture\\nWe perform a series of ablations in Table 7,\\nstarting from the default MOIRAISmall.\\n\\nFirstly, we ablate\\nthe multi patch size component, removing the constraints\\nby allowing any frequency to have any patch size during\\ntraining, and also simply fixing the patch size to 32.\\n\\nIn\\nboth cases, we observe a deterioration in normalized MAE.',\n",
       "  'Removing Any-variate Attention and using additive learned\\nembeddings (randomizing variate index during training\\nto encourage permutation invariance) instead, leads to\\nsuboptimal results, showcasing the strength of Any-variate\\nAttention.\\n\\nWe see similar deterioration when replacing\\n7\\nUnified Training of Universal Time Series Forecasting Transformers\\ntarget\\nprediction\\nprediction: 0.5\\nprediction: 0.9\\n(a) Mixture distribution.\\n\\n\\ntarget\\nprediction\\nprediction: 0.5\\nprediction: 0.9\\n(b) Students t-distribution.',\n",
       "  'Figure 4.\\n\\nVisualization of probabilistic forecasts by two variants\\nof MOIRAISmall on the Traffic Hourly dataset.\\n\\nBoth models fore-\\ncast peaks, however, the Students t-distribution has a symmetric\\ndistribution, giving inappropriate prediction intervals for a peak,\\nas highlighted in red.',\n",
       "  '100\\n250\\n500\\n750\\n1000\\n2000\\n3000\\n4000\\n5000\\n0.45\\n0.50\\n0.55\\n0.60\\n0.65\\nETTm1\\n100\\n250\\n500\\n750\\n1000\\n2000\\n3000\\n4000\\n5000\\n0.24\\n0.26\\n0.28\\n0.30\\n0.32\\nElectricity\\n100\\n250\\n500\\n750\\n1000\\n2000\\n3000\\n4000\\n5000\\n0.30\\n0.35\\n0.40\\n0.45\\n0.50\\nWeather\\nFigure 5.\\n\\nPlot of performance (MAE) against context length (x-\\naxis in log scale) with prediction length 96 and patch size 32 on\\nthe validation set of the ETTm1, Electricity, and Weather datasets.',\n",
       "  'the mixture distribution with a Students t-distribution, and\\nfurther visualize the necessity of flexible distributions for\\nprobabilistic forecasts in Figure 4.\\nTraining Methodology\\n\\n\\nWe study the impact of a large\\nand diverse dataset by training MOIRAISmall only on the Glu-\\nonTS and Monash datasets, observing that diversity of data\\nis critical for cross-domain training even on in-distribution\\nevaluation.',\n",
       "  'Finally, given the same batch size and training\\niterations, we show that packed training significantly boosts\\nperformance.\\n\\nThis is because packing increases the effec-\\ntive batch size and increases the number of observations the\\nmodel is trained on, given the same amount of compute.\\n4.4.\\n\\nFurther Analysis\\nContext Length\\nOur pre-training methodology varies\\ncontext length defined by the task distribution.',\n",
       "  'We verify\\nthat MOIRAI has the capability to take as input arbitrary\\ncontext lengths by visualizing in Figure 5 the relationship\\nbetween performance and increasing context lengths\\nover three datasets in the zero-shot setting.\\n\\nZeng et al.\\n(2023); Liu et al. (2023b) previously observed that the\\ndesiderata of continuously improving performance with\\nincreasing context length is not present in conventional\\nTransformer-based forecasters.',\n",
       "  'Here, we observe that\\nMOIRAI indeed achieves this desired property, in fact,\\ncapable of handling thousands of time steps.\\n0\\n100\\n200\\n300\\n400\\n500\\nSequence Length\\n0.000\\n0.002\\n0.004\\n0.006\\n0.008\\nDensity\\nFigure 6.\\n\\nHistogram of sequence length when sampling data from\\nLOTSA based on the proposed task distribution.\\n\\nSequence length\\nrefers to the number of tokens after patching and flattening.',\n",
       "  'Packing\\nPacking has long been applied in training LLMs\\nand other Transformer-based models, but not for time series\\nTransformers.\\n\\nWhile we can get away with inefficiencies\\nwhen dealing with small-scale data, we start to suffer from\\nlonger training times as we scale towards the paradigm of\\nFMs and LTMs.\\n\\nThis is further exacerbated by our flat-\\ntened setting which increases the disparity in sequence\\nlengths.\\n\\nAs evidenced in Section 4.3, keeping compute\\n(batch size, iterations, etc.)',\n",
       "  'constant, packing improves per-\\nformance by 16%.\\n\\nTo understand why this is the case, we\\nvisualize sequence length distribution in Figure 6.\\n\\nWith a\\nlarge portion of the data being shorter than the maximum se-\\nquence length, padding represents a whopping 61.08% of in-\\nput tokens without packed training, and only 0.38% with our\\npacked implementation (calculated over 1000 iterations).\\n\\n\\n5.',\n",
       "  '5.\\n\\nConclusion\\nIn this work, we introduced MOIRAI, a masked encoder-\\nbased universal time series forecasting Transformer which\\nalleviates the issues faced in the universal forecasting\\nparadigm.\\n\\nWe also introduce the LOTSA, the largest col-\\nlection of open-data for pre-training time series forecasting\\nmodels.\\n\\nMOIRAI is evaluated on the in-distribution and\\nout-of-distribution settings, and is capable of probabilistic\\nand long sequence forecasting.',\n",
       "  'We show that as a zero-\\nshot forecaster, MOIRAI achieves competitive or superior\\nperformance compared to full-shot models.\\n\\n\\nLimitations & Future Work\\nWhile MOIRAI achieves\\nphenomenal in and out-of-distribution performance, this is\\njust a first step in the universal forecasting paradigm.\\n\\nDue\\nto resource constraints, little to no hyperparameter tuning\\nwas performed  efficient tuning techniques such as P\\n(Yang et al., 2022a) can be applied.',\n",
       "  'In terms of architecture,\\nour approach to tackling cross-frequency learning with a\\nmulti patch size mapping is somewhat heuristic, and future\\nwork should design a more flexible and elegant approach.\\n\\n\\nAlso, the current architecture has limited support for high-\\ndimensional time series, and efficient methods for extending\\nTransformer input length can alleviate this issue.\\n\\nIn terms of\\ndata, LOTSA can be further enhanced with greater diversity\\nin terms of domain and frequency.',\n",
       "  'Finally, incorporating\\nmulti-modality such as tabular or text inputs is an exciting\\nnew direction which universal forecasting has unlocked.\\n\\n\\n8\\nUnified Training of Universal Time Series Forecasting Transformers\\nImpact Statement\\nThis paper presents work whose goal is to advance the field\\nof Machine Learning.\\n\\nThere are many potential societal\\nconsequences of our work, none which we feel must be\\nspecifically highlighted here.',\n",
       "  'In the era of foundation models (FMs) (Bommasani et al.,\\n2021), the landscape of deep learning for time series fore-\\ncasting is experiencing a revolution.\\n\\nIn contrast to FMs capa-\\nble of tackling a multitude of downstream tasks, the current\\ndeep forecasting paradigm, involving training a model on\\na single dataset with a fixed context and prediction length,\\nappears increasingly antiquated, lacking the capacity to gen-\\neralize or adapt to diverse scenarios or datasets.',\n",
       "  'Given\\nthe unreasonable effectiveness of large pre-trained models\\n1Salesforce AI Research 2School of Computing and Infor-\\nmation Systems, Singapore Management University.\\n\\nCorrespon-\\ndence to: Gerald Woo <gwoo@salesforce.com>, Chenghao Liu\\n<chenghao.liu@salesforce.com>.\\nPreprint.\\n\\n\\nUniversal Forecaster\\n1) Multiple Frequencies\\n2) Any-variate Forecasting\\n3) Varying Distributions\\nMultiple Domains\\nFigure 1.',\n",
       "  'A universal forecaster is a large pre-trained model capa-\\nble of handling any time series forecasting problem.\\n\\nIt is trained\\non a large-scale time series dataset spanning multiple domains.\\n\\n\\nCompared to the existing paradigm, universal forecasting faces the\\nthree key issues of i) multiple frequencies, ii) any-variate forecast-\\ning, and iii) varying distributions.\\nin improving performance and data efficiency via transfer\\nlearning in modalities like vision and language (Dosovitskiy\\net al., 2020;',\n",
       "  'Brown et al., 2020), we are starting to see a\\npush to transition away from the existing paradigm, towards\\na universal forecasting paradigm (see Figure 1), where a sin-\\ngle large pre-trained model is able to handle any time series\\nforecasting problem.\\n\\nHowever, the road to building a univer-\\nsal time series forecasting model is mired with challenges.\\n\\n\\nUnlike the modalities of vision and language which have the\\nunified formats of images and text respectively, time series\\ndata is highly heterogeneous.',\n",
       "  'Firstly, the frequency (e.g.\\nminutely, hourly, daily sampling rates) of time series plays\\nan important role in determining the patterns present in the\\ntime series.\\n\\nCross-frequency learning has been shown to be\\na challenging task due to negative interference (Van Ness\\net al., 2023), with existing work simply avoiding this\\nproblem for multi-frequency datasets by learning one model\\nper frequency (Oreshkin et al., 2020).',\n",
       "  'Secondly, time series\\ndata are heterogeneous in terms of dimensionality, whereby\\nmultivariate time series can have different number of\\nvariates.\\n\\nFurthermore, each variate measures a semantically\\ndifferent quantity across datasets.\\n\\nWhile considering each\\nvariate of a multivariate time series independently (Nie et al.,\\n2023; Ekambaram et al., 2023) can sidestep this problem,\\nwe expect a universal model to be sufficiently flexible to\\n1\\narXiv:2402.02592v1',\n",
       "  '[cs.LG]  4 Feb 2024\\nUnified Training of Universal Time Series Forecasting Transformers\\nTable 1.\\n\\nComparison between pre-trained forecasting models.\\n\\n\\nindicates a work in progress, where authors have indicated their\\nintention, and ? indicates unknown intention, as of writing.\\n\\n\\nAny-variate\\n(Zero-shot)\\nProbabilistic\\nForecasting\\nFlexible\\nDistribution\\nPre-training Data (Size)\\nOpen-source\\nMOIRAI\\n\\n\\n\\nLOTSA (> 27B)',\n",
       "  '\\nTimeGPT-1\\n\\n\\n\\nUnknown (100B)\\n\\nForecastPFN\\n\\n\\n-\\nSynthetic Data (60M)\\n\\nLag-Llama\\n\\n\\n\\nMonash (< 1B)\\n\\nPreDcT\\n\\n\\n-\\nGoogle Trends + Others (1B)\\n?\\nTTM\\n\\n\\n-\\nMonash (< 1B)\\n\\nLLMTime\\n\\n\\n\\nWeb-scale Text\\n\\nconsider multivariate interactions and take into account\\nexogenous covariates.',\n",
       "  'Thirdly, probabilistic forecasting is a\\ncritical feature often required by practitioners, yet, different\\ndatasets have differing support and distributional properties\\n for example, using a symmetric distribution (e.g. Normal,\\nStudent-T) as the predictive distribution is not suitable\\nfor positive time series  making standard approaches of\\npre-defining a simple parametric distribution (Salinas et al.,\\n2020) to be insufficiently flexible to capture a wide variety\\nof datasets.',\n",
       "  'Lastly, a large pre-trained model capable\\nof universal forecasting requires a large-scale dataset\\nfrom diverse domains.\\n\\nExisting time series datasets are\\ninsufficiently large to support the training of such models.',\n",
       "  'Starting from a masked encoder architecture which has\\nbeen shown to be a strong candidate architecture for scaling\\nup pre-trained time series forecasting models (Woo et al.,\\n2023), we alleviate the above issues by introducing novel\\nmodifications which allows the architecture to handle\\nthe heterogeneity of arbitrary time series data.\\n\\n\\nFirstly,\\nwe propose to learn multiple input and output projection\\nlayers to handle the differing patterns from time series of\\nvarying frequencies.',\n",
       "  'Using patch-based projections with\\nlarger patch sizes for high-frequency data and vice versa,\\nprojection layers are specialized to learn the patterns of that\\nfrequency.',\n",
       "  'Secondly, we address the problem of varying\\ndimensionality with our proposed Any-variate Attention,\\nwhich simultaneously considers both time and variate axes\\nas a single sequence, leveraging Rotary Position Embed-\\ndings (RoPE) (Su et al., 2024), and learned binary attention\\nbiases (Yang et al., 2022b) to encode time and variate axes\\nrespectively.\\n\\nImportantly, Any-variate Attention allows the\\nmodel to take as input arbitrary number of variates.',\n",
       "  'Thirdly,\\nwe overcome the issue of requiring flexible predictive\\ndistributions with a mixture of parametric distributions.\\n\\n\\nFurthermore, optimizing the negative log-likelihood of a\\nflexible distribution has the added benefit of being competi-\\ntive with target metric optimization (Awasthi et al., 2022), a\\npowerful feature for pre-training universal forecasters, given\\nthat it can be evaluated with any target metric subsequently.',\n",
       "  'To power the training of our Large Time Series Model\\n(LTM), we introduce the Large-scale Open Time Series\\nArchive (LOTSA), the largest collection of open time series\\ndatasets with 27B observations across nine domains.\\n\\nWe\\noptimize the negative log-likelihood of the mixture distri-\\nbution, and randomly sample context and prediction lengths\\nduring training, allowing for flexible downstream usage\\nof the pre-trained model.',\n",
       "  'We train our proposed method,\\nMasked EncOder-based UnIveRsAl TIme Series Forecast-\\ning Transformer (MOIRAI1), in three sizes  MOIRAISmall,\\nMOIRAIBase, and MOIRAILarge, with 14m, 91m, and\\n311m parameters respectively.\\n\\nWe perform experimental\\nevaluations on both in and out-of-distribution settings, and\\nshow that MOIRAI consistently achieves competitive or\\nsuperior performance compared to state-of-the-art full-shot\\nbaselines.\\n\\nOur contributions are summarized as follows:\\n\\n\\n1.',\n",
       "  '1.\\n\\nWe introduce a novel Transformer architecture to\\nsupport the requirements of universal forecasting.\\n\\n\\nCrucially, the components we propose extend beyond\\nmasked encoders and are versatile, applicable to a\\nbroad range of Transformer variants.\\n\\n\\n2.\\n\\nWe introduce LOTSA, a new large-scale collection of\\nopen time series datasets to empower pre-training of\\nLTMs.\\n\\nLOTSA, the model weights, and our library\\nfor unified training of universal time series models,\\nUNI2TS, will be fully open sourced.',\n",
       "  '3. Trained on LOTSA data, MOIRAI achieves competitive\\nor superior performance as a zero-shot forecaster when\\ncompared to full-shot models.\\n\\n\\n2.\\n\\nRelated Work\\nPre-training for Zero-shot Forecasting\\nTable 1 pro-\\nvides a summary of the key differences between recent\\npre-trained models with zero-shot forecasting capabilities,\\nwhich is a recently emerging field.',\n",
       "  'TimeGPT-1 (Garza\\n& Mergenthaler-Canseco, 2023) first presented a closed-\\nsource model, offering zero-shot forecasting capabilities\\nas well as supporting fine-tuning through an API, currently\\nonly available to their beta users.\\n\\nForecastPFN (Dooley\\net al., 2023) proposes to pre-train on synthetic time series,\\nwhich can be subsequently be leveraged as a zero-shot\\nforecaster, albeit specialized for data or time limited\\nsettings.',\n",
       "  'Lag-llama (Rasul et al., 2023) works towards a\\nfoundation model for time series forecasting, leveraging\\nthe LLaMA (Touvron et al., 2023) architecture design with\\nlagged time series features, and also presents neural scaling\\nlaws for time series forecasting.\\n\\nPreDcT\\n\\n(Das et al., 2023b)\\nis a patch-based decoder-only foundation model for time\\nseries forecasting, introducing a larger output patch size for\\nfaster decoding.',\n",
       "  'They collect a private dataset from Google\\nTrends to pre-train their model in combination with open-\\ndata.\\n\\nTiny Time Mixers (TTMs) (Ekambaram et al., 2024)\\n1In ancient Greek religion and mythology, the Moirai, often\\nknown in English as the Fates, were the personifications of destiny.\\n\\n\\n(Wikipedia contributors, 2024)\\n2\\nUnified Training of Universal Time Series Forecasting Transformers\\nis a concurrent work leveraging lightweight mixer-style\\narchitecture.',\n",
       "  'They perform data augmentation by downsam-\\npling high-frequency time series, and support multivariate\\ndownstream tasks by fine-tuning an exogenous mixer.\\n\\n\\nleverage Large Language Models (LLMs), pre-trained on\\nweb-scale text data, have been leveraged for zero-shot fore-\\ncasting.\\n\\nSpecifically, LLMTime (Gruver et al., 2023) treats\\ntime series as strings, applying careful pre-processing based\\non the specific LLMs tokenizer, showing that LLMs have\\nthe inherent capability to perform zero-shot forecasting.',\n",
       "  'Pre-train\\n\\n+\\n\\nFine-tune for Time Series Forecasting\\nPre-training with subsequent fine-tuning on downstream\\nforecasting tasks has predated the recent zero-shot fore-\\ncasting efforts.',\n",
       "  'Denoising autoencoders (Zerveas et al.,\\n2021) and contrastive learning (Yue et al., 2022; Woo et al.,\\n2022) have been shown to be effective pretext tasks for\\ntime series forecasting, but have largely been applied to\\nthe existing paradigm of pre-training and fine-tuning on\\nthe same dataset, without exploring their generalization\\ncapabilities.',\n",
       "  'More recently, Dong et al. (2023) explored\\ncombining both reconstruction and contrastive based\\npre-training approaches, and performed initial explorations\\ninto cross-dataset transfer.\\n\\nThe topic has been well explored,\\nand we refer readers to more comprehensive surveys (Zhang\\net al., 2023; Ma et al., 2023).\\n\\nReprogramming is a recent\\ndirection which involves fine-tuning the model weights\\nof an LLM which has been pre-trained on text data, for\\ndownstream tasks for other modalities.',\n",
       "  'Zhou et al. (2023);\\nJin et al. (2023) introduce modules and fine-tuning methods\\nto adapt LLMs for time series tasks including forecasting.\\n\\n\\n3. Method\\nProblem\\nFormulation\\nConsider\\na\\ndataset\\nof\\nN\\ntime series D\\n=\\n{(Y (i), Z(i))}N\\ni=1, where Y (i)\\n=\\n(y(i)\\n1 , y(i)\\n2 , . . .\\n\\n, y(i)\\nTi )  RdyiTi is a target time series of\\ndyi variates and Ti time steps.\\n\\nEach time series is associated\\nwith a set of covariates Z(i) = (z(i)\\n1 , z(i)\\n2 , . . .\\n\\n, z(i)\\n\\n\\nTi ) \\nRdziTi.',\n",
       "  'The goal is to forecast the predictive distribution\\np(Yt:t+h|) by predicting distribution parameters  via\\na learned model f : (Ytl:t, Ztl:t+h) 7\\n which\\nmaximizes the log-likelihood:\\nmax\\n\\nE\\n(Y,Z)p(D)\\n(t,l,h)p(T |D)\\nlog p(Yt:t+h| ),\\ns.t.  = f(Ytl:t, Ztl:t+h),\\n(1)\\nwhere p(D) is the data distribution which samples for a time\\nseries, (Y , Z), and p(T |D) is the task distribution which\\ndefines the lookback window, Ytl:t = (ytl, . . .',\n",
       "  ', yt1)\\nwith\\ncontext\\nlength\\nl\\nand\\nforecast\\nhorizon,\\nYt:t+h = (yt, . .\\n\\n. , yt+h1) with prediction length h.\\n3.1.\\n\\nArchitecture\\nIllustrated in Figure 2, MOIRAI follows a (non-overlapping)\\npatch-based approach to modeling time series with a masked\\nencoder architecture.\\n\\nOne of our proposed modifications\\nto extend the architecture to the any-variate setting is to\\nflatten multivariate time series, considering all variates as\\na single sequence.',\n",
       "  'Patches are subsequently projected into\\nvector representations via a multi patch size input projection\\nlayer.\\n\\nThe [mask] signifies a learnable embedding which\\nreplaces patches falling within the forecast horizon.\\n\\nThe out-\\nput tokens are then decoded via the multi patch size output\\nprojection into the parameters of the mixture distribution.',\n",
       "  'While not visualized, (non-learnable) instance normalization\\n(Kim et al., 2022) is applied to inputs/outputs, aligning with\\nthe current standard practice for deep forecasting models.\\n\\n\\nThe core Transformer module is an encoder-only Trans-\\nformer architecture, leveraging various improvements as\\nproposed by recent state-of-the-art LLM architectures.',\n",
       "  'We\\nuse pre-normalization (Xiong et al., 2020) and replace all\\nLayerNorms with RMSNorm (Zhang & Sennrich, 2019),\\nand also apply query-key normalization (Henry et al., 2020).\\n\\n\\nThe non-linearity in FFN layers are replaced with SwiGLU\\n(Shazeer, 2020), adjusting the hidden dimension to have\\nequal number of parameters as the original FFN layer.\\n\\nWe\\nomit biases in all layers of the Transformer module.\\n\\n\\n3.1.1.\\n\\nMULTI PATCH SIZE PROJECTION LAYERS',\n",
       "  'In the context of universal forecasting, a single model should\\npossess the capability to handle time series spanning a wide\\nrange of frequencies.\\n\\nExisting patch-based architectures\\nrely on a single patch size hyperparameter, a legacy feature\\nfrom the prevailing one-model-per-dataset paradigm.',\n",
       "  'In-\\nstead, we aim for a more flexible strategy: opting for a larger\\npatch size to handle high-frequency data, thereby lower the\\nburden of the quadratic computation cost of attention while\\nmaintaining a long context length.\\n\\nSimultaneously, we\\nadvocate for a smaller patch size for low-frequency data to\\ntransfer computation to the Transformer layers, rather than\\nrelying solely on simple linear embedding layers.',\n",
       "  'To im-\\nplement this approach, we propose learning multiple input\\nand output embedding layers, each associated with varying\\npatch sizes.\\n\\nThe selection of the appropriate patch size for\\na given time series frequency relies on pre-defined settings\\n(see Appendix B.1).\\n\\nNote that we only learn one set of\\nprojection weights per patch size, which is shared amongst\\nfrequencies if there is an overlap based on the settings.\\n\\n\\n3.1.2.',\n",
       "  '3.1.2.\\n\\nANY-VARIATE ATTENTION\\nUniversal forecasters must be equipped to handle arbitrary\\nmultivariate time series.\\n\\nExisting time series Transform-\\ners often rely on an independent variate assumption or are\\nlimited to a single dimensionality due to embedding layers\\n3\\nUnified Training of Universal Time Series Forecasting Transformers\\nVariate 0\\nVariate 1\\nVariate 2\\nPatch Size 8\\nPatch Size 16\\nPatch Size 32\\nPatch Size 64\\nPatch Size 128\\nMulti Patch Size\\nInput Projection\\n[mask]',\n",
       "  '[mask]\\nPatch\\nEmbedding\\n2\\n2\\n2\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n2\\n0\\n1\\n2\\n0\\n1\\n2\\nVariate ID\\nTime ID\\nPatch Size 8\\nPatch Size 16\\nPatch Size 32\\nPatch Size 64\\nPatch Size 128\\nMulti Patch Size\\nOutput Projection\\nTransformer (Full Self-Attention)\\nMixture\\nDistribution\\nFigure 2.\\n\\nOverall architecture of MOIRAI.\\n\\nVisualized is a 3-variate time series, where variates 0 and 1 are target variables (i.e. to be\\nforecasted, and variate 2 is a dynamic covariate (values in forecast horizon known).',\n",
       "  'Based on a patch size of 64, each variate is patchified\\ninto 3 tokens.\\n\\nThe patch embeddings along with sequence and variate id are fed into the Transformer.\\n\\nThe shaded patches represent the\\nforecast horizon to be forecasted, whose corresponding output representations are mapped into the mixture distribution parameters.\\n\\n\\nmapping Rdy  Rdh, where Rdh is the hidden dimension.',\n",
       "  'We overcome this limitation as shown in Figure 2, by flat-\\ntening a multivariate time series to consider all variates as a\\nsingle sequence.\\n\\nThis introduces a new requirement of hav-\\ning variate encodings to enable the model to disambiguate\\nbetween different variates in the sequence.\\n\\nFurthermore, we\\nneed to ensure that permutation equivariance w.r.t. variate\\nordering, and permutation invariance w.r.t.\\n\\nvariate indices\\nare respected.',\n",
       "  'Conventional approaches like sinusoidal or\\nlearned embeddings do not meet these requirements, and\\nare unable to handle an arbitrary number of variates.\\n\\nTo\\naddress this, we propose Any-variate Attention, leveraging\\nbinary attention biases to encode variate indices.',\n",
       "  'Dropping layer and attention head indices, and scaling factor\\nfor brevity, the attention score between the (i, m)-th query\\nwhere i represents the time index and m represents the\\nvariate index, and the (j, n)-th key, Aij,mn  R, is given by:\\nEij,mn = (W Qxi,m)T\\n\\nRij(W Kxj,n)\\n+ u(1)  1{m=n} + u(2)  1{m=n},\\n(2)\\nAij,mn =\\nexp{Eij,mn}\\nP\\nk,o exp{Eik,mo},\\n(3)\\nwhere W Qxi,m, W Kxj,n  Rdh are the respective query\\nand key vectors,\\n\\nRij \\n\\nRdhdh is the rotary matrix (Su\\net al., 2024), u(1), u(2)',\n",
       "  ' R are learnable scalars for each\\nhead in each layer, and 1{cond} =\\n\\x08 1, if cond\\n0, otherwise is the indica-\\ntor function.\\n\\nThe binary attention bias component allows\\nfor disambiguation between variates via attention scores,\\nfulfills the criteria of permutation equivariance/invariance\\nw.r.t.\\n\\nvariate ordering/indices, and can extend to arbitrary\\nnumber of variates.\\n\\n\\n3.1.3.',\n",
       "  '3.1.3.\\n\\nMIXTURE DISTRIBUTION\\nTo achieve the goal of having a flexible distribution, yet\\nensuring that operations of sampling and evaluating the\\nloss function remains simple, we propose to use a mixture\\nof parametric distributions.\\n\\nA mixture distribution of c\\ncomponents has p.d.f.:\\np(Yt:t+h| ) =\\nc\\nX\\ni=1\\nwipi(Yt:t+h| i),\\n(4)\\nwhere \\n=\\n{w1, 1, . . .\\n\\n, wc, c}, and pi is the i-th\\ncomponents p.d.f.',\n",
       "  'While the choice of mixture components\\nis flexible and implementing any combination of parametric\\ndistributions is straightforward, we specifically propose\\nto use the following mixture components: i) a Students\\nt-distribution which has shown to be a robust option for\\ngeneral time series, ii) a negative binomial distribution\\nfor positive count data, iii) a log-normal distribution to\\nmodel right-skewed data commonly across economic and\\nand natural phenomena, and iv) a low variance normal\\ndistribution for high confidence predictions.',\n",
       "  'Further details\\ncan be found in Appendix B.2.\\n\\n\\n3.2.\\n\\nUnified Training\\n3.2.1.\\n\\nLOTSA DATA\\nExisting work has predominantly relied on three primary\\nsources of data  the Monash Time Series Forecasting\\nArchive (Godahewa et al., 2021), datasets provided by the\\nGluonTS library (Alexandrov et al., 2020), and datasets\\nfrom the popular long sequence forecasting benchmark\\n(Lai et al., 2018; Wu et al., 2021).\\n\\nWhile Monash and\\n4\\nUnified Training of Universal Time Series Forecasting Transformers\\nTable 2.',\n",
       "  'Key statistics of LOTSA by domain.\\n\\n\\nEnergy\\nTransport\\nClimate\\nCloudOps\\nWeb\\nSales\\nNature\\nEcon/Fin\\nHealthcare\\n# Datasets\\n30\\n23\\n6\\n3\\n3\\n6\\n5\\n23\\n6\\n# Obs.\\n\\n\\n16,358,600,896\\n4,900,453,419\\n4,188,011,890\\n1,518,268,292\\n428,082,373\\n197,984,339\\n28,547,647\\n24,919,596\\n1,594,281\\n%\\n59.17%\\n17.73%\\n15.15%\\n5.49%\\n1.55%\\n0.72%\\n0.09%\\n0.10%\\n0.01%\\nTable 3.\\n\\nKey statistics of LOTSA by frequency.\\n\\n\\nYearly\\nQuarterly\\nMonthly\\nWeekly\\nDaily\\n(Multi) Hourly\\n(Multi) Minute-level\\n(Multi) Second-level\\n# Datasets\\n4\\n5\\n10\\n7\\n21\\n31\\n25\\n2\\n# Obs.',\n",
       "  '873,297\\n2,312,027\\n11,040,648\\n18,481,871\\n709,017,118\\n19,875,993,973\\n7,013,949,430\\n14,794,369\\n%\\n0.003%\\n0.008%\\n0.040%\\n0.067%\\n2.565%\\n71.893%\\n25.370%\\n0.054%\\nGluonTS comprise of datasets from diverse domains, they\\nare constrained in size, with approximately 1B observations\\ncombined.\\n\\nIn comparison, LLMs are trained on trillions\\nof tokens.\\n\\nDas et al. (2023b) builds a private dataset based\\non Google Trends, but lacks diversity and is similarly sized\\nat 1B observations.',\n",
       "  'The effectiveness of FMs heavily stem from large-scale pre-\\ntraining data.\\n\\nGiven that existing data sources fall short of\\nsupporting such a paradigm, attempting to train an LTM\\non them may result in misleading conclusions.\\n\\nThus, we\\ntackle this issue head-on by building a large-scale archive\\nof open time series datasets by collating publicly available\\nsources of time series datasets.\\n\\nThis effort aims to cover\\na broad spectrum of domains, consolidating datasets from\\ndiverse sources with varying formats.',\n",
       "  'We design a uni-\\nfied storage format using Arrow (Richardson et al., 2023)\\nwhich is ready for deep learning pipelines.\\n\\nThe result-\\ning collection, LOTSA, spans nine domains, with a total\\nof 27, 646, 462, 733 observations, with key statistics in Ta-\\nbles 2 and 3, and in-depth details in Appendix A.\\n3.2.2.\\n\\nPRE-TRAINING\\nAs introduced in Equation (1), our pre-training task is for-\\nmulated to optimize the mixture distribution log-likelihood.',\n",
       "  'The design of both the data distribution and task distribu-\\ntion are two critical aspects of the pre-training pipeline.\\n\\n\\nThis design imparts versatile capabilities to our LTM, en-\\nabling it to adapt to a range of downstream tasks.\\n\\nThis\\nflexibility stands in contrast to the prevailing deep forecast-\\ning paradigm, where models are typically specialized for\\nspecific datasets and settings.\\n\\n\\nData Distribution\\nThe data distribution, (Y, Z)  p(D),\\ndefines how time series are sampled from the dataset.',\n",
       "  'Trained on LOTSA, which is a dataset of datasets, we in-\\ntroduce the notion of sub-datasets, by decomposing the\\ndata distribution into a sub-dataset distribution, and a time\\nseries distribution conditioned on a sub-dataset, p(D) =\\np(Y, Z|D)p(D).\\n\\nThus, we first sample a sub-dataset from\\np(D), and given that sub-dataset, we sample a time series.\\n\\n\\nTable 4.\\n\\nDetails of MOIRAI model sizes.',\n",
       "  'Layers\\ndmodel\\ndff\\nHeads\\ndkv\\nParams\\nMOIRAISmall\\n6\\n384\\n1536\\n6\\n64\\n14m\\nMOIRAIBase\\n12\\n768\\n3072\\n12\\n64\\n91m\\nMOIRAILarge\\n24\\n1024\\n4096\\n16\\n64\\n311m\\n\\n\\nFor K sub-datasets, where Dk represents the set of indices\\nof time series belonging to sub-dataset k, the structure of\\np(Y (i), Z(i)|Dk) =\\nTi1{iDk}\\nP\\njDk Tj , proportionate to the num-\\nber of observations, is straightforward.',\n",
       "  'However, due to data imbalance across domains and fre-\\nquency, we avoid sampling sub-datasets proportionately,\\nand instead cap the contribution of each sub-dataset at\\n = 0.001, before re-normalizing: p(Dk) =\\nk\\nPK\\ni=1 i ,\\nwhere k = min(|Dk|,)\\n\\n\\nPK\\ni |Di| , and |Dk| = P\\niDk Ti.\\n\\n\\nTask Distribution\\nDifferent from the existing deep fore-\\ncasting paradigm, we aim to train a model with forecasting\\ncapabilities over varying context and prediction lengths.',\n",
       "  'Rather than defining a fixed context and prediction length,\\nwe sample from a task distribution, (t, l, h)  p(T |D)\\nwhich defines the lookback window and forecasting horizon,\\ngiven a time series.\\n\\nIn practice, rather than sampling t, l, h,\\ngiven a time series, we crop a uniformly sampled window,\\nwhose length is uniformly sampled from a range.\\n\\nThis range\\nis defined by a minimum sequence length per variate of 2,\\nand a total maximum sequence length of 512.',\n",
       "  'The window\\nis then split into lookback and horizon segments, where\\nthe prediction length is uniformly sampled as a proportion\\n(within the range [0.15, 0.5]) of the window.\\n\\nWe further\\naugment training by i) uniformly subsampling multivariate\\ntime series in the variate dimension, and ii) constructing\\nmultivariate time series from sub-datasets with univariate\\ntime series, by randomly concatenating them.',\n",
       "  'The number\\nof variates is sampled from a beta-binomial distribution\\nwith parameters n = 128, a = 2, b = 5 which supports a\\nmaximum of 128 variates, with mean\\n\\n 37 for efficiency.\\n\\n\\n5\\nUnified Training of Universal Time Series Forecasting Transformers\\nSES\\nNaive\\nTheta\\nARIMA\\nETS\\nPR\\nN-BEATS\\nTransformer\\nCatBoost\\nDeepAR\\nTBATS\\nWaveNet\\nFFNN\\nMoiraiSmall\\nMoiraiBase\\nMoiraiLarge\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nNormalized MAE\\nFigure 3.\\n\\nAggregate results of the Monash Time Series Forecasting\\nBenchmark.',\n",
       "  'The normalized MAE is reported, which normalizes\\nthe MAE of each dataset by the naive forecasts MAE, and aggre-\\ngated by taking the geometric mean across datasets.\\n\\n\\nTraining\\nWe train MOIRAI in three sizes  small, base,\\nand large, with key parameter details listed in Table 4.\\n\\n\\nThe small model is trained for 100, 000 steps, while\\nbase and large models are trained for 1, 000, 000 steps\\nwith a batch size of 256.',\n",
       "  'For optimization, we use the\\nAdamW optimizer with the following hyperparameters,\\nlr = 1e-3, weight decay = 1e-1, 1 = 0.9, 2 = 0.98.\\n\\nWe\\nalso apply a learning rate scheduler with linear warmup\\nfor the first 10, 000 steps, and cosine annealing thereafter.\\n\\n\\nModels are trained on NVIDIA A100-40G GPUs with\\nTF32 precision.',\n",
       "  'We implement sequence packing (Raffel\\net al., 2020) to avoid large amounts of padding due to\\nthe disparity of sequence lengths in the new setting with\\nvarying context, prediction, and variate lengths, thereby\\nincreasing the effective batch size.\\n\\n\\n4.\\n\\nExperiments\\n4.1.\\n\\nIn-distribution Forecasting\\nWe first perform an in-distribution evaluation using the\\nMonash benchmark, which aim to measure generaliza-\\ntion capability across diverse domains.',\n",
       "  'Described in Ap-\\npendix A, LOTSA includes the Monash Time Series Fore-\\ncasting Archive as a source of data.\\n\\nFor a large portion of\\nthese datasets, we only include the train set, holding out\\nthe test set which we now use for in-distribution evaluation.',\n",
       "  'In this evaluation, we consider a standard setting with a\\ncontext length of 1000, and a patch size of 32 for all fre-\\nquencies, except for quarterly data with a patch size of 8.\\nFigure 3 summarizes the results based on the normalized\\nmean absolute error (MAE), in comparison with the base-\\nlines presented in the Monash benchmark.\\n\\nIt is worth noting\\nthat each baseline in the Monash benchmark is typically\\ntrained individually per dataset or per time series within a\\ndataset.',\n",
       "  'In contrast, MOIRAI stands out by being a single\\nmodel evaluated across various datasets.\\n\\nFull results as well\\nas a comparison with LLMTime (Gruver et al., 2023) can\\nbe found in Appendix D.1.\\n\\n\\nWe observe that MOIRAI outperforms all baselines from the\\nMonash benchmark regardless of model size, displaying the\\nstrong in-distribution and cross-domain capabilities arising\\nfrom our unified training methodology.',\n",
       "  'We highlight that\\neach instance of MOIRAI is a single model evaluated across\\ndatasets, compared to baselines for which one model is\\ntrained per dataset.\\n\\n\\n4.2.\\n\\nOut-of-distribution / Zero-shot Forecasting\\nNext, we perform an out-of-distribution evaluation on un-\\nseen target datasets.\\n\\nHere, MOIRAI is a zero-shot fore-\\ncaster compared with state-of-the-art full-shot baselines\\nwhich have been trained on the individual target datasets.',\n",
       "  'While the ideal scenario would be to include other univer-\\nsal forecasters, this proves to be a challenging task.\\n\\nAs a\\nnascent field, most universal forecasters currently do not yet\\nhave open weights avaiable for evaluation.\\n\\nFurthermore, the\\nproblem of comparing zero-shot methods is exacerbated by\\nnot having a standard held-out test split, making it challeng-\\ning to collate a set of datasets which all the models have not\\nbeen trained on.',\n",
       "  'Thus, we establish the strong zero-shot ca-\\npabilities of MOIRAI by displaying competitive or stronger\\nresults compared with SOTA full-shot methods  datasets\\nused in the following have not been included in LOTSA.\\n\\n\\nProbabilistic Forecasting\\nWe evaluate on seven datasets\\nacross energy, transport, climate, and sales domains,\\nfollowing a rolling evaluation setup with stride equal to\\nprediction length.\\n\\nPrediction lengths and number of rolling\\nevaluations are defined for each dataset based on frequency.',\n",
       "  'We report the Continuous Ranked Probability Score (CRPS)\\nand Mean Scaled Interval Score (MSIS) metrics (definitions\\nin Appendix C), comparing against four full-shot baselines \\nDeepAR (Salinas et al., 2020), PatchTST (Nie et al., 2023),\\nand TiDE (Das et al., 2023a) with Students t-distribution\\nprediction heads, and TFT based on quantile prediction\\n(Lim et al., 2021), all implemented with the GluonTS\\nlibrary (Alexandrov et al., 2020), as well as simple\\nbaselines AutoARIMA (Garza et al., 2022) and Seasonal\\nNaive (Hyndman & Athanasopoulos, 2018).',\n",
       "  'For each\\ndataset and baseline, we perform hyperparameter tuning\\non a validation CRPS, and report results averaged over\\nfive training runs with different seeds.\\n\\nFor MOIRAI, we\\nperform inference time tuning, selecting context length\\nfrom {1000, 2000, 3000, 4000, 5000} and patch sizes based\\non frequency, on the validation CRPS.\\n\\nFull details of the\\nevaluation setting can be found in Appendix C.\\nTable 5 reports the CRPS and MSIS, with full results\\nincluding deterministic metrics in Appendix D.2.',\n",
       "  'We\\nobserve that MOIRAIBase and MOIRAILarge consistently\\nachieve strong zero-shot performance, obtaining either best\\nor second best results for all datasets except Walmart and\\nIstanbul Traffic.\\n\\nEven for these datasets, performance is\\nstill close to the best performance, despite being a single\\nzero-shot model compared to baselines which have been\\ntuned and trained on the train sets.\\n\\n\\n6\\nUnified Training of Universal Time Series Forecasting Transformers\\nTable 5.\\n\\nProbabilistic forecasting results.',\n",
       "  'Best results are highlighted in bold, and second best results are underlined.\\n\\nBaseline results are\\naggregated over five training runs with different seeds, reporting the mean and standard deviation.',\n",
       "  'Zero-shot\\nFull-shot\\nBaseline\\nMOIRAISmall\\nMOIRAIBase\\nMOIRAILarge\\nPatchTST\\nTiDE\\nTFT\\nDeepAR\\nAutoARIMA\\nSeasonal Naive\\nElectricity\\nCRPS\\n0.072\\n0.055\\n0.050\\n0.0520.00\\n0.0480.00\\n0.0500.00\\n0.0650.01\\n0.327\\n0.070\\nMSIS\\n7.999\\n6.172\\n5.875\\n5.7440.12\\n5.6720.08\\n6.2780.24\\n6.8930.82\\n29.412\\n35.251\\nSolar\\nCRPS\\n0.471\\n0.419\\n0.406\\n0.5180.09\\n0.4200.00\\n0.4460.03\\n0.4310.01\\n1.055\\n0.512\\nMSIS\\n8.425\\n7.011\\n6.250\\n8.4471.59\\n13.7540.32\\n8.0573.51\\n11.1810.67\\n25.849\\n48.130\\nWalmart\\nCRPS\\n0.103\\n0.093\\n0.098\\n0.0820.01\\n0.0770.00\\n0.0870.00\\n0.1210.00\\n0.124\\n0.151\\nMSIS\\n9.371\\n8.421\\n8.520\\n6.0050.21\\n6.2580.12\\n8.7180.10\\n12.5020.03\\n9.888\\n49.458\\nWeather\\nCRPS\\n0.049\\n0.041\\n0.051\\n0.0590.01\\n0.0540.00\\n0.0430.00\\n0.1320.11\\n0.252\\n0.068\\nMSIS\\n5.236\\n5.136\\n4.962\\n7.7590.49\\n8.0951.74\\n7.7910.44\\n21.65117.34\\n19.805\\n31.293\\nIstanbul Traffic\\nCRPS\\n0.173\\n0.116\\n0.112\\n0.1120.00\\n0.1100.01\\n0.1100.01\\n0.1080.00\\n0.589\\n0.257\\nMSIS\\n5.937\\n4.461\\n4.277\\n3.8130.09\\n4.7520.17\\n4.0570.44\\n4.0940.31\\n16.317\\n45.473\\nTurkey Power\\nCRPS\\n0.048\\n0.040\\n0.036\\n0.0540.01\\n0.0460.01\\n0.0390.00\\n0.0660.02\\n0.116\\n0.085\\nMSIS\\n7.127\\n6.766\\n6.341\\n8.9780.51\\n8.5790.52\\n7.9430.31\\n13.5201.17\\n14.863\\n36.256\\nTable 6.',\n",
       "  'Long sequence forecasting results.\\n\\nResults are averaged across prediction lengths {96, 192, 336, 720}.\\n\\nBest results are highlighted\\nin bold, and second best results are underlined.\\n\\nFull-shot results are obtained from Liu et al. (2023b).',\n",
       "  'Zero-shot\\nFull-shot\\nMOIRAISmall\\nMOIRAIBase\\nMOIRAILarge\\niTransformer\\nTimesNet\\nPatchTST\\nCrossformer\\nTiDE\\nDLinear\\nSCINet\\nFEDformer\\nETTh1\\nMSE\\n0.400\\n0.434\\n0.510\\n0.454\\n0.458\\n0.469\\n0.529\\n0.541\\n0.456\\n0.747\\n0.44\\nMAE\\n0.424\\n0.438\\n0.469\\n0.448\\n0.450\\n0.455\\n0.522\\n0.507\\n0.452\\n0.647\\n0.46\\nETTh2\\nMSE\\n0.341\\n0.345\\n0.354\\n0.383\\n0.414\\n0.387\\n0.942\\n0.611\\n0.559\\n0.954\\n0.437\\nMAE\\n0.379\\n0.382\\n0.376\\n0.407\\n0.497\\n0.407\\n0.684\\n0.550\\n0.515\\n0.723\\n0.449\\nETTm1\\nMSE\\n0.448\\n0.381\\n0.390\\n0.407\\n0.400\\n0.387\\n0.513\\n0.419\\n0.403\\n0.486\\n0.448\\nMAE\\n0.409\\n0.388\\n0.389\\n0.410\\n0.406\\n0.400\\n0.495\\n0.419\\n0.407\\n0.481\\n0.452\\nETTm2\\nMSE\\n0.300\\n0.272\\n0.276\\n0.288\\n0.291\\n0.281\\n0.757\\n0.358\\n0.35\\n0.571\\n0.305\\nMAE\\n0.341\\n0.321\\n0.320\\n0.332\\n0.333\\n0.326\\n0.611\\n0.404\\n0.401\\n0.537\\n0.349\\nElectricity\\nMSE\\n0.233\\n0.188\\n0.188\\n0.178\\n0.193\\n0.216\\n0.244\\n0.252\\n0.212\\n0.268\\n0.214\\nMAE\\n0.320\\n0.274\\n0.273\\n0.270\\n0.295\\n0.304\\n0.334\\n0.344\\n0.3\\n0.365\\n0.327\\nWeather\\nMSE\\n0.242\\n0.238\\n0.259\\n0.258\\n0.259\\n0.259\\n0.259\\n0.271\\n0.265\\n0.292\\n0.309\\nMAE\\n0.267\\n0.261\\n0.275\\n0.278\\n0.287\\n0.281\\n0.315\\n0.320\\n0.317\\n0.363\\n0.36\\nLong Sequence Forecasting\\nWe evaluate on a subset\\nof the popular long sequence forecasting benchmark (Wu\\net al., 2021), omitting datasets which have datasets from\\nthe same source present in our pre-training data and cannot\\nbe considered zero-shot.',\n",
       "  'We report the Mean Squared Er-\\nror (MSE) and MAE, comparing against six state-of-the-art\\nbaselines, iTransformer (Liu et al., 2023b), TimesNet (Wu\\net al., 2023), PatchTST, Crossformer (Zhang & Yan, 2023),\\nTiDE, DLinear (Zeng et al., 2023), SCINet (Liu et al., 2022),\\nand FEDformer (Zhou et al., 2022b).\\n\\nPoint forecasts are ob-\\ntained from MOIRAI by taking the median from the samples\\nof the predictive distribution.',\n",
       "  'Tuning for MOIRAI was based\\non the average validation MSE across prediction lengths,\\nfurther including the options between channel indepedent\\nand channel mixing strategies (Nie et al., 2023) for the low\\ndimension datasets (ETT and Weather).\\n\\n\\nTable 6 reports the average performance across prediction\\nlengths, with full results in Appendix D.3.\\n\\nWe observe\\nthat MOIRAI achieves strong results compared to full-shot\\nbaselines.',\n",
       "  'While MOIRAIBase consistently achieves strong\\nperformance across datasets with either best or second-best\\nperformance, the large model is less consistent, with\\nslightly weaker but competitive results.\\n\\nThe relationship\\nbetween performance and model size is tenuous in this\\nsetting, however, this does not constitute strong evidence\\nagainst the potential of scaling, since these results are\\nTable 7.\\n\\nAblation study on Monash benchmark.',\n",
       "  'The aggregated\\nnormalized MAE, similarly calculated as in Figure 3 is reported.\\n\\n\\nNormalized MAE\\nMOIRAISmall\\n0.655\\nw/o patch size constraints\\n0.720\\nw/o multi patch size\\n1.156\\n\\n\\nw/o Any-variate Attention\\n0.904\\nw/o mixture distribution\\n0.740\\nw/o LOTSA\\n0.809\\n\\n\\nw/o packing\\n0.785\\nbased on models trained on a fixed dataset size and settings.\\n\\n\\nRather, this calls for more comprehensive neural scaling\\nlaws (Kaplan et al., 2020) for LTMs, to build a stronger\\nunderstanding of their scaling behavior.\\n\\n\\n4.3.',\n",
       "  '4.3.\\n\\nAblation Study\\nArchitecture\\nWe perform a series of ablations in Table 7,\\nstarting from the default MOIRAISmall.\\n\\nFirstly, we ablate\\nthe multi patch size component, removing the constraints\\nby allowing any frequency to have any patch size during\\ntraining, and also simply fixing the patch size to 32.\\n\\nIn\\nboth cases, we observe a deterioration in normalized MAE.',\n",
       "  'Removing Any-variate Attention and using additive learned\\nembeddings (randomizing variate index during training\\nto encourage permutation invariance) instead, leads to\\nsuboptimal results, showcasing the strength of Any-variate\\nAttention.\\n\\nWe see similar deterioration when replacing\\n7\\nUnified Training of Universal Time Series Forecasting Transformers\\ntarget\\nprediction\\nprediction: 0.5\\nprediction: 0.9\\n(a) Mixture distribution.\\n\\n\\ntarget\\nprediction\\nprediction: 0.5\\nprediction: 0.9\\n(b) Students t-distribution.',\n",
       "  'Figure 4.\\n\\nVisualization of probabilistic forecasts by two variants\\nof MOIRAISmall on the Traffic Hourly dataset.\\n\\nBoth models fore-\\ncast peaks, however, the Students t-distribution has a symmetric\\ndistribution, giving inappropriate prediction intervals for a peak,\\nas highlighted in red.',\n",
       "  '100\\n250\\n500\\n750\\n1000\\n2000\\n3000\\n4000\\n5000\\n0.45\\n0.50\\n0.55\\n0.60\\n0.65\\nETTm1\\n100\\n250\\n500\\n750\\n1000\\n2000\\n3000\\n4000\\n5000\\n0.24\\n0.26\\n0.28\\n0.30\\n0.32\\nElectricity\\n100\\n250\\n500\\n750\\n1000\\n2000\\n3000\\n4000\\n5000\\n0.30\\n0.35\\n0.40\\n0.45\\n0.50\\nWeather\\nFigure 5.\\n\\nPlot of performance (MAE) against context length (x-\\naxis in log scale) with prediction length 96 and patch size 32 on\\nthe validation set of the ETTm1, Electricity, and Weather datasets.',\n",
       "  'the mixture distribution with a Students t-distribution, and\\nfurther visualize the necessity of flexible distributions for\\nprobabilistic forecasts in Figure 4.\\nTraining Methodology\\n\\n\\nWe study the impact of a large\\nand diverse dataset by training MOIRAISmall only on the Glu-\\nonTS and Monash datasets, observing that diversity of data\\nis critical for cross-domain training even on in-distribution\\nevaluation.',\n",
       "  'Finally, given the same batch size and training\\niterations, we show that packed training significantly boosts\\nperformance.\\n\\nThis is because packing increases the effec-\\ntive batch size and increases the number of observations the\\nmodel is trained on, given the same amount of compute.\\n4.4.\\n\\nFurther Analysis\\nContext Length\\nOur pre-training methodology varies\\ncontext length defined by the task distribution.',\n",
       "  'We verify\\nthat MOIRAI has the capability to take as input arbitrary\\ncontext lengths by visualizing in Figure 5 the relationship\\nbetween performance and increasing context lengths\\nover three datasets in the zero-shot setting.\\n\\nZeng et al.\\n(2023); Liu et al. (2023b) previously observed that the\\ndesiderata of continuously improving performance with\\nincreasing context length is not present in conventional\\nTransformer-based forecasters.',\n",
       "  'Here, we observe that\\nMOIRAI indeed achieves this desired property, in fact,\\ncapable of handling thousands of time steps.\\n0\\n100\\n200\\n300\\n400\\n500\\nSequence Length\\n0.000\\n0.002\\n0.004\\n0.006\\n0.008\\nDensity\\nFigure 6.\\n\\nHistogram of sequence length when sampling data from\\nLOTSA based on the proposed task distribution.\\n\\nSequence length\\nrefers to the number of tokens after patching and flattening.',\n",
       "  'Packing\\nPacking has long been applied in training LLMs\\nand other Transformer-based models, but not for time series\\nTransformers.\\n\\nWhile we can get away with inefficiencies\\nwhen dealing with small-scale data, we start to suffer from\\nlonger training times as we scale towards the paradigm of\\nFMs and LTMs.\\n\\nThis is further exacerbated by our flat-\\ntened setting which increases the disparity in sequence\\nlengths.\\n\\nAs evidenced in Section 4.3, keeping compute\\n(batch size, iterations, etc.)',\n",
       "  'constant, packing improves per-\\nformance by 16%.\\n\\nTo understand why this is the case, we\\nvisualize sequence length distribution in Figure 6.\\n\\nWith a\\nlarge portion of the data being shorter than the maximum se-\\nquence length, padding represents a whopping 61.08% of in-\\nput tokens without packed training, and only 0.38% with our\\npacked implementation (calculated over 1000 iterations).\\n\\n\\n5.',\n",
       "  '5.\\n\\nConclusion\\nIn this work, we introduced MOIRAI, a masked encoder-\\nbased universal time series forecasting Transformer which\\nalleviates the issues faced in the universal forecasting\\nparadigm.\\n\\nWe also introduce the LOTSA, the largest col-\\nlection of open-data for pre-training time series forecasting\\nmodels.\\n\\nMOIRAI is evaluated on the in-distribution and\\nout-of-distribution settings, and is capable of probabilistic\\nand long sequence forecasting.',\n",
       "  'We show that as a zero-\\nshot forecaster, MOIRAI achieves competitive or superior\\nperformance compared to full-shot models.\\n\\n\\nLimitations & Future Work\\nWhile MOIRAI achieves\\nphenomenal in and out-of-distribution performance, this is\\njust a first step in the universal forecasting paradigm.\\n\\nDue\\nto resource constraints, little to no hyperparameter tuning\\nwas performed  efficient tuning techniques such as P\\n(Yang et al., 2022a) can be applied.',\n",
       "  'In terms of architecture,\\nour approach to tackling cross-frequency learning with a\\nmulti patch size mapping is somewhat heuristic, and future\\nwork should design a more flexible and elegant approach.\\n\\n\\nAlso, the current architecture has limited support for high-\\ndimensional time series, and efficient methods for extending\\nTransformer input length can alleviate this issue.\\n\\nIn terms of\\ndata, LOTSA can be further enhanced with greater diversity\\nin terms of domain and frequency.',\n",
       "  'Finally, incorporating\\nmulti-modality such as tabular or text inputs is an exciting\\nnew direction which universal forecasting has unlocked.\\n\\n\\n8\\nUnified Training of Universal Time Series Forecasting Transformers\\nImpact Statement\\nThis paper presents work whose goal is to advance the field\\nof Machine Learning.\\n\\nThere are many potential societal\\nconsequences of our work, none which we feel must be\\nspecifically highlighted here.',\n",
       "  'Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\\nthe model of choice in natural language processing (NLP).\\n\\nThe dominant approach is to pre-train on\\na large text corpus and then ne-tune on a smaller task-specic dataset (Devlin et al., 2019).\\n\\nThanks\\nto Transformers computational efciency and scalability, it has become possible to train models of\\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020).',\n",
       "  'With the\\nmodels and datasets growing, there is still no sign of saturating performance.\\n\\n\\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\\nKrizhevsky et al., 2012; He et al., 2016).\\n\\nInspired by NLP successes, multiple works try combining\\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a).',\n",
       "  'The latter models, while\\ntheoretically efcient, have not yet been scaled effectively on modern hardware accelerators due to\\nthe use of specialized attention patterns.\\n\\nTherefore, in large-scale image recognition, classic ResNet-\\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\\n2020).\\n\\n\\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\\nTransformer directly to images, with the fewest possible modications.',\n",
       "  'To do so, we split an image\\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\\nformer.\\n\\nImage patches are treated the same way as tokens (words) in an NLP application.\\n\\nWe train\\nthe model on image classication in supervised fashion.\\n\\n\\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\\nels yield modest accuracies of a few percentage points below ResNets of comparable size.',\n",
       "  'This\\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\\n1Fine-tuning\\ncode\\nand\\npre-trained\\nmodels\\nare\\navailable\\nat\\nhttps://github.com/\\ngoogle-research/vision_transformer\\n1\\narXiv:2010.11929v2  \\n\\n[cs.CV]  3 Jun 2021\\nPublished as a conference paper at ICLR 2021\\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\\nwhen trained on insufcient amounts of data.',\n",
       "  'However, the picture changes if the models are trained on larger datasets (14M-300M images).\\n\\nWe\\nnd that large scale training trumps inductive bias.\\n\\nOur Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufcient scale and transferred to tasks with fewer datapoints.\\n\\nWhen\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks.',\n",
       "  'In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n\\n\\n2\\nRELATED WORK\\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\\ncome the state of the art method in many NLP tasks.',\n",
       "  'Large Transformer-based models are often\\npre-trained on large corpora and then ne-tuned for the task at hand: BERT (Devlin et al., 2019)\\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\\n\\n\\nNaive application of self-attention to images would require that each pixel attends to every other\\npixel.',\n",
       "  'With quadratic cost in the number of pixels, this does not scale to realistic input sizes.\\n\\nThus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past.\\n\\nParmar et al.\\n\\n(2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally.\\n\\nSuch local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019;\\n\\nZhao et al., 2020).',\n",
       "  'In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images.\\n\\nAn alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a).',\n",
       "  'Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefciently on hardware accelerators.\\n\\n\\nMost related to ours is the model of Cordonnier et al.\\n\\n(2020), which extracts patches of size 2  2\\nfrom the input image and applies full self-attention on top.',\n",
       "  'This model is very similar to ViT,\\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\\ncompetitive with (or even better than) state-of-the-art CNNs.\\n\\nMoreover, Cordonnier et al. (2020)\\nuse a small patch size of 2  2 pixels, which makes the model applicable only to small-resolution\\nimages, while we handle medium-resolution images as well.',\n",
       "  'There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\\nof self-attention, e.g. by augmenting feature maps for image classication (Bello et al., 2019) or by\\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classication (Wu\\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or unied text-vision tasks (Chen\\net al., 2020c; Lu et al., 2019;',\n",
       "  'Li et al., 2019).\\n\\n\\nAnother recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\\nto image pixels after reducing image resolution and color space.\\n\\nThe model is trained in an unsu-\\npervised fashion as a generative model, and the resulting representation can then be ne-tuned or\\nprobed linearly for classication performance, achieving a maximal accuracy of 72% on ImageNet.',\n",
       "  'Our work adds to the increasing collection of papers that explore image recognition at larger scales\\nthan the standard ImageNet dataset.\\n\\nThe use of additional data sources allows to achieve state-of-\\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\\n\\n\\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\\net al. (2020);\\n\\nDjolonga et al.',\n",
       "  '(2020) perform an empirical exploration of CNN transfer learning from\\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\\nwell, but train Transformers instead of ResNet-based models used in prior works.\\n\\n\\n2\\nPublished as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection of Flattened Patches\\n*',\n",
       "  'Extra learnable\\n     [ cl ass]  embedding\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\nPatch + Position \\nEmbedding\\nClass\\nBird\\nBall\\nCar\\n...\\nEmbedded \\nPatches\\nMulti-Head \\nAttention\\nNorm\\nMLP\\nNorm\\n+\\nL x\\n+\\nTransformer Encoder\\nFigure 1: Model overview.\\n\\nWe split an image into xed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder.',\n",
       "  'In order to perform classication, we use the standard approach of adding an extra learnable\\nclassication token to the sequence.\\n\\nThe illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n\\n\\n3\\nMETHOD\\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.',\n",
       "  'An advantage of this intentionally simple setup is that scalable NLP Transformer architectures  and\\ntheir efcient implementations  can be used almost out of the box.\\n3.1\\nVISION TRANSFORMER (VIT)\\nAn overview of the model is depicted in Figure 1.\\n\\nThe standard Transformer receives as input a 1D\\nsequence of token embeddings.',\n",
       "  'To handle 2D images, we reshape the image x  RHW C into a\\nsequence of attened 2D patches xp  RN(P 2C), where (H, W) is the resolution of the original\\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\\nis the resulting number of patches, which also serves as the effective input sequence length for the\\nTransformer.',\n",
       "  'The Transformer uses constant latent vector size D through all of its layers, so we\\natten the patches and map to D dimensions with a trainable linear projection (Eq. 1).\\n\\nWe refer to\\nthe output of this projection as the patch embeddings.\\n\\n\\nSimilar to BERTs\\n\\n[class] token, we prepend a learnable embedding to the sequence of embed-\\nded patches (z0\\n0 = xclass), whose state at the output of the Transformer encoder (z0\\nL) serves as the\\nimage representation y (Eq. 4).',\n",
       "  'Both during pre-training and ne-tuning, a classication head is at-\\ntached to z0\\nL.\\n\\nThe classication head is implemented by a MLP with one hidden layer at pre-training\\ntime and by a single linear layer at ne-tuning time.\\n\\n\\nPosition embeddings are added to the patch embeddings to retain positional information.\\n\\nWe use\\nstandard learnable 1D position embeddings, since we have not observed signicant performance\\ngains from using more advanced 2D-aware position embeddings (Appendix D.4).',\n",
       "  'The resulting\\nsequence of embedding vectors serves as input to the encoder.\\n\\n\\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3).\\n\\nLayernorm (LN) is applied before\\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\\n\\n\\n3\\nPublished as a conference paper at ICLR 2021\\nThe MLP contains two layers with a GELU non-linearity.\\n\\n\\nz0 =',\n",
       "  'z0 =\\n\\n[xclass; x1\\npE; x2\\npE;    ; xN\\np E] + Epos,\\nE  R(P\\n\\n2C)D, Epos  R(N+1)D\\n(1)\\nz\\n = MSA(LN(z1))\\n\\n+\\n\\nz1,\\n = 1 . . .\\n\\nL\\n(2)\\nz = MLP(LN(z\\n))\\n\\n+\\n\\nz\\n,\\n = 1 . . .\\n\\nL\\n(3)\\ny = LN(z0\\nL)\\n(4)\\nInductive bias.\\n\\n\\nWe note that Vision Transformer has much less image-specic inductive bias than\\nCNNs.\\n\\nIn CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\\nbaked into each layer throughout the whole model.',\n",
       "  'In ViT, only MLP layers are local and transla-\\ntionally equivariant, while the self-attention layers are global.\\n\\nThe two-dimensional neighborhood\\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\\nat ne-tuning time for adjusting the position embeddings for images of different resolution (as de-\\nscribed below).',\n",
       "  'Other than that, the position embeddings at initialization time carry no information\\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\\nfrom scratch.\\n\\n\\nHybrid Architecture.\\n\\n\\nAs an alternative to raw image patches, the input sequence can be formed\\nfrom feature maps of a CNN (LeCun et al., 1989).\\n\\nIn this hybrid model, the patch embedding\\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map.',\n",
       "  'As a special case,\\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\\nattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\\n\\n\\nThe classication input embedding and position embeddings are added as described above.\\n3.2\\nFINE-TUNING AND HIGHER RESOLUTION\\nTypically, we pre-train ViT on large datasets, and ne-tune to (smaller) downstream tasks.',\n",
       "  'For\\nthis, we remove the pre-trained prediction head and attach a zero-initialized D \\n\\nK feedforward\\nlayer, where K is the number of downstream classes.\\n\\nIt is often benecial to ne-tune at higher\\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020).\\n\\nWhen feeding images\\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\\nlength.',\n",
       "  'The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\\nhowever, the pre-trained position embeddings may no longer be meaningful.\\n\\nWe therefore perform\\n2D interpolation of the pre-trained position embeddings, according to their location in the original\\nimage.\\n\\nNote that this resolution adjustment and patch extraction are the only points at which an\\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.',\n",
       "  '4\\nEXPERIMENTS\\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid.\\n\\nTo understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks.\\n\\nWhen considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost.',\n",
       "  'Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n\\n\\n4.1\\nSETUP\\nDatasets.\\n\\nTo explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\\n303M high-resolution images.\\n\\nWe de-duplicate the pre-training datasets w.r.t.',\n",
       "  'the test sets of the\\ndownstream tasks following Kolesnikov et al. (2020).\\n\\nWe transfer the models trained on these\\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008).\\n\\nFor these datasets, pre-processing\\nfollows Kolesnikov et al. (2020).',\n",
       "  '4\\nPublished as a conference paper at ICLR 2021\\nModel\\nLayers\\nHidden size D\\nMLP size\\nHeads\\nParams\\nViT-Base\\n12\\n768\\n3072\\n12\\n86M\\nViT-Large\\n24\\n1024\\n4096\\n16\\n307M\\nViT-Huge\\n32\\n1280\\n5120\\n16\\n632M\\nTable 1: Details of Vision Transformer model variants.\\n\\n\\nWe also evaluate on the 19-task VTAB classication suite (Zhai et al., 2019b).\\n\\nVTAB evaluates\\nlow-data transfer to diverse tasks, using 1 000 training examples per task.',\n",
       "  'The tasks are divided into\\nthree groups: Natural  tasks like the above, Pets, CIFAR, etc. Specialized  medical and satellite\\nimagery, and Structured  tasks that require geometric understanding like localization.\\n\\n\\nModel Variants.\\n\\nWe base ViT congurations on those used for BERT (Devlin et al., 2019), as\\nsummarized in Table 1.\\n\\nThe Base and Large models are directly adopted from BERT and we\\nadd the larger Huge model.',\n",
       "  'In what follows we use brief notation to indicate the model size and\\nthe input patch size: for instance, ViT-L/16 means the Large variant with 1616 input patch size.\\n\\n\\nNote that the Transformers sequence length is inversely proportional to the square of the patch size,\\nthus models with smaller patch size are computationally more expensive.',\n",
       "  'For the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\\nconvolutions (Qiao et al., 2019).\\n\\nThese modications improve transfer (Kolesnikov et al., 2020),\\nand we denote the modied model ResNet (BiT).\\n\\nFor the hybrids, we feed the intermediate fea-\\nture maps into ViT with patch size of one pixel.',\n",
       "  'To experiment with different sequence lengths,\\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\\nstage 3.\\n\\nOption (ii) results in a 4x longer sequence length, and a more expensive ViT model.\\n\\n\\nTraining & Fine-tuning.',\n",
       "  'We train all models, including ResNets, using Adam (Kingma & Ba,\\n2015) with 1 = 0.9, 2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\\npractices, Adam works slightly better than SGD for ResNets in our setting).\\n\\nWe use a linear learning\\nrate warmup and decay, see Appendix B.1 for details.\\n\\nFor ne-tuning we use SGD with momentum,\\nbatch size 512, for all models, see Appendix B.1.1.',\n",
       "  'For ImageNet results in Table 2, we ne-tuned at\\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\\n\\n\\nMetrics.\\n\\nWe report results on downstream datasets either through few-shot or ne-tuning accuracy.\\n\\n\\nFine-tuning accuracies capture the performance of each model after ne-tuning it on the respective\\ndataset.',\n",
       "  'Few-shot accuracies are obtained by solving a regularized least-squares regression problem\\nthat maps the (frozen) representation of a subset of training images to {1, 1}K target vectors.\\n\\nThis\\nformulation allows us to recover the exact solution in closed form.\\n\\nThough we mainly focus on\\nne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-y evaluation\\nwhere ne-tuning would be too costly.',\n",
       "  '4.2\\nCOMPARISON TO STATE OF THE ART\\nWe rst compare our largest models  ViT-H/14 and ViT-L/16  to state-of-the-art CNNs from\\nthe literature.\\n\\nThe rst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\\nperforms supervised transfer learning with large ResNets.\\n\\nThe second is Noisy Student (Xie et al.,\\n2020), which is a large EfcientNet trained using semi-supervised learning on ImageNet and JFT-\\n300M with the labels removed.',\n",
       "  'Currently, Noisy Student is the state of the art on ImageNet and\\nBiT-L on the other datasets reported here.\\n\\nAll models were trained on TPUv3 hardware, and we\\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\\nv3 cores (2 per chip) used for training multiplied by the training time in days.\\n\\n\\nTable 2 shows the results.',\n",
       "  'The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\\ntional resources to train.\\n\\nThe larger model, ViT-H/14, further improves the performance, especially\\non the more challenging datasets  ImageNet, CIFAR-100, and the VTAB suite.',\n",
       "  'Interestingly, this\\n5\\nPublished as a conference paper at ICLR 2021\\nOurs-JFT\\nOurs-JFT\\nOurs-I21k\\nBiT-L\\nNoisy Student\\n(ViT-H/14)\\n(ViT-L/16)\\n(ViT-L/16)\\n(ResNet152x4)\\n(EfcientNet-L2)',\n",
       "  'ImageNet\\n88.55  0.04\\n87.76  0.03\\n85.30  0.02\\n87.54  0.02\\n88.4/88.5\\nImageNet ReaL\\n90.72  0.05\\n90.54  0.03\\n88.62  0.05\\n90.54\\n90.55\\nCIFAR-10\\n99.50  0.06\\n99.42  0.03\\n99.15  0.03\\n99.37  0.06\\n\\nCIFAR-100\\n94.55  0.04\\n93.90  0.05\\n93.25  0.05\\n93.51  0.08\\n\\nOxford-IIIT Pets\\n97.56  0.03\\n97.32  0.11\\n94.67  0.15\\n96.62  0.23\\n\\nOxford Flowers-102\\n99.68  0.02\\n99.74  0.00\\n99.61  0.02\\n99.63  0.03\\n\\nVTAB (19 tasks)\\n77.63  0.23\\n76.28  0.46\\n72.72  0.21\\n76.29  1.70\\n\\nTPUv3-core-days\\n2.5k\\n0.68k\\n0.23k\\n9.9k\\n12.3k\\nTable 2:\\nComparison with state of the art on popular image classication benchmarks.',\n",
       "  'We re-\\nport mean and standard deviation of the accuracies, averaged over three ne-tuning runs.\\n\\nVision\\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\\ndatasets, while taking substantially less computational resources to pre-train.\\n\\nViT pre-trained on the\\nsmaller public ImageNet-21k dataset performs well too.\\n\\nSlightly improved 88.5% result reported\\nin Touvron et al. (2020).\\n\\n\\nVTAB (19 tasks)\\n65\\n70\\n75\\n80\\nAccuracy',\n",
       "  '[%]\\nNatural (7 tasks)\\n70\\n80\\n90\\nSpecialized (4 tasks)\\n80\\n82\\n85\\n88\\n90\\nStructured (8 tasks)\\n\\n\\n50\\n60\\n70\\nViT-H/14\\nBiT-L (R152x4)\\nVIVI-Ex-100% (R50x3)\\n\\n\\nS4L (R50x1)\\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\\n\\n\\nmodel still took substantially less compute to pre-train than prior state of the art.',\n",
       "  'However, we note\\nthat pre-training efciency may be affected not only by the architecture choice, but also other pa-\\nrameters, such as training schedule, optimizer, weight decay, etc.\\n\\nWe provide a controlled study of\\nperformance vs. compute for different architectures in Section 4.4.',\n",
       "  'Finally, the ViT-L/16 model\\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\\nproximately 30 days.',\n",
       "  'Figure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\\nmethods on this benchmark: BiT, VIVI  a ResNet co-trained on ImageNet and Youtube (Tschannen\\net al., 2020), and S4L  supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\\n\\n\\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks.\\n\\nOn the\\nSpecialized the performance of the top two models is similar.',\n",
       "  '4.3\\nPRE-TRAINING DATA REQUIREMENTS\\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset.\\n\\nWith fewer\\ninductive biases for vision than ResNets, how crucial is the dataset size?\\n\\nWe perform two series of\\nexperiments.\\n\\n\\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\\n300M.\\n\\nTo boost the performance on the smaller datasets, we optimize three basic regularization\\nparameters  weight decay, dropout, and label smoothing.',\n",
       "  'Figure 3 shows the results after ne-\\ntuning to ImageNet (results on other datasets are shown in Table 5)2.\\n\\nWhen pre-trained on the\\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\\n(moderate) regularization.\\n\\nWith ImageNet-21k pre-training, their performances are similar.\\n\\nOnly\\nwith JFT-300M, do we see the full benet of larger models.\\n\\nFigure 3 also shows the performance\\n2Note that the ImageNet pre-trained models are also ne-tuned, but again on ImageNet.',\n",
       "  'This is because the\\nresolution increase during ne-tuning improves the performance.\\n\\n\\n6\\nPublished as a conference paper at ICLR 2021\\nImageNet\\nImageNet-21k\\n\\n\\nJFT-300M\\nPre-training dataset\\n70\\n75\\n80\\n85\\n90\\nImageNet Top1 Accuracy\\n\\n[%]\\nBiT\\nViT-B/32\\nViT-B/16\\nViT-L/32\\nViT-L/16\\nViT-H/14\\nFigure 3:\\nTransfer to ImageNet.\\n\\n\\nWhile\\nlarge ViT models perform worse than BiT\\nResNets (shaded area) when pre-trained on\\nsmall datasets, they shine when pre-trained on\\nlarger datasets.',\n",
       "  'Similarly, larger ViT variants\\novertake smaller ones as the dataset grows.\\n\\n\\n10 M\\n30 M\\n100 M\\n300 M\\nNumber of JFT pre-training samples\\n30\\n40\\n50\\n60\\n70\\nLinear 5-shot ImageNet Top1\\n\\n[%]\\nViT-L/16\\nViT-L/32\\nViT-B/32\\nViT-b/32\\nResNet50x1 (BiT)\\nResNet152x2 (BiT)\\nFigure 4: Linear few-shot evaluation on Ima-\\ngeNet versus pre-training size.\\n\\nResNets per-\\nform better with smaller pre-training datasets\\nbut plateau sooner than ViT, which performs\\nbetter with larger pre-training.',\n",
       "  'ViT-b is ViT-B\\nwith all hidden dimensions halved.\\n\\n\\n102\\n103\\n90\\n95\\nTransfer accuracy\\n\\n[%]\\nAverage-5\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\n102\\n103\\n75\\n80\\n85\\n90\\nImageNet\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\nTotal pre-training compute [exaFLOPs]\\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\\nResNets, and hybrids.\\n\\nVision Transformers generally outperform ResNets with the same compu-\\ntational budget.',\n",
       "  'Hybrids improve upon pure Transformers for smaller model sizes, but the gap\\nvanishes for larger models.\\n\\n\\nregion spanned by BiT models of different sizes.\\n\\nThe BiT CNNs outperform ViT on ImageNet, but\\nwith the larger datasets, ViT overtakes.\\n\\n\\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\\n300M dataset.\\n\\nWe do not perform additional regularization on the smaller subsets and use the same\\nhyper-parameters for all settings.',\n",
       "  'This way, we assess the intrinsic model properties, and not the\\neffect of regularization.\\n\\nWe do, however, use early-stopping, and report the best validation accuracy\\nachieved during training.\\n\\nTo save compute, we report few-shot linear accuracy instead of full ne-\\ntuning accuracy.\\n\\nFigure 4 contains the results.\\n\\nVision Transformers overt more than ResNets with\\ncomparable computational cost on smaller datasets.',\n",
       "  'For example, ViT-B/32 is slightly faster than\\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets.\\n\\nThe same is true\\nfor ResNet152x2 and ViT-L/16.\\n\\nThis result reinforces the intuition that the convolutional inductive\\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\\ndata is sufcient, even benecial.',\n",
       "  'Overall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\\n(Table 2) seem promising for very low-data transfer.\\n\\nFurther analysis of few-shot properties of ViT\\nis an exciting direction of future work.',\n",
       "  '7\\nPublished as a conference paper at ICLR 2021\\n4.4\\nSCALING STUDY\\nWe perform a controlled scaling study of different models by evaluating transfer performance from\\nJFT-300M. In this setting data size does not bottleneck the models performances, and we assess\\nperformance versus pre-training cost of each model.',\n",
       "  'The model set includes: 7 ResNets, R50x1,\\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\\nbackbone).',\n",
       "  'Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\\nfor details on computational costs).\\n\\nDetailed results per model are provided in Table 6 in the Ap-\\npendix.\\n\\nA few patterns can be observed.\\n\\nFirst, Vision Transformers dominate ResNets on the\\nperformance/compute trade-off.\\n\\nViT uses approximately 2  4 less compute to attain the same\\nperformance (average over 5 datasets).',\n",
       "  'Second, hybrids slightly outperform ViT at small compu-\\ntational budgets, but the difference vanishes for larger models.\\n\\nThis result is somewhat surprising,\\nsince one might expect convolutional local feature processing to assist ViT at any size.\\n\\nThird, Vision\\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\\n\\n\\n4.5\\nINSPECTING VISION TRANSFORMER\\nInput\\nAttention\\nFigure 6: Representative ex-\\namples of attention from the\\noutput token to the input\\nspace.',\n",
       "  'See Appendix D.7 for\\ndetails.\\n\\n\\nTo begin to understand how the Vision Transformer processes im-\\nage data, we analyze its internal representations.\\n\\nThe rst layer of\\nthe Vision Transformer linearly projects the attened patches into a\\nlower-dimensional space (Eq. 1).\\n\\nFigure 7 (left) shows the top prin-\\ncipal components of the the learned embedding lters.\\n\\nThe com-\\nponents resemble plausible basis functions for a low-dimensional\\nrepresentation of the ne structure within each patch.',\n",
       "  'After the projection, a learned position embedding is added to the\\npatch representations.\\n\\nFigure 7 (center) shows that the model learns\\nto encode distance within the image in the similarity of position em-\\nbeddings, i.e. closer patches tend to have more similar position em-\\nbeddings.\\n\\nFurther, the row-column structure appears; patches in the\\nsame row/column have similar embeddings.\\n\\nFinally, a sinusoidal\\nstructure is sometimes apparent for larger grids (Appendix D).',\n",
       "  'That\\nthe position embeddings learn to represent 2D image topology ex-\\nplains why hand-crafted 2D-aware embedding variants do not yield\\nimprovements (Appendix D.4).\\n\\n\\nSelf-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers.\\n\\nWe investigate to what degree\\nthe network makes use of this capability.\\n\\nSpecically, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right).',\n",
       "  'This\\nattention distance is analogous to receptive eld size in CNNs.\\n\\n\\nWe nd that some heads attend to most of the image already in the lowest layers, showing that\\nthe ability to integrate information globally is indeed used by the model.\\n\\nOther attention heads\\nhave consistently small attention distances in the low layers.',\n",
       "  'This highly localized attention is\\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\\nsuggesting that it may serve a similar function as early convolutional layers in CNNs.\\n\\nFurther, the\\nattention distance increases with network depth.\\n\\nGlobally, we nd that the model attends to image\\nregions that are semantically relevant for classication (Figure 6).\\n\\n\\n4.6\\nSELF-SUPERVISION\\nTransformers show impressive performance on NLP tasks.',\n",
       "  'However, much of their success stems\\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\\n8\\nPublished as a conference paper at ICLR 2021\\nRGB embedding filters\\n(first 28 principal components)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch row\\nPosition embedding similarity\\n1\\n1\\nCosine similarity\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)',\n",
       "  'ViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32.\\n\\nCenter:\\n\\nSim-\\nilarity of position embeddings of ViT-L/32.\\n\\nTiles show the cosine similarity between the position\\nembedding of the patch with the indicated row and column and the position embeddings of all other\\npatches.\\n\\nRight:\\n\\nSize of attended area by head and network depth.\\n\\nEach dot shows the mean attention\\ndistance across images for one of 16 heads at one layer.',\n",
       "  'See Appendix D.7 for details.\\n\\n\\net al., 2019;\\n\\nRadford et al., 2018).\\n\\nWe also perform a preliminary exploration on masked patch\\nprediction for self-supervision, mimicking the masked language modeling task used in BERT.\\n\\nWith\\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\\nsignicant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\\n\\n\\nAppendix B.1.2 contains further details.',\n",
       "  'We leave exploration of contrastive pre-training (Chen\\net al., 2020b; He et al., 2020; Bachman et al., 2019; Henaff et al., 2020) to future work.\\n\\n\\n5\\nCONCLUSION\\n\\n\\nWe have explored the direct application of Transformers to image recognition.\\n\\nUnlike prior works\\nusing self-attention in computer vision, we do not introduce image-specic inductive biases into\\nthe architecture apart from the initial patch extraction step.',\n",
       "  'Instead, we interpret an image as a\\nsequence of patches and process it by a standard Transformer encoder as used in NLP.\\n\\nThis simple,\\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\n\\n\\nThus, Vision Transformer matches or exceeds the state of the art on many image classication\\ndatasets, whilst being relatively cheap to pre-train.\\n\\n\\nWhile these initial results are encouraging, many challenges remain.',\n",
       "  'One is to apply ViT to other\\ncomputer vision tasks, such as detection and segmentation.\\n\\nOur results, coupled with those in Carion\\net al. (2020), indicate the promise of this approach.\\n\\nAnother challenge is to continue exploring self-\\nsupervised pre-training methods.\\n\\nOur initial experiments show improvement from self-supervised\\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\\ntraining.',\n",
       "  'Finally, further scaling of ViT would likely lead to improved performance.\\n\\n\\nACKNOWLEDGEMENTS\\nThe work was performed in Berlin, Zurich, and Amsterdam.',\n",
       "  'We thank many colleagues at Google\\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Lucic, Noam\\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.',\n",
       "  'Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\\nthe model of choice in natural language processing (NLP).\\n\\nThe dominant approach is to pre-train on\\na large text corpus and then ne-tune on a smaller task-specic dataset (Devlin et al., 2019).\\n\\nThanks\\nto Transformers computational efciency and scalability, it has become possible to train models of\\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020).',\n",
       "  'With the\\nmodels and datasets growing, there is still no sign of saturating performance.\\n\\n\\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\\nKrizhevsky et al., 2012; He et al., 2016).\\n\\nInspired by NLP successes, multiple works try combining\\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a).',\n",
       "  'The latter models, while\\ntheoretically efcient, have not yet been scaled effectively on modern hardware accelerators due to\\nthe use of specialized attention patterns.\\n\\nTherefore, in large-scale image recognition, classic ResNet-\\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\\n2020).\\n\\n\\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\\nTransformer directly to images, with the fewest possible modications.',\n",
       "  'To do so, we split an image\\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\\nformer.\\n\\nImage patches are treated the same way as tokens (words) in an NLP application.\\n\\nWe train\\nthe model on image classication in supervised fashion.\\n\\n\\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\\nels yield modest accuracies of a few percentage points below ResNets of comparable size.',\n",
       "  'This\\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\\n1Fine-tuning\\ncode\\nand\\npre-trained\\nmodels\\nare\\navailable\\nat\\nhttps://github.com/\\ngoogle-research/vision_transformer\\n1\\narXiv:2010.11929v2  \\n\\n[cs.CV]  3 Jun 2021\\nPublished as a conference paper at ICLR 2021\\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\\nwhen trained on insufcient amounts of data.',\n",
       "  'However, the picture changes if the models are trained on larger datasets (14M-300M images).\\n\\nWe\\nnd that large scale training trumps inductive bias.\\n\\nOur Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufcient scale and transferred to tasks with fewer datapoints.\\n\\nWhen\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks.',\n",
       "  'In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n\\n\\n2\\nRELATED WORK\\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\\ncome the state of the art method in many NLP tasks.',\n",
       "  'Large Transformer-based models are often\\npre-trained on large corpora and then ne-tuned for the task at hand: BERT (Devlin et al., 2019)\\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\\n\\n\\nNaive application of self-attention to images would require that each pixel attends to every other\\npixel.',\n",
       "  'With quadratic cost in the number of pixels, this does not scale to realistic input sizes.\\n\\nThus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past.\\n\\nParmar et al.\\n\\n(2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally.\\n\\nSuch local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019;\\n\\nZhao et al., 2020).',\n",
       "  'In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images.\\n\\nAn alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a).',\n",
       "  'Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefciently on hardware accelerators.\\n\\n\\nMost related to ours is the model of Cordonnier et al.\\n\\n(2020), which extracts patches of size 2  2\\nfrom the input image and applies full self-attention on top.',\n",
       "  'This model is very similar to ViT,\\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\\ncompetitive with (or even better than) state-of-the-art CNNs.\\n\\nMoreover, Cordonnier et al. (2020)\\nuse a small patch size of 2  2 pixels, which makes the model applicable only to small-resolution\\nimages, while we handle medium-resolution images as well.',\n",
       "  'There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\\nof self-attention, e.g. by augmenting feature maps for image classication (Bello et al., 2019) or by\\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classication (Wu\\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or unied text-vision tasks (Chen\\net al., 2020c; Lu et al., 2019;',\n",
       "  'Li et al., 2019).\\n\\n\\nAnother recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\\nto image pixels after reducing image resolution and color space.\\n\\nThe model is trained in an unsu-\\npervised fashion as a generative model, and the resulting representation can then be ne-tuned or\\nprobed linearly for classication performance, achieving a maximal accuracy of 72% on ImageNet.',\n",
       "  'Our work adds to the increasing collection of papers that explore image recognition at larger scales\\nthan the standard ImageNet dataset.\\n\\nThe use of additional data sources allows to achieve state-of-\\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\\n\\n\\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\\net al. (2020);\\n\\nDjolonga et al.',\n",
       "  '(2020) perform an empirical exploration of CNN transfer learning from\\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\\nwell, but train Transformers instead of ResNet-based models used in prior works.\\n\\n\\n2\\nPublished as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection of Flattened Patches\\n*',\n",
       "  'Extra learnable\\n     [ cl ass]  embedding\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\nPatch + Position \\nEmbedding\\nClass\\nBird\\nBall\\nCar\\n...\\nEmbedded \\nPatches\\nMulti-Head \\nAttention\\nNorm\\nMLP\\nNorm\\n+\\nL x\\n+\\nTransformer Encoder\\nFigure 1: Model overview.\\n\\nWe split an image into xed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder.',\n",
       "  'In order to perform classication, we use the standard approach of adding an extra learnable\\nclassication token to the sequence.\\n\\nThe illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n\\n\\n3\\nMETHOD\\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.',\n",
       "  'An advantage of this intentionally simple setup is that scalable NLP Transformer architectures  and\\ntheir efcient implementations  can be used almost out of the box.\\n3.1\\nVISION TRANSFORMER (VIT)\\nAn overview of the model is depicted in Figure 1.\\n\\nThe standard Transformer receives as input a 1D\\nsequence of token embeddings.',\n",
       "  'To handle 2D images, we reshape the image x  RHW C into a\\nsequence of attened 2D patches xp  RN(P 2C), where (H, W) is the resolution of the original\\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\\nis the resulting number of patches, which also serves as the effective input sequence length for the\\nTransformer.',\n",
       "  'The Transformer uses constant latent vector size D through all of its layers, so we\\natten the patches and map to D dimensions with a trainable linear projection (Eq. 1).\\n\\nWe refer to\\nthe output of this projection as the patch embeddings.\\n\\n\\nSimilar to BERTs\\n\\n[class] token, we prepend a learnable embedding to the sequence of embed-\\nded patches (z0\\n0 = xclass), whose state at the output of the Transformer encoder (z0\\nL) serves as the\\nimage representation y (Eq. 4).',\n",
       "  'Both during pre-training and ne-tuning, a classication head is at-\\ntached to z0\\nL.\\n\\nThe classication head is implemented by a MLP with one hidden layer at pre-training\\ntime and by a single linear layer at ne-tuning time.\\n\\n\\nPosition embeddings are added to the patch embeddings to retain positional information.\\n\\nWe use\\nstandard learnable 1D position embeddings, since we have not observed signicant performance\\ngains from using more advanced 2D-aware position embeddings (Appendix D.4).',\n",
       "  'The resulting\\nsequence of embedding vectors serves as input to the encoder.\\n\\n\\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3).\\n\\nLayernorm (LN) is applied before\\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\\n\\n\\n3\\nPublished as a conference paper at ICLR 2021\\nThe MLP contains two layers with a GELU non-linearity.\\n\\n\\nz0 =',\n",
       "  'z0 =\\n\\n[xclass; x1\\npE; x2\\npE;    ; xN\\np E] + Epos,\\nE  R(P\\n\\n2C)D, Epos  R(N+1)D\\n(1)\\nz\\n = MSA(LN(z1))\\n\\n+\\n\\nz1,\\n = 1 . . .\\n\\nL\\n(2)\\nz = MLP(LN(z\\n))\\n\\n+\\n\\nz\\n,\\n = 1 . . .\\n\\nL\\n(3)\\ny = LN(z0\\nL)\\n(4)\\nInductive bias.\\n\\n\\nWe note that Vision Transformer has much less image-specic inductive bias than\\nCNNs.\\n\\nIn CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\\nbaked into each layer throughout the whole model.',\n",
       "  'In ViT, only MLP layers are local and transla-\\ntionally equivariant, while the self-attention layers are global.\\n\\nThe two-dimensional neighborhood\\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\\nat ne-tuning time for adjusting the position embeddings for images of different resolution (as de-\\nscribed below).',\n",
       "  'Other than that, the position embeddings at initialization time carry no information\\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\\nfrom scratch.\\n\\n\\nHybrid Architecture.\\n\\n\\nAs an alternative to raw image patches, the input sequence can be formed\\nfrom feature maps of a CNN (LeCun et al., 1989).\\n\\nIn this hybrid model, the patch embedding\\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map.',\n",
       "  'As a special case,\\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\\nattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\\n\\n\\nThe classication input embedding and position embeddings are added as described above.\\n3.2\\nFINE-TUNING AND HIGHER RESOLUTION\\nTypically, we pre-train ViT on large datasets, and ne-tune to (smaller) downstream tasks.',\n",
       "  'For\\nthis, we remove the pre-trained prediction head and attach a zero-initialized D \\n\\nK feedforward\\nlayer, where K is the number of downstream classes.\\n\\nIt is often benecial to ne-tune at higher\\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020).\\n\\nWhen feeding images\\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\\nlength.',\n",
       "  'The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\\nhowever, the pre-trained position embeddings may no longer be meaningful.\\n\\nWe therefore perform\\n2D interpolation of the pre-trained position embeddings, according to their location in the original\\nimage.\\n\\nNote that this resolution adjustment and patch extraction are the only points at which an\\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.',\n",
       "  '4\\nEXPERIMENTS\\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid.\\n\\nTo understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks.\\n\\nWhen considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost.',\n",
       "  'Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n\\n\\n4.1\\nSETUP\\nDatasets.\\n\\nTo explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\\n303M high-resolution images.\\n\\nWe de-duplicate the pre-training datasets w.r.t.',\n",
       "  'the test sets of the\\ndownstream tasks following Kolesnikov et al. (2020).\\n\\nWe transfer the models trained on these\\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008).\\n\\nFor these datasets, pre-processing\\nfollows Kolesnikov et al. (2020).',\n",
       "  '4\\nPublished as a conference paper at ICLR 2021\\nModel\\nLayers\\nHidden size D\\nMLP size\\nHeads\\nParams\\nViT-Base\\n12\\n768\\n3072\\n12\\n86M\\nViT-Large\\n24\\n1024\\n4096\\n16\\n307M\\nViT-Huge\\n32\\n1280\\n5120\\n16\\n632M\\nTable 1: Details of Vision Transformer model variants.\\n\\n\\nWe also evaluate on the 19-task VTAB classication suite (Zhai et al., 2019b).\\n\\nVTAB evaluates\\nlow-data transfer to diverse tasks, using 1 000 training examples per task.',\n",
       "  'The tasks are divided into\\nthree groups: Natural  tasks like the above, Pets, CIFAR, etc. Specialized  medical and satellite\\nimagery, and Structured  tasks that require geometric understanding like localization.\\n\\n\\nModel Variants.\\n\\nWe base ViT congurations on those used for BERT (Devlin et al., 2019), as\\nsummarized in Table 1.\\n\\nThe Base and Large models are directly adopted from BERT and we\\nadd the larger Huge model.',\n",
       "  'In what follows we use brief notation to indicate the model size and\\nthe input patch size: for instance, ViT-L/16 means the Large variant with 1616 input patch size.\\n\\n\\nNote that the Transformers sequence length is inversely proportional to the square of the patch size,\\nthus models with smaller patch size are computationally more expensive.',\n",
       "  'For the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\\nconvolutions (Qiao et al., 2019).\\n\\nThese modications improve transfer (Kolesnikov et al., 2020),\\nand we denote the modied model ResNet (BiT).\\n\\nFor the hybrids, we feed the intermediate fea-\\nture maps into ViT with patch size of one pixel.',\n",
       "  'To experiment with different sequence lengths,\\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\\nstage 3.\\n\\nOption (ii) results in a 4x longer sequence length, and a more expensive ViT model.\\n\\n\\nTraining & Fine-tuning.',\n",
       "  'We train all models, including ResNets, using Adam (Kingma & Ba,\\n2015) with 1 = 0.9, 2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\\npractices, Adam works slightly better than SGD for ResNets in our setting).\\n\\nWe use a linear learning\\nrate warmup and decay, see Appendix B.1 for details.\\n\\nFor ne-tuning we use SGD with momentum,\\nbatch size 512, for all models, see Appendix B.1.1.',\n",
       "  'For ImageNet results in Table 2, we ne-tuned at\\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\\n\\n\\nMetrics.\\n\\nWe report results on downstream datasets either through few-shot or ne-tuning accuracy.\\n\\n\\nFine-tuning accuracies capture the performance of each model after ne-tuning it on the respective\\ndataset.',\n",
       "  'Few-shot accuracies are obtained by solving a regularized least-squares regression problem\\nthat maps the (frozen) representation of a subset of training images to {1, 1}K target vectors.\\n\\nThis\\nformulation allows us to recover the exact solution in closed form.\\n\\nThough we mainly focus on\\nne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-y evaluation\\nwhere ne-tuning would be too costly.',\n",
       "  '4.2\\nCOMPARISON TO STATE OF THE ART\\nWe rst compare our largest models  ViT-H/14 and ViT-L/16  to state-of-the-art CNNs from\\nthe literature.\\n\\nThe rst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\\nperforms supervised transfer learning with large ResNets.\\n\\nThe second is Noisy Student (Xie et al.,\\n2020), which is a large EfcientNet trained using semi-supervised learning on ImageNet and JFT-\\n300M with the labels removed.',\n",
       "  'Currently, Noisy Student is the state of the art on ImageNet and\\nBiT-L on the other datasets reported here.\\n\\nAll models were trained on TPUv3 hardware, and we\\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\\nv3 cores (2 per chip) used for training multiplied by the training time in days.\\n\\n\\nTable 2 shows the results.',\n",
       "  'The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\\ntional resources to train.\\n\\nThe larger model, ViT-H/14, further improves the performance, especially\\non the more challenging datasets  ImageNet, CIFAR-100, and the VTAB suite.',\n",
       "  'Interestingly, this\\n5\\nPublished as a conference paper at ICLR 2021\\nOurs-JFT\\nOurs-JFT\\nOurs-I21k\\nBiT-L\\nNoisy Student\\n(ViT-H/14)\\n(ViT-L/16)\\n(ViT-L/16)\\n(ResNet152x4)\\n(EfcientNet-L2)',\n",
       "  'ImageNet\\n88.55  0.04\\n87.76  0.03\\n85.30  0.02\\n87.54  0.02\\n88.4/88.5\\nImageNet ReaL\\n90.72  0.05\\n90.54  0.03\\n88.62  0.05\\n90.54\\n90.55\\nCIFAR-10\\n99.50  0.06\\n99.42  0.03\\n99.15  0.03\\n99.37  0.06\\n\\nCIFAR-100\\n94.55  0.04\\n93.90  0.05\\n93.25  0.05\\n93.51  0.08\\n\\nOxford-IIIT Pets\\n97.56  0.03\\n97.32  0.11\\n94.67  0.15\\n96.62  0.23\\n\\nOxford Flowers-102\\n99.68  0.02\\n99.74  0.00\\n99.61  0.02\\n99.63  0.03\\n\\nVTAB (19 tasks)\\n77.63  0.23\\n76.28  0.46\\n72.72  0.21\\n76.29  1.70\\n\\nTPUv3-core-days\\n2.5k\\n0.68k\\n0.23k\\n9.9k\\n12.3k\\nTable 2:\\nComparison with state of the art on popular image classication benchmarks.',\n",
       "  'We re-\\nport mean and standard deviation of the accuracies, averaged over three ne-tuning runs.\\n\\nVision\\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\\ndatasets, while taking substantially less computational resources to pre-train.\\n\\nViT pre-trained on the\\nsmaller public ImageNet-21k dataset performs well too.\\n\\nSlightly improved 88.5% result reported\\nin Touvron et al. (2020).\\n\\n\\nVTAB (19 tasks)\\n65\\n70\\n75\\n80\\nAccuracy',\n",
       "  '[%]\\nNatural (7 tasks)\\n70\\n80\\n90\\nSpecialized (4 tasks)\\n80\\n82\\n85\\n88\\n90\\nStructured (8 tasks)\\n\\n\\n50\\n60\\n70\\nViT-H/14\\nBiT-L (R152x4)\\nVIVI-Ex-100% (R50x3)\\n\\n\\nS4L (R50x1)\\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\\n\\n\\nmodel still took substantially less compute to pre-train than prior state of the art.',\n",
       "  'However, we note\\nthat pre-training efciency may be affected not only by the architecture choice, but also other pa-\\nrameters, such as training schedule, optimizer, weight decay, etc.\\n\\nWe provide a controlled study of\\nperformance vs. compute for different architectures in Section 4.4.',\n",
       "  'Finally, the ViT-L/16 model\\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\\nproximately 30 days.',\n",
       "  'Figure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\\nmethods on this benchmark: BiT, VIVI  a ResNet co-trained on ImageNet and Youtube (Tschannen\\net al., 2020), and S4L  supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\\n\\n\\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks.\\n\\nOn the\\nSpecialized the performance of the top two models is similar.',\n",
       "  '4.3\\nPRE-TRAINING DATA REQUIREMENTS\\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset.\\n\\nWith fewer\\ninductive biases for vision than ResNets, how crucial is the dataset size?\\n\\nWe perform two series of\\nexperiments.\\n\\n\\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\\n300M.\\n\\nTo boost the performance on the smaller datasets, we optimize three basic regularization\\nparameters  weight decay, dropout, and label smoothing.',\n",
       "  'Figure 3 shows the results after ne-\\ntuning to ImageNet (results on other datasets are shown in Table 5)2.\\n\\nWhen pre-trained on the\\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\\n(moderate) regularization.\\n\\nWith ImageNet-21k pre-training, their performances are similar.\\n\\nOnly\\nwith JFT-300M, do we see the full benet of larger models.\\n\\nFigure 3 also shows the performance\\n2Note that the ImageNet pre-trained models are also ne-tuned, but again on ImageNet.',\n",
       "  'This is because the\\nresolution increase during ne-tuning improves the performance.\\n\\n\\n6\\nPublished as a conference paper at ICLR 2021\\nImageNet\\nImageNet-21k\\n\\n\\nJFT-300M\\nPre-training dataset\\n70\\n75\\n80\\n85\\n90\\nImageNet Top1 Accuracy\\n\\n[%]\\nBiT\\nViT-B/32\\nViT-B/16\\nViT-L/32\\nViT-L/16\\nViT-H/14\\nFigure 3:\\nTransfer to ImageNet.\\n\\n\\nWhile\\nlarge ViT models perform worse than BiT\\nResNets (shaded area) when pre-trained on\\nsmall datasets, they shine when pre-trained on\\nlarger datasets.',\n",
       "  'Similarly, larger ViT variants\\novertake smaller ones as the dataset grows.\\n\\n\\n10 M\\n30 M\\n100 M\\n300 M\\nNumber of JFT pre-training samples\\n30\\n40\\n50\\n60\\n70\\nLinear 5-shot ImageNet Top1\\n\\n[%]\\nViT-L/16\\nViT-L/32\\nViT-B/32\\nViT-b/32\\nResNet50x1 (BiT)\\nResNet152x2 (BiT)\\nFigure 4: Linear few-shot evaluation on Ima-\\ngeNet versus pre-training size.\\n\\nResNets per-\\nform better with smaller pre-training datasets\\nbut plateau sooner than ViT, which performs\\nbetter with larger pre-training.',\n",
       "  'ViT-b is ViT-B\\nwith all hidden dimensions halved.\\n\\n\\n102\\n103\\n90\\n95\\nTransfer accuracy\\n\\n[%]\\nAverage-5\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\n102\\n103\\n75\\n80\\n85\\n90\\nImageNet\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\nTotal pre-training compute [exaFLOPs]\\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\\nResNets, and hybrids.\\n\\nVision Transformers generally outperform ResNets with the same compu-\\ntational budget.',\n",
       "  'Hybrids improve upon pure Transformers for smaller model sizes, but the gap\\nvanishes for larger models.\\n\\n\\nregion spanned by BiT models of different sizes.\\n\\nThe BiT CNNs outperform ViT on ImageNet, but\\nwith the larger datasets, ViT overtakes.\\n\\n\\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\\n300M dataset.\\n\\nWe do not perform additional regularization on the smaller subsets and use the same\\nhyper-parameters for all settings.',\n",
       "  'This way, we assess the intrinsic model properties, and not the\\neffect of regularization.\\n\\nWe do, however, use early-stopping, and report the best validation accuracy\\nachieved during training.\\n\\nTo save compute, we report few-shot linear accuracy instead of full ne-\\ntuning accuracy.\\n\\nFigure 4 contains the results.\\n\\nVision Transformers overt more than ResNets with\\ncomparable computational cost on smaller datasets.',\n",
       "  'For example, ViT-B/32 is slightly faster than\\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets.\\n\\nThe same is true\\nfor ResNet152x2 and ViT-L/16.\\n\\nThis result reinforces the intuition that the convolutional inductive\\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\\ndata is sufcient, even benecial.',\n",
       "  'Overall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\\n(Table 2) seem promising for very low-data transfer.\\n\\nFurther analysis of few-shot properties of ViT\\nis an exciting direction of future work.',\n",
       "  '7\\nPublished as a conference paper at ICLR 2021\\n4.4\\nSCALING STUDY\\nWe perform a controlled scaling study of different models by evaluating transfer performance from\\nJFT-300M. In this setting data size does not bottleneck the models performances, and we assess\\nperformance versus pre-training cost of each model.',\n",
       "  'The model set includes: 7 ResNets, R50x1,\\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\\nbackbone).',\n",
       "  'Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\\nfor details on computational costs).\\n\\nDetailed results per model are provided in Table 6 in the Ap-\\npendix.\\n\\nA few patterns can be observed.\\n\\nFirst, Vision Transformers dominate ResNets on the\\nperformance/compute trade-off.\\n\\nViT uses approximately 2  4 less compute to attain the same\\nperformance (average over 5 datasets).',\n",
       "  'Second, hybrids slightly outperform ViT at small compu-\\ntational budgets, but the difference vanishes for larger models.\\n\\nThis result is somewhat surprising,\\nsince one might expect convolutional local feature processing to assist ViT at any size.\\n\\nThird, Vision\\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\\n\\n\\n4.5\\nINSPECTING VISION TRANSFORMER\\nInput\\nAttention\\nFigure 6: Representative ex-\\namples of attention from the\\noutput token to the input\\nspace.',\n",
       "  'See Appendix D.7 for\\ndetails.\\n\\n\\nTo begin to understand how the Vision Transformer processes im-\\nage data, we analyze its internal representations.\\n\\nThe rst layer of\\nthe Vision Transformer linearly projects the attened patches into a\\nlower-dimensional space (Eq. 1).\\n\\nFigure 7 (left) shows the top prin-\\ncipal components of the the learned embedding lters.\\n\\nThe com-\\nponents resemble plausible basis functions for a low-dimensional\\nrepresentation of the ne structure within each patch.',\n",
       "  'After the projection, a learned position embedding is added to the\\npatch representations.\\n\\nFigure 7 (center) shows that the model learns\\nto encode distance within the image in the similarity of position em-\\nbeddings, i.e. closer patches tend to have more similar position em-\\nbeddings.\\n\\nFurther, the row-column structure appears; patches in the\\nsame row/column have similar embeddings.\\n\\nFinally, a sinusoidal\\nstructure is sometimes apparent for larger grids (Appendix D).',\n",
       "  'That\\nthe position embeddings learn to represent 2D image topology ex-\\nplains why hand-crafted 2D-aware embedding variants do not yield\\nimprovements (Appendix D.4).\\n\\n\\nSelf-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers.\\n\\nWe investigate to what degree\\nthe network makes use of this capability.\\n\\nSpecically, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right).',\n",
       "  'This\\nattention distance is analogous to receptive eld size in CNNs.\\n\\n\\nWe nd that some heads attend to most of the image already in the lowest layers, showing that\\nthe ability to integrate information globally is indeed used by the model.\\n\\nOther attention heads\\nhave consistently small attention distances in the low layers.',\n",
       "  'This highly localized attention is\\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\\nsuggesting that it may serve a similar function as early convolutional layers in CNNs.\\n\\nFurther, the\\nattention distance increases with network depth.\\n\\nGlobally, we nd that the model attends to image\\nregions that are semantically relevant for classication (Figure 6).\\n\\n\\n4.6\\nSELF-SUPERVISION\\nTransformers show impressive performance on NLP tasks.',\n",
       "  'However, much of their success stems\\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\\n8\\nPublished as a conference paper at ICLR 2021\\nRGB embedding filters\\n(first 28 principal components)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch row\\nPosition embedding similarity\\n1\\n1\\nCosine similarity\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)',\n",
       "  'ViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32.\\n\\nCenter:\\n\\nSim-\\nilarity of position embeddings of ViT-L/32.\\n\\nTiles show the cosine similarity between the position\\nembedding of the patch with the indicated row and column and the position embeddings of all other\\npatches.\\n\\nRight:\\n\\nSize of attended area by head and network depth.\\n\\nEach dot shows the mean attention\\ndistance across images for one of 16 heads at one layer.',\n",
       "  'See Appendix D.7 for details.\\n\\n\\net al., 2019;\\n\\nRadford et al., 2018).\\n\\nWe also perform a preliminary exploration on masked patch\\nprediction for self-supervision, mimicking the masked language modeling task used in BERT.\\n\\nWith\\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\\nsignicant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\\n\\n\\nAppendix B.1.2 contains further details.',\n",
       "  'We leave exploration of contrastive pre-training (Chen\\net al., 2020b; He et al., 2020; Bachman et al., 2019; Henaff et al., 2020) to future work.\\n\\n\\n5\\nCONCLUSION\\n\\n\\nWe have explored the direct application of Transformers to image recognition.\\n\\nUnlike prior works\\nusing self-attention in computer vision, we do not introduce image-specic inductive biases into\\nthe architecture apart from the initial patch extraction step.',\n",
       "  'Instead, we interpret an image as a\\nsequence of patches and process it by a standard Transformer encoder as used in NLP.\\n\\nThis simple,\\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\n\\n\\nThus, Vision Transformer matches or exceeds the state of the art on many image classication\\ndatasets, whilst being relatively cheap to pre-train.\\n\\n\\nWhile these initial results are encouraging, many challenges remain.',\n",
       "  'One is to apply ViT to other\\ncomputer vision tasks, such as detection and segmentation.\\n\\nOur results, coupled with those in Carion\\net al. (2020), indicate the promise of this approach.\\n\\nAnother challenge is to continue exploring self-\\nsupervised pre-training methods.\\n\\nOur initial experiments show improvement from self-supervised\\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\\ntraining.',\n",
       "  'Finally, further scaling of ViT would likely lead to improved performance.\\n\\n\\nACKNOWLEDGEMENTS\\nThe work was performed in Berlin, Zurich, and Amsterdam.',\n",
       "  'We thank many colleagues at Google\\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Lucic, Noam\\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chromadb.PersistentClient('arxiv_vdb').get_collection('arxiv').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=\"attention is all you need\",\n",
    "    max_results=5\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = client.results(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./pdfs/', exist_ok=True)\n",
    "os.makedirs('./source/', exist_ok=True)\n",
    "for rs in res:\n",
    "    rs.download_pdf('./pdfs/', filename=rs.entry_id.split('/')[-1]+'.pdf')\n",
    "    # rs.download_source('./source/', filename=rs.entry_id.split('/')[-1]+'.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from typing import Literal, List\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "import chainlit as cl\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 11:13:01 - Created default translation directory at /home/raihan/projects/arxiv-chatbot/.chainlit/translations\n",
      "2024-01-24 11:13:01 - Created default translation file at /home/raihan/projects/arxiv-chatbot/.chainlit/translations/pt-BR.json\n",
      "2024-01-24 11:13:01 - Created default translation file at /home/raihan/projects/arxiv-chatbot/.chainlit/translations/en-US.json\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup #type: ignore\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from grobid_client.grobid_client import GrobidClient #type: ignore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from typing import  Literal\n",
    "import arxiv #type: ignore\n",
    "import chainlit as cl\n",
    "import chromadb\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import List\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"IndexNewArxivPapers\").setLevel(logging.INFO)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def read_tei(tei_file):\n",
    "    with open(tei_file, 'r', encoding='utf-8') as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "        return soup\n",
    "    raise RuntimeError('Cannot generate a soup from the input')\n",
    "\n",
    "def elem_to_text(elem, default=''):\n",
    "    if elem:\n",
    "        return elem.getText()\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "class TEIFile(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.soup = read_tei(filename)\n",
    "        self._text = None\n",
    "        self._title = ''\n",
    "        self._abstract = ''\n",
    "\n",
    "    @property\n",
    "    def doi(self):\n",
    "        idno_elem = self.soup.find('idno', type='DOI')\n",
    "        if not idno_elem:\n",
    "            return ''\n",
    "        else:\n",
    "            return idno_elem.getText()\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        if not self._title:\n",
    "            self._title = self.soup.title.getText() #type: ignore\n",
    "        return self._title\n",
    "\n",
    "    @property\n",
    "    def abstract(self):\n",
    "        if not self._abstract:\n",
    "            abstract = self.soup.abstract.getText(separator=' ', strip=True) #type: ignore\n",
    "            self._abstract = abstract\n",
    "        return self._abstract\n",
    "\n",
    "    @property\n",
    "    def authors(self):\n",
    "        authors_in_header = self.soup.analytic.find_all('author') #type: ignore\n",
    "\n",
    "        result = []\n",
    "        for author in authors_in_header:\n",
    "            persname = author.persname\n",
    "            if not persname:\n",
    "                continue\n",
    "            firstname = elem_to_text(persname.find(\"forename\", type=\"first\")).strip()\n",
    "            middlename = elem_to_text(persname.find(\"forename\", type=\"middle\")).strip()\n",
    "            surname = elem_to_text(persname.surname).strip()\n",
    "            if middlename == '':\n",
    "                full_name = f\"{firstname} {surname}\".strip()\n",
    "            else:\n",
    "                full_name = f\"{firstname} {middlename} {surname}\".strip()\n",
    "            result.append(full_name)\n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        if not self._text:\n",
    "            divs_text = []\n",
    "            for div in self.soup.body.find_all(\"div\"): #type: ignore\n",
    "                # div is neither an appendix nor references, just plain text.\n",
    "                # if not div.get(\"type\"):\n",
    "                #     text = div.getText(separator=': ', strip=True).replace(\"\\n\", \"\")\n",
    "                    \n",
    "                #     divs_text.append(text)\n",
    "                print(div)\n",
    "            plain_text = \"\\n\\n\".join(divs_text)\n",
    "            self._text = plain_text\n",
    "        return self._text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    }
   ],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "import os\n",
    "\n",
    "client = GrobidClient(config_path=\"./grobid_client_python/config.json\")\n",
    "\n",
    "os.makedirs(\"./output/\", exist_ok=True)\n",
    "client.process(\n",
    "    \"processFulltextDocument\",\n",
    "    input_path = \"./pdfs/\",\n",
    "    output=\"./output/\",\n",
    "    force=True,\n",
    "    consolidate_citations=True,\n",
    "    include_raw_citations=True,\n",
    "    tei_coordinates=True,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import grobid_tei_xml\n",
    "for file in os.listdir('./output/'):\n",
    "    if file.endswith('.tei.xml'):\n",
    "        xml_path = os.path.join('./output/', file)\n",
    "        with open(xml_path, 'r') as xml_file:\n",
    "            doc = grobid_tei_xml.parse_document_xml(xml_file.read())\n",
    "                \n",
    "        with open('./output/'+file+'.json', 'w') as json_file:\n",
    "            json_file.write(json.dumps(doc.to_dict(), indent=2))\n",
    "# xml_path = \"./output/1912.11959v2.grobid.tei.xml\"\n",
    "\n",
    "# with open(xml_path, 'r') as xml_file:\n",
    "#     doc = grobid_tei_xml.parse_document_xml(xml_file.read())\n",
    "\n",
    "# with open('./output/2107.08000v1.grobid.tei.json', 'w') as json_file:\n",
    "#     json_file.write(json.dumps(doc.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction The previous state-of-the-art sequence model, the recurrent neural network, has been largely supplanted by the Transformer model  [Vaswani et al., 2017] , which is primarily built atop a self-attention mechanism. Given a task to train upon, the self-attention mechanism focuses on one token per attention head within the entire sequence at each time-step; the key to the self-attention mechanism's success is the mechanism's ability to learn which token within the entire sequence to focus on in order to achieve the best results. The self-attention mechanism has proven successful on a variety of natural language processing tasks, but has not achieved ubiquitous success. The authors of  [Kaiser and Bengio, 2016]  pointed out that an attention mechanism would likely struggle to solve a task which required a model to focus on multiple tokens at a given time-step. Further, the authors of  [Kaiser and Sutskever, 2015]  recommended that an attention mechanism could be replaced by active-memory to alleviate these concerns. Unlike attention, active-memory allows a model to access and change any and all elements of its memory at each timestep. The active-memory mechanism can access more than one element at each time step. In  [Kaiser and Bengio, 2016] , Figure  1 : The active memory mechanism. In this case, the activememory is implemented in a unidirectional manner, with a kernel size 3. the authors used an active-memory system to translate English to French, and was capable of outperforming an RNN model, both with and without an attention mechanism. Motivated by the success of attention mechanism  [Vaswani et al., 2017]  and active-memory  [Kaiser and Bengio, 2016] , in this paper we investigate the Transformer's self-attention mechanism in comparison to a variety of active-memory mechanisms. We experiment on two types of tasks: the language modeling task and a set of algorithmic tasks. For the language modelling task, the self-attention mechanism out-performs an active-memory mechanism used alone by a slim margin. However, a combination of both selfattention and active-memory reliably outperform both mechanisms used alone. We also evaluated the self-attention mechanism and various active-memory mechanisms on a variety of algorithmic tasks, which can also be expressed as a sequence modeling task. Across most of the algorithmic tasks tested, the activememory mechanisms achieve equal, or superior, results to a traditional self-attention mechanism. This would appear to vindicate the hypothesis stated by  [Kaiser and Bengio, 2016] , suggesting that the nature of the attention mechanism does indeed limit the effectiveness and accuracy of the model. Finally, we note that, for several algorithmic tasks, the mere addition of the self-attention mechanism hinders results; the active-memory mechanism alone outperforms a combination of the two separate mechanisms. This raises an unsolved problem; it would appear that, for deep learning sequence models, there is still no unambiguous model that can optimally solve all possible problems. \n",
      " Related Work The Transformer model  [Vaswani et al., 2017]  is built with two separate modules, the self-attention mechanism and the feedforward mechanism, which are stacked atop each other for multiple layers. The feedforward mechanism is an intrasequence analysis, where the output for each token in the sequence is dependent only on the token at the same timestep, and independent of all other time-steps. On the other hand, the self-attention mechanism is an inter-sequence analysis, where the output for each time-step is dependent upon the entire sequence. The self-attention mechanism is defined, mathematically, as: Q t , K t , V t = x t y t = concat(head 1,t , head 2,t , . . . , head n,t )W o head i = Attention(Q t W Q i , K t W K i , V t W V i ) Attention(Q, K, V ) = sof tmax( Q t K T t  d k )V t W o R d k * k,d , W K,Q,V i R d,d k * k The feed-forward module is defined as: y t = W l,2 (max(W l,1 x t + b l,1 , 0)) + b l,2 W l,2 R d,d * 4 , W l,1 R d * 4,d The Transformer model, and its variants  [Dai et al., 2019] , have achieved remarkable results across a variety of natural language processing tasks since its inception  [Zhenzhong et al., 2019 ] [Delvin et al., 2018 ] [Yang et al., 2019b]  , and are currently investigated heavily by both academia and industry. The Neural GPU  [Freivalds and Liepins, 2017 ] [Kaiser and Bengio, 2016 ] [Kaiser and Sutskever, 2015] , which introduced an active-memory model, achieved impressive algorithmic results in  [Kaiser and Sutskever, 2015] , and also achieved impressive machine translation results in  [Kaiser and Bengio, 2016] . A Neural GPU contains a CGRU (Convolution Gated Recurrent Unit) module which is iterated repeatedly. This allows the entire sequence to be analyzed in a parallelizable and computationally efficient manner. The CGRU module is defined as: u = sigmoid(U 1 * x + B 1 ) r = sigmoid(U 2 * x + B 2 ) y = u  x + (1 -u)  tanh(U 0 * (r  x) + B 0 )) where U * x refers to applying a convolutional operator over x, using U as a trainable kernel bank and B is a trainable bias vector. The CGRU has, since its introduction, been used in other models  [Resende et al., 2016] . Convolutional operators are traditionally used for image processing  [Alom et al., 2018] , and have also been used in relation to sequential analysis in previous papers  [Yang et al., 2019a ] [Wu et al., 2019 ] [Gehring et al., 2017 ] [Dauphin et al., 2016] . To the best of our knowledge they have not been used explicitly to replace, or augment, the self-attention mechanism. The first sequence-to-sequence model, based on convolutional operators, was, to the best of our knowledge, introduced in  [Gehring et al., 2017] , which replaced the thentraditional LSTM block with a series of convolutions and gated convolutional networks  [13] , and outperformed RNNbased models in terms of both speed and accuracy. However, the model introduced in  [Gehring et al., 2017]  was followed shortly afterwards by the Transformer model, which outperformed the convolutional-based model. The convolutional self-attention network  [Yang et al., 2019a]  was recently introduced, and bares a passing similarity to the traditional convolutional operator described in this paper. The layer of the convolutional self-attention is similar to a traditional self-attention mechanism, but where the key and value tensors are calculated as: K h = (K h i-M/2 , ..., K h i , ..., K h i+M/2 ) V h = (V h i-M/2 , ..., V h i , ..., V h i+M/2 ) From this point, the convolutional self-attention mechanism acts in an identical manner to the traditional selfattention mechanism. This is in direct comparison to the convolutional operator described in this paper, which explicitly avoids the use of the self-attention mechanism and relies entirely on a purely convolutional operator. \n",
      " Approach In this paper, we investigate whether various active-memory mechanisms could replace self-attention in a Transformer. We also evaluate the combination of self-attention and activememory mechanisms for language modelling tasks. All the active-memory mechanisms introduced in this paper were inspired by the Neural GPU, as introduced in  [Kaiser and Sutskever, 2015] . The key allure of the Neural GPU is that the inputs of each time-step can be analyzed and altered, and we were inspired to apply a similar form of sequence modelling alongside a self-attention mechanism. We describe various convolution-based active-memory mechanisms in this section. \n",
      " The Convolutional Operators The Traditional Convolutional Operator The first, and most simple, active-memory mechanism is the simple convolutional operator. The traditional convolutional operator was formally defined in  [Bai et al., 2019] . If the task requires the sequence to be analyzed in a unidirectional manner, such as the case for language modelling, then a zerosvector of size k -1 is concatenated to the left of the input tensor so that, for the nth output token, the model only has access to the first n input tokens. This feature is crucial to avoid allowing the model 'seeing' forward through the sequence and having access to information that the model, in practice, would not yet have. This has an identical function to the masking operation of the self-attention mechanism. If the task can be analyzed in a bidirectional manner, then the model uses a convolutional filter using the SAMEpadding, which allows for the vector to maintain its shape throughout the convolutional operator. However, when the convolutional operator is performed in this manner, the token at time-step t is dependent on the input tokens h [t-k/2,t+k/2] , where k is the kernel size. The primary flaw of a convolutional operator, in comparison to a self-attention mechanism, is that, given n layers where each kernel has a k kernel size, each token can only see k * n -n + 1 or k/2 * n -n + 1 time-steps across for unidirectional and bidirectional tasks respectively. For example, in our experiments on language modeling (Section 4), the kernel size was set to 20 and was iterated over 8 layers. Therefore, at each time-step t, the final output is capable of analyzing the input from 153 previous time-steps, well above the average sequence-size (90 tokens) in the dataset. The self-attention mechanism, in comparison, can see across a theoretically infinite context size, even using only a single layer. Given this information, the self-attention mechanism is capable of handling theoretically greater long-term dependencies than the active-memory mechanism. However, in practice, the ability of an active-memory mechanism to access and change its entire memory could overcome this limitation. The convolutional operator is assisted further by the fact that the convolutional operator's complexity grows linearly with the sequence size, while the self-attention mechanism's complexity grows quadratically. Numerous papers have noted that, while Transformers are parallelizable and capable of capturing long-range dependencies, the Transformer network suffers from the inability of model tokens in a recurrent manner  [Wang et al., 2019 ] [Hao et al., 2019] . This is in direct comparison to traditional RNN models, which can capture long-range dependencies, but can struggle to capture long-range dependencies. The use of active-memory, in theory, would accomplish this task, given that the output at time-step t h t is dependent of the inputs x  [t-k,t]  where k is the kernel size. Therefore, this operation can, in theory, model recurrence. We did not explicitly test whether this does model recurrence in practice, but will focus on this in future work. The convolutional operator is followed by the ReLU activation function. \n",
      " The Persistent-Convolutional Operator The Persistent-Convolutional operator is similar to the traditional convolutional operator described above, except that the zeros vector is replaced by the a trainable vector of identical shape to the zeros vector. This allows the operator to, identical to the traditional convolutional operator, maintain an identical shape across the convolution. To keep parameterization to a minimum, the same persistent vector is used across all convolution operators in the entire model. The persistentconvolutional operator is defined as: p  W kernel size-1,hiddensize x = [p, x], y = W * x + b where [.,.] denotes the concatenation function and p is the trainable persistent vector. Persistent vectors have been used previously in language modelling tasks  [Sukhbaatar et al., 2019] , but never as an augmentation for convolutional operators, as far as we know. If the model is to be analyzed in a bidirectional manner, rather then a unidirectional manner, then the persistentconvolutional operator can be redefined as: The use of a persistent vector allows for the model to have a permanent memory that, given the fact the vector is trainable, can be expressed in an optimal manner for the model. This is the equivalent of a permanent memory for the deep learning model. p 1 , p 2  W (kernel size-1)//2,hiddensize x = [p 1 , x, p 2 ] \n",
      " The Highway-Convolutional Operator The Highway-Convolutional operator is based on the highway network architecture  [5] , which can be defined as: a = U 0 * x + B 0 b = sigmoid(U 1 * x + B 1 ) y = a  b + x  (1 -b) The key allure of the highway network, as described in  [Srivastava et al., 2015] , is the fact that a highway network can be trained for a large number of layers, even hundreds of layers, because information can pass, unimpeded, across each layer. The authors of  [Srivastava et al., 2015]  described these paths as 'information highways'. The use of these 'information highways' allows information to pass through the self-attention mechanism in an equally efficient manner. In this paper, we use the hard-sigmoid function  [Kaiser and Bengio, 2016]  to stabilize gradients, which is defined as: y = max(0, min(1, 1.2 * sigmoid(x) -0.1)) \n",
      " Self-Attention + Convolutional Operators The operator calculates the results of the self-attention mechanism and results of the convolutional operator independently, and then adds them together to produce the final output of the operator. This operator would allow the model to analyze the input using both the self-attention mechanism and active-memory mechanism and decide which features from both mechanisms would be most optimal. This approach has the obvious advantage of being able to take the 'best of both worlds', where the optimal features that can only be detected  The loss-per-token of the self-attention mechanism and the active-memory mechanisms on the WT3 dataset, and the difference of loss between the self-attention and the active-memory mechanisms. The lower the loss, the better the model performed. With the exception of the CGRU, all purely active-memory operators achieve a test loss less then 1.2% higher then the self-attention mechanism. The optimal models combined the self-attention mechanism and an active-memory mechanism, and achieved a lower test loss than the self-attention mechanism and active-memory mechanisms alone. by the self-attention mechanism, and the optimal features that can only be detected by the convolutional operator, are both available to the model. The architecture of a single layer of the \"self-attention + convolution\" operator is shown in Figure  2 . This architecture, without the convolutional operator, is a simple Transformer layer. The output of the convolutional operator is added, element-wise, to the output of the self-attention mechanism. This allows, hypothetically, for the best-of-bothworlds, where the model has access to the self-attention mechanism and the active-memory mechanism. Similarly, we also add the self-attention mechanism to the persistent-convolutional operator and the highwayconvolutional operator, respectively. \n",
      " Experiments To evaluate the effectiveness of the various convolution-based active-memory mechanisms, we used two separate experiments; a language modelling task that is traditionally associated with attention-based mechanisms  [Shoeybi et al., 2019] , and algorithmic tasks that are associated with active-memory models  [Kaiser and Sutskever, 2015] . The active-memory mechanisms are experimented both independently and alongside a self-attention mechanism. \n",
      " Language Modelling Experimental Setup The first task that the operators were tested with was a unidirectional language modelling task; the WikiText-3 (WT3) dataset  [Merity et al., 2016] , tokenized using BPEtokenization  [Sennrich et al., 2016] . The WikiText-3 dataset was sourced entirely from Wikipedia articles, contains over 3.6 millions lines of text, and is split into a training dataset, valid dataset and test dataset. The train dataset contains 103M tokens, while the valid and test dataset contain 250K tokens each. The models used were all 8-layer models, with a hidden size of 256 and a filter size of 1024, a vocab size of 32,000, (x, y) 1 0 1 1 + 0 0 1 1 (x + y) 0 0 0 0 0 1 1 1 0 Table 2: The binary addition task. Given two numbers (in this case, the two numbers are 11 and 3), the final output is the binary version of the addition of the two input numbers (in this case, 14). kernel size of 20 and a dropout rate of 0.9. No further regularization was used. The optimizer was the Adam Optimizer  [Kingma and Ba, 2014]  and a warmup-learning rate was used, as specified in  [Vaswani et al., 2017] . All models were implemented using Tensorflow, version 1.07, on a V100 GPU card. Notable preprocessing was used for analyzing the WikiText-3 dataset; every character was explicitly denoted as lower-case, each hyphenwas replaced by @-@ and punctuation marks, such as fullstops and commas, were seperated by white-space. This was done to discourage the BPE to tokenize sets of characters that included punctuation marks, forcing the model to tokenize sets of characters that were only letters, therefore tokenizing a greater set of words. \n",
      " Experimental Results With the exception of the CGRU operator, all active-memory mechanisms, when combined with the self-attention mechanism, outperformed the self-attention mechanism alone, achieving a lower loss-per-token. This would appear to vindicate the proposition of both this paper and [2], suggesting that, indeed, active-memory mechanisms and self-attention are comparable. However, no model that purely used an active-memory mechanism outperformed the self-attention mechanism for language modelling. We note that, if the dropout rate was decreased to 0.7, all operators, with the exception of CGRU, all models achieved superior results to the self-attention mechanism at a the same dropout rate. However, these models did not achieve superior results to the self-attention model with a dropout-rate of 0.9. This would imply that self-attention mechanisms are more sensitive to dropout rates compared to active-memory mechanisms. Further, each operator, except for the CGRU operator, benefited from combining it with self-attention, allowing both operators to operate independently and concurrently. The model with the lowest loss-per-token had a self-attention mechanism and a highway-convolutional operator. It is further worth noting that the highway-convolutional operator outperformed both other convolutional operators, both with and without the addition of the self-attention mechanism. \n",
      " Algorithmic Tasks Experimental Setup The second experiment for evaluating the active-memory mechanisms was on various algorithmic tasks:  Reverse: Given an array X of size L, the model is trained to return the array Y, where Y[0] = X[-1]. In order to effectively perform this task, the model must be capable of analyzing the start of the input vector at the very end, and vice-versa. (x, y) 1 0 1 0 1  0 1 1 0 0 (x  y) 0 0 0 1 1 1 1 1 1 0 0  Sort: Given an array of randomly order integers, the model is trained to return an array that accurately order the input integers. The entire vector must be remembered and analyzed at each time-step.  Addition: Given two binary numbers, the model is trained to return an array that represents the addition in the form of a third binary number. An example of the addition task is shown in Table  2 .  Multiply: Multiplies two binary numbers, as shown in Table  3 .  Not: If the input is 1, then not returns 0. Else, the not function returns 1. The output relies only on the input at the current time-step.  Remember: Given a series of random numbers of sequence size N , followed by a sequence of zeros of identical size, the model is trained to output a series of zeros of size N , followed by the random numbers. In order for this task to be performed, the model must be able to remember tokens over an increasingly long sequence. All data for the algorithmic tasks were generated in an online manner. For three of the tasks, Sort, Addition and Multiply, the model must focus on multiple tokens at every timestep. In comparison, the Reverse task, the Not task and the Remember task only require the model to focus on a single token at every time-step. The model that was used for algorithmic tasks contains 4 layers, with a hidden size of 128 units, a filter size of 512 and a kernel size of 20. Each model was trained for a maximum of 100 epochs, where each epoch contains 100 iterations. At the end of each epoch, the model was exposed to an online batch, containing 32 test cases. If the model achieved an accuracy of 100% on the online test batch, the sequence-size of the data is increased, therefore increasing its complexity. For the Reverse, Sort, Not and Remember task, when the model achieved a 100% accuracy, the sequence was increased by 1. For the Addition and Multiply task, the sequence was increased by 2. The model was initially trained only for sequences that are 5 tokens long and was not introduced to a larger sequence until the model was capable of achieving 100% accuracy on this sequence-size. We found that this form of curriculum learning was essential: if a model was initially trained on a sequence of several dozen tokens, each operator was incapable of achieving a reasonable accuracy. The vocabulary size was different for each task. The Reverse task had a vocabulary size of 100, while the Sort task and the Remember task had a vocabulary size of 20. We noted that whenever the vocab size was increased the model would achieve less accurate results. Because all tokens in the Addition, Multiply and Not tasks are either 0, 1, or the separator, the vocabulary size is set to 3. \n",
      " Experimental Results Each model was tested for each task, and the highest sequence that the model could achieve within 100 epochs was recorded. Each experiment was performed three times, and the average sequence size is presented in Table  4 . For example, the selfattention mechanism managed to achieve a 100% accuracy for a sequence of 41 tokens for the Reverse task, but could not achieve a 100% accuracy for both the Sort task and the Addition task for a sequence size of 20 (the Sort task achieved a maximum sequence size of 14, while the Addition task achieved a maximum size of 7). Of the six algorithmic tasks tested, active-memory mechanisms were used, either solely or in combination with the self-attention mechanism, in the best-performing model of five of these tasks. For example, the self-attention mechanism achieved an average sequence size of 41.0 for the Reverse Task and 14.0 for the Sort Task, which are lower than those achieved by the \"self-attention + persistent-convolution\" mechanism  (43.7 and 23.3, respectively) . Furthermore, for the Addition and Multiply Tasks, the active-memory mechanisms across the board outperformed both the self-attention mechanism and the combination of the self-attention mechanism and the active-memory mechanism. For example, the traditional convolution operator, for the Addition Task, outperformed the self-attention mechanism and the \"self-attention + convolutional\" mechanism by 34.0 and 4.7 respectively. The results show that the active-memory mechanisms achieve equal, or superior, results to a traditional self-attention mechanism. Self-attention, used alone, only performed optimally on the Remember task, and equally well on the Not task. Interestingly, across all models for the Addition and Multiply tasks, the self-attention mechanism reliably led to poor results; not only does the self-attention mechanism, alone, achieve the poorest results, but the combination of the self-attention mechanism and any active-memory system performed worse then the active-memory system alone. This is in direct contrast to the Sort task and the Reverse task, where the combination of self-attention mechanism and the active-memory achieve the best results. The self-attention mechanism would, in theory, outperform active-memory mechanisms for the Remember task. This is because, in order to adequately perform the Remember task, the model must be capable of calculating an output based on long-range dependencies, which active-memory cannot match at a large enough sequence length. Other tasks do require a long-range dependency in order to operate well at large sequence sizes, but are dependent on the model performing other tasks as well. For example, the addition task requires to model long range-dependencies and perform binary addition. The self-attention mechanism, although it can learn these long-range dependencies, cannot access all necessary tokens at a given time to adequately perform binary addition. This is vindicated by the experimental results. In Table  4 , the self-attention mechanism achieved the highest results on the Remember task. This would suggest that, if the algorithmic task only requires a long-range dependency, then the selfattention mechanism will outperform active-memory mech- The average sequence length that each operator was capable of gaining 100% accuracy within 100 epochs over 3 runs. The higher the sequence size, the better the model learned. For the Reverse and Sort tasks, the combination of self-attention mechanism and persistent-convolution achieved the best results. For the Sort, Addition, and Multiply tasks, the self-attention mechanism was beaten by the active-memory mechanisms. For the Addition and Multiply tasks, the mere use of a self-attention mechanism alongside an active-memory mechanism actively decreased results. The highest possible sequence that can be learned over these epochs is 104 in the Not task. The self-attention mechanism achieved the best result only for the Remember task. anisms when used alone. In comparison, the self-attention mechanism is incapable of matching the results of activememory for all other tasks. These findings appear to vindicate the statement made by  Kaiser et. al [2] ; whenever the sequential task requires the model to focus on multiple tokens at every time-step, using an attention mechanism will lead to extremely poor results, especially in comparison to active-memory models. It is worth noting that, for each of the active-memory mechanisms operating alone, none of the three achieved a 100% accuracy for any sequence over a size of 37 for the Remember task. This is because, given the kernel size of 20 and 4 layers, the model is only capable of seeing 37 time-steps across. Therefore, the model cannot see 37 time-steps across and, therefore, cannot perform the Remember task at this sequence size or any larger sequence size. This displays the importance of utilizing both a self-attention mechanism, which can be utilized for analyzing long-range dependencies, and an active-memory mechanism, which can extract features that the self-attention mechanism cannot. \n",
      " Discussion of Results The experiments above suggest that, across most tasks, a combination of a self-attention mechanism and an activememory mechanism, at worst, perform comparably to a purely attention-based model, and at best surpass an attention model, with the exception of the Remember task. However, for some algorithmic tasks, we note that the mere inclusion of a self-attention mechanism actively hinders performance. Models that combine both the attention mechanism and active-memory mechanisms outperformed both attentiononly and active-memory-only models for language modelling. This suggests that, for language modelling tasks, both active-memory mechanisms and attention mechanisms are capable of extracting features that the other mechanism is not capable of extracting, and that both mechanisms operate optimally when used alongside each other. The findings are further abstracted by studying the effect of various algorithmic tasks; in cases where only a single token needs to be focused on, the self-attention mechanism matches the most ardent active-memory, while active-memory mecha-nisms radically outperform self-attention for other tasks. This would imply that various time-dependencies that cannot be analyzed by a self-attention mechanism can be analyzed by active-memory. It is worth noting that, for the Not function, all models learn optimally. This is likely due to the fact that the output of each time-step depends only on the input at this time-step, and each model can analyze this dependency equally efficiently. Also, based on the results of the Remember task, the self-attention mechanism can attain greater long-range dependency in comparison to the active-memory mechanisms. Finally, we note that, for the Remember function, both mechanisms, when used alone, outperform the two mechanisms used together. For every other task, a combination of the self-attention and active-memory would improve upon at least one of the mechanisms when used alone. We are unsure exactly what has led to this result. This will require further investigation in the future. \n",
      " Conclusion In this paper we investigate the Transformer's self-attention mechanism in comparison to a variety of active-memory mechanisms. We experiment on two types of tasks: the language modeling task and the algorithmic task. Our results show that the self-attention mechanism can be improved by an active-memory mechanism alone or by a combination of the two. Our results have implications for wider sequence modeling tasks, which are currently dominated by self-attention based models. Our code and models used in experiments are available at: https://github.com/Anon-111/Active-Memory. In the future, we will further explore the use of activememory for sequence-to-sequence tasks, such as machine translation. We will also analyze the empirical differences between the studied algorithmic tasks, and investigate why the self-attention mechanism may assist one task but harm another. Figure 2 : 2 Figure 2: The Self-Attention + Convolutional Operator Transformer. \n",
      " \n",
      " Table 1 : 1 \n",
      " Table 3 : 3 The binary multiplication task. The above example contains two input numbers, 12 and 21, and the output number 252. \n",
      " Table 4 : 4 Model Reverse Sort Addition Multiply Not Remember CGRU 7.7 5.7 16.3 9.0 104.0 9.0 Self-Attention 41.0 14.0 7.0 7.0 104.0 57.0 Convolution 17.7 20.7 41.0 17.0 104.0 36.0 Persistent-Convolution 25.0 20.3 38.3 16.3 104.0 35.0 Highway-Convolution 19.7 16.7 35.7 13.7 104.0 33.0 Self-Attention + Convolution 41.0 23.3 36.3 12.3 104.0 26.0 Self-Attention + Persistent-Convolution 43.7 23.3 35.0 12.3 104.0 27.0 Self-Attention + Highway-Convolution 41.0 20.0 34.3 11.7 104.0 24.7\n"
     ]
    }
   ],
   "source": [
    "print(doc.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/2306.01926v1.grobid.tei.xml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"lxml-xml\")\n",
    "chunks = []\n",
    "content = \"\"\n",
    "for div in soup.body.find_all([\"div\", \"figure\"]):\n",
    "    if div.name == \"figure\":\n",
    "        head = div.find(\"head\")\n",
    "        fig_desc = div.find(\"figDesc\")\n",
    "        # if ('Figure' not in head.getText()) and ('Table' not in head.getText()):\n",
    "        content += head.getText().replace(\" \", \"\") + fig_desc.getText() + \"\\n\"\n",
    "    elif div.name == \"div\":\n",
    "        section_num = div.head.get(\"n\")\n",
    "        section_title = div.head.getText(separator=' ', strip=True)\n",
    "        if section_num is None:\n",
    "            section_num = \"\"\n",
    "        content += section_num + \" \" + div.getText(separator=': ', strip=True) + \"\\n\"\n",
    "    content += \"=======================================\\n\"\n",
    "    # for head in div.find_all(\"head\"):\n",
    "    #     section_num = head.get(\"n\")\n",
    "    #     section_title = head.getText(separator=' ', strip=True)\n",
    "    #     print(section_num, section_title)\n",
    "    # content = \"\"\n",
    "    \n",
    "    # for p in div.find_all(\"p\"):\n",
    "    #     content += p.getText(separator=' ', strip=True)\n",
    "    # print(content)\n",
    "    # print(\"=================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip files in /source/ in a folder with the same name as the pdf\n",
    "\n",
    "import tarfile\n",
    "\n",
    "for file in os.listdir('./source/'):\n",
    "    if file.endswith('.tar.gz'):\n",
    "        tar = tarfile.open('./source/'+file, \"r:gz\")\n",
    "        tar.extractall('./source/'+(file.replace('.tar.gz', '')))\n",
    "        tar.close()\n",
    "        os.remove('./source/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Included file #1.tex not found.\n",
      "Warning: Included file tex/fig-glam.tex not found.\n",
      "Warning: Included file tex/fig-styles.tex not found.\n",
      "Warning: Included file tex/fig-local-channel.tex not found.\n",
      "Warning: Included file tex/fig-local-spatial.tex not found.\n",
      "Warning: Included file tex/fig-global-channel.tex not found.\n",
      "Warning: Included file tex/fig-global-spatial.tex not found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from TexSoup import TexSoup\n",
    "\n",
    "def read_and_parse_tex_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return parse_tex_content(content, os.path.dirname(file_path))\n",
    "\n",
    "def parse_tex_content(content, base_dir):\n",
    "    soup = TexSoup(content)\n",
    "    full_content = str(soup)\n",
    "\n",
    "    # Find all \\input and \\include commands and process them\n",
    "    for command in soup.find_all('input') + soup.find_all('include'):\n",
    "        filename = str(command.args[0]).replace('{', '').replace('}', '')\n",
    "        if not filename.endswith('.tex'):\n",
    "            filename += '.tex'\n",
    "        \n",
    "        included_file_path = os.path.join(base_dir, filename)\n",
    "        if os.path.exists(included_file_path):\n",
    "            included_content = read_and_parse_tex_file(included_file_path)\n",
    "            full_content = full_content.replace(str(command), included_content)\n",
    "        else:\n",
    "            print(f\"Warning: Included file {filename} not found.\")\n",
    "\n",
    "    return full_content\n",
    "\n",
    "main_file_path = 'source/2107.08000v1/main.tex'\n",
    "complete_document = read_and_parse_tex_file(main_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TexSoup import TexSoup\n",
    "\n",
    "soup = TexSoup(complete_document, tolerance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\\section{Introduction}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\documentclass[10pt,twocolumn,letterpaper]{article}\n",
      "\n",
      "\\usepackage{iccv}\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "%------------------------------------------------------------------------------\n",
      "% hack for CVPR/ICCV style\n",
      "\\makeatletter\n",
      "\\@namedef{ver@everyshi.sty}{}\n",
      "\\makeatother\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "% main packages\n",
      "\\usepackage[dvipsnames,svgnames,x11names]{xcolor}\n",
      "\\usepackage{tikz}\n",
      "\\usetikzlibrary{arrows.meta,shapes,calc,matrix,fit,positioning,backgrounds,decorations.markings,fadings}\n",
      "\\usepackage{pgfplots}\n",
      "\\usepackage{pgfplotstable}\n",
      "\\pgfplotsset{compat=1.9}\n",
      "\\usepackage{xstring}\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "% externalization: requires defining \\finalcopy first\n",
      "\n",
      "\\usepgfplotslibrary{external}\n",
      "% \\tikzexternalize[prefix=fig/extern/]\n",
      "\\newcommand{\\extfig}[2]{\\tikzsetnextfilename{#1}{#2}}\n",
      "\\newcommand{\\noextfig}[1]{\\tikzset{external/export next={false}}{#1}}\n",
      "\\newcommand{\\extdata}[1]{\\input{#1}}\n",
      "\\IfBeginWith*{\\jobname}{fig/extern/}{\\finalcopy}{}\n",
      "\n",
      "%-----------------------------------------------------------------------------\n",
      "% tikz styles\n",
      "\n",
      "\\tikzstyle{every picture}+=[\n",
      "\tremember picture,\n",
      "\tevery text node part/.style={align=center},\n",
      "\tevery matrix/.append style={ampersand replacement=\\&},\n",
      "]\n",
      "\\tikzstyle{tight} = [inner sep=0pt,outer sep=0pt]\n",
      "\\tikzstyle{node}  = [draw,circle,tight,minimum size=12pt,anchor=center]\n",
      "\\tikzstyle{op}    = [draw,circle,tight]\n",
      "\\tikzstyle{dot}   = [fill,draw,circle,inner sep=1pt,outer sep=0]\n",
      "\\tikzstyle{pt}    = [fill,draw,circle,inner sep=1.5pt,outer sep=.2pt]\n",
      "\\tikzstyle{box}   = [draw,thick,rectangle,inner sep=3pt]\n",
      "\\tikzstyle{high}  = [black!60]\n",
      "\\tikzstyle{group} = [high,box,opacity=.5]\n",
      "\\tikzstyle{dim1}  = [fill opacity=.3,text opacity=1]\n",
      "\\tikzstyle{dim2}  = [fill opacity=.5,text opacity=1]\n",
      "\\tikzstyle{dim3}  = [fill opacity=.7,text opacity=1]\n",
      "\\tikzstyle{rectc} = [tight,transform shape]\n",
      "\\tikzstyle{rect}  = [rectc,anchor=south west]\n",
      "\n",
      "%-----------------------------------------------------------------------------\n",
      "% framed figures\n",
      "\n",
      "\\newcommand{\\framed}[3][1]{\\extfig{#2}{\\tikz{\n",
      "\t\\node[tight](a){\\fig[#1]{#3}};\n",
      "\t\\node[tight,draw=gray,fit=(a)]{};\n",
      "}}}\n",
      "\n",
      "%-----------------------------------------------------------------------------\n",
      "% pgfplots general options\n",
      "\n",
      "\\newcommand{\\leg}[1]{\\addlegendentry{#1}}\n",
      "\n",
      "\\tikzset{every mark/.append style={solid}}\n",
      "\\pgfplotsset{%smooth,\n",
      "\tgrid=both, width=\\columnwidth, try min ticks=5,\n",
      "\tevery axis/.append style={font=\\small},\n",
      "\tevery axis plot/.append style={thick,mark=none,mark size=1.8,tension=0.18},\n",
      "\tlegend cell align=left, legend style={fill opacity=0.8},\n",
      "\txticklabel={\\pgfmathprintnumber[assume math mode=true]{\\tick}},\n",
      "\tyticklabel={\\pgfmathprintnumber[assume math mode=true]{\\tick}},\n",
      "\tnodes near coords math/.style={\n",
      "\t\tnodes near coords={\\pgfmathprintnumber[assume math mode=true]{\\pgfplotspointmeta}},\n",
      "\t},\n",
      "}\n",
      "\n",
      "\\pgfplotsset{\n",
      "\tdash/.style={mark=o,dashed,opacity=0.6},\n",
      "\tdott/.style={mark=o,dotted,opacity=0.6},\n",
      "\tnolim/.style={enlargelimits=false},\n",
      "\tplain/.style={every axis plot/.append style={},nolim,grid=none},\n",
      "}\n",
      "\\newcommand{\\kilo}[1]{\\thisrow{#1}/1000}\n",
      "\n",
      "%--------------------------------------------------------------------\n",
      "\n",
      "% layers\n",
      "\\pgfdeclarelayer{bg4}\n",
      "\\pgfdeclarelayer{bg3}\n",
      "\\pgfdeclarelayer{bg2}\n",
      "\\pgfdeclarelayer{bg1}\n",
      "\\pgfdeclarelayer{fg1}\n",
      "\\pgfdeclarelayer{fg2}\n",
      "\\pgfdeclarelayer{fg3}\n",
      "\\pgfdeclarelayer{fg4}\n",
      "\\pgfsetlayers{bg4,bg3,bg2,bg1,main,fg1,fg2,fg3,fg4}\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "% 3d drawing\n",
      "\n",
      "\\pgfkeys{/tikz/.cd, aspect/.store in=\\aspect, aspect=1}\n",
      "\\pgfkeys{/tikz/.cd, depth/.store in=\\depth, depth=.5}\n",
      "\\pgfkeys{/tikz/.cd, stepx/.store in=\\step, stepx=1}\n",
      "\n",
      "\\tikzstyle{geom} = [line join=bevel,aspect=1,depth=.5,z={(\\depth*\\aspect,\\depth)}]\n",
      "\\tikzstyle{wire} = [geom,draw,thick]\n",
      "\n",
      "% 3d coordinates\n",
      "\\def{\\}{cx}[#1,#2,#3]{#1}\n",
      "\\def{\\}{cy}[#1,#2,#3]{#2}\n",
      "\\def{\\}{cz}[#1,#2,#3]{#3}\n",
      "\\def{\\}{ex}[#1,#2,#3]{#1,0,0}\n",
      "\\def{\\}{ey}[#1,#2,#3]{0,#2,0}\n",
      "\\def{\\}{ez}[#1,#2,#3]{0,0,#3}\n",
      "\n",
      "% lines along x, y, or z\n",
      "\\newcommand{\\zline}[3][]{%\n",
      "\\path[geom,#1] #2 -- +(\\cx[#3],\\cy[#3]);\n",
      "}\n",
      "\\newcommand{\\yline}[3][]{%\n",
      "\\path[geom,#1,shift={#2},xslant=\\aspect]\n",
      "\t(0,0) -- +(\\cx[#3],\\depth*\\cz[#3]);\n",
      "}\n",
      "\\newcommand{\\xline}[3][]{%\n",
      "\\path[geom,#1,shift={#2},yslant=1/\\aspect]\n",
      "\t(0,0) -- +(\\aspect*\\depth*\\cz[#3],\\cy[#3]);\n",
      "}\n",
      "\n",
      "% rectangles / grids along x, y, or z\n",
      "\\newcommand{\\zrect}[3][]{%\n",
      "\\path[geom,#1] #2 rectangle +(\\cx[#3],\\cy[#3]);\n",
      "}\n",
      "\\newcommand{\\yrect}[3][]{%\n",
      "\\path[geom,#1,shift={#2},xslant=\\aspect]\n",
      "\t(0,0) rectangle +(\\cx[#3],\\depth*\\cz[#3]);\n",
      "}\n",
      "\\newcommand{\\xrect}[3][]{%\n",
      "\\path[geom,#1,shift={#2},yslant=1/\\aspect]\n",
      "\t(0,0) rectangle +(\\aspect*\\depth*\\cz[#3],\\cy[#3]);\n",
      "}\n",
      "\\newcommand{\\xgrid}[3][]{%\n",
      "\\path[geom,#1,shift={#2},yslant=1/\\aspect,xstep=\\aspect*\\depth*\\step]\n",
      "\t(0,0) grid +(\\aspect*\\depth*\\cz[#3],\\cy[#3]);\n",
      "}\n",
      "\n",
      "% parallepiped\n",
      "\\newcommand{\\para}[4][]{%\n",
      "\\zrect[#1]{(#3)}{#4}                 % front\n",
      "\\yrect[#1]{($(#3)+(\\ey[#4])$)}{#4}   % top\n",
      "\\xrect[#1]{($(#3)+(\\ex[#4])$)}{#4}   % right\n",
      "\\path[geom]\n",
      "\t(#3) coordinate(#2-southwest)\n",
      "\t($(#3)+(#4)$) coordinate(#2-northeast)\n",
      "\t($(#3)+(\\ey[#4])$) coordinate(#2-northwest)\n",
      "\t($(#3)+(#4)-(\\ey[#4])$) coordinate(#2-southeast)\n",
      "\t($(#3)+.5*(\\ex[#4])$) coordinate(#2-south)\n",
      "\t($(#3)+(#4)-.5*(\\ex[#4])$) coordinate(#2-north)\n",
      "\t($(#3)+.5*(\\ex[#4])+.5*(#4)$) coordinate(#2-center)\n",
      "\t(#2-southwest |- #2-center) coordinate(#2-west)\n",
      "\t(#2-center -| #2-northeast) coordinate(#2-east)\n",
      "\t;\n",
      "}\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "% space before \\paragraph (default 4.05ex)\n",
      "\\makeatletter\n",
      "\\renewcommand\\paragraph{\\@startsection{paragraph}{4}{\\z@}{1ex}{-1em}{\\normalfont\\normalsize\\bfseries}}\n",
      "\\makeatother\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\usepackage{times}\n",
      "\\usepackage{epsfig}\n",
      "\\usepackage{graphicx}\n",
      "\\usepackage{amsmath}\n",
      "\\usepackage{amssymb}\n",
      "\n",
      "\\usepackage{times}\n",
      "\\usepackage{helvet}\n",
      "\\usepackage{courier}\n",
      "\\usepackage{url}\n",
      "\\usepackage{graphicx}\n",
      "\\usepackage{booktabs}\n",
      "\\usepackage{subcaption}\n",
      "\\usepackage{amsmath}\n",
      "\\usepackage{amsfonts}\n",
      "\\usepackage{mathtools}\n",
      "\\usepackage{multirow}\n",
      "\\usepackage{enumitem}\n",
      "\n",
      "%\n",
      "\\usepackage{times}\n",
      "\\usepackage{epsfig}\n",
      "\\usepackage{graphicx}\n",
      "\\usepackage{amsmath}\n",
      "\\usepackage{amssymb}\n",
      "\\usepackage{tabularx}\n",
      "\n",
      "\\usepackage{verbatim}\n",
      "\\usepackage{multirow}\n",
      "\\usepackage{array}\n",
      "\n",
      "\\usepackage{amsthm}\n",
      "\\usepackage{xspace}\n",
      "\\usepackage[svgnames]{xcolor}\n",
      "\n",
      "\\usepackage{pifont}\n",
      "\n",
      "\\frenchspacing\n",
      "\\setlength{\\pdfpagewidth}{8.5in}\n",
      "\\setlength{\\pdfpageheight}{11in}\n",
      "\n",
      "\\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}\n",
      "\n",
      "\\iccvfinalcopy\n",
      "\n",
      "\\def{\\}{httilde}{\\mbox{\\tt\\raisebox{-.5ex}{\\symbol{126}}}}\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "\\title{All the attention you need:\\\\Global-local, spatial-channel attention for image retrieval}\n",
      "\n",
      "\\author{Chull Hwan Song\\\\\n",
      "{\\small Odd Concepts}\n",
      "\\and\n",
      "Hye Joo Han\\\\\n",
      "{\\small Odd Concepts}\n",
      "\\and\n",
      "Yannis Avrithis\\\\\n",
      "{\\small Inria, Univ Rennes, CNRS, IRISA}\n",
      "}\n",
      "\n",
      "\\maketitle\n",
      "\n",
      "% !TEX root = ../paper.tex\n",
      "\n",
      "\\newcommand{\\head}[1]{{\\smallskip\\noindent\\textbf{#1}}}\n",
      "\\newcommand{\\alert}[1]{{\\color{red}{#1}}}\n",
      "\\newcommand{\\sm}{\\scriptsize}\n",
      "\\newcommand{\\eq}[1]{(\\ref{eq:#1})}\n",
      "\n",
      "% \\newcommand{\\Th}[1]{\\tableheadline{#1}}\n",
      "\\newcommand{\\Th}[1]{\\textsc{#1}}\n",
      "\\newcommand{\\mr}[2]{\\multirow{#1}{*}{#2}}\n",
      "\\newcommand{\\mc}[2]{\\multicolumn{#1}{c}{#2}}\n",
      "\\newcommand{\\mca}[3]{\\multicolumn{#1}{#2}{#3}}\n",
      "\\newcommand{\\tb}[1]{\\textbf{#1}}\n",
      "\\newcommand{\\ch}{\\checkmark}\n",
      "\n",
      "\\newcommand{\\red}[1]{{\\color{red}{#1}}}\n",
      "\\newcommand{\\blue}[1]{{\\color{blue}{#1}}}\n",
      "\\newcommand{\\green}[1]{\\color{green}{#1}}\n",
      "\\newcommand{\\gray}[1]{{\\color{gray}{#1}}}\n",
      "\n",
      "\\newcommand{\\citeme}[1]{\\red{[XX]}}\n",
      "\\newcommand{\\refme}[1]{\\red{(XX)}}\n",
      "\n",
      "\\newcommand{\\fig}[2][1]{\\includegraphics[width=#1\\linewidth]{fig/#2}}\n",
      "\\newcommand{\\figh}[2][1]{\\includegraphics[height=#1\\linewidth]{fig/#2}}\n",
      "\n",
      "%--------------------------------------------------------------------\n",
      "\n",
      "\\newcommand{\\tran}{^\\top}\n",
      "\\newcommand{\\mtran}{^{-\\top}}\n",
      "\\newcommand{\\zcol}{\\mathbf{0}}\n",
      "\\newcommand{\\zrow}{\\zcol\\tran}\n",
      "\n",
      "\\newcommand{\\ind}{\\mathbbm{1}}\n",
      "\\newcommand{\\expect}{\\mathbb{E}}\n",
      "\\newcommand{\\nat}{\\mathbb{N}}\n",
      "\\newcommand{\\zahl}{\\mathbb{Z}}\n",
      "\\newcommand{\\real}{\\mathbb{R}}\n",
      "\\newcommand{\\proj}{\\mathbb{P}}\n",
      "\\newcommand{\\prob}{\\mathbf{Pr}}\n",
      "\\newcommand{\\normal}{\\mathcal{N}}\n",
      "\n",
      "\\newcommand{\\mif}{\\textrm{if}\\}\n",
      "\\newcommand{\\other}{\\textrm{otherwise}}\n",
      "\\newcommand{\\minimize}{\\textrm{minimize}\\}\n",
      "\\newcommand{\\maximize}{\\textrm{maximize}\\}\n",
      "\\newcommand{\\st}{\\textrm{subject\\to}\\}\n",
      "\n",
      "\\newcommand{\\id}{\\operatorname{id}}\n",
      "\\newcommand{\\const}{\\operatorname{const}}\n",
      "\\newcommand{\\sgn}{\\operatorname{sgn}}\n",
      "\\newcommand{\\var}{\\operatorname{Var}}\n",
      "\\newcommand{\\mean}{\\operatorname{mean}}\n",
      "\\newcommand{\\trace}{\\operatorname{tr}}\n",
      "\\newcommand{\\diag}{\\operatorname{diag}}\n",
      "\\newcommand{\\vect}{\\operatorname{vec}}\n",
      "\\newcommand{\\cov}{\\operatorname{cov}}\n",
      "\\newcommand{\\sign}{\\operatorname{sign}}\n",
      "\\newcommand{\\prj}{\\operatorname{proj}}\n",
      "\n",
      "\\newcommand{\\sigmoid}{\\operatorname{sigmoid}}\n",
      "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
      "\\newcommand{\\clip}{\\operatorname{clip}}\n",
      "\n",
      "\\newcommand{\\defn}{\\mathrel{:=}}\n",
      "\\newcommand{\\peq}{\\mathrel{+\\!=}}\n",
      "\\newcommand{\\meq}{\\mathrel{-\\!=}}\n",
      "\n",
      "\\newcommand{\\floor}[1]{\\left\\lfloor{#1}\\right\\rfloor}\n",
      "\\newcommand{\\ceil}[1]{\\left\\lceil{#1}\\right\\rceil}\n",
      "\\newcommand{\\inner}[1]{\\left\\langle{#1}\\right\\rangle}\n",
      "\\newcommand{\\norm}[1]{\\left\\|{#1}\\right\\|}\n",
      "\\newcommand{\\abs}[1]{\\left|{#1}\\right|}\n",
      "\\newcommand{\\frob}[1]{\\norm{#1}_F}\n",
      "\\newcommand{\\card}[1]{\\left|{#1}\\right|\\xspace}\n",
      "\\newcommand{\\diff}{\\mathrm{d}}\n",
      "\\newcommand{\\der}[3][]{\\frac{d^{#1}#2}{d#3^{#1}}}\n",
      "\\newcommand{\\pder}[3][]{\\frac{\\partial^{#1}{#2}}{\\partial{#3^{#1}}}}\n",
      "\\newcommand{\\ipder}[3][]{\\partial^{#1}{#2}/\\partial{#3^{#1}}}\n",
      "\\newcommand{\\dder}[3]{\\frac{\\partial^2{#1}}{\\partial{#2}\\partial{#3}}}\n",
      "\n",
      "\\newcommand{\\wb}[1]{\\overline{#1}}\n",
      "\\newcommand{\\wt}[1]{\\widetilde{#1}}\n",
      "\n",
      "\\def{\\}{xssp}{\\hspace{1pt}}\n",
      "\\def{\\}{ssp}{\\hspace{3pt}}\n",
      "\\def{\\}{msp}{\\hspace{5pt}}\n",
      "\\def{\\}{lsp}{\\hspace{12pt}}\n",
      "\n",
      "\\newcommand{\\cA}{\\mathcal{A}}\n",
      "\\newcommand{\\cB}{\\mathcal{B}}\n",
      "\\newcommand{\\cC}{\\mathcal{C}}\n",
      "\\newcommand{\\cD}{\\mathcal{D}}\n",
      "\\newcommand{\\cE}{\\mathcal{E}}\n",
      "\\newcommand{\\cF}{\\mathcal{F}}\n",
      "\\newcommand{\\cG}{\\mathcal{G}}\n",
      "\\newcommand{\\cH}{\\mathcal{H}}\n",
      "\\newcommand{\\cI}{\\mathcal{I}}\n",
      "\\newcommand{\\cJ}{\\mathcal{J}}\n",
      "\\newcommand{\\cK}{\\mathcal{K}}\n",
      "\\newcommand{\\cL}{\\mathcal{L}}\n",
      "\\newcommand{\\cM}{\\mathcal{M}}\n",
      "\\newcommand{\\cN}{\\mathcal{N}}\n",
      "\\newcommand{\\cO}{\\mathcal{O}}\n",
      "\\newcommand{\\cP}{\\mathcal{P}}\n",
      "\\newcommand{\\cQ}{\\mathcal{Q}}\n",
      "\\newcommand{\\cR}{\\mathcal{R}}\n",
      "\\newcommand{\\cS}{\\mathcal{S}}\n",
      "\\newcommand{\\cT}{\\mathcal{T}}\n",
      "\\newcommand{\\cU}{\\mathcal{U}}\n",
      "\\newcommand{\\cV}{\\mathcal{V}}\n",
      "\\newcommand{\\cW}{\\mathcal{W}}\n",
      "\\newcommand{\\cX}{\\mathcal{X}}\n",
      "\\newcommand{\\cY}{\\mathcal{Y}}\n",
      "\\newcommand{\\cZ}{\\mathcal{Z}}\n",
      "\n",
      "\\newcommand{\\vA}{\\mathbf{A}}\n",
      "\\newcommand{\\vB}{\\mathbf{B}}\n",
      "\\newcommand{\\vC}{\\mathbf{C}}\n",
      "\\newcommand{\\vD}{\\mathbf{D}}\n",
      "\\newcommand{\\vE}{\\mathbf{E}}\n",
      "\\newcommand{\\vF}{\\mathbf{F}}\n",
      "\\newcommand{\\vG}{\\mathbf{G}}\n",
      "\\newcommand{\\vH}{\\mathbf{H}}\n",
      "\\newcommand{\\vI}{\\mathbf{I}}\n",
      "\\newcommand{\\vJ}{\\mathbf{J}}\n",
      "\\newcommand{\\vK}{\\mathbf{K}}\n",
      "\\newcommand{\\vL}{\\mathbf{L}}\n",
      "\\newcommand{\\vM}{\\mathbf{M}}\n",
      "\\newcommand{\\vN}{\\mathbf{N}}\n",
      "\\newcommand{\\vO}{\\mathbf{O}}\n",
      "\\newcommand{\\vP}{\\mathbf{P}}\n",
      "\\newcommand{\\vQ}{\\mathbf{Q}}\n",
      "\\newcommand{\\vR}{\\mathbf{R}}\n",
      "\\newcommand{\\vS}{\\mathbf{S}}\n",
      "\\newcommand{\\vT}{\\mathbf{T}}\n",
      "\\newcommand{\\vU}{\\mathbf{U}}\n",
      "\\newcommand{\\vV}{\\mathbf{V}}\n",
      "\\newcommand{\\vW}{\\mathbf{W}}\n",
      "\\newcommand{\\vX}{\\mathbf{X}}\n",
      "\\newcommand{\\vY}{\\mathbf{Y}}\n",
      "\\newcommand{\\vZ}{\\mathbf{Z}}\n",
      "\n",
      "\\newcommand{\\va}{\\mathbf{a}}\n",
      "\\newcommand{\\vb}{\\mathbf{b}}\n",
      "\\newcommand{\\vc}{\\mathbf{c}}\n",
      "\\newcommand{\\vd}{\\mathbf{d}}\n",
      "\\newcommand{\\ve}{\\mathbf{e}}\n",
      "\\newcommand{\\vf}{\\mathbf{f}}\n",
      "\\newcommand{\\vg}{\\mathbf{g}}\n",
      "\\newcommand{\\vh}{\\mathbf{h}}\n",
      "\\newcommand{\\vi}{\\mathbf{i}}\n",
      "\\newcommand{\\vj}{\\mathbf{j}}\n",
      "\\newcommand{\\vk}{\\mathbf{k}}\n",
      "\\newcommand{\\vl}{\\mathbf{l}}\n",
      "\\newcommand{\\vm}{\\mathbf{m}}\n",
      "\\newcommand{\\vn}{\\mathbf{n}}\n",
      "\\newcommand{\\vo}{\\mathbf{o}}\n",
      "\\newcommand{\\vp}{\\mathbf{p}}\n",
      "\\newcommand{\\vq}{\\mathbf{q}}\n",
      "\\newcommand{\\vr}{\\mathbf{r}}\n",
      "\\newcommand{\\Vs}{\\mathbf{s}}\n",
      "\\newcommand{\\vt}{\\mathbf{t}}\n",
      "\\newcommand{\\vu}{\\mathbf{u}}\n",
      "\\newcommand{\\vv}{\\mathbf{v}}\n",
      "\\newcommand{\\vw}{\\mathbf{w}}\n",
      "\\newcommand{\\vx}{\\mathbf{x}}\n",
      "\\newcommand{\\vy}{\\mathbf{y}}\n",
      "\\newcommand{\\vz}{\\mathbf{z}}\n",
      "\n",
      "\\newcommand{\\vone}{\\mathbf{1}}\n",
      "\\newcommand{\\vzero}{\\mathbf{0}}\n",
      "\n",
      "\\newcommand{\\valpha}{{\\boldsymbol{\\alpha}}}\n",
      "\\newcommand{\\vbeta}{{\\boldsymbol{\\beta}}}\n",
      "\\newcommand{\\vgamma}{{\\boldsymbol{\\gamma}}}\n",
      "\\newcommand{\\vdelta}{{\\boldsymbol{\\delta}}}\n",
      "\\newcommand{\\vepsilon}{{\\boldsymbol{\\epsilon}}}\n",
      "\\newcommand{\\vzeta}{{\\boldsymbol{\\zeta}}}\n",
      "\\newcommand{\\veta}{{\\boldsymbol{\\eta}}}\n",
      "\\newcommand{\\vtheta}{{\\boldsymbol{\\theta}}}\n",
      "\\newcommand{\\viota}{{\\boldsymbol{\\iota}}}\n",
      "\\newcommand{\\vkappa}{{\\boldsymbol{\\kappa}}}\n",
      "\\newcommand{\\vlambda}{{\\boldsymbol{\\lambda}}}\n",
      "\\newcommand{\\vmu}{{\\boldsymbol{\\mu}}}\n",
      "\\newcommand{\\vnu}{{\\boldsymbol{\\nu}}}\n",
      "\\newcommand{\\vxi}{{\\boldsymbol{\\xi}}}\n",
      "\\newcommand{\\vomikron}{{\\boldsymbol{\\omikron}}}\n",
      "\\newcommand{\\vpi}{{\\boldsymbol{\\pi}}}\n",
      "\\newcommand{\\vrho}{{\\boldsymbol{\\rho}}}\n",
      "\\newcommand{\\vsigma}{{\\boldsymbol{\\sigma}}}\n",
      "\\newcommand{\\vtau}{{\\boldsymbol{\\tau}}}\n",
      "\\newcommand{\\vupsilon}{{\\boldsymbol{\\upsilon}}}\n",
      "\\newcommand{\\vphi}{{\\boldsymbol{\\phi}}}\n",
      "\\newcommand{\\vchi}{{\\boldsymbol{\\chi}}}\n",
      "\\newcommand{\\vpsi}{{\\boldsymbol{\\psi}}}\n",
      "\\newcommand{\\vomega}{{\\boldsymbol{\\omega}}}\n",
      "\n",
      "\\newcommand{\\rLambda}{\\mathrm{\\Lambda}}\n",
      "\\newcommand{\\rSigma}{\\mathrm{\\Sigma}}\n",
      "\n",
      "\\newcommand{\\vLambda}{\\bm{\\rLambda}}\n",
      "\\newcommand{\\vSigma}{\\bm{\\rSigma}}\n",
      "\n",
      "% big cdot\n",
      "\\makeatletter\n",
      "\\newcommand*\\bdot{\\mathpalette\\bdot@{.7}}\n",
      "\\newcommand*\\bdot@[2]{\\mathbin{\\vcenter{\\hbox{\\scalebox{#2}{$\\m@th#1\\bullet$}}}}}\n",
      "\\makeatother\n",
      "\n",
      "%--------------------------------------------------------------------\n",
      "% Add a period to the end of an abbreviation unless there's one\n",
      "% already, then \\xspace.\n",
      "\\makeatletter\n",
      "\\DeclareRobustCommand\\onedot{\\futurelet\\@let@token\\@onedot}\n",
      "\\def{\\@}{onedot}{\\ifx\\@let@token.\\else.\\null\\fi\\xspace}\n",
      "\n",
      "\\def{\\}{eg}{\\emph{e.g}\\onedot} \\def{\\}{Eg}{\\emph{E.g}\\onedot}\n",
      "\\def{\\}{ie}{\\emph{i.e}\\onedot} \\def{\\}{Ie}{\\emph{I.e}\\onedot}\n",
      "\\def{\\}{cf}{\\emph{cf}\\onedot} \\def{\\}{Cf}{\\emph{Cf}\\onedot}\n",
      "\\def{\\}{etc}{\\emph{etc}\\onedot} \\def{\\}{vs}{\\emph{vs}\\onedot}\n",
      "\\def{\\}{wrt}{w.r.t\\onedot} \\def{\\}{dof}{d.o.f\\onedot} \\def{\\}{aka}{a.k.a\\onedot}\n",
      "\\def{\\}{etal}{\\emph{et al}\\onedot}\n",
      "\\makeatother\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "% method shortcuts\n",
      "\n",
      "\\def{\\}{vlad}{VLAD\\xspace}\n",
      "\\def{\\}{smk}{SMK$^{\\star}$\\xspace}\n",
      "\\def{\\}{asmk}{ASMK$^{\\star}$\\xspace}\n",
      "\\def{\\}{sp}{SP\\xspace}\n",
      "\\def{\\}{qe}{QE\\xspace}\n",
      "\\def{\\}{hqe}{HQE\\xspace}\n",
      "\\def{\\}{dfs}{DFS\\xspace}\n",
      "\\def{\\}{off}{O}\n",
      "\\def{\\}{hesaff}{HesAff\\xspace}\n",
      "\\def{\\}{rsift}{rSIFT\\xspace}\n",
      "\\def{\\}{delf}{DELF\\xspace}\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "% data shortcuts\n",
      "\n",
      "\\def{\\}{oxf}5k{Ox5k\\xspace}\n",
      "\\def{\\}{paris}6k{Par6k\\xspace}\n",
      "\n",
      "\\def{\\}{roxf}{$\\mathcal{R}$Oxford}\n",
      "\\def{\\}{rox}{$\\mathcal{R}$Oxf}\n",
      "\\def{\\}{ro}{$\\mathcal{R}$O}\n",
      "\\def{\\}{rpar}{$\\mathcal{R}$Paris}\n",
      "\\def{\\}{rpa}{$\\mathcal{R}$Par}\n",
      "\\def{\\}{rp}{$\\mathcal{R}$Pe}\n",
      "\\def{\\}{r}1m{$\\mathcal{R}$1M}\n",
      "\\def{\\}{rs}{$\\mathcal{R}$100k}\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "% colors\n",
      "\n",
      "\\definecolor{greenn}{rgb}{0.30,0.69,0.31}\n",
      "\\definecolor{redd}{rgb}{0.3,0.2,0.7}\n",
      "\\definecolor{red}{rgb}{0.8,0.1,0.1}\n",
      "\n",
      "\\newcommand{\\ok}[1]{\\color{redd}{#1}}\n",
      "\\newcommand{\\okg}[1]{\\color{greenn}{#1}}\n",
      "\n",
      "\n",
      "\\begin{abstract}\n",
      "We address representation learning for large-scale instance-level image retrieval. Apart from backbone, training pipelines and loss functions, popular approaches have focused on different spatial pooling and attention mechanisms, which are at the core of learning a powerful global image representation. There are different forms of attention according to the interaction of elements of the feature tensor (local and global) and the dimensions where it is applied (spatial and channel). Unfortunately, each study addresses only one or two forms of attention and applies it to different problems like classification, detection or retrieval.\n",
      "\n",
      "We present \\emph{global-local attention module} (GLAM), which is attached at the end of a backbone network and incorporates all four forms of attention: local and global, spatial and channel. We obtain a new feature tensor and, by spatial pooling, we learn a powerful embedding for image retrieval.\n",
      "Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.\n",
      "\\end{abstract}\n",
      "\n",
      "\\section{Introduction}\n",
      "\\label{sec:intro}\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{figure*}\n",
      "\\centering\n",
      "\\input{tex/fig-glam}\n",
      "\\caption{Our \\emph{global-local attention module} (GLAM) involves both {\\color{blue}channel} and {\\color{red}spatial} attention, as well as both {\\color{yellow!60!red}local} attention (channels/locations weighted independently, based on contextual information obtained by pooling) and {\\color{green!60!black}global} attention (based on pairwise interaction between channels/locations). As a result, four attention maps are used: \\emph{local channel} ($\\vA_c^l$), \\emph{local spatial} ($\\vA_s^l$), \\emph{global channel} ($\\vA_c^g$) and \\emph{global spatial} ($\\vA_s^g$). The input feature map $\\vF$ is weighted into local ($\\vF^l$) and global ($\\vF^g$) attention feature maps, which are fused with $\\vF$ to yield the \\emph{global-local attention feature map} $\\vF^{gl}$. The diagram is abstract: The four attention modules are shown in more detail in Figures \\ref{fig:fig4}, \\ref{fig:fig3}, \\ref{fig:fig6}, \\ref{fig:fig5}.}\n",
      "\\label{fig:glam}\n",
      "\\end{figure*}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "Instance-level image retrieval is at the core of visual representation learning and is connected with many problems of visual recognition and machine learning, for instance \\emph{metric learning}~\\cite{oh2016deep,KKCK20}, \\emph{few-shot learning}~\\cite{SnellSZ17} and \\emph{unsupervised learning}~\\cite{chen2020simple}. Many large-scale open datasets~\\cite{Babenko01, Radenovic01, Gordo01, Noh01, Weyand01}, and competitions\\footnote{https://www.kaggle.com/c/landmark-retrieval-2020} have accelerated progress in instance-level image retrieval, which has been transformed by deep learning~\\cite{Babenko01}.\n",
      "\n",
      "Many studies on instance-level image retrieval focus on learning features from \\emph{convolutional neural networks} (CNN), while others focus on \\emph{re-ranking}, for instance by graph-based methods~\\cite{Donoser01}. The former can be distinguished according to feature types: \\emph{local descriptors}, reminiscent of SIFT~\\cite{Lowe01}, where an image is mapped to a few hundred vectors; and \\emph{global descriptors}, where an image is mapped to a single vector. In fact, deep learning has brought global descriptors with astounding performance, while allowing efficient search. Our study belongs to this type.\n",
      "\n",
      "Studies on global descriptors have focused on \\emph{spatial pooling}~\\cite{Babenko03,Radenovic01}. The need for compact, discriminative representations that are resistant to clutter has naturally given rise to \\emph{spatial attention} methods~\\cite{Kalantidis01,Ng01}. Different kinds of attention have been studied in many areas of computer vision research. There is also \\emph{channel attention}~\\cite{Hu01,ChenKLYF18}; \\emph{local attention}, applied independently to elements of the representation (feature map)~\\cite{woo01,Kim01}; \\emph{global attention}, based on interaction between elements~\\cite{Wang02,ChenKLYF18}; and combinations thereof. Unfortunately, each study has been limited to one or two kinds of attention only; attention is not always learned; and applications vary.\n",
      "\n",
      "It is the objective of our work to perform a comprehensive study of all forms of attention above, apply them to instance-level image retrieval and provide a detailed account of their interaction and impact on performance. As shown in \\autoref{fig:glam}, we collect contextual information from images with both \\emph{local} and \\emph{global} attention, giving rise to two parallel network streams. Importantly, each operates on both \\emph{spatial locations} and \\emph{feature channels}. Local attention is about individual locations and channels; global is about interaction between locations and between channels. The extracted information is separately embedded in local and global attention feature maps, which are combined in a \\emph{global-local attention feature map} before pooling.\n",
      "\n",
      "Our contributions can be summarized as follows:\n",
      "\\begin{enumerate}[itemsep=2pt, parsep=0pt, topsep=0pt]\n",
      "\t \\item We propose a novel network that consists of both global and local attention for image retrieval. This is the first study that employs both mechanisms.\n",
      "\t \\item Each of the global and local attention mechanisms comprises both spatial and channel attention.\n",
      "\t \\item Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.\n",
      "\\end{enumerate}\n",
      "\n",
      "\\section{Related work}\n",
      "\\label{sec:related}\n",
      "\n",
      "\\paragraph{Instance-level image retrieval}\n",
      "\n",
      "Studies on instance-level image retrieval can be roughly, but not exclusively, divided into three types: (1) studies on \\emph{global descriptors} \\cite{Babenko01, Gordo01, Kalantidis01, Weyand01, Babenko03, Radenovic01}; (2) studies on \\emph{local descriptors} and geometry-based re-ranking \\cite{Noh01, Teichmann01, simeoni2019local, Weyand01}; (3) \\emph{re-ranking} by graph-based methods \\cite{Donoser01, iscen2017efficient, Yang01}.\n",
      "The first two types of studies focus on the feature representation, while the last type focuses on re-ranking extracted features.\n",
      "\n",
      "Studies on global descriptors focus on \\emph{spatial pooling} of CNN feature maps into vectors, including MAC~\\cite{Razavian2015VisualIR}, SPoC~\\cite{Babenko03}, CroW~\\cite{Kalantidis01}, R-MAC~\\cite{ToliasSJ15, Gordo00, Gordo01}, GeM~\\cite{Radenovic01}, and NetVLAD~\\cite{Arandjelovic01, Kim01}, as well as \\emph{learning the representation}~\\cite{Babenko01, Gordo00, Gordo01, Radenovi01, Radenovic01}. Studies before deep learning dominated image retrieval were mostly based on \\emph{local descriptors} like SIFT~\\cite{Lowe01} and \\emph{bag-of-words} representation~\\cite{Philbin01} or aggregated descriptors like VLAD~\\cite{JPD+11} or ASMK~\\cite{TAJ13}. Local descriptors have been revived in deep learning, \\eg with DELF~\\cite{Noh01}, DELG~\\cite{ECCV2020_912} and ASMK extensions~\\cite{Teichmann01, tolias2020learning}.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{2.6pt}\n",
      "\\begin{tabular}{lcccccc} \\toprule\n",
      "\t \\mr{2}{\\Th{Method}}                          & \\mc{2}{\\Th{Local}}    & \\mc{2}{\\Th{Global}} & \\mr{2}{\\Th{Lrn}} & \\mr{2}{\\Th{Ret}} \\\\ \\cmidrule{2-5}\n",
      "\t                                              & Spatial   & Channel   & Spatial  & Channel  &                  &                  \\\\ \\midrule\n",
      "\t SENet~\\cite{Hu01}                            &           & \\ch       &          &          & \\ch              &                  \\\\\n",
      "\t ECA-Net~\\cite{wang01}                        &           & \\ch       &          &          & \\ch              &                  \\\\\n",
      "\t GCNet~\\cite{Cao01}                           &           & \\ch       &          &          & \\ch              &                  \\\\\n",
      "\t CBAM~\\cite{woo01}                            & \\ch       & \\ch       &          &          & \\ch              &                  \\\\\n",
      "\t GE~\\cite{HuSASV18}                           & \\ch       &           &          &          & \\ch              &                  \\\\\n",
      "\t NL-Net~\\cite{Wang02}                         &           &           & \\ch      &          & \\ch              &                  \\\\\n",
      "\t AA-Net~\\cite{Bello_2019_ICCV}                &           &           & \\ch      &          & \\ch              &                  \\\\\n",
      "\t SAN~\\cite{zhao2020exploring}                 &           &           & \\ch      &          & \\ch              &                  \\\\\n",
      "\t N$^3$Net~\\cite{plotz2018neural}              &           &           & \\ch      &          & \\ch              &                  \\\\\n",
      "\t A$^2$-Net~\\cite{ChenKLYF18}                  &           &           &          & \\ch      & \\ch              &                  \\\\\n",
      "\t GSoP~\\cite{Gao_2019_CVPR}                    &           &           &          & \\ch      & \\ch              &                  \\\\ \\midrule\n",
      "\t OnA~\\cite{JimenezAN17}                       & \\ch       &           &          &          &                  & \\ch              \\\\\n",
      "\t AGeM~\\cite{gu2018attention}                  & \\ch       &           &          &          &                  & \\ch              \\\\\n",
      "\t CroW~\\cite{Kalantidis01}                     & \\ch       & \\ch       &          &          &                  & \\ch              \\\\\n",
      "\t CRN~\\cite{Kim01}                             & \\ch       &           &          &          & \\ch              & \\ch              \\\\\n",
      "\t DELF~\\cite{Noh01}                            & \\ch       &           &          &          & \\ch              & \\ch              \\\\\n",
      "\t DELG~\\cite{ECCV2020_912}                     & \\ch       &           &          &          & \\ch              & \\ch              \\\\\n",
      "\t Tolias \\etal~\\cite{tolias2020learning}       & \\ch       &           &          &          & \\ch              & \\ch              \\\\\n",
      "\t SOLAR~\\cite{Ng01}                            &           &           & \\ch      &          & \\ch              & \\ch              \\\\ \\midrule\n",
      "\t \\tb{Ours}                                    & \\ch       & \\ch       & \\ch      & \\ch      & \\ch              & \\ch              \\\\ \\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Related work on attention. LRN: learned; RET: applied to instance-level image retrieval.}\n",
      "\\label{tab:rel}\n",
      "\\end{table}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "We focus on learning a global descriptor in this work, because it is the most efficient in terms of storage and search. However, our generic attention mechanism produces a feature tensor and could be applicable to local descriptors as well, if global pooling were replaced by local feature detection. Re-ranking methods are complementary to the representation and we do not consider them in this work.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\input{tex/fig-styles}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{figure}\n",
      "\\centering\n",
      "\\input{tex/fig-local-channel}\n",
      "\\caption{Local channel attention.}\n",
      "\\label{fig:fig4}\n",
      "\\end{figure}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Attention}\n",
      "\n",
      "Attention mechanisms have been first proposed in \\emph{image classification} studies focusing on \\emph{channel attention}~\\cite{Hu01, wang01, Cao01}, \\emph{spatial attention}~\\cite{HuSASV18} or both, like CBAM~\\cite{woo01}. In \\emph{image retrieval}, CroW \\cite{Kalantidis01} also  employs both spatial and channel attention and can be seen as a precursor of CBAM, but, like other studies of spatial attention on retrieval~\\cite{simeoni2019graph, JimenezAN17, gu2018attention}, it is not learned. CRN~\\cite{Kim01} applies spatial attention for feature reweighting and is learned. Learned spatial attention mechanisms are common for local descriptors~\\cite{Noh01, ECCV2020_912, tolias2020learning}.\n",
      "\n",
      "We call the above methods \\emph{local attention}, in the sense that elements of the feature tensor (channels / spatial locations), are weighted independently, based on contextual information obtained by pooling or learned. By constrast, by \\emph{global attention} we refer to mechanisms that model interaction between elements of the feature tensor, for example between channels or between locations.\n",
      "\n",
      "In \\emph{image classification}, \\emph{non-local neural network} (NLNet)~\\cite{Wang02} is maybe the first global attention mechanism, followed by similar studies~\\cite{Bello_2019_ICCV,zhao2020exploring,plotz2018neural}. It is global \\emph{spatial attention}, allowing interaction between any pair of spatial locations. Similarly, there are studies of global \\emph{channel attention}, allowing interaction between channels~\\cite{ChenKLYF18, Gao_2019_CVPR}. Global attention has focused mostly on image recognition and has been applied to either spatial or channel attention so far, not both. In \\emph{image retrieval}, SOLAR~\\cite{Ng01} is a direct application of the global spatial attention mechanism of~\\cite{Wang02}.\n",
      "\n",
      "\\autoref{tab:rel} attempts to categorize related work on attention according to whether attention is local or global, spatial or channel, whether it is learned and whether it is applied to instance-level image retrieval. We observe that all methods limit to one or two forms of attention only. Of those studies that focus on image retrieval, many are not learned~\\cite{JimenezAN17, gu2018attention, Kalantidis01}, and of those that are, some are designed for local descriptors~\\cite{Noh01,tolias2020learning}.\n",
      "\n",
      "By contrast, we provide a comprehensive study of \\emph{all forms} of attention, global and local, spatial and channel, to obtain a learned representation in the form of a tensor that can be used in any way. We spatially pool it into a global descriptor and we study the relative gain of different forms of attention in image retrieval.\n",
      "\n",
      "\\section{Global-local attention}\n",
      "\\label{sec:method}\n",
      "\n",
      "We design a \\emph{global-local attention module} (GLAM), which is attached at the end of a backbone network. \\autoref{fig:glam} illustrates its main components. We are given a $c\\times h \\times w$ feature tensor $\\vF$, where $c$ is the number of channels, and $h \\times w$ is the spatial resolution. Local attention collects context from the image and applies pooling to obtain a $c \\times 1 \\times 1$ \\emph{local channel attention map} $\\vA_c^l$ and a $1 \\times h \\times w$ \\emph{local spatial attention map} $\\vA_s^l$. Global attention allows interaction between channels, resulting in a $c \\times c$ \\emph{global channel attention map} $\\vA_c^g$, and between spatial locations, resulting in a $hw \\times hw$ \\emph{global spatial attention map} $\\vA_s^g$. The feature maps produced by the two attention streams are combined with the original one by a learned fusion mechanism into the \\emph{global-local attention feature map} $\\vF^{gl}$ before being spatially pooled into a global image descriptor.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{figure}\n",
      "\\centering\n",
      "\\input{tex/fig-local-spatial}\n",
      "\\caption{Local spatial attention. Convolutional layers in blue implemented by dilated convolutions with kernel size $3 \\times 3$ and dilation factors $1,3,5$.}\n",
      "\\label{fig:fig3}\n",
      "\\end{figure}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\subsection{Local attention}\n",
      "\\label{sec:local}\n",
      "\n",
      "We extract an 1D channel and a 2D spatial attention map to weigh the feature map in the corresponding dimensions.\n",
      "\n",
      "\\paragraph{Local channel attention}\n",
      "\n",
      "Following ECA-Net~\\cite{wang01}, this attention captures local channel information. As shown in \\autoref{fig:fig4}, we are given a $c\\times h\\times w$ feature tensor $\\vF$ from our backbone. We first reduce it to a $c \\times 1 \\times 1$ tensor by \\emph{global average pooling} (GAP). Channel attention is then captured by a 1D convolution of kernel size $k$ along the channel dimension, where $k$ controls the extent of cross-channel interaction. This is followed by a sigmoid function, resulting in the $c\\times 1\\times 1$ \\emph{local channel attention map} $\\vA_c^l$.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Local spatial attention}\n",
      "\n",
      "Inspired by the inception module~\\cite{Szegedy01} and similar to~\\cite{Kim01}, this attention map captures local spatial information at different scales. As shown in \\autoref{fig:fig3},\n",
      "given the same $c\\times h\\times w$ feature tensor $\\vF$ from our backbone, we obtain a new tensor $\\vF'$ with channels reduced to $c'$, using a ${1 \\times 1}$ convolution. We then extract local spatial contextual information using convolutional filters of kernel size ${3\\times 3}$, ${5\\times 5}$, and ${7\\times 7}$, which are efficiently implemented by ${3\\times 3}$ dilated convolutions~\\cite{chen2017rethinking,Yu_2017_CVPR} with dilation parameter 1, 2, and 3 respectively. The resulting features, along with one obtained by ${1\\times 1}$ convolution on $\\vF'$, are concatenated into a $4c' \\times h \\times w$ tensor. Finally, we obtain the $1 \\times h \\times w$ \\emph{local spatial attention map} $\\vA_s^l$ by a ${1\\times 1}$ convolution that reduces the channel dimension to $1$.\n",
      "\n",
      "The middle column of \\autoref{fig:fig7} shows heat maps of local spatial attention, localizing target objects in images.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Local attention feature map}\n",
      "\n",
      "We use the local channel attention map $\\vA_c^l$ to weigh $\\vF$ in the channel dimension\n",
      "\\begin{equation}\n",
      "\t\\vF_c^l \\defn \\vF \\odot \\vA_c^l + \\vF.\n",
      "\\label{eq:eq3}\n",
      "\\end{equation}\n",
      "We then use local spatial attention map $\\vA_s^l$ to weigh $\\vF_c^l$ in the spatial dimensions, resulting in the $c \\times h \\times w$ \\emph{local attention feature map}\n",
      "\\begin{equation}\n",
      "\t\\vF^l = \\vF_c^l \\odot \\vA_s^l + \\vF_c^l.\n",
      "\\label{eq:eq3-1}\n",
      "\\end{equation}\n",
      "Here, $\\vA \\odot \\vB$ denotes an element-wise multiplication of tensors $\\vA$ and $\\vB$, with broadcasting when one tensor is smaller. We adopt the choice of applying channel followed by spatial attention from \\emph{convolutional block attention module} CBAM~\\cite{woo01}. However, apart from computing $\\vA_s^l$ at different scales, both attention maps are obtained from the original tensor $\\vF$ rather than sequentially. In addition, both~\\eq{eq3} and~\\eq{eq3-1} include residual connections, while CBAM includes a single residual connection over both steps.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{figure}\n",
      "\\centering\n",
      "\\input{tex/fig-global-channel}\n",
      "\\caption{Global channel attention.}\n",
      "\\label{fig:fig6}\n",
      "\\end{figure}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\subsection{Global attention}\n",
      "\\label{sec:global}\n",
      "\n",
      "We extract two matrices capturing global pairwise channel and spatial interaction to weigh the feature map.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Global channel attention}\n",
      "\n",
      "We introduce a \\emph{global channel attention} mechanism that captures global channel interaction. This mechanism is based on the non-local neural network~\\cite{Wang02}, but with the idea of 1D convolution from ECA-Net~\\cite{wang01}. As shown in \\autoref{fig:fig6}, we are given the $c\\times h\\times w$ feature tensor $\\vF$ from our backbone. We apply GAP and squeeze spatial dimensions, followed by a 1D convolution of kernel size $k$ and a sigmoid function, to obtain $1 \\times c$ \\emph{query} $\\vQ_c$ and \\emph{key} $\\vK_c$ tensors. The \\emph{value} tensor $\\vV_c$ is obtained by mere reshaping of $\\vF$ to $hw \\times c$, without GAP. Next, we form the outer product of $\\vK_c$ and $\\vQ_c$, followed by softmax over channels to obtain a $c \\times c$ \\emph{global channel attention map}\n",
      "\\begin{equation}\n",
      "\t\\vA_c^g = \\softmax({\\vK_c}\\tran \\vQ_c).\n",
      "\\label{eq:eq6-1}\n",
      "\\end{equation}\n",
      "Finally, this attention map is multiplied with $\\vV_c$ and the matrix product $\\vV_c \\vA_c^g$ is reshaped back to $c \\times h \\times w$ to give the \\emph{global channel attention feature map} $\\vG_c$. In GSoP~\\cite{Gao_2019_CVPR} and A$^2$-Net~\\cite{ChenKLYF18}, a $c \\times c$ global channel attention map is obtained by multiplication of $hw \\times c$ matrices; \\eq{eq6-1} is more efficient, using only an outer product of $1 \\times c$ vectors.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{figure}\n",
      "\\centering\n",
      "\\input{tex/fig-global-spatial}\n",
      "\\caption{Global spatial attention.}\n",
      "\\label{fig:fig5}\n",
      "\\end{figure}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Global spatial attention}\n",
      "\n",
      "Since ordinary convolution applies only a local neighborhood at a time, it cannot capture global contextual information. Thus, we apply \\emph{non-local filtering}~\\cite{Wang02}, which is a form of \\emph{self-attention}~\\cite{Vaswani01} in the spatial dimensions. As shown in \\autoref{fig:fig5}, we are given the same $c\\times h\\times w$ feature tensor $\\vF$ from our backbone. By using three $1\\times 1$ convolutions, which reduce channels to $c'$, and flattening spatial dimensions to $hw$, we obtain $c' \\times hw$ \\emph{query} $\\vQ_s$, \\emph{key} $\\vK_s$, and \\emph{value} $\\vV_s$ tensors, where each column is a feature vector corresponding to a particular spatial location. We capture pairwise similarities of these vectors by matrix multiplication of $\\vK_s$ and $\\vQ_s$, followed by softmax over locations to obtain a $hw \\times hw$ \\emph{global spatial attention map}:\n",
      "\\begin{equation}\n",
      "\t\\vA_s^g = \\softmax(\\vK_s\\tran \\vQ_s).\n",
      "\\label{eq:eq4-1}\n",
      "\\end{equation}\n",
      "This attention map is multiplied with $\\vV_s$ and the matrix product $\\vV_s \\vA_s^g$ is reshaped back to $c' \\times h \\times w$ by expanding the spatial dimensions. Finally, using a ${1\\times 1}$ convolution, which increases channels back to $c$, we obtain the $c \\times h\\times w$ \\emph{global spatial attention feature map} $\\vG_s$.\n",
      "\n",
      "The right column of \\autoref{fig:fig7} shows heat maps for global spatial attention, localizing target objects in images.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Global attention feature map}\n",
      "\n",
      "We use the global channel attention feature map $\\vF_c$ to weigh $\\vF$ element-wise\n",
      "\\begin{equation}\n",
      "\t\\vF_c^g = \\vF \\odot \\vG_c.\n",
      "\\label{eq:eq8}\n",
      "\\end{equation}\n",
      "We then use global spatial attention feature map $\\vG_s$ to weigh $\\vF_c^g$ element-wise, resulting in the $c \\times h \\times w$ \\emph{global attention feature map}\n",
      "\\begin{equation}\n",
      "\t\\vF^g = \\vF_c^g \\odot \\vG_s + \\vF_c^g.\n",
      "\\label{eq:eq8-1}\n",
      "\\end{equation}\n",
      "Similarly to $\\mathbf{F}^{l}$ in~\\eq{eq3} and~\\eq{eq3-1}, we apply channel attention first, followed by spatial attention. However, unlike \\eq{eq3}, there is no residual connection in~\\eq{eq8}. This choice is supported by early experiments.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{figure}\n",
      "\\centering\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{2pt}\n",
      "\\newcommand{\\heat}[1]{%\n",
      "\t\\fig[.28]{heatmap/#1/src.png} &\n",
      "\t\\fig[.28]{heatmap/#1/l.png} &\n",
      "\t\\fig[.28]{heatmap/#1/g.png} \\\\\n",
      "}\n",
      "\\begin{tabular}{ccc}\n",
      "\t\\heat{1}\n",
      "\t\\heat{2}\n",
      "\t\\heat{4}\n",
      "\t(a) input &\n",
      "\t(b) local &\n",
      "\t(c) global\n",
      "\\end{tabular}\n",
      "\\caption{\\emph{Local and global spatial attention}. Left: input images. Middle: local spatial attention heat maps. Right: global spatial attention heat maps. Red (blue) means higher (lower) attention weight.}\n",
      "\\label{fig:fig7}\n",
      "\\end{figure}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\subsection{Global-local attention}\n",
      "\\label{sec:embed}\n",
      "\n",
      "\\paragraph{Feature fusion}\n",
      "\n",
      "As shown in \\autoref{fig:glam}, we combine the local and global attention feature maps, $\\vF^l$ and $\\vF^g$, with the original feature $\\vF$. While concatenation and summation are common operations for feature combination, we use a weighted average with weights $w_l$, $w_g$, $w$ respectively, obtained by softmax over three learnable scalar parameters, to obtain a $c \\times h \\times w$ \\emph{global-local attention feature map}\n",
      "\\begin{equation}\n",
      "\t\\vF^{gl} = w_l \\vF^l + w_g \\vF^l + w \\vF.\n",
      "\\label{eq:eq10}\n",
      "\\end{equation}\n",
      "EfficientDet~\\cite{Tan01} has shown that this is the most effective, among a number of choices, for fusion of features across different scales.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Pooling}\n",
      "\n",
      "We apply GeM~\\cite{Radenovic01}, a learnable spatial pooling mechanism, to feature map $\\vF^{gl}$~\\eq{eq10}, followed by a fully-connected (FC) layer with dropout and batch normalization. The final embedding is obtained by $\\ell_2$-normalization.\n",
      "\n",
      "\\section{Experiments}\n",
      "\\label{sec:exp}\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{figure*}\n",
      "\\centering\n",
      "\\fig{10_2}\n",
      "\\caption{Examples of our ranking results. In each row, the first image on the left (pink dotted outline) is a query image with a target object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images for the query; red solid outline: negative.}\n",
      "\\label{fig:fig8}\n",
      "\\end{figure*}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\subsection{Datasets}\n",
      "\n",
      "\\paragraph{Training set}\n",
      "\n",
      "There are a number of open landmark datasets commonly used for training in image retrieval studies, including \\emph{neural code} (NC)~\\cite{Babenko01}, \\emph{neural code clean} (NC-clean)~\\cite{Gordo01}, as well as Google Landmarks v1 (GLDv1)~\\cite{Noh01} and v2 (GLDv2)~\\cite{Weyand01}. \\autoref{tab:table1} shows relevant statistics. These datasets can be categorized into noisy and clean. The clean sets were obtained from the original noisy sets for more effective training~\\cite{Gordo01, Weyand01}. The original noisy datasets are much larger, but they have high intra-class variability. Each class can include visually dissimilar images such as exterior and interior views of a building or landmark, including floor plans and paintings inside. The clean datasets focus on views directly relevant to landmark recognition but have a much smaller number of images.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Evaluation set and metrics}\n",
      "\n",
      "We use four common evaluation datasets for landmark image retrieval: Oxford5k (\\oxf5k)~\\cite{Philbin01}, Paris6k (\\paris6k)~\\cite{Philbin02}, as well as Revisited Oxford (\\roxf~or \\rox) and Paris (\\rpar~or \\rpa)~\\cite{RITAC18}. \\roxf~and \\rpar~are used with and without one million distractors (\\r1m)~\\cite{Ng01} and evaluated using the Medium and Hard protocols~\\cite{RITAC18}. We evaluate using \\emph{mean Average Precision} (mAP) and \\emph{mean precision at} 10 (mP@10).\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\subsection{Implementation details}\n",
      "\n",
      "We train on 8 TITAN RTX 2080Ti GPUs. All models are pre-trained on ImageNet~\\cite{Russakovsky01} and implemented in PyTorch \\cite{Paszke01}. For fair comparisons, we set a training environment similar to the those  of compared studies~\\cite{Yokoo01, Weyand01, Ng01, RITAC18}. We employ ResNet101~\\cite{Zhang01} as a backbone model. The kernel size $k$ of ECANet in \\autoref{sec:local} is set to 3. The parameter $p$ of GeM in \\autoref{sec:embed} is set to 3 and the dimension $d$ of final embeddings to 512. We adopt ArcFace~\\cite{Deng01}, a cosine-softmax based loss, with a margin of 0.3. We use stochastic gradient descent with initial learning rate $10^{-3}$, momentum 0.9 and weight decay $10^{-5}$.\n",
      "\n",
      "We adopt the batch sampling of Yokoo \\etal~\\cite{Yokoo01} where mini-batch samples with similar aspect ratios are resized to a particular size. Here, we use a batch size of 64. For image augmentation, we apply scaling, random cropping, and varied illumination. At inference, we apply a multi-resolution representation~\\cite{Gordo01} to query and database images.\n",
      "\n",
      "Our method is denoted as GLAM (\\emph{global-local attention module}). Using the backbone model alone is referred to as \\emph{baseline}. It is compatible with recent models based on ResNet101-GeM trained with ArcFace~\\cite{Weyand01, Ng01}. Adding our local attention (\\autoref{sec:local}) to the baseline model is denoted \\emph{+local}, while adding our global attention (\\autoref{sec:global}) is denoted \\emph{+global}. Since we focus on representation learning, we do not consider post-processing methods like geometry-based re-ranking \\cite{Noh01, simeoni2019local, Weyand01} or graph-based re-ranking~\\cite{Donoser01, iscen2017efficient, Yang01}.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\small\n",
      "\\begin{tabular}{lcc} \\toprule\n",
      "\\Th{Train Set} & \\Th{\\#Images} & \\Th{\\#Classes} \\\\ \\midrule\n",
      "NC-noisy &  213,678 & 672 \\\\\n",
      "NC-clean & 27,965 & 581 \\\\\n",
      "SfM-120k & 117,369 & 713 \\\\\n",
      "GLDv1-noisy & 1,225,029  &  14, 951 \\\\\n",
      "GLDv2-noisy & 4,132,914  &  203,094 \\\\\n",
      "GLDv2-clean & 1,580,470 & 81,313 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Statistics of different training sets.}\n",
      "\\label{tab:table1}\n",
      "\\end{table}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\\subsection{Benchmarking}\n",
      "\\label{sec:SOTA}\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\scriptsize\n",
      "\\setlength{\\tabcolsep}{0.6pt}\n",
      "\\begin{tabular}{l*{8}{c}} \\toprule\n",
      "\\mr{2}{\\Th{Method}} & \\mr{2}{\\Th{Train Set}} & \\mr{2}{\\Th{dim}} & \\mr{2}{\\Th{Oxf5k}} & \\mr{2}{\\Th{Par6k}} & \\mc{2}{\\Th{$\\cR$Medium}} & \\mc{2}{\\Th{$\\cR$Hard}} \\\\ \\cmidrule(l){6-9}\n",
      " & & & & & \\rox & \\rpa & \\rox & \\rpa \\\\ \\midrule\n",
      "GeM-Siamese \\cite{Radenovic01, RITAC18} & SfM-120k    & 2048 & 87.8 & 92.7 & 64.7 & 77.2 & 38.5 & 56.3 \\\\\n",
      "SOLAR~\\cite{Ng01}                       & GLDv1-noisy & 2048 & --   & --   & 69.9 & 81.6 & 47.9 & 64.5 \\\\\n",
      "GLDv2~\\cite{Weyand01}                   & GLDv2-clean & 2048 & --   & --   & 74.2 & 84.9 & 51.6 & 70.3 \\\\\n",
      "\\midrule\n",
      "GLAM (Ours) &  NC-clean & 512 &  77.8 & 85.8 & 51.6 &  68.1 &  20.9 & 44.7 \\\\\n",
      " &  GLDv1-noisy & 512 & 92.8 & 95.0 & \\ok{\\tb{73.7}}  &  \\ok{\\tb{83.5}}  &  \\ok{\\tb{49.8}} & \\ok{\\tb{69.4}} \\\\\n",
      " &  GLDv2-noisy & 512 & 93.3 & 95.3 & 75.7 & 86.0 & 53.1 & 73.8 \\\\\n",
      " &  GLDv2-clean & 512 & \\red{\\tb{94.2}} & \\red{\\tb{95.6}} & \\red{\\tb{78.6}} & \\red{\\tb{88.5}} & \\red{\\tb{60.2}} & \\red{\\tb{76.8}} \\\\ \\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{mAP comparison of our best model (baseline+local+global) trained on different \\emph{training sets} against \\cite{Weyand01,Ng01}. All models use ResNet101-GeM. Red: best results. Blue: GLAM higher than SOLAR~\\cite{Ng01} on GLDv1-noisy.}\n",
      "\\label{tab:table11}\n",
      "\\end{table}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{table*}\n",
      "\\centering\n",
      "\\scriptsize\n",
      "\\setlength{\\tabcolsep}{2pt}\n",
      "\\begin{tabular}{l|cc|cc|cccccccc|cccccccc} \\toprule\n",
      "\t\\mr{3}{\\Th{Method}} & \\mr{3}{\\Th{Train Set}} & \\mr{3}{\\Th{Dim}} & \\mca{2}{c|}{\\Th{Base}} & \\mca{8}{c|}{\\Th{Medium}} & \\mc{8}{\\Th{Hard}} \\\\\n",
      "\t                                                   &             &          & \\oxf5k & \\paris6k & \\mc{2}{\\rox} & \\mc{2}{+\\r1m} & \\mc{2}{\\rpa} & \\multicolumn{2}{c|}{+\\r1m} & \\mc{2}{\\rox} & \\mc{2}{+\\r1m} & \\mc{2}{\\rpa} & \\mc{2}{+\\r1m} \\\\\n",
      "\t                                                   &             &          & mAP &  mAP & mAP & mP & mAP & mP & mAP & mP & mAP & mP & mAP & mP & mAP & mP & mAP & mP & mAP & mP \\\\ \\midrule\n",
      "\tSPoC-V16 \\cite{Babenko03, RITAC18}                 & [O]         & 512      & 53.1$^*$ & -- & 38.0 & 54.6 & 17.1 & 33.3 & 59.8 & 93.0 & 30.3 & 83.0 & 11.4 & 20.9 & 0.9 & 2.9 & 32.4 & 69.7 & 7.6 & 30.6 \\\\\n",
      "\tSPoC-R101 \\cite{RITAC18}                           & [O]         & 2048     & -- & -- & 39.8 & 61.0 & 21.5 & 40.4 & 69.2 & 96.7 & 41.6 & 92.0 & 12.4 & 23.8 & 2.8 & 5.6 & 44.7 & 78.0 & 15.3 & 54.4 \\\\\n",
      "\tCroW-V16 \\cite{Kalantidis01,RITAC18}               & [O]         & 512      & 70.8 & 79.7 & 41.4 & 58.8 & 22.5 & 40.5 & 62.9 & 94.4 & 34.1 & 87.1 & 13.9 & 25.7 & 3.0 & 6.6 & 36.9 & 77.9 & 10.3 & 45.1 \\\\\n",
      "\tCroW-R101 \\cite{RITAC18}                           & [O]         & 2048     & -- & -- & 42.4 & 61.9 & 21.2 & 39.4 & 70.4 & 97.1 & 42.7 & 92.9 & 13.3 & 27.7 & 3.3 & 9.3 & 47.2 & 83.6 & 16.3 & 61.6 \\\\\n",
      "\tMAC-V16-Siamese \\cite{Radenovi01, RITAC18}         & [O]         & 512      & 80.0 & 82.9 & 37.8 & 57.8  & 21.8  & 39.7 & 59.2 & 93.3  & 33.6 & 87.1 & 14.6  & 27.0  & 7.4  & 11.9 & 35.9  & 78.4  & 13.2 & 54.7 \\\\\n",
      "\tMAC-R101-Siamese \\cite{RITAC18}                    & [O]         & 2048     & -- & -- & 41.7 & 65.0 & 24.2 & 43.7 & 66.2 & 96.4 & 40.8 & 93.0 & 18.0 & 32.9 & 5.7 & 14.4 & 44.1 & 86.3 & 18.2 & 67.7 \\\\\n",
      "\tRMAC-V16-Siamese \\cite{Radenovi01, RITAC18}        & [O]         & 512      & 80.1 & 85.0 & 42.5 & 62.8 & 21.7 & 40.3 & 66.2 & 95.4 & 39.9 & 88.9 & 12.0 & 26.1 & 1.7 & 5.8 & 40.9 & 77.1 & 14.8 & 54.0 \\\\\n",
      "\tRMAC-R101-Siamese \\cite{RITAC18}                   & [O]         & 2048     & -- & -- & 49.8 & 68.9 &  29.2 &  48.9 &  74.0 &  97.7 &  49.3 &  93.7 &  18.5 & 32.2 &  4.5 &  13.0 &  52.1 &  87.1 &  21.3 &  67.4 \\\\\n",
      "\tRMAC-R101-Triplet \\cite{Gordo01, RITAC18}          & NC-clean    & 2048     & 86.1 & \\tb{94.5} & 60.9 & 78.1 & 39.3 & 62.1 & 78.9 & 96.9 & 54.8 & 93.9 & 32.4 & 50.0 & 12.5 & 24.9 & 59.4 & 86.1 & 28.0 & 70.0 \\\\\n",
      "\tGeM-R101-Siamese \\cite{Radenovic01, RITAC18}       & SfM-120k    & 2048     & \\tb{87.8} & 92.7 & 64.7 & 84.7 & 45.2 & 71.7 & 77.2 & \\red{\\tb{98.1}} & 52.3 & \\red{\\tb{95.3}} & 38.5 & 53.0 & 19.9 & 34.9 & 56.3 & 89.1 & 24.7 & 73.3 \\\\\n",
      "\tAGeM-R101-Siamese \\cite{gu2018attention}           & SfM-120k    & 2048     & -- & -- & 67.0 & -- & -- & -- & 78.1 & --  & -- & -- & 40.7 & -- & -- & -- & 57.3 & -- & -- & -- \\\\\n",
      "\tSOLAR-GeM-R101-Triplet/SOS \\cite{Ng01}             & GLDv1-noisy & 2048     & -- & -- & 69.9 & \\tb{86.7} & 53.5 & \\tb{76.7} & 81.6  & 97.1 & 59.2 & 94.9 & 47.9 & \\tb{63.0} & 29.9 & \\tb{48.9} &  64.5 & \\tb{93.0} & 33.4 & \\tb{81.6} \\\\\n",
      "\tDELG-GeM-R101-ArcFace \\cite{ECCV2020_912}          & GLDv1-noisy & 2048     & -- & -- & 73.2 & -- & \\tb{54.8} & -- & 82.4 & --  & \\tb{61.8} & -- & 51.2 & -- & \\tb{30.3} & -- & 64.7 & -- & \\tb{35.5} & -- \\\\\n",
      "\tGeM-R101-ArcFace \\cite{Weyand01}                   & GLDv2-clean & 2048     & -- & -- & \\tb{74.2} & -- & -- & -- & \\tb{84.9} & --  & -- & -- &  \\tb{51.6}  & -- & -- & -- &  \\tb{70.3} & -- & -- & -- \\\\\n",
      "\t\\midrule\n",
      "\tGLAM-GeM-R101-ArcFace baseline   & GLDv2-clean & 512 & \\ok{\\tb{91.9}} & \\ok{\\tb{94.5}} & 72.8 & \\ok{\\tb{86.7}} &  \\ok{\\tb{58.1}} & \\ok{\\tb{78.2}} & 84.2 & 95.9 &  \\ok{\\tb{63.9}} &  93.3 &  49.9 & 62.1 & \\ok{\\tb{31.6}} & \\ok{\\tb{49.7}} & 69.7 & 88.4 & \\ok{\\tb{37.7}} & 73.7  \\\\\n",
      "\t+local                           & GLDv2-clean & 512 & \\ok{\\tb{91.2}} & \\ok{\\tb{95.4}} & 73.7 & \\ok{\\tb{86.2}} & \\ok{\\tb{60.5}} & \\ok{\\tb{77.4}} & \\ok{\\tb{86.5}} & 95.6 & \\ok{\\tb{68.0}} & 93.9  &  \\ok{\\tb{52.6}} & \\ok{\\tb{65.3}} & \\ok{\\tb{36.1}} & \\ok{\\tb{55.6}} & \\ok{\\tb{73.7}} & 89.3 & \\ok{\\tb{44.7}} & 79.1  \\\\\n",
      "\t+global                          & GLDv2-clean & 512 & \\ok{\\tb{92.3}} & \\ok{\\tb{95.3}} & \\ok{\\tb{77.2}} & \\ok{\\tb{87.0}} & \\ok{\\tb{63.8}} & \\ok{\\tb{79.3}} & \\ok{\\tb{86.7}} & 95.4 & \\ok{\\tb{67.8}} & 93.7  & \\ok{\\tb{57.4}} & \\ok{\\tb{69.6}} & \\ok{\\tb{38.7}} & \\ok{\\tb{57.9}} & \\ok{\\tb{75.0}} & 89.4 & \\ok{\\tb{45.0}} & 77.0  \\\\\n",
      "\t+global+local                    & GLDv2-clean & 512 & \\red{\\tb{94.2}} & \\red{\\tb{95.6}} & \\red{\\tb{78.6}} & \\red{\\tb{88.2}} & \\red{\\tb{68.0}} & \\red{\\tb{82.4}} & \\red{\\tb{88.5}}  & 97.0 &  \\red{\\tb{73.5}} & 94.9 & \\red{\\tb{60.2}} & \\red{\\tb{72.9}} & \\red{\\tb{43.5}} & \\red{\\tb{62.1}} & \\red{\\tb{76.8}} & \\red{\\tb{93.4}} & \\red{\\tb{53.1}} & \\red{\\tb{84.0}} \\\\\n",
      "\t\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16: VGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). $^*$: dimension $d=256$~\\cite{Babenko03}. mP: mP@10. Red: best results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand \\etal~\\cite{Weyand01} is the only model other than ours trained on GLDv2-clean, while~\\cite{Ng01} is trained on GLDv1-noisy and compared in \\autoref{tab:table11}.}\n",
      "\\label{tab:table_exp_all}\n",
      "\\end{table*}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Noisy \\vs clean training sets}\n",
      "\n",
      "We begin by training our best model (baseline+local+global)\n",
      "on all training sets of \\autoref{tab:table1}, except NC-noisy because some images are currently unavailable. As shown in \\autoref{tab:table11}, even though GLDv2-noisy has 2.6 times more images than GLDv2-clean, the latter is superior by a large margin. This shows that, in training, a cleaner dataset can be more important than a larger one. By contrast, NC-clean has the worst performance despite being clean, aparently because it is too small. To achieve best possible performance, we use GLDv2-clean as a training set in the remaining experiments.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Comparisons on same training set}\n",
      "\n",
      "It is common to compare methods regardless of training sets as more become available, \\eg,~\\cite{RITAC18, Ng01}. Since GLDv2-clean is relatively new, Weyand \\etal~\\cite{Weyand01}, which introduced the dataset, is the only study that has trained the same backbone with the same settings (ResNet101-GeM with ArcFace) on GLDv2-clean. Our baseline is lower than~\\cite{Weyand01}, because our dimensinality is 512, while other models based on ResNet101 use 2048. Yet, \\autoref{tab:table11} shows that our best model trained on GLDv2-clean outperforms~\\cite{Weyand01} by a large margin. But the most important comparison is with SOLAR~\\cite{Ng01}, also based on self-attention, which has trained ResNet101-GeM on GLDv1-noisy. On this training set, our best model clearly outperforms~\\cite{Ng01} despite lower dimensionality.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Comparison with state of the art}\n",
      "\n",
      "\\autoref{tab:table_exp_all} shows the performance of four variants of our model, \\ie baseline with or without local/global attention, and compares them against state-of-the-art (SOTA) methods based on global descriptors without re-ranking on the complete set of benchmarks, including distractors. Both local and global attention bring significant gain over the baseline. The effect of global is stronger, while the gain of the two is additive in the combination. The best results are achieved by the global-local attention network (baseline+global+local). With this model, we outperform previous best methods on most benchmarks except mP@10 on \\rpar~(medium) and \\rpar$+$\\r1m~(medium), where we are outperformed by~\\cite{Radenovic01, RITAC18}. These results demonstrate that our approach is effective for landmark image retrieval. \\autoref{fig:fig8} shows some examples of our ranking results.\n",
      "\n",
      "\\subsection{Ablation study}\n",
      "\\label{sec:Ablation}\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{1.5pt}\n",
      "\\begin{tabular}{l*{6}{c}} \\toprule\n",
      "\\mr{2}{\\Th{Method}} & \\mr{2}{\\Th{Oxf5k}} & \\mr{2}{\\Th{Par6k}} & \\mc{2}{\\Th{$\\cR$Medium}} & \\mc{2}{\\Th{$\\cR$Hard}} \\\\ \\cmidrule(l){4-7}\n",
      " & & & \\rox & \\rpa & \\rox & \\rpa \\\\ \\midrule\n",
      "GLAM baseline  & 91.9 & 94.5 & 72.8 &  84.2 & 49.9 & 69.7 \\\\\n",
      "+local-channel & 91.3 & 95.3 & 72.2 & 85.8 & 48.3 & 73.1 \\\\\n",
      "+local-spatial & 91.0 & 95.1 & 72.1 & 85.3 & 48.3 &  71.9 \\\\\n",
      "+local & 91.2 & 95.4 & 73.7 & 86.5 & 52.6 & 75.0 \\\\\n",
      "+global-channel & 92.5 & 94.4 & 73.3 & 84.4 & 49.8 & 70.1 \\\\\n",
      "+global-spatial & 92.4 & 95.1 & 73.2 & 86.3 & 50.0 & 72.7 \\\\\n",
      "+global & 92.3 & 95.3 & 77.2 & 86.7 & 57.4 & 75.0 \\\\\n",
      "+global+local  & \\tb{94.2} & \\tb{95.6} & \\tb{78.6}  & \\tb{88.5}  & \\tb{60.2} & \\tb{76.8} \\\\ \\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{mAP comparison of spatial and channel variants of our local (+local, \\autoref{sec:local}) and global (+global, \\autoref{sec:local}) attention modules to the baseline.}\n",
      "\\label{tab:table10}\n",
      "\\end{table}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{3.4pt}\n",
      "\\begin{tabular}{l*{6}{c}} \\toprule\n",
      "\\mr{2}{\\Th{Method}} & \\mr{2}{\\Th{Oxf5k}} & \\mr{2}{\\Th{Par6k}} & \\mc{2}{\\Th{$\\cR$Medium}} & \\mc{2}{\\Th{$\\cR$Hard}} \\\\ \\cmidrule(l){4-7}\n",
      " & & & \\rox & \\rpa & \\rox & \\rpa \\\\ \\midrule\n",
      "CBAM style  & 93.8 & \\tb{95.7} & 75.6 &  88.4 & 53.3 & \\tb{76.8} \\\\\n",
      "GLAM (Ours)  & \\tb{94.2} & 95.6 & \\tb{78.6}  & \\tb{88.5}  & \\tb{60.2} & \\tb{76.8} \\\\ \\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{mAP comparison between CBAM style and our local spatial attention.}\n",
      "\\label{tab:table7}\n",
      "\\end{table}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{4.2pt}\n",
      "\\begin{tabular}{l*{6}{c}} \\toprule\n",
      "\\mr{2}{\\Th{Method}} & \\mr{2}{\\Th{Oxf5k}} & \\mr{2}{\\Th{Par6k}} & \\mc{2}{\\Th{$\\cR$Medium}} & \\mc{2}{\\Th{$\\cR$Hard}} \\\\ \\cmidrule(l){4-7}\n",
      " & & & \\rox & \\rpa & \\rox & \\rpa \\\\ \\midrule\n",
      "Concatenate  & 89.5 & 95.1 & 73.6 & 86.5 & 54.0 & 73.7 \\\\\n",
      "Sum (Ours) & \\tb{94.2} & \\tb{95.6} & \\tb{78.6}  & \\tb{88.5}  & \\tb{60.2} & \\tb{76.8} \\\\ \\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{mAP comparison between weighted concatenation and weighted average for feature fusion.}\n",
      "\\label{tab:table5}\n",
      "\\end{table}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{2.5pt}\n",
      "\\begin{tabular}{l*{6}{c}} \\toprule\n",
      "\\mr{2}{\\Th{Method}} & \\mr{2}{\\Th{Oxf5k}} & \\mr{2}{\\Th{Par6k}} & \\mc{2}{\\Th{$\\cR$Medium}} & \\mc{2}{\\Th{$\\cR$Hard}} \\\\ \\cmidrule(l){4-7}\n",
      " & & & \\rox & \\rpa & \\rox & \\rpa \\\\ \\midrule\n",
      "Fixed-size  & 76.1 & 82.6 & 55.7 & 68.4 & 29.2 & 47.5 \\\\\n",
      "Group-size (Ours) & \\tb{94.2} & \\tb{95.6} & \\tb{78.6}  & \\tb{88.5}  & \\tb{60.2} & \\tb{76.8} \\\\ \\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{mAP comparison between fixed-size ($224 \\times 224$) and group-size sampling methods.}\n",
      "\\label{tab:table4}\n",
      "\\end{table}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{1.9pt}\n",
      "\\begin{tabular}{l*{7}{c}} \\toprule\n",
      "\\mr{2}{\\Th{Query}} & \\mr{2}{\\Th{Database}} & \\mr{2}{\\Th{Oxf5k}} & \\mr{2}{\\Th{Par6k}} & \\mc{2}{\\Th{$\\cR$Medium}} & \\mc{2}{\\Th{$\\cR$Hard}} \\\\ \\cmidrule(l){5-8}\n",
      " & & & & \\rox & \\rpa & \\rox & \\rpa \\\\ \\midrule\n",
      "Single & Single & 93.3 & 95.2 & 76.9 & 87.1 & 58.6 & 74.7 \\\\\n",
      "Multi & Single & 93.9 & 95.4 & 78.0 & 87.7 & 59.0 & 75.5 \\\\\n",
      "Single & Multi & 93.6 & \\tb{95.6} & 77.0 & 87.8 & 57.1 & 76.0 \\\\\n",
      "Multi & Multi & \\tb{94.2} & \\tb{95.6} & \\tb{78.6}  & \\tb{88.5}  & \\tb{60.2} & \\tb{76.8} \\\\ \\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{mAP comparison of using multiresolution representation (Multi) or not (Single) on query or database.}\n",
      "\\label{tab:table6}\n",
      "\\end{table}\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "Our ablation study uses the Google Landmark v2 clean dataset (GLDv2-clean)~\\cite{Weyand01} for training, which is shown to be the most effective in \\autoref{tab:table11}.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Effect of attention modules}\n",
      "\n",
      "We ablate the effect of our local and global attention networks as well as their combination. \\autoref{tab:table10} shows the results, which are more fine-grained than those of \\autoref{tab:table_exp_all}. In particular, it shows the effect of the channel and spatial variants of both local and global attention. We observe that, when used alone, the channel and spatial variants of local attention are harmful in most cases. Even the combination, baseline+local, is not always effective. By contrast, when used alone, the channel and spatial variants of global attention are mostly beneficial, especially the latter. Their combination, baseline+global, is impressive, bringing gain of up to 7.5\\%. Importantly, the combination baseline+global+local improves further by up to another 2.8\\%. This result shows the necessity of local attention in the final model.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{CBAM \\vs our local spatial attention}\n",
      "\n",
      "We experiment with the local spatial attention of CBAM~\\cite{woo01}. CBAM applies average and max-pooling to input features and concatenates the two for spatial attention. We apply this variant to our local spatial attention module for comparison. For the CBAM style module, we keep the overall design of our module as shown in \\autoref{fig:fig3}, but apply average and max-pooling to each of the four convolutional layer outputs before concatenation. \\autoref{tab:table7} shows that the CBAM style module is considerably worse than ours on all benchmarks except Paris6k, where it is only slightly better.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Concatenation \\vs sum for feature fusion}\n",
      "\n",
      "We use a softmax-based weighted average of local and global attention feature maps with the original feature map~\\eq{eq10}. Here, we compare this weighted average with weighted concatenation, where concatenation replaces the sum operation in~\\eq{eq10}. As shown in \\autoref{tab:table5}, the weighted average outperforms the weighted concatenation.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Fixed-size \\vs group-size sampling}\n",
      "\n",
      "Numerous studies have proposed methods for constructing batches according to image size for efficient training. For instance, Gordo \\etal~\\cite{Gordo01}, DELF~\\cite{Noh01}, and Yokoo~\\etal\\cite{Yokoo01} employed different image sizes per batch for training instead of a single fixed size. We adopt the method of Yokoo \\etal, which constructs a batch with images of similar aspect ratio, so that the images can be resized to a size with an aspect ratio that is similar to their own. We call this method \\emph{group-size sampling}. \\autoref{tab:table4} compares fixed-size ($224 \\times 224$) with group-size sampling. We observe that maintaining aspect ratios by using dynamic input sizes is much more effective.\n",
      "\n",
      "%------------------------------------------------------------------------------\n",
      "\n",
      "\\paragraph{Multi-resolution}\n",
      "\n",
      "We use the multi-resolution representation~\\cite{Gordo01} for the final feature of an image at inference time. This method: (1) resizes an image into multiple scales; (2) extracts features from the resized images; and (3) averages the features to obtain the final feature of the image. The method is applied to both query and database images to enhance ranking results, especially for small target objects. \\autoref{tab:table6} compares the four cases of applying this method or not to query or database images.\n",
      "\n",
      "\\section{Conclusion}\n",
      "\\label{sec:conclusion}\n",
      "\n",
      "We have introduced a novel approach that extracts global and local contextual information using attention mechanisms for instance-level image retrieval. It is manifested as a network architecture consisting of global and local attention components, each operating on both spatial and channel dimensions. This constitutes a comprehensive study and empirical evaluation of all four forms of attention that have previously been studied only in isolation. Our findings indicate that the gain (or loss) brought by one form of attention alone strongly depends on the presence of the others, with the maximum gain appearing when all forms are present. The output is a modified feature tensor that can be used in any way, for instance with local feature detection instead of spatial pooling for image retrieval.\n",
      "\n",
      "With the advent of \\emph{vision transformers}~\\cite{dosovitskiy2020image,2101.11986} and their recent application to image retrieval~\\cite{2102.05644}, attention is expected to play a more and more significant role in vision. According to our classification, transformers perform global spatial attention alone. It is of great interest to investigate the role of the other forms of attention, where our approach may yield a basic building block of such architectures. One may even envision an extension to language models, where transformers originate from~\\cite{VSP+17}.\n",
      "\n",
      "\n",
      "{\\small\n",
      "\\bibliographystyle{ieee_fullname}\n",
      "\\bibliography{main}\n",
      "}\n",
      "\n",
      "\\end{document}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(complete_document)\n",
    "\n",
    "with open('output.txt', 'w') as f:\n",
    "    f.write(complete_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sections_from_string(tex_content):\n",
    "    def remove_latex_commands(text):\n",
    "        # Regular expression to match LaTeX commands\n",
    "        func_regex = r'\\\\[a-zA-Z]+(\\[[^\\]]*\\])?(\\{[^\\}]*\\})?'\n",
    "        return re.sub(func_regex, '', text)\n",
    "\n",
    "    def remove_tables(text):\n",
    "        # Remove content between \\begin{table} and \\end{table}\n",
    "        regex = r'\\\\begin\\{table\\}.*?\\\\end\\{table\\}'\n",
    "        return re.sub(regex, '', text, flags=re.DOTALL)\n",
    "\n",
    "    def remove_comments(text):\n",
    "        # Remove content between \\begin{comment} and \\end{comment}\n",
    "        regex = r'%-*'\n",
    "        return re.sub(regex, '', text)\n",
    "\n",
    "    lines = tex_content.split('\\n')\n",
    "\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    section_content = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Check for a new section, subsection, or paragraph\n",
    "        match = re.match(r'\\\\(sub)*section{(.+?)}|\\\\paragraph{(.+?)}', line)\n",
    "        if match:\n",
    "            # Save the previous section\n",
    "            if current_section is not None:\n",
    "                content = ''.join(section_content).strip()\n",
    "                content = remove_latex_commands(content)\n",
    "                content = remove_tables(content)\n",
    "                content = remove_comments(content)\n",
    "                sections[current_section] = content\n",
    "                section_content = []\n",
    "\n",
    "            # Extract the new section name\n",
    "            current_section = match.group(2) or match.group(3)\n",
    "        elif current_section is not None:\n",
    "            section_content.append(line)\n",
    "\n",
    "    # Add the last section\n",
    "    if current_section is not None:\n",
    "        content = ''.join(section_content).strip()\n",
    "        content = remove_latex_commands(content)\n",
    "        content = remove_tables(content)\n",
    "        content = remove_comments(content)\n",
    "        sections[current_section] = content\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Introduction': ' (GLAM) involves both {channel} and {spatial} attention, as well as both {local} attention (channels/locations weighted independently, based on contextual information obtained by pooling) and {global} attention (based on pairwise interaction between channels/locations). As a result, four attention maps are used:  ($_c^l$),  ($_s^l$),  ($_c^g$) and  ($_s^g$). The input feature map $$ is weighted into local ($^l$) and global ($^g$) attention feature maps, which are fused with $$ to yield the  $^{gl}$. The diagram is abstract: The four attention modules are shown in more detail in Figures , , , .}Instance-level image retrieval is at the core of visual representation learning and is connected with many problems of visual recognition and machine learning, for instance ~, ~ and ~. Many large-scale open datasets~, and competitions have accelerated progress in instance-level image retrieval, which has been transformed by deep learning~.Many studies on instance-level image retrieval focus on learning features from  (CNN), while others focus on , for instance by graph-based methods~. The former can be distinguished according to feature types: , reminiscent of SIFT~, where an image is mapped to a few hundred vectors; and , where an image is mapped to a single vector. In fact, deep learning has brought global descriptors with astounding performance, while allowing efficient search. Our study belongs to this type.Studies on global descriptors have focused on ~. The need for compact, discriminative representations that are resistant to clutter has naturally given rise to  methods~. Different kinds of attention have been studied in many areas of computer vision research. There is also ~; , applied independently to elements of the representation (feature map)~; , based on interaction between elements~; and combinations thereof. Unfortunately, each study has been limited to one or two kinds of attention only; attention is not always learned; and applications vary.It is the objective of our work to perform a comprehensive study of all forms of attention above, apply them to instance-level image retrieval and provide a detailed account of their interaction and impact on performance. As shown in , we collect contextual information from images with both  and  attention, giving rise to two parallel network streams. Importantly, each operates on both  and . Local attention is about individual locations and channels; global is about interaction between locations and between channels. The extracted information is separately embedded in local and global attention feature maps, which are combined in a  before pooling.Our contributions can be summarized as follows:[itemsep=2pt, parsep=0pt, topsep=0pt]\\t  We propose a novel network that consists of both global and local attention for image retrieval. This is the first study that employs both mechanisms.\\t  Each of the global and local attention mechanisms comprises both spatial and channel attention.\\t  Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.',\n",
       " 'Related work': '',\n",
       " 'Instance-level image retrieval': 'Studies on instance-level image retrieval can be roughly, but not exclusively, divided into three types: (1) studies on  ; (2) studies on  and geometry-based re-ranking ; (3)  by graph-based methods .The first two types of studies focus on the feature representation, while the last type focuses on re-ranking extracted features.Studies on global descriptors focus on  of CNN feature maps into vectors, including MAC~, SPoC~, CroW~, R-MAC~, GeM~, and NetVLAD~, as well as ~. Studies before deep learning dominated image retrieval were mostly based on  like SIFT~ and  representation~ or aggregated descriptors like VLAD~ or ASMK~. Local descriptors have been revived in deep learning,  with DELF~, DELG~ and ASMK extensions~.{2.6pt}{lcccccc} \\t {}                          & {}    & {} & {} & {} \\\\\\\\ \\t                                              & Spatial   & Channel   & Spatial  & Channel  &                  &                  \\\\\\\\ \\t SENet~                            &           &        &          &          &               &                  \\\\\\\\\\t ECA-Net~                        &           &        &          &          &               &                  \\\\\\\\\\t GCNet~                           &           &        &          &          &               &                  \\\\\\\\\\t CBAM~                            &        &        &          &          &               &                  \\\\\\\\\\t GE~                           &        &           &          &          &               &                  \\\\\\\\\\t NL-Net~                         &           &           &       &          &               &                  \\\\\\\\\\t AA-Net~                &           &           &       &          &               &                  \\\\\\\\\\t SAN~                 &           &           &       &          &               &                  \\\\\\\\\\t N$^3$Net~              &           &           &       &          &               &                  \\\\\\\\\\t A$^2$-Net~                  &           &           &          &       &               &                  \\\\\\\\\\t GSoP~                    &           &           &          &       &               &                  \\\\\\\\ \\t OnA~                       &        &           &          &          &                  &               \\\\\\\\\\t AGeM~                  &        &           &          &          &                  &               \\\\\\\\\\t CroW~                     &        &        &          &          &                  &               \\\\\\\\\\t CRN~                             &        &           &          &          &               &               \\\\\\\\\\t DELF~                            &        &           &          &          &               &               \\\\\\\\\\t DELG~                     &        &           &          &          &               &               \\\\\\\\\\t Tolias ~       &        &           &          &          &               &               \\\\\\\\\\t SOLAR~                            &           &           &       &          &               &               \\\\\\\\ \\t                                     &        &        &       &       &               &               \\\\\\\\ We focus on learning a global descriptor in this work, because it is the most efficient in terms of storage and search. However, our generic attention mechanism produces a feature tensor and could be applicable to local descriptors as well, if global pooling were replaced by local feature detection. Re-ranking methods are complementary to the representation and we do not consider them in this work.',\n",
       " 'Attention': 'Attention mechanisms have been first proposed in  studies focusing on ~, ~ or both, like CBAM~. In , CroW  also  employs both spatial and channel attention and can be seen as a precursor of CBAM, but, like other studies of spatial attention on retrieval~, it is not learned. CRN~ applies spatial attention for feature reweighting and is learned. Learned spatial attention mechanisms are common for local descriptors~.We call the above methods , in the sense that elements of the feature tensor (channels / spatial locations), are weighted independently, based on contextual information obtained by pooling or learned. By constrast, by  we refer to mechanisms that model interaction between elements of the feature tensor, for example between channels or between locations.In ,  (NLNet)~ is maybe the first global attention mechanism, followed by similar studies~. It is global , allowing interaction between any pair of spatial locations. Similarly, there are studies of global , allowing interaction between channels~. Global attention has focused mostly on image recognition and has been applied to either spatial or channel attention so far, not both. In , SOLAR~ is a direct application of the global spatial attention mechanism of~. attempts to categorize related work on attention according to whether attention is local or global, spatial or channel, whether it is learned and whether it is applied to instance-level image retrieval. We observe that all methods limit to one or two forms of attention only. Of those studies that focus on image retrieval, many are not learned~, and of those that are, some are designed for local descriptors~.By contrast, we provide a comprehensive study of  of attention, global and local, spatial and channel, to obtain a learned representation in the form of a tensor that can be used in any way. We spatially pool it into a global descriptor and we study the relative gain of different forms of attention in image retrieval.',\n",
       " 'Global-local attention': '',\n",
       " 'Local attention': 'We extract an 1D channel and a 2D spatial attention map to weigh the feature map in the corresponding dimensions.',\n",
       " 'Local channel attention': 'Following ECA-Net~, this attention captures local channel information. As shown in , we are given a $c h w$ feature tensor $$ from our backbone. We first reduce it to a $c  1  1$ tensor by  (GAP). Channel attention is then captured by a 1D convolution of kernel size $k$ along the channel dimension, where $k$ controls the extent of cross-channel interaction. This is followed by a sigmoid function, resulting in the $c 1 1$  $_c^l$.',\n",
       " 'Local spatial attention': \"Inspired by the inception module~ and similar to~, this attention map captures local spatial information at different scales. As shown in ,given the same $c h w$ feature tensor $$ from our backbone, we obtain a new tensor $'$ with channels reduced to $c'$, using a ${1  1}$ convolution. We then extract local spatial contextual information using convolutional filters of kernel size ${3 3}$, ${5 5}$, and ${7 7}$, which are efficiently implemented by ${3 3}$ dilated convolutions~ with dilation parameter 1, 2, and 3 respectively. The resulting features, along with one obtained by ${1 1}$ convolution on $'$, are concatenated into a $4c'  h  w$ tensor. Finally, we obtain the $1  h  w$  $_s^l$ by a ${1 1}$ convolution that reduces the channel dimension to $1$.The middle column of  shows heat maps of local spatial attention, localizing target objects in images.\",\n",
       " 'Local attention feature map': 'We use the local channel attention map $_c^l$ to weigh $$ in the channel dimension\\t_c^l    _c^l + .We then use local spatial attention map $_s^l$ to weigh $_c^l$ in the spatial dimensions, resulting in the $c  h  w$ \\t^l = _c^l  _s^l + _c^l.Here, $  $ denotes an element-wise multiplication of tensors $$ and $$, with broadcasting when one tensor is smaller. We adopt the choice of applying channel followed by spatial attention from  CBAM~. However, apart from computing $_s^l$ at different scales, both attention maps are obtained from the original tensor $$ rather than sequentially. In addition, both~ and~ include residual connections, while CBAM includes a single residual connection over both steps.',\n",
       " 'Global attention': 'We extract two matrices capturing global pairwise channel and spatial interaction to weigh the feature map.',\n",
       " 'Global channel attention': 'We introduce a  mechanism that captures global channel interaction. This mechanism is based on the non-local neural network~, but with the idea of 1D convolution from ECA-Net~. As shown in , we are given the $c h w$ feature tensor $$ from our backbone. We apply GAP and squeeze spatial dimensions, followed by a 1D convolution of kernel size $k$ and a sigmoid function, to obtain $1  c$  $_c$ and  $_c$ tensors. The  tensor $_c$ is obtained by mere reshaping of $$ to $hw  c$, without GAP. Next, we form the outer product of $_c$ and $_c$, followed by softmax over channels to obtain a $c  c$ \\t_c^g = ({_c} _c).Finally, this attention map is multiplied with $_c$ and the matrix product $_c _c^g$ is reshaped back to $c  h  w$ to give the  $_c$. In GSoP~ and A$^2$-Net~, a $c  c$ global channel attention map is obtained by multiplication of $hw  c$ matrices;  is more efficient, using only an outer product of $1  c$ vectors.',\n",
       " 'Global spatial attention': \"Since ordinary convolution applies only a local neighborhood at a time, it cannot capture global contextual information. Thus, we apply ~, which is a form of ~ in the spatial dimensions. As shown in , we are given the same $c h w$ feature tensor $$ from our backbone. By using three $1 1$ convolutions, which reduce channels to $c'$, and flattening spatial dimensions to $hw$, we obtain $c'  hw$  $_s$,  $_s$, and  $_s$ tensors, where each column is a feature vector corresponding to a particular spatial location. We capture pairwise similarities of these vectors by matrix multiplication of $_s$ and $_s$, followed by softmax over locations to obtain a $hw  hw$ :\\t_s^g = (_s _s).This attention map is multiplied with $_s$ and the matrix product $_s _s^g$ is reshaped back to $c'  h  w$ by expanding the spatial dimensions. Finally, using a ${1 1}$ convolution, which increases channels back to $c$, we obtain the $c  h w$  $_s$.The right column of  shows heat maps for global spatial attention, localizing target objects in images.\",\n",
       " 'Global attention feature map': 'We use the global channel attention feature map $_c$ to weigh $$ element-wise\\t_c^g =   _c.We then use global spatial attention feature map $_s$ to weigh $_c^g$ element-wise, resulting in the $c  h  w$ \\t^g = _c^g  _s + _c^g.Similarly to $^{l}$ in~ and~, we apply channel attention first, followed by spatial attention. However, unlike , there is no residual connection in~. This choice is supported by early experiments.{2pt}[1]{\\t &\\t &\\t \\\\\\\\}{ccc}\\t\\t\\t\\t(a) input &\\t(b) local &\\t(c) global. Left: input images. Middle: local spatial attention heat maps. Right: global spatial attention heat maps. Red (blue) means higher (lower) attention weight.}',\n",
       " 'Feature fusion': 'As shown in , we combine the local and global attention feature maps, $^l$ and $^g$, with the original feature $$. While concatenation and summation are common operations for feature combination, we use a weighted average with weights $w_l$, $w_g$, $w$ respectively, obtained by softmax over three learnable scalar parameters, to obtain a $c  h  w$ \\t^{gl} = w_l ^l + w_g ^l + w .EfficientDet~ has shown that this is the most effective, among a number of choices, for fusion of features across different scales.',\n",
       " 'Pooling': 'We apply GeM~, a learnable spatial pooling mechanism, to feature map $^{gl}$~, followed by a fully-connected (FC) layer with dropout and batch normalization. The final embedding is obtained by $_2$-normalization.',\n",
       " 'Experiments': '',\n",
       " 'Datasets': '',\n",
       " 'Training set': 'There are a number of open landmark datasets commonly used for training in image retrieval studies, including  (NC)~,  (NC-clean)~, as well as Google Landmarks v1 (GLDv1)~ and v2 (GLDv2)~.  shows relevant statistics. These datasets can be categorized into noisy and clean. The clean sets were obtained from the original noisy sets for more effective training~. The original noisy datasets are much larger, but they have high intra-class variability. Each class can include visually dissimilar images such as exterior and interior views of a building or landmark, including floor plans and paintings inside. The clean datasets focus on views directly relevant to landmark recognition but have a much smaller number of images.',\n",
       " 'Evaluation set and metrics': 'We use four common evaluation datasets for landmark image retrieval: Oxford5k (5k)~, Paris6k (6k)~, as well as Revisited Oxford (~or ) and Paris (~or )~. ~and ~are used with and without one million distractors (1m)~ and evaluated using the Medium and Hard protocols~. We evaluate using  (mAP) and  10 (mP@10).',\n",
       " 'Implementation details': 'We train on 8 TITAN RTX 2080Ti GPUs. All models are pre-trained on ImageNet~ and implemented in PyTorch . For fair comparisons, we set a training environment similar to the those  of compared studies~. We employ ResNet101~ as a backbone model. The kernel size $k$ of ECANet in  is set to 3. The parameter $p$ of GeM in  is set to 3 and the dimension $d$ of final embeddings to 512. We adopt ArcFace~, a cosine-softmax based loss, with a margin of 0.3. We use stochastic gradient descent with initial learning rate $10^{-3}$, momentum 0.9 and weight decay $10^{-5}$.We adopt the batch sampling of Yokoo ~ where mini-batch samples with similar aspect ratios are resized to a particular size. Here, we use a batch size of 64. For image augmentation, we apply scaling, random cropping, and varied illumination. At inference, we apply a multi-resolution representation~ to query and database images.Our method is denoted as GLAM (). Using the backbone model alone is referred to as . It is compatible with recent models based on ResNet101-GeM trained with ArcFace~. Adding our local attention () to the baseline model is denoted , while adding our global attention () is denoted . Since we focus on representation learning, we do not consider post-processing methods like geometry-based re-ranking  or graph-based re-ranking~.{lcc}  &  &  \\\\\\\\ -noisy &  213,678 & 672 \\\\-clean & 27,965 & 581 \\\\-120k & 117,369 & 713 \\\\1-noisy & 1,225,029  &  14, 951 \\\\2-noisy & 4,132,914  &  203,094 \\\\2-clean & 1,580,470 & 81,313 \\\\\\\\',\n",
       " 'Benchmarking': '{0.6pt}{l*{8}{c}} {} & {} & {} & {} & {} & {} & {} \\\\\\\\ (l){6-9} & & & & &  &  &  &  \\\\\\\\ -Siamese  & SfM-120k    & 2048 & 87.8 & 92.7 & 64.7 & 77.2 & 38.5 & 56.3 \\\\~                       & GLDv1-noisy & 2048 & --   & --   & 69.9 & 81.6 & 47.9 & 64.5 \\\\2~                   & GLDv2-clean & 2048 & --   & --   & 74.2 & 84.9 & 51.6 & 70.3 \\\\\\\\ (Ours) &  NC-clean & 512 &  77.8 & 85.8 & 51.6 &  68.1 &  20.9 & 44.7 \\\\\\\\ &  GLDv1-noisy & 512 & 92.8 & 95.0 & }  &  }  &  } & } \\\\\\\\ &  GLDv2-noisy & 512 & 93.3 & 95.3 & 75.7 & 86.0 & 53.1 & 73.8 \\\\\\\\ &  GLDv2-clean & 512 & } & } & } & } & } & } \\\\\\\\  against . All models use ResNet101-GeM. Red: best results. Blue: GLAM higher than SOLAR~ on GLDv1-noisy.}{2pt}{l|cc|cc|cccccccc|cccccccc} \\t{} & {} & {} & {c|}{} & {c|}{} & {} \\\\\\\\\\t                                                   &             &          & 5k & 6k & {} & {+1m} & {} & {c|}{+1m} & {} & {+1m} & {} & {+1m} \\\\\\\\\\t                                                   &             &          & mAP &  mAP & mAP & mP & mAP & mP & mAP & mP & mAP & mP & mAP & mP & mAP & mP & mAP & mP & mAP & mP \\\\\\\\ \\tSPoC-V16                  & [O]         & 512      & 53.1$^*$ & -- & 38.0 & 54.6 & 17.1 & 33.3 & 59.8 & 93.0 & 30.3 & 83.0 & 11.4 & 20.9 & 0.9 & 2.9 & 32.4 & 69.7 & 7.6 & 30.6 \\\\\\\\\\tSPoC-R101                            & [O]         & 2048     & -- & -- & 39.8 & 61.0 & 21.5 & 40.4 & 69.2 & 96.7 & 41.6 & 92.0 & 12.4 & 23.8 & 2.8 & 5.6 & 44.7 & 78.0 & 15.3 & 54.4 \\\\\\\\\\tCroW-V16                & [O]         & 512      & 70.8 & 79.7 & 41.4 & 58.8 & 22.5 & 40.5 & 62.9 & 94.4 & 34.1 & 87.1 & 13.9 & 25.7 & 3.0 & 6.6 & 36.9 & 77.9 & 10.3 & 45.1 \\\\\\\\\\tCroW-R101                            & [O]         & 2048     & -- & -- & 42.4 & 61.9 & 21.2 & 39.4 & 70.4 & 97.1 & 42.7 & 92.9 & 13.3 & 27.7 & 3.3 & 9.3 & 47.2 & 83.6 & 16.3 & 61.6 \\\\\\\\\\tMAC-V16-Siamese          & [O]         & 512      & 80.0 & 82.9 & 37.8 & 57.8  & 21.8  & 39.7 & 59.2 & 93.3  & 33.6 & 87.1 & 14.6  & 27.0  & 7.4  & 11.9 & 35.9  & 78.4  & 13.2 & 54.7 \\\\\\\\\\tMAC-R101-Siamese                     & [O]         & 2048     & -- & -- & 41.7 & 65.0 & 24.2 & 43.7 & 66.2 & 96.4 & 40.8 & 93.0 & 18.0 & 32.9 & 5.7 & 14.4 & 44.1 & 86.3 & 18.2 & 67.7 \\\\\\\\\\tRMAC-V16-Siamese         & [O]         & 512      & 80.1 & 85.0 & 42.5 & 62.8 & 21.7 & 40.3 & 66.2 & 95.4 & 39.9 & 88.9 & 12.0 & 26.1 & 1.7 & 5.8 & 40.9 & 77.1 & 14.8 & 54.0 \\\\\\\\\\tRMAC-R101-Siamese                    & [O]         & 2048     & -- & -- & 49.8 & 68.9 &  29.2 &  48.9 &  74.0 &  97.7 &  49.3 &  93.7 &  18.5 & 32.2 &  4.5 &  13.0 &  52.1 &  87.1 &  21.3 &  67.4 \\\\\\\\\\tRMAC-R101-Triplet           & NC-clean    & 2048     & 86.1 &  & 60.9 & 78.1 & 39.3 & 62.1 & 78.9 & 96.9 & 54.8 & 93.9 & 32.4 & 50.0 & 12.5 & 24.9 & 59.4 & 86.1 & 28.0 & 70.0 \\\\\\\\\\tGeM-R101-Siamese        & SfM-120k    & 2048     &  & 92.7 & 64.7 & 84.7 & 45.2 & 71.7 & 77.2 & } & 52.3 & } & 38.5 & 53.0 & 19.9 & 34.9 & 56.3 & 89.1 & 24.7 & 73.3 \\\\\\\\\\tAGeM-R101-Siamese            & SfM-120k    & 2048     & -- & -- & 67.0 & -- & -- & -- & 78.1 & --  & -- & -- & 40.7 & -- & -- & -- & 57.3 & -- & -- & -- \\\\\\\\\\tSOLAR-GeM-R101-Triplet/SOS              & GLDv1-noisy & 2048     & -- & -- & 69.9 &  & 53.5 &  & 81.6  & 97.1 & 59.2 & 94.9 & 47.9 &  & 29.9 &  &  64.5 &  & 33.4 &  \\\\\\\\\\tDELG-GeM-R101-ArcFace           & GLDv1-noisy & 2048     & -- & -- & 73.2 & -- &  & -- & 82.4 & --  &  & -- & 51.2 & -- &  & -- & 64.7 & -- &  & -- \\\\\\\\\\tGeM-R101-ArcFace                    & GLDv2-clean & 2048     & -- & -- &  & -- & -- & -- &  & --  & -- & -- &    & -- & -- & -- &   & -- & -- & -- \\\\\\\\\\t\\tGLAM-GeM-R101-ArcFace baseline   & GLDv2-clean & 512 & } & } & 72.8 & } &  } & } & 84.2 & 95.9 &  } &  93.3 &  49.9 & 62.1 & } & } & 69.7 & 88.4 & } & 73.7  \\\\\\\\\\t+local                           & GLDv2-clean & 512 & } & } & 73.7 & } & } & } & } & 95.6 & } & 93.9  &  } & } & } & } & } & 89.3 & } & 79.1  \\\\\\\\\\t+global                          & GLDv2-clean & 512 & } & } & } & } & } & } & } & 95.4 & } & 93.7  & } & } & } & } & } & 89.4 & } & 77.0  \\\\\\\\\\t+global+local                    & GLDv2-clean & 512 & } & } & } & } & } & } & }  & 97.0 &  } & 94.9 & } & } & } & } & } & } & } & } \\\\\\\\\\t. mP: mP@10. Red: best results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand ~ is the only model other than ours trained on GLDv2-clean, while~ is trained on GLDv1-noisy and compared in .}',\n",
       " 'Noisy \\\\vs clean training sets': 'We begin by training our best model (baseline+local+global)on all training sets of , except NC-noisy because some images are currently unavailable. As shown in , even though GLDv2-noisy has 2.6 times more images than GLDv2-clean, the latter is superior by a large margin. This shows that, in training, a cleaner dataset can be more important than a larger one. By contrast, NC-clean has the worst performance despite being clean, aparently because it is too small. To achieve best possible performance, we use GLDv2-clean as a training set in the remaining experiments.',\n",
       " 'Comparisons on same training set': 'It is common to compare methods regardless of training sets as more become available, ,~. Since GLDv2-clean is relatively new, Weyand ~, which introduced the dataset, is the only study that has trained the same backbone with the same settings (ResNet101-GeM with ArcFace) on GLDv2-clean. Our baseline is lower than~, because our dimensinality is 512, while other models based on ResNet101 use 2048. Yet,  shows that our best model trained on GLDv2-clean outperforms~ by a large margin. But the most important comparison is with SOLAR~, also based on self-attention, which has trained ResNet101-GeM on GLDv1-noisy. On this training set, our best model clearly outperforms~ despite lower dimensionality.',\n",
       " 'Comparison with state of the art': ' shows the performance of four variants of our model,  baseline with or without local/global attention, and compares them against state-of-the-art (SOTA) methods based on global descriptors without re-ranking on the complete set of benchmarks, including distractors. Both local and global attention bring significant gain over the baseline. The effect of global is stronger, while the gain of the two is additive in the combination. The best results are achieved by the global-local attention network (baseline+global+local). With this model, we outperform previous best methods on most benchmarks except mP@10 on ~(medium) and $+$1m~(medium), where we are outperformed by~. These results demonstrate that our approach is effective for landmark image retrieval.  shows some examples of our ranking results.',\n",
       " 'Ablation study': '{1.5pt}{l*{6}{c}} {} & {} & {} & {} & {} \\\\\\\\ (l){4-7} & & &  &  &  &  \\\\\\\\  baseline  & 91.9 & 94.5 & 72.8 &  84.2 & 49.9 & 69.7 \\\\\\\\+local-channel & 91.3 & 95.3 & 72.2 & 85.8 & 48.3 & 73.1 \\\\\\\\+local-spatial & 91.0 & 95.1 & 72.1 & 85.3 & 48.3 &  71.9 \\\\\\\\+local & 91.2 & 95.4 & 73.7 & 86.5 & 52.6 & 75.0 \\\\\\\\+global-channel & 92.5 & 94.4 & 73.3 & 84.4 & 49.8 & 70.1 \\\\\\\\+global-spatial & 92.4 & 95.1 & 73.2 & 86.3 & 50.0 & 72.7 \\\\\\\\+global & 92.3 & 95.3 & 77.2 & 86.7 & 57.4 & 75.0 \\\\\\\\+global+local  &  &  &   &   &  &  \\\\\\\\ ) and global (+global, ) attention modules to the baseline.}{3.4pt}{l*{6}{c}} {} & {} & {} & {} & {} \\\\\\\\ (l){4-7} & & &  &  &  &  \\\\\\\\  style  & 93.8 &  & 75.6 &  88.4 & 53.3 &  \\\\ (Ours)  &  & 95.6 &   &   &  &  \\\\\\\\ {4.2pt}{l*{6}{c}} {} & {} & {} & {} & {} \\\\\\\\ (l){4-7} & & &  &  &  &  \\\\\\\\   & 89.5 & 95.1 & 73.6 & 86.5 & 54.0 & 73.7 \\\\ (Ours) &  &  &   &   &  &  \\\\\\\\ {2.5pt}{l*{6}{c}} {} & {} & {} & {} & {} \\\\\\\\ (l){4-7} & & &  &  &  &  \\\\\\\\ -size  & 76.1 & 82.6 & 55.7 & 68.4 & 29.2 & 47.5 \\\\-size (Ours) &  &  &   &   &  &  \\\\\\\\ {1.9pt}{l*{7}{c}} {} & {} & {} & {} & {} & {} \\\\\\\\ (l){5-8} & & & &  &  &  &  \\\\\\\\  & Single & 93.3 & 95.2 & 76.9 & 87.1 & 58.6 & 74.7 \\\\ & Single & 93.9 & 95.4 & 78.0 & 87.7 & 59.0 & 75.5 \\\\ & Multi & 93.6 &  & 77.0 & 87.8 & 57.1 & 76.0 \\\\ & Multi &  &  &   &   &  &  \\\\\\\\ Our ablation study uses the Google Landmark v2 clean dataset (GLDv2-clean)~ for training, which is shown to be the most effective in .',\n",
       " 'Effect of attention modules': 'We ablate the effect of our local and global attention networks as well as their combination.  shows the results, which are more fine-grained than those of . In particular, it shows the effect of the channel and spatial variants of both local and global attention. We observe that, when used alone, the channel and spatial variants of local attention are harmful in most cases. Even the combination, baseline+local, is not always effective. By contrast, when used alone, the channel and spatial variants of global attention are mostly beneficial, especially the latter. Their combination, baseline+global, is impressive, bringing gain of up to 7.5\\\\. Importantly, the combination baseline+global+local improves further by up to another 2.8\\\\. This result shows the necessity of local attention in the final model.',\n",
       " 'CBAM \\\\vs our local spatial attention': 'We experiment with the local spatial attention of CBAM~. CBAM applies average and max-pooling to input features and concatenates the two for spatial attention. We apply this variant to our local spatial attention module for comparison. For the CBAM style module, we keep the overall design of our module as shown in , but apply average and max-pooling to each of the four convolutional layer outputs before concatenation.  shows that the CBAM style module is considerably worse than ours on all benchmarks except Paris6k, where it is only slightly better.',\n",
       " 'Concatenation \\\\vs sum for feature fusion': 'We use a softmax-based weighted average of local and global attention feature maps with the original feature map~. Here, we compare this weighted average with weighted concatenation, where concatenation replaces the sum operation in~. As shown in , the weighted average outperforms the weighted concatenation.',\n",
       " 'Fixed-size \\\\vs group-size sampling': 'Numerous studies have proposed methods for constructing batches according to image size for efficient training. For instance, Gordo ~, DELF~, and Yokoo~ employed different image sizes per batch for training instead of a single fixed size. We adopt the method of Yokoo , which constructs a batch with images of similar aspect ratio, so that the images can be resized to a size with an aspect ratio that is similar to their own. We call this method .  compares fixed-size ($224  224$) with group-size sampling. We observe that maintaining aspect ratios by using dynamic input sizes is much more effective.',\n",
       " 'Multi-resolution': 'We use the multi-resolution representation~ for the final feature of an image at inference time. This method: (1) resizes an image into multiple scales; (2) extracts features from the resized images; and (3) averages the features to obtain the final feature of the image. The method is applied to both query and database images to enhance ranking results, especially for small target objects.  compares the four cases of applying this method or not to query or database images.',\n",
       " 'Conclusion': 'We have introduced a novel approach that extracts global and local contextual information using attention mechanisms for instance-level image retrieval. It is manifested as a network architecture consisting of global and local attention components, each operating on both spatial and channel dimensions. This constitutes a comprehensive study and empirical evaluation of all four forms of attention that have previously been studied only in isolation. Our findings indicate that the gain (or loss) brought by one form of attention alone strongly depends on the presence of the others, with the maximum gain appearing when all forms are present. The output is a modified feature tensor that can be used in any way, for instance with local feature detection instead of spatial pooling for image retrieval.With the advent of ~ and their recent application to image retrieval~, attention is expected to play a more and more significant role in vision. According to our classification, transformers perform global spatial attention alone. It is of great interest to investigate the role of the other forms of attention, where our approach may yield a basic building block of such architectures. One may even envision an extension to language models, where transformers originate from~.{}'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_sections_from_string(complete_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "for file in os.listdir('./pdfs/'):\n",
    "    INTRO_DELIMITERS = ['Introduction\\n', 'INTRODUCTION\\n']\n",
    "    REF_DELIMITERS = ['References\\n','REFERENCES\\n']\n",
    "    \n",
    "    intro_pattern = '|'.join(map(re.escape, INTRO_DELIMITERS))\n",
    "    ref_delimiters = '|'.join(map(re.escape, REF_DELIMITERS))\n",
    "    \n",
    "    doc = fitz.open(f'./pdfs/{file}')\n",
    "    id = file.replace('.pdf', '')\n",
    "    content = \"\"\n",
    "    for page in doc:\n",
    "        content += page.get_text()\n",
    "    text = re.split(intro_pattern, content)[-1]\n",
    "    text = re.split(ref_delimiters, text)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.txt\", \"w\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Assuming 'delimiters' is a list of your delimiters\n",
    "\n",
    "# Create a regular expression pattern that matches any of the delimiters\n",
    "\n",
    "# Assuming 'content' is your string\n",
    "sections = re.split(pattern, content, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All the attention you need:\\nGlobal-local, spatial-channel attention for image retrieval\\nChull Hwan Song\\nOdd Concepts\\nHye Joo Han\\nOdd Concepts\\nYannis Avrithis\\nInria, Univ Rennes, CNRS, IRISA\\nAbstract\\nWe address representation learning for large-scale\\ninstance-level image retrieval. Apart from backbone, train-\\ning pipelines and loss functions, popular approaches have\\nfocused on different spatial pooling and attention mecha-\\nnisms, which are at the core of learning a powerful global\\nimage representation. There are different forms of attention\\naccording to the interaction of elements of the feature tensor\\n(local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses\\nonly one or two forms of attention and applies it to different\\nproblems like classication, detection or retrieval.\\nWe present global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network and\\nincorporates all four forms of attention: local and global,\\nspatial and channel. We obtain a new feature tensor and, by\\nspatial pooling, we learn a powerful embedding for image\\nretrieval. Focusing on global descriptors, we provide em-\\npirical evidence of the interaction of all forms of attention\\nand improve the state of the art on standard benchmarks.',\n",
       " 'Instance-level image retrieval is at the core of visual rep-\\nresentation learning and is connected with many problems\\nof visual recognition and machine learning, for instance\\nmetric learning [30, 26], few-shot learning [42] and unsu-\\npervised learning [8]. Many large-scale open datasets [3,\\n37, 16, 29, 53], and competitions1 have accelerated progress\\nin instance-level image retrieval, which has been trans-\\nformed by deep learning [3].\\nMany studies on instance-level image retrieval focus\\non learning features from convolutional neural networks\\n(CNN), while others focus on re-ranking, for instance by\\ngraph-based methods [11]. The former can be distinguished\\naccording to feature types: local descriptors, reminiscent of\\nSIFT [27], where an image is mapped to a few hundred vec-\\ntors; and global descriptors, where an image is mapped to a\\n1https://www.kaggle.com/c/landmark-retrieval-2020\\nsingle vector. In fact, deep learning has brought global de-\\nscriptors with astounding performance, while allowing ef-\\ncient search. Our study belongs to this type.\\nStudies on global descriptors have focused on spatial\\npooling [2, 37]. The need for compact, discriminative rep-\\nresentations that are resistant to clutter has naturally given\\nrise to spatial attention methods [24, 28]. Different kinds\\nof attention have been studied in many areas of computer\\nvision research. There is also channel attention [20, 9]; lo-\\ncal attention, applied independently to elements of the rep-\\nresentation (feature map) [54, 25]; global attention, based\\non interaction between elements [52, 9]; and combinations\\nthereof. Unfortunately, each study has been limited to one or\\ntwo kinds of attention only; attention is not always learned;\\nand applications vary.\\nIt is the objective of our work to perform a compre-\\nhensive study of all forms of attention above, apply them\\nto instance-level image retrieval and provide a detailed ac-\\ncount of their interaction and impact on performance. As\\nshown in Figure 1, we collect contextual information from\\nimages with both local and global attention, giving rise to\\ntwo parallel network streams. Importantly, each operates\\non both spatial locations and feature channels. Local at-\\ntention is about individual locations and channels; global is\\nabout interaction between locations and between channels.\\nThe extracted information is separately embedded in local\\nand global attention feature maps, which are combined in a\\nglobal-local attention feature map before pooling.\\nOur contributions can be summarized as follows:\\n1. We propose a novel network that consists of both\\nglobal and local attention for image retrieval. This is\\nthe rst study that employs both mechanisms.\\n2. Each of the global and local attention mechanisms\\ncomprises both spatial and channel attention.\\n3. Focusing on global descriptors, we provide empirical\\nevidence of the interaction of all forms of attention and\\nimprove the state of the art on standard benchmarks.\\n1\\narXiv:2107.08000v1  [cs.CV]  16 Jul 2021\\nAl\\nc\\nc  1  1\\n\\n+\\nFl\\nc\\nAl\\ns\\n1  h  w\\n\\n+\\nFl\\n\\nc  h  w\\nF\\n\\n+\\nc  h  w\\nFgl\\nAg\\nc\\nc  c\\n\\nFg\\nc\\nAg\\ns\\nhw  hw\\n\\n+\\nFg\\n\\nwl\\nw\\nwg\\nchannel attention\\nspatial attention\\nfusion\\nlocal attention\\nglobal attention\\nFigure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\\ntion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\\n(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\\nc),\\nlocal spatial (Al\\ns), global channel (Ag\\nc) and global spatial (Ag\\ns). The input feature map F is weighted into local (Fl) and\\nglobal (Fg) attention feature maps, which are fused with F to yield the global-local attention feature map Fgl. The diagram\\nis abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\\n2. Related work\\nInstance-level image retrieval\\nStudies on instance-level\\nimage retrieval can be roughly, but not exclusively, di-\\nvided into three types: (1) studies on global descriptors\\n[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\\ngeometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\\nby graph-based methods [11, 21, 55]. The rst two types\\nof studies focus on the feature representation, while the last\\ntype focuses on re-ranking extracted features.\\nStudies on global descriptors focus on spatial pooling\\nof CNN feature maps into vectors, including MAC [38],\\nSPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\\nand NetVLAD [1, 25], as well as learning the representa-\\ntion [3, 15, 16, 36, 37]. Studies before deep learning dom-\\ninated image retrieval were mostly based on local descrip-\\ntors like SIFT [27] and bag-of-words representation [32] or\\naggregated descriptors like VLAD [22] or ASMK [46]. Lo-\\ncal descriptors have been revived in deep learning, e.g. with\\nDELF [29], DELG [5] and ASMK extensions [45, 47].\\nWe focus on learning a global descriptor in this work, be-\\ncause it is the most efcient in terms of storage and search.\\nHowever, our generic attention mechanism produces a fea-\\nture tensor and could be applicable to local descriptors as\\nwell, if global pooling were replaced by local feature detec-\\ntion. Re-ranking methods are complementary to the repre-\\nsentation and we do not consider them in this work.\\nAttention\\nAttention mechanisms have been rst proposed\\nin image classication studies focusing on channel at-\\nMETHOD\\nLOCAL\\nGLOBAL\\nLRN RET\\nSpatial Channel Spatial Channel\\nSENet [20]\\n\\n\\nECA-Net [51]\\n\\n\\nGCNet [6]\\n\\n\\nCBAM [54]\\n\\n\\n\\nGE [19]\\n\\n\\nNL-Net [52]\\n\\n\\nAA-Net [4]\\n\\n\\nSAN [59]\\n\\n\\nN3Net [34]\\n\\n\\nA2-Net [9]\\n\\n\\nGSoP [14]\\n\\n\\nOnA [23]\\n\\n\\nAGeM [17]\\n\\n\\nCroW [24]\\n\\n\\n\\nCRN [25]\\n\\n\\n\\nDELF [29]\\n\\n\\n\\nDELG [5]\\n\\n\\n\\nTolias et al. [47]\\n\\n\\n\\nSOLAR [28]\\n\\n\\n\\nOurs\\n\\n\\n\\n\\n\\n\\nTable 1: Related work on attention. LRN: learned; RET: ap-\\nplied to instance-level image retrieval.\\ntention [20, 51, 6], spatial attention [19] or both, like\\nCBAM [54]. In image retrieval, CroW [24] also employs\\n2\\nfeature map\\nGAP\\nconv1d(k)\\nsigmoid\\nattention map\\nc  h  w\\nc  1  1\\nc  1  1\\nF\\nAl\\nc\\nFigure 2: Local channel attention.\\nboth spatial and channel attention and can be seen as a pre-\\ncursor of CBAM, but, like other studies of spatial attention\\non retrieval [41, 23, 17], it is not learned. CRN [25] ap-\\nplies spatial attention for feature reweighting and is learned.\\nLearned spatial attention mechanisms are common for local\\ndescriptors [29, 5, 47].\\nWe call the above methods local attention, in the sense\\nthat elements of the feature tensor (channels / spatial loca-\\ntions), are weighted independently, based on contextual in-\\nformation obtained by pooling or learned. By constrast, by\\nglobal attention we refer to mechanisms that model inter-\\naction between elements of the feature tensor, for example\\nbetween channels or between locations.\\nIn image classication, non-local neural network (NL-\\nNet) [52] is maybe the rst global attention mechanism, fol-\\nlowed by similar studies [4, 59, 34]. It is global spatial at-\\ntention, allowing interaction between any pair of spatial lo-\\ncations. Similarly, there are studies of global channel atten-\\ntion, allowing interaction between channels [9, 14]. Global\\nattention has focused mostly on image recognition and has\\nbeen applied to either spatial or channel attention so far, not\\nboth. In image retrieval, SOLAR [28] is a direct application\\nof the global spatial attention mechanism of [52].\\nTable 1 attempts to categorize related work on atten-\\ntion according to whether attention is local or global, spa-\\ntial or channel, whether it is learned and whether it is ap-\\nplied to instance-level image retrieval. We observe that all\\nmethods limit to one or two forms of attention only. Of\\nthose studies that focus on image retrieval, many are not\\nlearned [23, 17, 24], and of those that are, some are de-\\nsigned for local descriptors [29, 47].\\nBy contrast, we provide a comprehensive study of all\\nforms of attention, global and local, spatial and channel, to\\nobtain a learned representation in the form of a tensor that\\ncan be used in any way. We spatially pool it into a global\\ndescriptor and we study the relative gain of different forms\\nof attention in image retrieval.\\nfeature map\\nconv 1  1\\nconv 3  3\\nconv 5  5\\nconv 7  7\\nconcat\\nconv 1  1\\nattention map\\nc  h  w\\n4c  h  w\\n1  h  w\\nc  h  w\\ndilated\\nconv\\nF\\nF\\nAl\\ns\\nFigure 3: Local spatial attention. Convolutional layers in\\nblue implemented by dilated convolutions with kernel size\\n3  3 and dilation factors 1, 3, 5.\\n3. Global-local attention\\nWe design a global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network. Figure 1\\nillustrates its main components. We are given a c  h  w\\nfeature tensor F, where c is the number of channels, and\\nh  w is the spatial resolution. Local attention collects con-\\ntext from the image and applies pooling to obtain a c11\\nlocal channel attention map Al\\nc and a 1  h  w local spa-\\ntial attention map Al\\ns. Global attention allows interaction\\nbetween channels, resulting in a c  c global channel at-\\ntention map Ag\\nc, and between spatial locations, resulting in\\na hw  hw global spatial attention map Ag\\ns. The feature\\nmaps produced by the two attention streams are combined\\nwith the original one by a learned fusion mechanism into\\nthe global-local attention feature map Fgl before being spa-\\ntially pooled into a global image descriptor.\\n3.1. Local attention\\nWe extract an 1D channel and a 2D spatial attention map\\nto weigh the feature map in the corresponding dimensions.\\nLocal channel attention\\nFollowing ECA-Net [51], this\\nattention captures local channel information. As shown in\\nFigure 2, we are given a chw feature tensor F from our\\nbackbone. We rst reduce it to a c  1  1 tensor by global\\naverage pooling (GAP). Channel attention is then captured\\nby a 1D convolution of kernel size k along the channel di-\\nmension, where k controls the extent of cross-channel inter-\\naction. This is followed by a sigmoid function, resulting in\\nthe c  1  1 local channel attention map Al\\nc.\\nLocal spatial attention\\nInspired by the inception mod-\\nule [43] and similar to [25], this attention map captures local\\nspatial information at different scales. As shown in Figure 3,\\n3\\nfeature map\\nGAP\\nconv1d(k)\\nconv1d(k)\\nsigmoid\\nsigmoid\\n\\n\\nsoftmax\\nattention feature map\\n1  c\\n1  c\\n1  c\\nQc\\nc  c\\nhw  c\\nVc\\nAg\\nc\\nc  h  w\\n1  c\\n1  c\\nKc\\nF\\nGc\\nFigure 4: Global channel attention.\\ngiven the same c  h  w feature tensor F from our back-\\nbone, we obtain a new tensor F with channels reduced to\\nc, using a 1  1 convolution. We then extract local spatial\\ncontextual information using convolutional lters of kernel\\nsize 3  3, 5  5, and 7  7, which are efciently imple-\\nmented by 3  3 dilated convolutions [7, 57] with dilation\\nparameter 1, 2, and 3 respectively. The resulting features,\\nalong with one obtained by 1  1 convolution on F, are\\nconcatenated into a 4c  h  w tensor. Finally, we obtain\\nthe 1  h  w local spatial attention map Al\\ns by a 1  1\\nconvolution that reduces the channel dimension to 1.\\nThe middle column of Figure 6 shows heat maps of local\\nspatial attention, localizing target objects in images.\\nLocal attention feature map\\nWe use the local channel\\nattention map Al\\nc to weigh F in the channel dimension\\nFl\\nc := F  Al\\nc + F.\\n(1)\\nWe then use local spatial attention map Al\\ns to weigh Fl\\nc\\nin the spatial dimensions, resulting in the c  h  w local\\nattention feature map\\nFl = Fl\\nc  Al\\ns + Fl\\nc.\\n(2)\\nHere, AB denotes an element-wise multiplication of ten-\\nsors A and B, with broadcasting when one tensor is smaller.\\nWe adopt the choice of applying channel followed by spa-\\ntial attention from convolutional block attention module\\nCBAM [54]. However, apart from computing Al\\ns at differ-\\nent scales, both attention maps are obtained from the orig-\\ninal tensor F rather than sequentially. In addition, both (1)\\nand (2) include residual connections, while CBAM includes\\na single residual connection over both steps.\\n3.2. Global attention\\nWe extract two matrices capturing global pairwise chan-\\nnel and spatial interaction to weigh the feature map.\\nfeature map\\nconv 1  1\\nconv 1  1\\nconv 1  1\\n\\n\\nsoftmax\\nconv 1  1\\nattention feature map\\nc  hw\\nQs\\nhw  hw\\nc  h  w\\nc  hw\\nVs\\nc  h  w\\nAg\\ns\\nc  h  w\\nc  hw\\nKc\\nF\\nGs\\nFigure 5: Global spatial attention.\\nGlobal channel attention\\nWe introduce a global channel\\nattention mechanism that captures global channel interac-\\ntion. This mechanism is based on the non-local neural net-\\nwork [52], but with the idea of 1D convolution from ECA-\\nNet [51]. As shown in Figure 4, we are given the c  h  w\\nfeature tensor F from our backbone. We apply GAP and\\nsqueeze spatial dimensions, followed by a 1D convolution\\nof kernel size k and a sigmoid function, to obtain 1c query\\nQc and key Kc tensors. The value tensor Vc is obtained by\\nmere reshaping of F to hwc, without GAP. Next, we form\\nthe outer product of Kc and Qc, followed by softmax over\\nchannels to obtain a c  c global channel attention map\\nAg\\nc = softmax(Kc\\nQc).\\n(3)\\nFinally, this attention map is multiplied with Vc and the ma-\\ntrix product VcAg\\nc is reshaped back to chw to give the\\nglobal channel attention feature map Gc. In GSoP [14] and\\nA2-Net [9], a cc global channel attention map is obtained\\nby multiplication of hw  c matrices; (3) is more efcient,\\nusing only an outer product of 1  c vectors.\\nGlobal spatial attention\\nSince ordinary convolution ap-\\nplies only a local neighborhood at a time, it cannot capture\\nglobal contextual information. Thus, we apply non-local l-\\ntering [52], which is a form of self-attention [49] in the spa-\\ntial dimensions. As shown in Figure 5, we are given the\\nsame c  h  w feature tensor F from our backbone. By\\nusing three 11 convolutions, which reduce channels to c,\\nand attening spatial dimensions to hw, we obtain c  hw\\nquery Qs, key Ks, and value Vs tensors, where each col-\\numn is a feature vector corresponding to a particular spatial\\nlocation. We capture pairwise similarities of these vectors\\nby matrix multiplication of Ks and Qs, followed by soft-\\nmax over locations to obtain a hw  hw global spatial at-\\ntention map:\\nAg\\ns = softmax(K\\ns Qs).\\n(4)\\n4\\nThis attention map is multiplied with Vs and the matrix\\nproduct VsAg\\ns is reshaped back to c hw by expanding\\nthe spatial dimensions. Finally, using a 1  1 convolution,\\nwhich increases channels back to c, we obtain the chw\\nglobal spatial attention feature map Gs.\\nThe right column of Figure 6 shows heat maps for global\\nspatial attention, localizing target objects in images.\\nGlobal attention feature map\\nWe use the global channel\\nattention feature map Fc to weigh F element-wise\\nFg\\nc = F  Gc.\\n(5)\\nWe then use global spatial attention feature map Gs to\\nweigh Fg\\nc element-wise, resulting in the c  h  w global\\nattention feature map\\nFg = Fg\\nc  Gs + Fg\\nc.\\n(6)\\nSimilarly to Fl in (1) and (2), we apply channel attention\\nrst, followed by spatial attention. However, unlike (1),\\nthere is no residual connection in (5). This choice is sup-\\nported by early experiments.\\n3.3. Global-local attention\\nFeature fusion\\nAs shown in Figure 1, we combine the\\nlocal and global attention feature maps, Fl and Fg, with\\nthe original feature F. While concatenation and summation\\nare common operations for feature combination, we use a\\nweighted average with weights wl, wg, w respectively, ob-\\ntained by softmax over three learnable scalar parameters, to\\nobtain a c  h  w global-local attention feature map\\nFgl = wlFl + wgFl + wF.\\n(7)\\nEfcientDet [44] has shown that this is the most effective,\\namong a number of choices, for fusion of features across\\ndifferent scales.\\nPooling\\nWe apply GeM [37], a learnable spatial pooling\\nmechanism, to feature map Fgl (7), followed by a fully-\\nconnected (FC) layer with dropout and batch normalization.\\nThe nal embedding is obtained by 2-normalization.\\n4. Experiments\\n4.1. Datasets\\nTraining set\\nThere are a number of open landmark\\ndatasets commonly used for training in image retrieval stud-\\nies, including neural code (NC) [3], neural code clean (NC-\\nclean) [16], as well as Google Landmarks v1 (GLDv1) [29]\\nand v2 (GLDv2) [53]. Table 2 shows relevant statistics.\\nThese datasets can be categorized into noisy and clean. The\\nclean sets were obtained from the original noisy sets for\\nmore effective training [16, 53]. The original noisy datasets\\nare much larger, but they have high intra-class variability.\\n(a) input\\n(b) local\\n(c) global\\nFigure 6: Local and global spatial attention. Left: input\\nimages. Middle: local spatial attention heat maps. Right:\\nglobal spatial attention heat maps. Red (blue) means higher\\n(lower) attention weight.\\nEach class can include visually dissimilar images such as\\nexterior and interior views of a building or landmark, in-\\ncluding oor plans and paintings inside. The clean datasets\\nfocus on views directly relevant to landmark recognition but\\nhave a much smaller number of images.\\nEvaluation set and metrics\\nWe use four common eval-\\nuation datasets for landmark image retrieval: Oxford5k\\n(Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox-\\nford (ROxford or ROxf) and Paris (RParis or RPar) [35].\\nROxford and RParis are used with and without one million\\ndistractors (R1M) [28] and evaluated using the Medium and\\nHard protocols [35]. We evaluate using mean Average Pre-\\ncision (mAP) and mean precision at 10 (mP@10).\\n4.2. Implementation details\\nWe train on 8 TITAN RTX 2080Ti GPUs. All models are\\npre-trained on ImageNet [39] and implemented in PyTorch\\n[31]. For fair comparisons, we set a training environment\\n5\\nFigure 7: Examples of our ranking results. In each row, the rst image on the left (pink dotted outline) is a query image with a\\ntarget object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images\\nfor the query; red solid outline: negative.\\nsimilar to the those of compared studies [56, 53, 28, 35]. We\\nemploy ResNet101 [18] as a backbone model. The kernel\\nsize k of ECANet in subsection 3.1 is set to 3. The param-\\neter p of GeM in subsection 3.3 is set to 3 and the dimen-\\nsion d of nal embeddings to 512. We adopt ArcFace [10],\\na cosine-softmax based loss, with a margin of 0.3. We use\\nstochastic gradient descent with initial learning rate 103,\\nmomentum 0.9 and weight decay 105.\\nWe adopt the batch sampling of Yokoo et al. [56] where\\nmini-batch samples with similar aspect ratios are resized to\\na particular size. Here, we use a batch size of 64. For image\\naugmentation, we apply scaling, random cropping, and var-\\nied illumination. At inference, we apply a multi-resolution\\nrepresentation [16] to query and database images.\\nOur method is denoted as GLAM (global-local atten-\\ntion module). Using the backbone model alone is referred\\nto as baseline. It is compatible with recent models based\\non ResNet101-GeM trained with ArcFace [53, 28]. Adding\\nour local attention (subsection 3.1) to the baseline model is\\ndenoted +local, while adding our global attention (subsec-\\ntion 3.2) is denoted +global. Since we focus on representa-\\ntion learning, we do not consider post-processing methods\\nlike geometry-based re-ranking [29, 40, 53] or graph-based\\nre-ranking [11, 21, 55].\\n4.3. Benchmarking\\nNoisy vs. clean training sets\\nWe begin by training our\\nbest model (baseline+local+global) on all training sets of\\nTable 2, except NC-noisy because some images are cur-\\nrently unavailable. As shown in Table 3, even though\\nTRAIN SET\\n#IMAGES\\n#CLASSES\\nNC-noisy\\n213,678\\n672\\nNC-clean\\n27,965\\n581\\nSfM-120k\\n117,369\\n713\\nGLDv1-noisy\\n1,225,029\\n14, 951\\nGLDv2-noisy\\n4,132,914\\n203,094\\nGLDv2-clean\\n1,580,470\\n81,313\\nTable 2: Statistics of different training sets.\\nMETHOD\\nTRAIN SET\\nDIM OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGeM-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7\\n77.2\\n38.5\\n56.3\\nSOLAR [28]\\nGLDv1-noisy 2048\\n\\n\\n69.9\\n81.6\\n47.9\\n64.5\\nGLDv2 [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n84.9\\n51.6\\n70.3\\nGLAM (Ours)\\nNC-clean\\n512\\n77.8\\n85.8\\n51.6\\n68.1\\n20.9\\n44.7\\nGLDv1-noisy 512\\n92.8\\n95.0\\n73.7\\n83.5\\n49.8\\n69.4\\nGLDv2-noisy 512\\n93.3\\n95.3\\n75.7\\n86.0\\n53.1\\n73.8\\nGLDv2-clean 512\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 3: mAP comparison of our best model (base-\\nline+local+global) trained on different training sets against\\n[53, 28]. All models use ResNet101-GeM. Red: best results.\\nBlue: GLAM higher than SOLAR [28] on GLDv1-noisy.\\nGLDv2-noisy has 2.6 times more images than GLDv2-\\nclean, the latter is superior by a large margin. This shows\\nthat, in training, a cleaner dataset can be more important\\nthan a larger one. By contrast, NC-clean has the worst\\nperformance despite being clean, aparently because it is\\n6\\nMETHOD\\nTRAIN SET\\nDIM\\nBASE\\nMEDIUM\\nHARD\\nOx5k Par6k\\nROxf\\n+R1M\\nRPar\\n+R1M\\nROxf\\n+R1M\\nRPar\\n+R1M\\nmAP\\nmAP mAP mP mAP mP mAP mP mAP mP\\nmAP mP mAP mP mAP mP mAP mP\\nSPoC-V16 [2, 35]\\n[O]\\n512\\n53.1\\n\\n38.0 54.6 17.1 33.3 59.8 93.0 30.3 83.0 11.4 20.9\\n0.9\\n2.9\\n32.4 69.7\\n7.6\\n30.6\\nSPoC-R101 [35]\\n[O]\\n2048\\n\\n\\n39.8 61.0 21.5 40.4 69.2 96.7 41.6 92.0 12.4 23.8\\n2.8\\n5.6\\n44.7 78.0 15.3 54.4\\nCroW-V16 [24, 35]\\n[O]\\n512\\n70.8\\n79.7\\n41.4 58.8 22.5 40.5 62.9 94.4 34.1 87.1 13.9 25.7\\n3.0\\n6.6\\n36.9 77.9 10.3 45.1\\nCroW-R101 [35]\\n[O]\\n2048\\n\\n\\n42.4 61.9 21.2 39.4 70.4 97.1 42.7 92.9 13.3 27.7\\n3.3\\n9.3\\n47.2 83.6 16.3 61.6\\nMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.0\\n82.9\\n37.8 57.8 21.8 39.7 59.2 93.3 33.6 87.1 14.6 27.0\\n7.4\\n11.9 35.9 78.4 13.2 54.7\\nMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n41.7 65.0 24.2 43.7 66.2 96.4 40.8 93.0 18.0 32.9\\n5.7\\n14.4 44.1 86.3 18.2 67.7\\nRMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.1\\n85.0\\n42.5 62.8 21.7 40.3 66.2 95.4 39.9 88.9 12.0 26.1\\n1.7\\n5.8\\n40.9 77.1 14.8 54.0\\nRMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n49.8 68.9 29.2 48.9 74.0 97.7 49.3 93.7 18.5 32.2\\n4.5\\n13.0 52.1 87.1 21.3 67.4\\nRMAC-R101-Triplet [16, 35]\\nNC-clean\\n2048\\n86.1\\n94.5\\n60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0\\nGeM-R101-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3\\nAGeM-R101-Siamese [17]\\nSfM-120k\\n2048\\n\\n\\n67.0\\n\\n\\n\\n78.1\\n\\n\\n\\n40.7\\n\\n\\n\\n57.3\\n\\n\\n\\nSOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048\\n\\n\\n69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6\\nDELG-GeM-R101-ArcFace [5]\\nGLDv1-noisy 2048\\n\\n\\n73.2\\n\\n54.8\\n\\n82.4\\n\\n61.8\\n\\n51.2\\n\\n30.3\\n\\n64.7\\n\\n35.5\\n\\nGeM-R101-ArcFace [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n\\n\\n\\n84.9\\n\\n\\n\\n51.6\\n\\n\\n\\n70.3\\n\\n\\n\\nGLAM-GeM-R101-ArcFace baseline\\nGLDv2-clean\\n512\\n91.9\\n94.5\\n72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7\\n+local\\nGLDv2-clean\\n512\\n91.2\\n95.4\\n73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1\\n+global\\nGLDv2-clean\\n512\\n92.3\\n95.3\\n77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0\\n+global+local\\nGLDv2-clean\\n512\\n94.2\\n95.6\\n78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0\\nTable 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16:\\nVGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). : dimension d = 256 [2]. mP: mP@10. Red:\\nbest results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only\\nmodel other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\\ntoo small. To achieve best possible performance, we use\\nGLDv2-clean as a training set in the remaining experiments.\\nComparisons on same training set\\nIt is common to com-\\npare methods regardless of training sets as more become\\navailable, e.g., [35, 28]. Since GLDv2-clean is relatively\\nnew, Weyand et al. [53], which introduced the dataset, is the\\nonly study that has trained the same backbone with the same\\nsettings (ResNet101-GeM with ArcFace) on GLDv2-clean.\\nOur baseline is lower than [53], because our dimensinality is\\n512, while other models based on ResNet101 use 2048. Yet,\\nTable 3 shows that our best model trained on GLDv2-clean\\noutperforms [53] by a large margin. But the most impor-\\ntant comparison is with SOLAR [28], also based on self-\\nattention, which has trained ResNet101-GeM on GLDv1-\\nnoisy. On this training set, our best model clearly outper-\\nforms [28] despite lower dimensionality.\\nComparison with state of the art\\nTable 4 shows the\\nperformance of four variants of our model, i.e. baseline\\nwith or without local/global attention, and compares them\\nagainst state-of-the-art (SOTA) methods based on global de-\\nscriptors without re-ranking on the complete set of bench-\\nmarks, including distractors. Both local and global atten-\\ntion bring signicant gain over the baseline. The effect\\nof global is stronger, while the gain of the two is addi-\\ntive in the combination. The best results are achieved by\\nthe global-local attention network (baseline+global+local).\\nWith this model, we outperform previous best methods\\non most benchmarks except mP@10 on RParis (medium)\\nand RParis+R1M (medium), where we are outperformed\\nby [37, 35]. These results demonstrate that our approach is\\neffective for landmark image retrieval. Figure 7 shows some\\nMETHOD\\nOXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGLAM baseline\\n91.9\\n94.5\\n72.8\\n84.2\\n49.9\\n69.7\\n+local-channel\\n91.3\\n95.3\\n72.2\\n85.8\\n48.3\\n73.1\\n+local-spatial\\n91.0\\n95.1\\n72.1\\n85.3\\n48.3\\n71.9\\n+local\\n91.2\\n95.4\\n73.7\\n86.5\\n52.6\\n75.0\\n+global-channel\\n92.5\\n94.4\\n73.3\\n84.4\\n49.8\\n70.1\\n+global-spatial\\n92.4\\n95.1\\n73.2\\n86.3\\n50.0\\n72.7\\n+global\\n92.3\\n95.3\\n77.2\\n86.7\\n57.4\\n75.0\\n+global+local\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 5: mAP comparison of spatial and channel variants\\nof our local (+local, subsection 3.1) and global (+global,\\nsubsection 3.1) attention modules to the baseline.\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nCBAM style\\n93.8\\n95.7\\n75.6\\n88.4\\n53.3\\n76.8\\nGLAM (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 6: mAP comparison between CBAM style and our\\nlocal spatial attention.\\nexamples of our ranking results.\\n4.4. Ablation study\\nOur ablation study uses the Google Landmark v2 clean\\ndataset (GLDv2-clean) [53] for training, which is shown to\\nbe the most effective in Table 3.\\n7\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf\\nRPar\\nROxf\\nRPar\\nConcatenate\\n89.5\\n95.1\\n73.6\\n86.5\\n54.0\\n73.7\\nSum (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 7: mAP comparison between weighted concatenation\\nand weighted average for feature fusion.\\nMETHOD\\nOXF5K PAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nFixed-size\\n76.1\\n82.6\\n55.7\\n68.4\\n29.2\\n47.5\\nGroup-size (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 8: mAP comparison between xed-size (224  224)\\nand group-size sampling methods.\\nQUERY DATABASE OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nSingle\\nSingle\\n93.3\\n95.2\\n76.9\\n87.1\\n58.6\\n74.7\\nMulti\\nSingle\\n93.9\\n95.4\\n78.0\\n87.7\\n59.0\\n75.5\\nSingle\\nMulti\\n93.6\\n95.6\\n77.0\\n87.8\\n57.1\\n76.0\\nMulti\\nMulti\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 9: mAP comparison of using multiresolution repre-\\nsentation (Multi) or not (Single) on query or database.\\nEffect of attention modules\\nWe ablate the effect of our\\nlocal and global attention networks as well as their com-\\nbination. Table 5 shows the results, which are more ne-\\ngrained than those of Table 4. In particular, it shows the ef-\\nfect of the channel and spatial variants of both local and\\nglobal attention. We observe that, when used alone, the\\nchannel and spatial variants of local attention are harmful\\nin most cases. Even the combination, baseline+local, is not\\nalways effective. By contrast, when used alone, the channel\\nand spatial variants of global attention are mostly benecial,\\nespecially the latter. Their combination, baseline+global, is\\nimpressive, bringing gain of up to 7.5%. Importantly, the\\ncombination baseline+global+local improves further by up\\nto another 2.8%. This result shows the necessity of local\\nattention in the nal model.\\nCBAM vs. our local spatial attention\\nWe experiment\\nwith the local spatial attention of CBAM [54]. CBAM ap-\\nplies average and max-pooling to input features and con-\\ncatenates the two for spatial attention. We apply this vari-\\nant to our local spatial attention module for comparison.\\nFor the CBAM style module, we keep the overall design\\nof our module as shown in Figure 3, but apply average and\\nmax-pooling to each of the four convolutional layer outputs\\nbefore concatenation. Table 6 shows that the CBAM style\\nmodule is considerably worse than ours on all benchmarks\\nexcept Paris6k, where it is only slightly better.\\nConcatenation vs. sum for feature fusion\\nWe use a\\nsoftmax-based weighted average of local and global atten-\\ntion feature maps with the original feature map (7). Here,\\nwe compare this weighted average with weighted concate-\\nnation, where concatenation replaces the sum operation\\nin (7). As shown in Table 7, the weighted average outper-\\nforms the weighted concatenation.\\nFixed-size vs. group-size sampling\\nNumerous studies\\nhave proposed methods for constructing batches according\\nto image size for efcient training. For instance, Gordo et\\nal. [16], DELF [29], and Yokoo et al. [56] employed dif-\\nferent image sizes per batch for training instead of a single\\nxed size. We adopt the method of Yokoo et al., which con-\\nstructs a batch with images of similar aspect ratio, so that\\nthe images can be resized to a size with an aspect ratio that\\nis similar to their own. We call this method group-size sam-\\npling. Table 8 compares xed-size (224  224) with group-\\nsize sampling. We observe that maintaining aspect ratios by\\nusing dynamic input sizes is much more effective.\\nMulti-resolution\\nWe use the multi-resolution representa-\\ntion [16] for the nal feature of an image at inference time.\\nThis method: (1) resizes an image into multiple scales; (2)\\nextracts features from the resized images; and (3) averages\\nthe features to obtain the nal feature of the image. The\\nmethod is applied to both query and database images to en-\\nhance ranking results, especially for small target objects.\\nTable 9 compares the four cases of applying this method or\\nnot to query or database images.\\n5. Conclusion\\nWe have introduced a novel approach that extracts global\\nand local contextual information using attention mecha-\\nnisms for instance-level image retrieval. It is manifested as\\na network architecture consisting of global and local atten-\\ntion components, each operating on both spatial and chan-\\nnel dimensions. This constitutes a comprehensive study and\\nempirical evaluation of all four forms of attention that have\\npreviously been studied only in isolation. Our ndings indi-\\ncate that the gain (or loss) brought by one form of attention\\nalone strongly depends on the presence of the others, with\\nthe maximum gain appearing when all forms are present.\\nThe output is a modied feature tensor that can be used in\\nany way, for instance with local feature detection instead of\\nspatial pooling for image retrieval.\\nWith the advent of vision transformers [12, 58] and their\\nrecent application to image retrieval [13], attention is ex-\\npected to play a more and more signicant role in vi-\\nsion. According to our classication, transformers perform\\nglobal spatial attention alone. It is of great interest to in-\\nvestigate the role of the other forms of attention, where our\\n8\\napproach may yield a basic building block of such archi-\\ntectures. One may even envision an extension to language\\nmodels, where transformers originate from [50].\\nReferences\\n[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-\\njdla, and Josef Sivic.\\nNetVLAD: CNN architecture for\\nweakly supervised place recognition. In CVPR, 2016. 2\\n[2] Artem Babenko and Victor Lempitsky. Aggregating Local\\nDeep Features for Image Retrieval. In ICCV, 2015. 1, 2, 7\\n[3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and\\nVictor Lempitsky.\\nNeural Codes for Image Retrieval.\\nIn\\nECCV, 2014. 1, 2, 5\\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\\nand Quoc V. Le.\\nAttention augmented convolutional net-\\nworks. In ICCV, 2019. 2, 3\\n[5] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep\\nlocal and global features for image search. In ECCV, 2020.\\n2, 3, 7\\n[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.\\nGCNet: Non-Local Networks Meet Squeeze-Excitation Net-\\nworks and Beyond. In ICCV, 2019. 2\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for seman-\\ntic image segmentation. arXiv preprint arXiv:1706.05587,\\n2017. 4\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In ICML, 2020. 1\\n[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. A2-nets: Double attention networks.\\nIn NeurIPS, 2018. 1, 2, 3, 4\\n[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. ArcFace: Additive Angular Margin Loss for Deep\\nFace Recognition. In CVPR, 2019. 6\\n[11] Michael Donoser and Horst Bischof. Diffusion Processes for\\nRetrieval Revisited. In CVPR, 2013. 1, 2, 6\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale.\\narXiv preprint\\narXiv:2010.11929, 2020. 8\\n[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\\nHerve Jegou.\\nTraining vision transformers for image re-\\ntrieval. Technical report, 2021. 8\\n[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\\nsecond-order pooling convolutional networks.\\nIn CVPR,\\n2019. 2, 3, 4\\n[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. Deep image retrieval: Learning global representations\\nfor image search. In ECCV, 2016. 2\\n[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. End-to-end learning of deep visual representations for\\nimage retrieval. IJCV, 2017. 1, 2, 5, 6, 7, 8\\n[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie.\\nAttention-\\naware generalized mean pooling for image retrieval. arXiv\\npreprint arXiv:1811.00202, 2018. 2, 3, 7\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition.\\nIn CVPR,\\n2016. 6\\n[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in con-\\nvolutional neural networks. In NeurIPS, 2018. 2\\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\\nSqueeze-and-Excitation Networks. In CVPR, 2018. 1, 2\\n[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,\\nand Ondrej Chum. Efcient diffusion on region manifolds:\\nRecovering small objects with compact cnn representations.\\nIn CVPR, 2017. 2, 6\\n[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into com-\\npact codes. PAMI, (99):11, 2011. 2\\n[23] Albert Jimenez, Jose M. Alvarez, and Xavier Giro-i-Nieto.\\nClass weighted convolutional features for visual instance\\nsearch. In BMVC, 2017. 2, 3\\n[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.\\nCrossdimensional weighting for aggregated deep convolu-\\ntional features. In ECCV, 2016. 1, 2, 3, 7\\n[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.\\nLearned Contextual Feature Reweighting for Image Geo-\\nLocalization. In CVPR, 2017. 1, 2, 3\\n[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\\nProxy anchor loss for deep metric learning. In CVPR, 2020.\\n1\\n[27] David G. Lowe.\\nDistinctive image features from scale-\\ninvariant keypoints. In IJCV, 2004. 1, 2\\n[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian\\nMikolajczyk. SOLAR: Second-Order Loss and Attention for\\nImage Retrieval. In ECCV, 2020. 1, 2, 3, 5, 6, 7\\n[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,\\nand Bohyung Han. Large Scale Image Retrieval with Atten-\\ntive Deep Local Features. In ICCV, 2017. 1, 2, 3, 5, 6, 8\\n[30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\\nSavarese. Deep metric learning via lifted structured feature\\nembedding. In CVPR, 2016. 1\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas Kopf, Edward Yang, Zach DeVito, Martin Raison,\\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\\nperative style, high-performance deep learning. In NeurIPS,\\n2019. 5\\n[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Object retrieval with large vocabularies\\nand fast spatial matching. In CVPR, 2007. 2, 5\\n[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Lost in quantization:Improving particu-\\nlar object retrieval in large scale image databases. In CVPR,\\n2008. 5\\n9\\n[34] Tobias Plotz and Stefan Roth. Neural nearest neighbors net-\\nworks. In NeurIPS, 2018. 2, 3\\n[35] Filip Radenovic, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ondrej Chum. Revisiting Oxford and Paris:\\nLarge-Scale Image Retrieval Benchmarking. In CVPR, 2018.\\n5, 6, 7\\n[36] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. CNN\\nimage retrieval learns from BoW: Unsupervised ne-tuning\\nwith hard examples. In ECCV, 2016. 2, 7\\n[37] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. Fine-\\nTuning CNN Image Retrieval with No Human Annotation.\\nIn TPAMI, 2019. 1, 2, 5, 6, 7\\n[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson,\\nand Atsuto Maki. Visual Instance Retrieval with Deep Con-\\nvolutional Networks. In CoRR, 2015. 2\\n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge. In International booktitle of Computer Vision, 2015.\\n5\\n[40] Oriane Simeoni, Yannis Avrithis, and Ondrej Chum. Local\\nfeatures and visual words emerge in activations. In CVPR,\\n2019. 2, 6\\n[41] O. Simeoni, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.\\nGraph-based particular object discovery. Machine Vision and\\nApplications, 30(2):243254, 3 2019. 3\\n[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\\nical networks for few-shot learning. In NeurIPS, 2017. 1\\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich.\\nGoing deeper with\\nconvolutions. In CVPR, 2015. 3\\n[44] Mingxing Tan, Ruoming Pang, and Quoc V. Le. EfcientDet:\\nScalable and Efcient Object Detection. In CVPR, 2020. 5\\n[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack\\nSim.\\nDetect-to-retrieve: Efcient regional aggregation for\\nimage search. In CVPR, 2019. 2\\n[46] Giorgios Tolias, Yannis Avrithis, and Herve Jegou. To aggre-\\ngate or not to aggregate: Selective match kernels for image\\nsearch. In ICCV, 2013. 2\\n[47] Giorgos Tolias, Tomas Jenicek, and Ondrej Chum. Learn-\\ning and aggregating deep local descriptors for instance-level\\nrecognition. In ECCV, 2020. 2, 3\\n[48] Giorgos Tolias, Ronan Sicre, and Herve Jegou. Particular ob-\\nject retrieval with integral max-pooling of CNN activations.\\nIn ICLR, 2016. 2\\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 4\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 9\\n[51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\\nmeng Zuo, and Qinghua Hu.\\nECA-Net: Efcient Chan-\\nnel Attention for Deep Convolutional Neural Networks. In\\nCVPR, 2020. 2, 3, 4\\n[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local Neural Networks. In CVPR, 2018. 1, 2,\\n3, 4\\n[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle Landmarks Dataset v2 - A Large-Scale Benchmark\\nfor Instance-Level Recognition and Retrieval.\\nIn CVPR,\\n2020. 1, 2, 5, 6, 7\\n[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\\nKweon. CBAM: Convolutional Block Attention Module. In\\nECCV, 2018. 1, 2, 4, 8\\n[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and\\nShinichi Satoh. Efcient image retrieval via decoupling dif-\\nfusion into online and ofine processing. In AAAI, 2019. 2,\\n6\\n[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi\\nIizuka. Two-stage Discriminative Re-ranking for Large-scale\\nLandmark Retrieval. In arXiv:2003.11211, 2020. 6, 8\\n[57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\\nresidual networks. In CVPR, 2017. 4\\n[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\\nto-token vit: Training vision transformers from scratch on\\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 8\\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\\nself-attention for image recognition. In CVPR, 2020. 2, 3\\n10\\n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All the attention you need:\\nGlobal-local, spatial-channel attention for image retrieval\\nChull Hwan Song\\nOdd Concepts\\nHye Joo Han\\nOdd Concepts\\nYannis Avrithis\\nInria, Univ Rennes, CNRS, IRISA\\nAbstract\\nWe address representation learning for large-scale\\ninstance-level image retrieval. Apart from backbone, train-\\ning pipelines and loss functions, popular approaches have\\nfocused on different spatial pooling and attention mecha-\\nnisms, which are at the core of learning a powerful global\\nimage representation. There are different forms of attention\\naccording to the interaction of elements of the feature tensor\\n(local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses\\nonly one or two forms of attention and applies it to different\\nproblems like classication, detection or retrieval.\\nWe present global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network and\\nincorporates all four forms of attention: local and global,\\nspatial and channel. We obtain a new feature tensor and, by\\nspatial pooling, we learn a powerful embedding for image\\nretrieval. Focusing on global descriptors, we provide em-\\npirical evidence of the interaction of all forms of attention\\nand improve the state of the art on standard benchmarks.\\n1. ',\n",
       " '\\nInstance-level image retrieval is at the core of visual rep-\\nresentation learning and is connected with many problems\\nof visual recognition and machine learning, for instance\\nmetric learning [30, 26], few-shot learning [42] and unsu-\\npervised learning [8]. Many large-scale open datasets [3,\\n37, 16, 29, 53], and competitions1 have accelerated progress\\nin instance-level image retrieval, which has been trans-\\nformed by deep learning [3].\\nMany studies on instance-level image retrieval focus\\non learning features from convolutional neural networks\\n(CNN), while others focus on re-ranking, for instance by\\ngraph-based methods [11]. The former can be distinguished\\naccording to feature types: local descriptors, reminiscent of\\nSIFT [27], where an image is mapped to a few hundred vec-\\ntors; and global descriptors, where an image is mapped to a\\n1https://www.kaggle.com/c/landmark-retrieval-2020\\nsingle vector. In fact, deep learning has brought global de-\\nscriptors with astounding performance, while allowing ef-\\ncient search. Our study belongs to this type.\\nStudies on global descriptors have focused on spatial\\npooling [2, 37]. The need for compact, discriminative rep-\\nresentations that are resistant to clutter has naturally given\\nrise to spatial attention methods [24, 28]. Different kinds\\nof attention have been studied in many areas of computer\\nvision research. There is also channel attention [20, 9]; lo-\\ncal attention, applied independently to elements of the rep-\\nresentation (feature map) [54, 25]; global attention, based\\non interaction between elements [52, 9]; and combinations\\nthereof. Unfortunately, each study has been limited to one or\\ntwo kinds of attention only; attention is not always learned;\\nand applications vary.\\nIt is the objective of our work to perform a compre-\\nhensive study of all forms of attention above, apply them\\nto instance-level image retrieval and provide a detailed ac-\\ncount of their interaction and impact on performance. As\\nshown in Figure 1, we collect contextual information from\\nimages with both local and global attention, giving rise to\\ntwo parallel network streams. Importantly, each operates\\non both spatial locations and feature channels. Local at-\\ntention is about individual locations and channels; global is\\nabout interaction between locations and between channels.\\nThe extracted information is separately embedded in local\\nand global attention feature maps, which are combined in a\\nglobal-local attention feature map before pooling.\\nOur contributions can be summarized as follows:\\n1. We propose a novel network that consists of both\\nglobal and local attention for image retrieval. This is\\nthe rst study that employs both mechanisms.\\n2. Each of the global and local attention mechanisms\\ncomprises both spatial and channel attention.\\n3. Focusing on global descriptors, we provide empirical\\nevidence of the interaction of all forms of attention and\\nimprove the state of the art on standard benchmarks.\\n1\\narXiv:2107.08000v1  [cs.CV]  16 Jul 2021\\nAl\\nc\\nc  1  1\\n\\n+\\nFl\\nc\\nAl\\ns\\n1  h  w\\n\\n+\\nFl\\n\\nc  h  w\\nF\\n\\n+\\nc  h  w\\nFgl\\nAg\\nc\\nc  c\\n\\nFg\\nc\\nAg\\ns\\nhw  hw\\n\\n+\\nFg\\n\\nwl\\nw\\nwg\\nchannel attention\\nspatial attention\\nfusion\\nlocal attention\\nglobal attention\\nFigure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\\ntion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\\n(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\\nc),\\nlocal spatial (Al\\ns), global channel (Ag\\nc) and global spatial (Ag\\ns). The input feature map F is weighted into local (Fl) and\\nglobal (Fg) attention feature maps, which are fused with F to yield the global-local attention feature map Fgl. The diagram\\nis abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\\n2. Related work\\nInstance-level image retrieval\\nStudies on instance-level\\nimage retrieval can be roughly, but not exclusively, di-\\nvided into three types: (1) studies on global descriptors\\n[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\\ngeometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\\nby graph-based methods [11, 21, 55]. The rst two types\\nof studies focus on the feature representation, while the last\\ntype focuses on re-ranking extracted features.\\nStudies on global descriptors focus on spatial pooling\\nof CNN feature maps into vectors, including MAC [38],\\nSPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\\nand NetVLAD [1, 25], as well as learning the representa-\\ntion [3, 15, 16, 36, 37]. Studies before deep learning dom-\\ninated image retrieval were mostly based on local descrip-\\ntors like SIFT [27] and bag-of-words representation [32] or\\naggregated descriptors like VLAD [22] or ASMK [46]. Lo-\\ncal descriptors have been revived in deep learning, e.g. with\\nDELF [29], DELG [5] and ASMK extensions [45, 47].\\nWe focus on learning a global descriptor in this work, be-\\ncause it is the most efcient in terms of storage and search.\\nHowever, our generic attention mechanism produces a fea-\\nture tensor and could be applicable to local descriptors as\\nwell, if global pooling were replaced by local feature detec-\\ntion. Re-ranking methods are complementary to the repre-\\nsentation and we do not consider them in this work.\\nAttention\\nAttention mechanisms have been rst proposed\\nin image classication studies focusing on channel at-\\nMETHOD\\nLOCAL\\nGLOBAL\\nLRN RET\\nSpatial Channel Spatial Channel\\nSENet [20]\\n\\n\\nECA-Net [51]\\n\\n\\nGCNet [6]\\n\\n\\nCBAM [54]\\n\\n\\n\\nGE [19]\\n\\n\\nNL-Net [52]\\n\\n\\nAA-Net [4]\\n\\n\\nSAN [59]\\n\\n\\nN3Net [34]\\n\\n\\nA2-Net [9]\\n\\n\\nGSoP [14]\\n\\n\\nOnA [23]\\n\\n\\nAGeM [17]\\n\\n\\nCroW [24]\\n\\n\\n\\nCRN [25]\\n\\n\\n\\nDELF [29]\\n\\n\\n\\nDELG [5]\\n\\n\\n\\nTolias et al. [47]\\n\\n\\n\\nSOLAR [28]\\n\\n\\n\\nOurs\\n\\n\\n\\n\\n\\n\\nTable 1: Related work on attention. LRN: learned; RET: ap-\\nplied to instance-level image retrieval.\\ntention [20, 51, 6], spatial attention [19] or both, like\\nCBAM [54]. In image retrieval, CroW [24] also employs\\n2\\nfeature map\\nGAP\\nconv1d(k)\\nsigmoid\\nattention map\\nc  h  w\\nc  1  1\\nc  1  1\\nF\\nAl\\nc\\nFigure 2: Local channel attention.\\nboth spatial and channel attention and can be seen as a pre-\\ncursor of CBAM, but, like other studies of spatial attention\\non retrieval [41, 23, 17], it is not learned. CRN [25] ap-\\nplies spatial attention for feature reweighting and is learned.\\nLearned spatial attention mechanisms are common for local\\ndescriptors [29, 5, 47].\\nWe call the above methods local attention, in the sense\\nthat elements of the feature tensor (channels / spatial loca-\\ntions), are weighted independently, based on contextual in-\\nformation obtained by pooling or learned. By constrast, by\\nglobal attention we refer to mechanisms that model inter-\\naction between elements of the feature tensor, for example\\nbetween channels or between locations.\\nIn image classication, non-local neural network (NL-\\nNet) [52] is maybe the rst global attention mechanism, fol-\\nlowed by similar studies [4, 59, 34]. It is global spatial at-\\ntention, allowing interaction between any pair of spatial lo-\\ncations. Similarly, there are studies of global channel atten-\\ntion, allowing interaction between channels [9, 14]. Global\\nattention has focused mostly on image recognition and has\\nbeen applied to either spatial or channel attention so far, not\\nboth. In image retrieval, SOLAR [28] is a direct application\\nof the global spatial attention mechanism of [52].\\nTable 1 attempts to categorize related work on atten-\\ntion according to whether attention is local or global, spa-\\ntial or channel, whether it is learned and whether it is ap-\\nplied to instance-level image retrieval. We observe that all\\nmethods limit to one or two forms of attention only. Of\\nthose studies that focus on image retrieval, many are not\\nlearned [23, 17, 24], and of those that are, some are de-\\nsigned for local descriptors [29, 47].\\nBy contrast, we provide a comprehensive study of all\\nforms of attention, global and local, spatial and channel, to\\nobtain a learned representation in the form of a tensor that\\ncan be used in any way. We spatially pool it into a global\\ndescriptor and we study the relative gain of different forms\\nof attention in image retrieval.\\nfeature map\\nconv 1  1\\nconv 3  3\\nconv 5  5\\nconv 7  7\\nconcat\\nconv 1  1\\nattention map\\nc  h  w\\n4c  h  w\\n1  h  w\\nc  h  w\\ndilated\\nconv\\nF\\nF\\nAl\\ns\\nFigure 3: Local spatial attention. Convolutional layers in\\nblue implemented by dilated convolutions with kernel size\\n3  3 and dilation factors 1, 3, 5.\\n3. Global-local attention\\nWe design a global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network. Figure 1\\nillustrates its main components. We are given a c  h  w\\nfeature tensor F, where c is the number of channels, and\\nh  w is the spatial resolution. Local attention collects con-\\ntext from the image and applies pooling to obtain a c11\\nlocal channel attention map Al\\nc and a 1  h  w local spa-\\ntial attention map Al\\ns. Global attention allows interaction\\nbetween channels, resulting in a c  c global channel at-\\ntention map Ag\\nc, and between spatial locations, resulting in\\na hw  hw global spatial attention map Ag\\ns. The feature\\nmaps produced by the two attention streams are combined\\nwith the original one by a learned fusion mechanism into\\nthe global-local attention feature map Fgl before being spa-\\ntially pooled into a global image descriptor.\\n3.1. Local attention\\nWe extract an 1D channel and a 2D spatial attention map\\nto weigh the feature map in the corresponding dimensions.\\nLocal channel attention\\nFollowing ECA-Net [51], this\\nattention captures local channel information. As shown in\\nFigure 2, we are given a chw feature tensor F from our\\nbackbone. We rst reduce it to a c  1  1 tensor by global\\naverage pooling (GAP). Channel attention is then captured\\nby a 1D convolution of kernel size k along the channel di-\\nmension, where k controls the extent of cross-channel inter-\\naction. This is followed by a sigmoid function, resulting in\\nthe c  1  1 local channel attention map Al\\nc.\\nLocal spatial attention\\nInspired by the inception mod-\\nule [43] and similar to [25], this attention map captures local\\nspatial information at different scales. As shown in Figure 3,\\n3\\nfeature map\\nGAP\\nconv1d(k)\\nconv1d(k)\\nsigmoid\\nsigmoid\\n\\n\\nsoftmax\\nattention feature map\\n1  c\\n1  c\\n1  c\\nQc\\nc  c\\nhw  c\\nVc\\nAg\\nc\\nc  h  w\\n1  c\\n1  c\\nKc\\nF\\nGc\\nFigure 4: Global channel attention.\\ngiven the same c  h  w feature tensor F from our back-\\nbone, we obtain a new tensor F with channels reduced to\\nc, using a 1  1 convolution. We then extract local spatial\\ncontextual information using convolutional lters of kernel\\nsize 3  3, 5  5, and 7  7, which are efciently imple-\\nmented by 3  3 dilated convolutions [7, 57] with dilation\\nparameter 1, 2, and 3 respectively. The resulting features,\\nalong with one obtained by 1  1 convolution on F, are\\nconcatenated into a 4c  h  w tensor. Finally, we obtain\\nthe 1  h  w local spatial attention map Al\\ns by a 1  1\\nconvolution that reduces the channel dimension to 1.\\nThe middle column of Figure 6 shows heat maps of local\\nspatial attention, localizing target objects in images.\\nLocal attention feature map\\nWe use the local channel\\nattention map Al\\nc to weigh F in the channel dimension\\nFl\\nc := F  Al\\nc + F.\\n(1)\\nWe then use local spatial attention map Al\\ns to weigh Fl\\nc\\nin the spatial dimensions, resulting in the c  h  w local\\nattention feature map\\nFl = Fl\\nc  Al\\ns + Fl\\nc.\\n(2)\\nHere, AB denotes an element-wise multiplication of ten-\\nsors A and B, with broadcasting when one tensor is smaller.\\nWe adopt the choice of applying channel followed by spa-\\ntial attention from convolutional block attention module\\nCBAM [54]. However, apart from computing Al\\ns at differ-\\nent scales, both attention maps are obtained from the orig-\\ninal tensor F rather than sequentially. In addition, both (1)\\nand (2) include residual connections, while CBAM includes\\na single residual connection over both steps.\\n3.2. Global attention\\nWe extract two matrices capturing global pairwise chan-\\nnel and spatial interaction to weigh the feature map.\\nfeature map\\nconv 1  1\\nconv 1  1\\nconv 1  1\\n\\n\\nsoftmax\\nconv 1  1\\nattention feature map\\nc  hw\\nQs\\nhw  hw\\nc  h  w\\nc  hw\\nVs\\nc  h  w\\nAg\\ns\\nc  h  w\\nc  hw\\nKc\\nF\\nGs\\nFigure 5: Global spatial attention.\\nGlobal channel attention\\nWe introduce a global channel\\nattention mechanism that captures global channel interac-\\ntion. This mechanism is based on the non-local neural net-\\nwork [52], but with the idea of 1D convolution from ECA-\\nNet [51]. As shown in Figure 4, we are given the c  h  w\\nfeature tensor F from our backbone. We apply GAP and\\nsqueeze spatial dimensions, followed by a 1D convolution\\nof kernel size k and a sigmoid function, to obtain 1c query\\nQc and key Kc tensors. The value tensor Vc is obtained by\\nmere reshaping of F to hwc, without GAP. Next, we form\\nthe outer product of Kc and Qc, followed by softmax over\\nchannels to obtain a c  c global channel attention map\\nAg\\nc = softmax(Kc\\nQc).\\n(3)\\nFinally, this attention map is multiplied with Vc and the ma-\\ntrix product VcAg\\nc is reshaped back to chw to give the\\nglobal channel attention feature map Gc. In GSoP [14] and\\nA2-Net [9], a cc global channel attention map is obtained\\nby multiplication of hw  c matrices; (3) is more efcient,\\nusing only an outer product of 1  c vectors.\\nGlobal spatial attention\\nSince ordinary convolution ap-\\nplies only a local neighborhood at a time, it cannot capture\\nglobal contextual information. Thus, we apply non-local l-\\ntering [52], which is a form of self-attention [49] in the spa-\\ntial dimensions. As shown in Figure 5, we are given the\\nsame c  h  w feature tensor F from our backbone. By\\nusing three 11 convolutions, which reduce channels to c,\\nand attening spatial dimensions to hw, we obtain c  hw\\nquery Qs, key Ks, and value Vs tensors, where each col-\\numn is a feature vector corresponding to a particular spatial\\nlocation. We capture pairwise similarities of these vectors\\nby matrix multiplication of Ks and Qs, followed by soft-\\nmax over locations to obtain a hw  hw global spatial at-\\ntention map:\\nAg\\ns = softmax(K\\ns Qs).\\n(4)\\n4\\nThis attention map is multiplied with Vs and the matrix\\nproduct VsAg\\ns is reshaped back to c hw by expanding\\nthe spatial dimensions. Finally, using a 1  1 convolution,\\nwhich increases channels back to c, we obtain the chw\\nglobal spatial attention feature map Gs.\\nThe right column of Figure 6 shows heat maps for global\\nspatial attention, localizing target objects in images.\\nGlobal attention feature map\\nWe use the global channel\\nattention feature map Fc to weigh F element-wise\\nFg\\nc = F  Gc.\\n(5)\\nWe then use global spatial attention feature map Gs to\\nweigh Fg\\nc element-wise, resulting in the c  h  w global\\nattention feature map\\nFg = Fg\\nc  Gs + Fg\\nc.\\n(6)\\nSimilarly to Fl in (1) and (2), we apply channel attention\\nrst, followed by spatial attention. However, unlike (1),\\nthere is no residual connection in (5). This choice is sup-\\nported by early experiments.\\n3.3. Global-local attention\\nFeature fusion\\nAs shown in Figure 1, we combine the\\nlocal and global attention feature maps, Fl and Fg, with\\nthe original feature F. While concatenation and summation\\nare common operations for feature combination, we use a\\nweighted average with weights wl, wg, w respectively, ob-\\ntained by softmax over three learnable scalar parameters, to\\nobtain a c  h  w global-local attention feature map\\nFgl = wlFl + wgFl + wF.\\n(7)\\nEfcientDet [44] has shown that this is the most effective,\\namong a number of choices, for fusion of features across\\ndifferent scales.\\nPooling\\nWe apply GeM [37], a learnable spatial pooling\\nmechanism, to feature map Fgl (7), followed by a fully-\\nconnected (FC) layer with dropout and batch normalization.\\nThe nal embedding is obtained by 2-normalization.\\n4. Experiments\\n4.1. Datasets\\nTraining set\\nThere are a number of open landmark\\ndatasets commonly used for training in image retrieval stud-\\nies, including neural code (NC) [3], neural code clean (NC-\\nclean) [16], as well as Google Landmarks v1 (GLDv1) [29]\\nand v2 (GLDv2) [53]. Table 2 shows relevant statistics.\\nThese datasets can be categorized into noisy and clean. The\\nclean sets were obtained from the original noisy sets for\\nmore effective training [16, 53]. The original noisy datasets\\nare much larger, but they have high intra-class variability.\\n(a) input\\n(b) local\\n(c) global\\nFigure 6: Local and global spatial attention. Left: input\\nimages. Middle: local spatial attention heat maps. Right:\\nglobal spatial attention heat maps. Red (blue) means higher\\n(lower) attention weight.\\nEach class can include visually dissimilar images such as\\nexterior and interior views of a building or landmark, in-\\ncluding oor plans and paintings inside. The clean datasets\\nfocus on views directly relevant to landmark recognition but\\nhave a much smaller number of images.\\nEvaluation set and metrics\\nWe use four common eval-\\nuation datasets for landmark image retrieval: Oxford5k\\n(Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox-\\nford (ROxford or ROxf) and Paris (RParis or RPar) [35].\\nROxford and RParis are used with and without one million\\ndistractors (R1M) [28] and evaluated using the Medium and\\nHard protocols [35]. We evaluate using mean Average Pre-\\ncision (mAP) and mean precision at 10 (mP@10).\\n4.2. Implementation details\\nWe train on 8 TITAN RTX 2080Ti GPUs. All models are\\npre-trained on ImageNet [39] and implemented in PyTorch\\n[31]. For fair comparisons, we set a training environment\\n5\\nFigure 7: Examples of our ranking results. In each row, the rst image on the left (pink dotted outline) is a query image with a\\ntarget object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images\\nfor the query; red solid outline: negative.\\nsimilar to the those of compared studies [56, 53, 28, 35]. We\\nemploy ResNet101 [18] as a backbone model. The kernel\\nsize k of ECANet in subsection 3.1 is set to 3. The param-\\neter p of GeM in subsection 3.3 is set to 3 and the dimen-\\nsion d of nal embeddings to 512. We adopt ArcFace [10],\\na cosine-softmax based loss, with a margin of 0.3. We use\\nstochastic gradient descent with initial learning rate 103,\\nmomentum 0.9 and weight decay 105.\\nWe adopt the batch sampling of Yokoo et al. [56] where\\nmini-batch samples with similar aspect ratios are resized to\\na particular size. Here, we use a batch size of 64. For image\\naugmentation, we apply scaling, random cropping, and var-\\nied illumination. At inference, we apply a multi-resolution\\nrepresentation [16] to query and database images.\\nOur method is denoted as GLAM (global-local atten-\\ntion module). Using the backbone model alone is referred\\nto as baseline. It is compatible with recent models based\\non ResNet101-GeM trained with ArcFace [53, 28]. Adding\\nour local attention (subsection 3.1) to the baseline model is\\ndenoted +local, while adding our global attention (subsec-\\ntion 3.2) is denoted +global. Since we focus on representa-\\ntion learning, we do not consider post-processing methods\\nlike geometry-based re-ranking [29, 40, 53] or graph-based\\nre-ranking [11, 21, 55].\\n4.3. Benchmarking\\nNoisy vs. clean training sets\\nWe begin by training our\\nbest model (baseline+local+global) on all training sets of\\nTable 2, except NC-noisy because some images are cur-\\nrently unavailable. As shown in Table 3, even though\\nTRAIN SET\\n#IMAGES\\n#CLASSES\\nNC-noisy\\n213,678\\n672\\nNC-clean\\n27,965\\n581\\nSfM-120k\\n117,369\\n713\\nGLDv1-noisy\\n1,225,029\\n14, 951\\nGLDv2-noisy\\n4,132,914\\n203,094\\nGLDv2-clean\\n1,580,470\\n81,313\\nTable 2: Statistics of different training sets.\\nMETHOD\\nTRAIN SET\\nDIM OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGeM-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7\\n77.2\\n38.5\\n56.3\\nSOLAR [28]\\nGLDv1-noisy 2048\\n\\n\\n69.9\\n81.6\\n47.9\\n64.5\\nGLDv2 [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n84.9\\n51.6\\n70.3\\nGLAM (Ours)\\nNC-clean\\n512\\n77.8\\n85.8\\n51.6\\n68.1\\n20.9\\n44.7\\nGLDv1-noisy 512\\n92.8\\n95.0\\n73.7\\n83.5\\n49.8\\n69.4\\nGLDv2-noisy 512\\n93.3\\n95.3\\n75.7\\n86.0\\n53.1\\n73.8\\nGLDv2-clean 512\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 3: mAP comparison of our best model (base-\\nline+local+global) trained on different training sets against\\n[53, 28]. All models use ResNet101-GeM. Red: best results.\\nBlue: GLAM higher than SOLAR [28] on GLDv1-noisy.\\nGLDv2-noisy has 2.6 times more images than GLDv2-\\nclean, the latter is superior by a large margin. This shows\\nthat, in training, a cleaner dataset can be more important\\nthan a larger one. By contrast, NC-clean has the worst\\nperformance despite being clean, aparently because it is\\n6\\nMETHOD\\nTRAIN SET\\nDIM\\nBASE\\nMEDIUM\\nHARD\\nOx5k Par6k\\nROxf\\n+R1M\\nRPar\\n+R1M\\nROxf\\n+R1M\\nRPar\\n+R1M\\nmAP\\nmAP mAP mP mAP mP mAP mP mAP mP\\nmAP mP mAP mP mAP mP mAP mP\\nSPoC-V16 [2, 35]\\n[O]\\n512\\n53.1\\n\\n38.0 54.6 17.1 33.3 59.8 93.0 30.3 83.0 11.4 20.9\\n0.9\\n2.9\\n32.4 69.7\\n7.6\\n30.6\\nSPoC-R101 [35]\\n[O]\\n2048\\n\\n\\n39.8 61.0 21.5 40.4 69.2 96.7 41.6 92.0 12.4 23.8\\n2.8\\n5.6\\n44.7 78.0 15.3 54.4\\nCroW-V16 [24, 35]\\n[O]\\n512\\n70.8\\n79.7\\n41.4 58.8 22.5 40.5 62.9 94.4 34.1 87.1 13.9 25.7\\n3.0\\n6.6\\n36.9 77.9 10.3 45.1\\nCroW-R101 [35]\\n[O]\\n2048\\n\\n\\n42.4 61.9 21.2 39.4 70.4 97.1 42.7 92.9 13.3 27.7\\n3.3\\n9.3\\n47.2 83.6 16.3 61.6\\nMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.0\\n82.9\\n37.8 57.8 21.8 39.7 59.2 93.3 33.6 87.1 14.6 27.0\\n7.4\\n11.9 35.9 78.4 13.2 54.7\\nMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n41.7 65.0 24.2 43.7 66.2 96.4 40.8 93.0 18.0 32.9\\n5.7\\n14.4 44.1 86.3 18.2 67.7\\nRMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.1\\n85.0\\n42.5 62.8 21.7 40.3 66.2 95.4 39.9 88.9 12.0 26.1\\n1.7\\n5.8\\n40.9 77.1 14.8 54.0\\nRMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n49.8 68.9 29.2 48.9 74.0 97.7 49.3 93.7 18.5 32.2\\n4.5\\n13.0 52.1 87.1 21.3 67.4\\nRMAC-R101-Triplet [16, 35]\\nNC-clean\\n2048\\n86.1\\n94.5\\n60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0\\nGeM-R101-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3\\nAGeM-R101-Siamese [17]\\nSfM-120k\\n2048\\n\\n\\n67.0\\n\\n\\n\\n78.1\\n\\n\\n\\n40.7\\n\\n\\n\\n57.3\\n\\n\\n\\nSOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048\\n\\n\\n69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6\\nDELG-GeM-R101-ArcFace [5]\\nGLDv1-noisy 2048\\n\\n\\n73.2\\n\\n54.8\\n\\n82.4\\n\\n61.8\\n\\n51.2\\n\\n30.3\\n\\n64.7\\n\\n35.5\\n\\nGeM-R101-ArcFace [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n\\n\\n\\n84.9\\n\\n\\n\\n51.6\\n\\n\\n\\n70.3\\n\\n\\n\\nGLAM-GeM-R101-ArcFace baseline\\nGLDv2-clean\\n512\\n91.9\\n94.5\\n72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7\\n+local\\nGLDv2-clean\\n512\\n91.2\\n95.4\\n73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1\\n+global\\nGLDv2-clean\\n512\\n92.3\\n95.3\\n77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0\\n+global+local\\nGLDv2-clean\\n512\\n94.2\\n95.6\\n78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0\\nTable 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16:\\nVGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). : dimension d = 256 [2]. mP: mP@10. Red:\\nbest results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only\\nmodel other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\\ntoo small. To achieve best possible performance, we use\\nGLDv2-clean as a training set in the remaining experiments.\\nComparisons on same training set\\nIt is common to com-\\npare methods regardless of training sets as more become\\navailable, e.g., [35, 28]. Since GLDv2-clean is relatively\\nnew, Weyand et al. [53], which introduced the dataset, is the\\nonly study that has trained the same backbone with the same\\nsettings (ResNet101-GeM with ArcFace) on GLDv2-clean.\\nOur baseline is lower than [53], because our dimensinality is\\n512, while other models based on ResNet101 use 2048. Yet,\\nTable 3 shows that our best model trained on GLDv2-clean\\noutperforms [53] by a large margin. But the most impor-\\ntant comparison is with SOLAR [28], also based on self-\\nattention, which has trained ResNet101-GeM on GLDv1-\\nnoisy. On this training set, our best model clearly outper-\\nforms [28] despite lower dimensionality.\\nComparison with state of the art\\nTable 4 shows the\\nperformance of four variants of our model, i.e. baseline\\nwith or without local/global attention, and compares them\\nagainst state-of-the-art (SOTA) methods based on global de-\\nscriptors without re-ranking on the complete set of bench-\\nmarks, including distractors. Both local and global atten-\\ntion bring signicant gain over the baseline. The effect\\nof global is stronger, while the gain of the two is addi-\\ntive in the combination. The best results are achieved by\\nthe global-local attention network (baseline+global+local).\\nWith this model, we outperform previous best methods\\non most benchmarks except mP@10 on RParis (medium)\\nand RParis+R1M (medium), where we are outperformed\\nby [37, 35]. These results demonstrate that our approach is\\neffective for landmark image retrieval. Figure 7 shows some\\nMETHOD\\nOXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGLAM baseline\\n91.9\\n94.5\\n72.8\\n84.2\\n49.9\\n69.7\\n+local-channel\\n91.3\\n95.3\\n72.2\\n85.8\\n48.3\\n73.1\\n+local-spatial\\n91.0\\n95.1\\n72.1\\n85.3\\n48.3\\n71.9\\n+local\\n91.2\\n95.4\\n73.7\\n86.5\\n52.6\\n75.0\\n+global-channel\\n92.5\\n94.4\\n73.3\\n84.4\\n49.8\\n70.1\\n+global-spatial\\n92.4\\n95.1\\n73.2\\n86.3\\n50.0\\n72.7\\n+global\\n92.3\\n95.3\\n77.2\\n86.7\\n57.4\\n75.0\\n+global+local\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 5: mAP comparison of spatial and channel variants\\nof our local (+local, subsection 3.1) and global (+global,\\nsubsection 3.1) attention modules to the baseline.\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nCBAM style\\n93.8\\n95.7\\n75.6\\n88.4\\n53.3\\n76.8\\nGLAM (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 6: mAP comparison between CBAM style and our\\nlocal spatial attention.\\nexamples of our ranking results.\\n4.4. Ablation study\\nOur ablation study uses the Google Landmark v2 clean\\ndataset (GLDv2-clean) [53] for training, which is shown to\\nbe the most effective in Table 3.\\n7\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf\\nRPar\\nROxf\\nRPar\\nConcatenate\\n89.5\\n95.1\\n73.6\\n86.5\\n54.0\\n73.7\\nSum (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 7: mAP comparison between weighted concatenation\\nand weighted average for feature fusion.\\nMETHOD\\nOXF5K PAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nFixed-size\\n76.1\\n82.6\\n55.7\\n68.4\\n29.2\\n47.5\\nGroup-size (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 8: mAP comparison between xed-size (224  224)\\nand group-size sampling methods.\\nQUERY DATABASE OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nSingle\\nSingle\\n93.3\\n95.2\\n76.9\\n87.1\\n58.6\\n74.7\\nMulti\\nSingle\\n93.9\\n95.4\\n78.0\\n87.7\\n59.0\\n75.5\\nSingle\\nMulti\\n93.6\\n95.6\\n77.0\\n87.8\\n57.1\\n76.0\\nMulti\\nMulti\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 9: mAP comparison of using multiresolution repre-\\nsentation (Multi) or not (Single) on query or database.\\nEffect of attention modules\\nWe ablate the effect of our\\nlocal and global attention networks as well as their com-\\nbination. Table 5 shows the results, which are more ne-\\ngrained than those of Table 4. In particular, it shows the ef-\\nfect of the channel and spatial variants of both local and\\nglobal attention. We observe that, when used alone, the\\nchannel and spatial variants of local attention are harmful\\nin most cases. Even the combination, baseline+local, is not\\nalways effective. By contrast, when used alone, the channel\\nand spatial variants of global attention are mostly benecial,\\nespecially the latter. Their combination, baseline+global, is\\nimpressive, bringing gain of up to 7.5%. Importantly, the\\ncombination baseline+global+local improves further by up\\nto another 2.8%. This result shows the necessity of local\\nattention in the nal model.\\nCBAM vs. our local spatial attention\\nWe experiment\\nwith the local spatial attention of CBAM [54]. CBAM ap-\\nplies average and max-pooling to input features and con-\\ncatenates the two for spatial attention. We apply this vari-\\nant to our local spatial attention module for comparison.\\nFor the CBAM style module, we keep the overall design\\nof our module as shown in Figure 3, but apply average and\\nmax-pooling to each of the four convolutional layer outputs\\nbefore concatenation. Table 6 shows that the CBAM style\\nmodule is considerably worse than ours on all benchmarks\\nexcept Paris6k, where it is only slightly better.\\nConcatenation vs. sum for feature fusion\\nWe use a\\nsoftmax-based weighted average of local and global atten-\\ntion feature maps with the original feature map (7). Here,\\nwe compare this weighted average with weighted concate-\\nnation, where concatenation replaces the sum operation\\nin (7). As shown in Table 7, the weighted average outper-\\nforms the weighted concatenation.\\nFixed-size vs. group-size sampling\\nNumerous studies\\nhave proposed methods for constructing batches according\\nto image size for efcient training. For instance, Gordo et\\nal. [16], DELF [29], and Yokoo et al. [56] employed dif-\\nferent image sizes per batch for training instead of a single\\nxed size. We adopt the method of Yokoo et al., which con-\\nstructs a batch with images of similar aspect ratio, so that\\nthe images can be resized to a size with an aspect ratio that\\nis similar to their own. We call this method group-size sam-\\npling. Table 8 compares xed-size (224  224) with group-\\nsize sampling. We observe that maintaining aspect ratios by\\nusing dynamic input sizes is much more effective.\\nMulti-resolution\\nWe use the multi-resolution representa-\\ntion [16] for the nal feature of an image at inference time.\\nThis method: (1) resizes an image into multiple scales; (2)\\nextracts features from the resized images; and (3) averages\\nthe features to obtain the nal feature of the image. The\\nmethod is applied to both query and database images to en-\\nhance ranking results, especially for small target objects.\\nTable 9 compares the four cases of applying this method or\\nnot to query or database images.\\n5. Conclusion\\nWe have introduced a novel approach that extracts global\\nand local contextual information using attention mecha-\\nnisms for instance-level image retrieval. It is manifested as\\na network architecture consisting of global and local atten-\\ntion components, each operating on both spatial and chan-\\nnel dimensions. This constitutes a comprehensive study and\\nempirical evaluation of all four forms of attention that have\\npreviously been studied only in isolation. Our ndings indi-\\ncate that the gain (or loss) brought by one form of attention\\nalone strongly depends on the presence of the others, with\\nthe maximum gain appearing when all forms are present.\\nThe output is a modied feature tensor that can be used in\\nany way, for instance with local feature detection instead of\\nspatial pooling for image retrieval.\\nWith the advent of vision transformers [12, 58] and their\\nrecent application to image retrieval [13], attention is ex-\\npected to play a more and more signicant role in vi-\\nsion. According to our classication, transformers perform\\nglobal spatial attention alone. It is of great interest to in-\\nvestigate the role of the other forms of attention, where our\\n8\\napproach may yield a basic building block of such archi-\\ntectures. One may even envision an extension to language\\nmodels, where transformers originate from [50].\\nReferences\\n[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-\\njdla, and Josef Sivic.\\nNetVLAD: CNN architecture for\\nweakly supervised place recognition. In CVPR, 2016. 2\\n[2] Artem Babenko and Victor Lempitsky. Aggregating Local\\nDeep Features for Image Retrieval. In ICCV, 2015. 1, 2, 7\\n[3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and\\nVictor Lempitsky.\\nNeural Codes for Image Retrieval.\\nIn\\nECCV, 2014. 1, 2, 5\\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\\nand Quoc V. Le.\\nAttention augmented convolutional net-\\nworks. In ICCV, 2019. 2, 3\\n[5] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep\\nlocal and global features for image search. In ECCV, 2020.\\n2, 3, 7\\n[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.\\nGCNet: Non-Local Networks Meet Squeeze-Excitation Net-\\nworks and Beyond. In ICCV, 2019. 2\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for seman-\\ntic image segmentation. arXiv preprint arXiv:1706.05587,\\n2017. 4\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In ICML, 2020. 1\\n[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. A2-nets: Double attention networks.\\nIn NeurIPS, 2018. 1, 2, 3, 4\\n[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. ArcFace: Additive Angular Margin Loss for Deep\\nFace Recognition. In CVPR, 2019. 6\\n[11] Michael Donoser and Horst Bischof. Diffusion Processes for\\nRetrieval Revisited. In CVPR, 2013. 1, 2, 6\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale.\\narXiv preprint\\narXiv:2010.11929, 2020. 8\\n[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\\nHerve Jegou.\\nTraining vision transformers for image re-\\ntrieval. Technical report, 2021. 8\\n[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\\nsecond-order pooling convolutional networks.\\nIn CVPR,\\n2019. 2, 3, 4\\n[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. Deep image retrieval: Learning global representations\\nfor image search. In ECCV, 2016. 2\\n[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. End-to-end learning of deep visual representations for\\nimage retrieval. IJCV, 2017. 1, 2, 5, 6, 7, 8\\n[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie.\\nAttention-\\naware generalized mean pooling for image retrieval. arXiv\\npreprint arXiv:1811.00202, 2018. 2, 3, 7\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition.\\nIn CVPR,\\n2016. 6\\n[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in con-\\nvolutional neural networks. In NeurIPS, 2018. 2\\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\\nSqueeze-and-Excitation Networks. In CVPR, 2018. 1, 2\\n[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,\\nand Ondrej Chum. Efcient diffusion on region manifolds:\\nRecovering small objects with compact cnn representations.\\nIn CVPR, 2017. 2, 6\\n[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into com-\\npact codes. PAMI, (99):11, 2011. 2\\n[23] Albert Jimenez, Jose M. Alvarez, and Xavier Giro-i-Nieto.\\nClass weighted convolutional features for visual instance\\nsearch. In BMVC, 2017. 2, 3\\n[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.\\nCrossdimensional weighting for aggregated deep convolu-\\ntional features. In ECCV, 2016. 1, 2, 3, 7\\n[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.\\nLearned Contextual Feature Reweighting for Image Geo-\\nLocalization. In CVPR, 2017. 1, 2, 3\\n[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\\nProxy anchor loss for deep metric learning. In CVPR, 2020.\\n1\\n[27] David G. Lowe.\\nDistinctive image features from scale-\\ninvariant keypoints. In IJCV, 2004. 1, 2\\n[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian\\nMikolajczyk. SOLAR: Second-Order Loss and Attention for\\nImage Retrieval. In ECCV, 2020. 1, 2, 3, 5, 6, 7\\n[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,\\nand Bohyung Han. Large Scale Image Retrieval with Atten-\\ntive Deep Local Features. In ICCV, 2017. 1, 2, 3, 5, 6, 8\\n[30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\\nSavarese. Deep metric learning via lifted structured feature\\nembedding. In CVPR, 2016. 1\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas Kopf, Edward Yang, Zach DeVito, Martin Raison,\\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\\nperative style, high-performance deep learning. In NeurIPS,\\n2019. 5\\n[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Object retrieval with large vocabularies\\nand fast spatial matching. In CVPR, 2007. 2, 5\\n[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Lost in quantization:Improving particu-\\nlar object retrieval in large scale image databases. In CVPR,\\n2008. 5\\n9\\n[34] Tobias Plotz and Stefan Roth. Neural nearest neighbors net-\\nworks. In NeurIPS, 2018. 2, 3\\n[35] Filip Radenovic, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ondrej Chum. Revisiting Oxford and Paris:\\nLarge-Scale Image Retrieval Benchmarking. In CVPR, 2018.\\n5, 6, 7\\n[36] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. CNN\\nimage retrieval learns from BoW: Unsupervised ne-tuning\\nwith hard examples. In ECCV, 2016. 2, 7\\n[37] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. Fine-\\nTuning CNN Image Retrieval with No Human Annotation.\\nIn TPAMI, 2019. 1, 2, 5, 6, 7\\n[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson,\\nand Atsuto Maki. Visual Instance Retrieval with Deep Con-\\nvolutional Networks. In CoRR, 2015. 2\\n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge. In International booktitle of Computer Vision, 2015.\\n5\\n[40] Oriane Simeoni, Yannis Avrithis, and Ondrej Chum. Local\\nfeatures and visual words emerge in activations. In CVPR,\\n2019. 2, 6\\n[41] O. Simeoni, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.\\nGraph-based particular object discovery. Machine Vision and\\nApplications, 30(2):243254, 3 2019. 3\\n[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\\nical networks for few-shot learning. In NeurIPS, 2017. 1\\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich.\\nGoing deeper with\\nconvolutions. In CVPR, 2015. 3\\n[44] Mingxing Tan, Ruoming Pang, and Quoc V. Le. EfcientDet:\\nScalable and Efcient Object Detection. In CVPR, 2020. 5\\n[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack\\nSim.\\nDetect-to-retrieve: Efcient regional aggregation for\\nimage search. In CVPR, 2019. 2\\n[46] Giorgios Tolias, Yannis Avrithis, and Herve Jegou. To aggre-\\ngate or not to aggregate: Selective match kernels for image\\nsearch. In ICCV, 2013. 2\\n[47] Giorgos Tolias, Tomas Jenicek, and Ondrej Chum. Learn-\\ning and aggregating deep local descriptors for instance-level\\nrecognition. In ECCV, 2020. 2, 3\\n[48] Giorgos Tolias, Ronan Sicre, and Herve Jegou. Particular ob-\\nject retrieval with integral max-pooling of CNN activations.\\nIn ICLR, 2016. 2\\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 4\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 9\\n[51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\\nmeng Zuo, and Qinghua Hu.\\nECA-Net: Efcient Chan-\\nnel Attention for Deep Convolutional Neural Networks. In\\nCVPR, 2020. 2, 3, 4\\n[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local Neural Networks. In CVPR, 2018. 1, 2,\\n3, 4\\n[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle Landmarks Dataset v2 - A Large-Scale Benchmark\\nfor Instance-Level Recognition and Retrieval.\\nIn CVPR,\\n2020. 1, 2, 5, 6, 7\\n[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\\nKweon. CBAM: Convolutional Block Attention Module. In\\nECCV, 2018. 1, 2, 4, 8\\n[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and\\nShinichi Satoh. Efcient image retrieval via decoupling dif-\\nfusion into online and ofine processing. In AAAI, 2019. 2,\\n6\\n[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi\\nIizuka. Two-stage Discriminative Re-ranking for Large-scale\\nLandmark Retrieval. In arXiv:2003.11211, 2020. 6, 8\\n[57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\\nresidual networks. In CVPR, 2017. 4\\n[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\\nto-token vit: Training vision transformers from scratch on\\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 8\\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\\nself-attention for image recognition. In CVPR, 2020. 2, 3\\n10\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.split(\"Introduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstructured IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yolox_l0.05.onnx: 100%|| 217M/217M [00:02<00:00, 98.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 16:26:16 - Reading PDF for file: pdfs/2107.08000v1.pdf ...\n",
      "2024-01-24 16:26:16 - Detecting page elements ...\n",
      "2024-01-24 16:26:17 - Detecting page elements ...\n",
      "2024-01-24 16:26:18 - Detecting page elements ...\n",
      "2024-01-24 16:26:18 - Detecting page elements ...\n",
      "2024-01-24 16:26:19 - Detecting page elements ...\n",
      "2024-01-24 16:26:19 - Detecting page elements ...\n",
      "2024-01-24 16:26:20 - Detecting page elements ...\n",
      "2024-01-24 16:26:21 - Detecting page elements ...\n",
      "2024-01-24 16:26:21 - Detecting page elements ...\n",
      "2024-01-24 16:26:22 - Detecting page elements ...\n",
      "2024-01-24 16:26:24 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:26 - Loading the Table agent ...\n",
      "2024-01-24 16:26:26 - Loading the table structure model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|| 1.47k/1.47k [00:00<00:00, 9.55MB/s]\n",
      "model.safetensors: 100%|| 115M/115M [00:01<00:00, 72.4MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 16:26:29 - Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "model.safetensors: 100%|| 46.8M/46.8M [00:00<00:00, 92.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 16:26:29 - [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 16:26:31 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:33 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:33 - padding image by 20 for structure detection\n",
      "2024-01-24 16:26:36 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:38 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:41 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:43 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:45 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:45 - padding image by 20 for structure detection\n",
      "2024-01-24 16:26:45 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:46 - padding image by 20 for structure detection\n",
      "2024-01-24 16:26:46 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:50 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:51 - padding image by 20 for structure detection\n",
      "2024-01-24 16:26:52 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:52 - padding image by 20 for structure detection\n",
      "2024-01-24 16:26:53 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:53 - padding image by 20 for structure detection\n",
      "2024-01-24 16:26:53 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:56 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:56 - padding image by 20 for structure detection\n",
      "2024-01-24 16:26:56 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:56 - padding image by 20 for structure detection\n",
      "2024-01-24 16:26:56 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:26:56 - padding image by 20 for structure detection\n",
      "2024-01-24 16:26:57 - Processing entire page OCR with tesseract...\n",
      "2024-01-24 16:27:00 - Processing entire page OCR with tesseract...\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "doc = partition_pdf(\n",
    "    filename = \"pdfs/2107.08000v1.pdf\",\n",
    "    strategy = \"hi_res\",\n",
    "    infer_table_structure=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 0 2\n"
     ]
    }
   ],
   "source": [
    "print(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the attention you need: Global-local, spatial-channel attention for image retrieval\n",
      "Chull Hwan Song Odd Concepts\n",
      "Hye Joo Han Odd Concepts\n",
      "Yannis Avrithis Inria, Univ Rennes, CNRS, IRISA\n",
      "Abstract\n",
      "Abstract\n",
      "1. Introduction\n",
      "1https://www.kaggle.com/c/landmark-retrieval-2020\n",
      "local attention\n",
      "2. Related work\n",
      "3. Global-local attention\n",
      "3.1. Local attention\n",
      "F\n",
      "Gc\n",
      "3.2. Global attention\n",
      "Gs\n",
      "3.3. Global-local attention\n",
      "4. Experiments\n",
      "4.1. Datasets\n",
      "4.2. Implementation details\n",
      "4.3. Benchmarking\n",
      "4.4. Ablation study\n",
      "5. Conclusion\n",
      "References\n"
     ]
    }
   ],
   "source": [
    "for el in doc:\n",
    "    if 'Title' in repr(el):\n",
    "        print(el.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 0 2\n",
      "l u J\n",
      "6 1\n",
      "]\n",
      "V C . s c [ 1 v 0 0 0 8 0 . 7 0 1 2 : v i X r a\n",
      "All the attention you need: Global-local, spatial-channel attention for image retrieval\n",
      "Chull Hwan Song Odd Concepts\n",
      "Hye Joo Han Odd Concepts\n",
      "Yannis Avrithis Inria, Univ Rennes, CNRS, IRISA\n",
      "Abstract\n",
      "Abstract\n",
      "We address representation learning for large-scale instance-level image retrieval. Apart from backbone, train- ing pipelines and loss functions, popular approaches have focused on different spatial pooling and attention mecha- nisms, which are at the core of learning a powerful global image representation. There are different forms of attention according to the interaction of elements of the feature tensor (local and global) and the dimensions where it is applied (spatial and channel). Unfortunately, each study addresses only one or two forms of attention and applies it to different problems like classication, detection or retrieval.\n",
      "We present global-local attention module (GLAM), which is attached at the end of a backbone network and incorporates all four forms of attention: local and global, spatial and channel. We obtain a new feature tensor and, by spatial pooling, we learn a powerful embedding for image retrieval. Focusing on global descriptors, we provide em- pirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.\n",
      "1. Introduction\n",
      "Instance-level image retrieval is at the core of visual rep- resentation learning and is connected with many problems of visual recognition and machine learning, for instance metric learning [30, 26], few-shot learning [42] and unsu- pervised learning [8]. Many large-scale open datasets [3, 37, 16, 29, 53], and competitions1 have accelerated progress in instance-level image retrieval, which has been trans- formed by deep learning [3].\n",
      "single vector. In fact, deep learning has brought global de- scriptors with astounding performance, while allowing ef- cient search. Our study belongs to this type.\n",
      "Studies on global descriptors have focused on spatial pooling [2, 37]. The need for compact, discriminative rep- resentations that are resistant to clutter has naturally given rise to spatial attention methods [24, 28]. Different kinds of attention have been studied in many areas of computer vision research. There is also channel attention [20, 9]; lo- cal attention, applied independently to elements of the rep- resentation (feature map) [54, 25]; global attention, based on interaction between elements [52, 9]; and combinations thereof. Unfortunately, each study has been limited to one or two kinds of attention only; attention is not always learned; and applications vary.\n",
      "It is the objective of our work to perform a compre- hensive study of all forms of attention above, apply them to instance-level image retrieval and provide a detailed ac- count of their interaction and impact on performance. As shown in Figure 1, we collect contextual information from images with both local and global attention, giving rise to two parallel network streams. Importantly, each operates on both spatial locations and feature channels. Local at- tention is about individual locations and channels; global is about interaction between locations and between channels. The extracted information is separately embedded in local and global attention feature maps, which are combined in a global-local attention feature map before pooling.\n",
      "Our contributions can be summarized as follows:\n",
      "Many studies on instance-level image retrieval focus on learning features from convolutional neural networks (CNN), while others focus on re-ranking, for instance by graph-based methods [11]. The former can be distinguished according to feature types: local descriptors, reminiscent of SIFT [27], where an image is mapped to a few hundred vec- tors; and global descriptors, where an image is mapped to a\n",
      "1https://www.kaggle.com/c/landmark-retrieval-2020\n",
      "1. We propose a novel network that consists of both global and local attention for image retrieval. This is the rst study that employs both mechanisms.\n",
      "2. Each of the global and local attention mechanisms comprises both spatial and channel attention.\n",
      "3. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.\n",
      "1\n",
      "local attention\n",
      "Fl c  Al s  Fl  wl  Al c    +    +    F  c  1  1  w  Fgl  1  h  w  Ag c  Fg c  Ag s  Fg   wg  +  c  h  w      +    c  h  w  c  c  hw  hw  fusion \n",
      "channel attention\n",
      "spatial attention\n",
      "global attention\n",
      "Figure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten- tion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention (based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al c), s). The input feature map F is weighted into local (Fl) and local spatial (Al global (Fg) attention feature maps, which are fused with F to yield the global-local attention feature map Fgl. The diagram is abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\n",
      "2. Related work\n",
      "Instance-level image retrieval Studies on instance-level image retrieval can be roughly, but not exclusively, di- vided into three types: (1) studies on global descriptors [3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and geometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking by graph-based methods [11, 21, 55]. The rst two types of studies focus on the feature representation, while the last type focuses on re-ranking extracted features.\n",
      "Studies on global descriptors focus on spatial pooling of CNN feature maps into vectors, including MAC [38], SPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37], and NetVLAD [1, 25], as well as learning the representa- tion [3, 15, 16, 36, 37]. Studies before deep learning dom- inated image retrieval were mostly based on local descrip- tors like SIFT [27] and bag-of-words representation [32] or aggregated descriptors like VLAD [22] or ASMK [46]. Lo- cal descriptors have been revived in deep learning, e.g. with DELF [29], DELG [5] and ASMK extensions [45, 47].\n",
      "LOCAL GLOBAL METHOD LRN RET Spatial Channel Spatial Channel SENet [20] ECA-Net [51] GCNet [6] CBAM [54] v GE [19] v NL-Net [52] AA-Net [4] SAN [59] N Net [34] A?-Net [9] v GSoP [14] v KAKK KAN KAKA K KAKA AKA OnA [23] AGeM [17] CroW [24] CRN [25] DELF [29] DELG [5] Tolias et al. [47] SOLAR [28] v KAA KKK KI KK KKK KIS KKK KKK K v v v v\n",
      "We focus on learning a global descriptor in this work, be- cause it is the most efcient in terms of storage and search. However, our generic attention mechanism produces a fea- ture tensor and could be applicable to local descriptors as well, if global pooling were replaced by local feature detec- tion. Re-ranking methods are complementary to the repre- sentation and we do not consider them in this work.\n",
      "Ours\n",
      "Table 1: Related work on attention. LRN: learned; RET: ap- plied to instance-level image retrieval.\n",
      "Attention Attention mechanisms have been rst proposed in image classication studies focusing on channel at-\n",
      "tention [20, 51, 6], spatial attention [19] or both, like CBAM [54]. In image retrieval, CroW [24] also employs\n",
      "2\n",
      "F  feature map  c  h  w  GAP  c  1  1  conv1d(k)  sigmoid  c  1  1  Al c  attention map \n",
      "Figure 2: Local channel attention.\n",
      "both spatial and channel attention and can be seen as a pre- cursor of CBAM, but, like other studies of spatial attention on retrieval [41, 23, 17], it is not learned. CRN [25] ap- plies spatial attention for feature reweighting and is learned. Learned spatial attention mechanisms are common for local descriptors [29, 5, 47].\n",
      "We call the above methods local attention, in the sense that elements of the feature tensor (channels / spatial loca- tions), are weighted independently, based on contextual in- formation obtained by pooling or learned. By constrast, by global attention we refer to mechanisms that model inter- action between elements of the feature tensor, for example between channels or between locations.\n",
      "In image classication, non-local neural network (NL- Net) [52] is maybe the rst global attention mechanism, fol- lowed by similar studies [4, 59, 34]. It is global spatial at- tention, allowing interaction between any pair of spatial lo- cations. Similarly, there are studies of global channel atten- tion, allowing interaction between channels [9, 14]. Global attention has focused mostly on image recognition and has been applied to either spatial or channel attention so far, not both. In image retrieval, SOLAR [28] is a direct application of the global spatial attention mechanism of [52].\n",
      "Table 1 attempts to categorize related work on atten- tion according to whether attention is local or global, spa- tial or channel, whether it is learned and whether it is ap- plied to instance-level image retrieval. We observe that all methods limit to one or two forms of attention only. Of those studies that focus on image retrieval, many are not learned [23, 17, 24], and of those that are, some are de- signed for local descriptors [29, 47].\n",
      "By contrast, we provide a comprehensive study of all forms of attention, global and local, spatial and channel, to obtain a learned representation in the form of a tensor that can be used in any way. We spatially pool it into a global descriptor and we study the relative gain of different forms of attention in image retrieval.\n",
      "3\n",
      "F | feature map exhxu F \\ convl x1 cd xhxw conv 3 x 3 conv 5 x 5 conv 7 x 7 dilated com u w Al | attention map\n",
      "Figure 3: Local spatial attention. Convolutional layers in blue implemented by dilated convolutions with kernel size 3  3 and dilation factors 1, 3, 5.\n",
      "3. Global-local attention\n",
      "We design a global-local attention module (GLAM), which is attached at the end of a backbone network. Figure 1 illustrates its main components. We are given a c  h  w feature tensor F, where c is the number of channels, and h  w is the spatial resolution. Local attention collects con- text from the image and applies pooling to obtain a c  1  1 local channel attention map Al c and a 1  h  w local spa- tial attention map Al s. Global attention allows interaction between channels, resulting in a c  c global channel at- tention map Ag c , and between spatial locations, resulting in a hw  hw global spatial attention map Ag s. The feature maps produced by the two attention streams are combined with the original one by a learned fusion mechanism into the global-local attention feature map Fgl before being spa- tially pooled into a global image descriptor.\n",
      "3.1. Local attention\n",
      "We extract an 1D channel and a 2D spatial attention map to weigh the feature map in the corresponding dimensions.\n",
      "Local channel attention Following ECA-Net [51], this attention captures local channel information. As shown in Figure 2, we are given a c  h  w feature tensor F from our backbone. We rst reduce it to a c  1  1 tensor by global average pooling (GAP). Channel attention is then captured by a 1D convolution of kernel size k along the channel di- mension, where k controls the extent of cross-channel inter- action. This is followed by a sigmoid function, resulting in the c  1  1 local channel attention map Al c.\n",
      "Local spatial attention Inspired by the inception mod- ule [43] and similar to [25], this attention map captures local spatial information at different scales. As shown in Figure 3,\n",
      "feature map  GAP  1  c  conv1d(k)  conv1d(k)  1  c  1  c  Vc  hw  c  sigmoid  sigmoid  Qc  1  c  Kc  1  c    c  c    Ag c  softmax  c  h  w  attention feature map \n",
      "F\n",
      "Gc\n",
      "|\n",
      "Figure 4: Global channel attention.\n",
      "given the same c x h x w feature tensor F from our back- bone, we obtain a new tensor F with channels reduced to c, using a 1 x 1 convolution. We then extract local spatial contextual information using convolutional filters of kernel size 3 x 3, 5 x 5, and 7 x 7, which are efficiently imple- mented by 3 x 3 dilated convolutions [7, 57] with dilation parameter 1, 2, and 3 respectively. The resulting features, along with one obtained by 1 x 1 convolution on F, are concatenated into a 4c x h x w tensor. Finally, we obtain the 1 x h x w local spatial attention map AL by a1 x 1 convolution that reduces the channel dimension to 1.\n",
      "The middle column of Figure 6 shows heat maps of local spatial attention, localizing target objects in images.\n",
      "Local attention feature map We use the local channel attention map Al\n",
      "Fl\n",
      "FU :=FOAL+F.\n",
      "We then use local spatial attention map Al s to weigh Fl c in the spatial dimensions, resulting in the c  h  w local attention feature map\n",
      "F=FLOAL+Fi. (2) Here, A @B denotes an element-wise multiplication of ten- sors A and B, with broadcasting when one tensor is smaller. We adopt the choice of applying channel followed by spa- tial attention from convolutional block attention module CBAM [54]. However, apart from computing Ad at differ- ent scales, both attention maps are obtained from the orig- inal tensor F rather than sequentially. In addition, both (1) and (2) include residual connections, while CBAM includes a single residual connection over both steps.\n",
      "3.2. Global attention\n",
      "We extract two matrices capturing global pairwise chan-\n",
      "nel and spatial interaction to weigh the feature map.\n",
      "(1)\n",
      "4\n",
      "F | feature map exhxw conv 1 x1 conv 1 x 1 conv 1 x 1 Qs | x hw Ke | x hw Vs |c x hw s hw x hu AS x softmax dxhxu / conv 1 x1 \\ exhxw attention feature map\n",
      "Gs\n",
      "|\n",
      "Figure 5: Global spatial attention.\n",
      "Global channel attention We introduce a global channel attention mechanism that captures global channel interac- tion. This mechanism is based on the non-local neural net- work [52], but with the idea of 1D convolution from ECA- Net [51]. As shown in Figure 4, we are given the c  h  w feature tensor F from our backbone. We apply GAP and squeeze spatial dimensions, followed by a 1D convolution of kernel size k and a sigmoid function, to obtain 1c query Qc and key Kc tensors. The value tensor Vc is obtained by mere reshaping of F to hwc, without GAP. Next, we form the outer product of Kc and Qc, followed by softmax over channels to obtain a c  c global channel attention map\n",
      "AY = softmax(K.'Q.). 3)\n",
      "Finally, this attention map is multiplied with Vc and the ma- trix product VcAg c is reshaped back to c  h  w to give the global channel attention feature map Gc. In GSoP [14] and A2-Net [9], a c  c global channel attention map is obtained by multiplication of hw  c matrices; (3) is more efcient, using only an outer product of 1  c vectors.\n",
      "Global spatial attention Since ordinary convolution ap- plies only a local neighborhood at a time, it cannot capture global contextual information. Thus, we apply non-local fil- tering [52], which is a form of self-attention [49] in the spa- tial dimensions. As shown in Figure 5, we are given the same c xX h x w feature tensor F from our backbone. By using three 1 x 1 convolutions, which reduce channels to c, and flattening spatial dimensions to hw, we obtain c! x hw query Qs, key Kx, and value V., tensors, where each col- umn is a feature vector corresponding to a particular spatial location. We capture pairwise similarities of these vectors by matrix multiplication of K, and Q,, followed by soft- max over locations to obtain a hw x hw global spatial at- tention map:\n",
      "AY = softmax(K! Q,). (4)\n",
      "This attention map is multiplied with V, and the matrix product VA is reshaped back to c x h x w by expanding the spatial dimensions. Finally, using a 1 x 1 convolution, which increases channels back to c, we obtain the c x h x w global spatial attention feature map Gs.\n",
      "The right column of Figure 6 shows heat maps for global\n",
      "spatial attention, localizing target objects in images.\n",
      "Global attention feature map We use the global channel attention feature map Fc to weigh F element-wise\n",
      "Fo =FoG.. (5)\n",
      "We then use global spatial attention feature map Gs to weigh Fg c element-wise, resulting in the c  h  w global attention feature map\n",
      "FY =F20G,+F%. (6)\n",
      "Similarly to Fl in (1) and (2), we apply channel attention rst, followed by spatial attention. However, unlike (1), there is no residual connection in (5). This choice is sup- ported by early experiments.\n",
      "3.3. Global-local attention\n",
      "Feature fusion As shown in Figure 1, we combine the local and global attention feature maps, Fl and Fg, with the original feature F. While concatenation and summation are common operations for feature combination, we use a weighted average with weights wl, wg, w respectively, ob- tained by softmax over three learnable scalar parameters, to obtain a c  h  w global-local attention feature map\n",
      "Fgl = wlFl + wgFl + wF. (7)\n",
      "EfcientDet [44] has shown that this is the most effective, among a number of choices, for fusion of features across different scales.\n",
      "Pooling We apply GeM [37], a learnable spatial pooling mechanism, to feature map F%! (7), followed by a fully- connected (FC) layer with dropout and batch normalization. The final embedding is obtained by 2-normalization.\n",
      "4. Experiments\n",
      "4.1. Datasets\n",
      "Training set There are a number of open landmark datasets commonly used for training in image retrieval stud- ies, including neural code (NC) [3], neural code clean (NC- clean) [16], as well as Google Landmarks v1 (GLDv1) [29] and v2 (GLDv2) [53]. Table 2 shows relevant statistics. These datasets can be categorized into noisy and clean. The clean sets were obtained from the original noisy sets for more effective training [16, 53]. The original noisy datasets are much larger, but they have high intra-class variability.\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(a) input\n",
      "(b) local\n",
      "(c) global\n",
      "Figure 6: Local and global spatial attention. Left: input images. Middle: local spatial attention heat maps. Right: global spatial attention heat maps. Red (blue) means higher (lower) attention weight.\n",
      "Each class can include visually dissimilar images such as exterior and interior views of a building or landmark, in- cluding oor plans and paintings inside. The clean datasets focus on views directly relevant to landmark recognition but have a much smaller number of images.\n",
      "Evaluation set and metrics We use four common eval- uation datasets for landmark image retrieval: Oxford5k (Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox- ford (ROxford or ROxf) and Paris (RParis or RPar) [35]. ROxford and RParis are used with and without one million distractors (R1M) [28] and evaluated using the Medium and Hard protocols [35]. We evaluate using mean Average Pre- cision (mAP) and mean precision at 10 (mP@10).\n",
      "4.2. Implementation details\n",
      "We train on 8 TITAN RTX 2080Ti GPUs. All models are pre-trained on ImageNet [39] and implemented in PyTorch [31]. For fair comparisons, we set a training environment\n",
      "\n",
      "Figure 7: Examples of our ranking results. In each row, the rst image on the left (pink dotted outline) is a query image with a target object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images for the query; red solid outline: negative.\n",
      "similar to the those of compared studies [56, 53, 28, 35]. We employ ResNet101 [18] as a backbone model. The kernel size k of ECANet in subsection 3.1 is set to 3. The param- eter p of GeM in subsection 3.3 is set to 3 and the dimen- sion d of nal embeddings to 512. We adopt ArcFace [10], a cosine-softmax based loss, with a margin of 0.3. We use stochastic gradient descent with initial learning rate 103, momentum 0.9 and weight decay 105.\n",
      "TRAIN SET NC-noisy NC-clean SfM-120k GLDv1-noisy GLDv2-noisy GLDv2-clean #IMAGES 213,678 27,965 117,369 1,225,029 4,132,914 1,580,470 #CLASSES 672 581 713 14, 951 203,094 81,313\n",
      "We adopt the batch sampling of Yokoo et al. [56] where mini-batch samples with similar aspect ratios are resized to a particular size. Here, we use a batch size of 64. For image augmentation, we apply scaling, random cropping, and var- ied illumination. At inference, we apply a multi-resolution representation [16] to query and database images.\n",
      "Our method is denoted as GLAM (global-local atten- tion module). Using the backbone model alone is referred to as baseline. It is compatible with recent models based on ResNet101-GeM trained with ArcFace [53, 28]. Adding our local attention (subsection 3.1) to the baseline model is denoted +local, while adding our global attention (subsec- tion 3.2) is denoted +global. Since we focus on representa- tion learning, we do not consider post-processing methods like geometry-based re-ranking [29, 40, 53] or graph-based re-ranking [11, 21, 55].\n",
      "Table 2: Statistics of different training sets.\n",
      "METHOD TRAIN SET DIM OXF5K PAR6K RMEDIUM RHARD ROxf RPar ROxf RPar GeM-Siamese [37, 35] SfM-120k 2048 87.8 SOLAR [28] GLDv2 [53] GLDv1-noisy 2048 GLDv2-clean 2048   92.7   64.7 77.2 38.5 56.3 69.9 81.6 47.9 64.5 74.2 84.9 51.6 70.3 GLAM (Ours) 512 GLDv1-noisy 512 GLDv2-noisy 512 GLDv2-clean 512 NC-clean 77.8 92.8 93.3 94.2 85.8 95.0 95.3 95.6 51.6 68.1 20.9 44.7 73.7 83.5 49.8 69.4 75.7 86.0 53.1 73.8 78.6 88.5 60.2 76.8\n",
      "Table 3: mAP comparison of our best model (base- line+local+global) trained on different training sets against [53, 28]. All models use ResNet101-GeM. Red: best results. Blue: GLAM higher than SOLAR [28] on GLDv1-noisy.\n",
      "4.3. Benchmarking\n",
      "Noisy vs. clean training sets We begin by training our best model (baseline+local+global) on all training sets of Table 2, except NC-noisy because some images are cur- rently unavailable. As shown in Table 3, even though\n",
      "GLDv2-noisy has 2.6 times more images than GLDv2- clean, the latter is superior by a large margin. This shows that, in training, a cleaner dataset can be more important than a larger one. By contrast, NC-clean has the worst performance despite being clean, aparently because it is\n",
      "6\n",
      "BASE\n",
      "MEDIUM\n",
      "METHOD TRAIN SET DIM +R1M RPar ROxf +R1M RPar +R1M 512 53.1 SPoC-V16 [2, 35] SPoC-R101 [35] 2048 CroW-V16 [24, 35] 512 2048 CroW-R101 [35] MAC-V16-Siamese [36, 35] 512 MAC-R101-Siamese [35] 2048 RMAC-V16-Siamese [36, 35] 512 2048 RMAC-R101-Siamese [35] RMAC-R101-Triplet [16, 35] 2048 86.1 2048 87.8 GeM-R101-Siamese [37, 35] 2048 AGeM-R101-Siamese [17] SOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048 GLDv1-noisy 2048 DELG-GeM-R101-ArcFace [5] GLDv2-clean 2048 GeM-R101-ArcFace [53] [O] [O] [O] [O] [O] [O] [O] [O] NC-clean SfM-120k SfM-120k  70.8  80.0  80.1        79.7  82.9  85.0  94.5 92.7      57.3    40.7     78.1    61.8      64.7 70.3 54.8    30.3      51.2 51.6 35.5  82.4 84.9        GLAM-GeM-R101-ArcFace baseline GLDv2-clean 512 GLDv2-clean 512 +local GLDv2-clean 512 +global GLDv2-clean 512 +global+local 91.9 91.2 92.3 94.2 94.5 95.4 95.3 95.6\n",
      "Table 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16: VGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). : dimension d = 256 [2]. mP: mP@10. Red: best results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only model other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\n",
      "too small. To achieve best possible performance, we use GLDv2-clean as a training set in the remaining experiments.\n",
      "Comparisons on same training set It is common to com- pare methods regardless of training sets as more become available, e.g., [35, 28]. Since GLDv2-clean is relatively new, Weyand et al. [53], which introduced the dataset, is the only study that has trained the same backbone with the same settings (ResNet101-GeM with ArcFace) on GLDv2-clean. Our baseline is lower than [53], because our dimensinality is 512, while other models based on ResNet101 use 2048. Yet, Table 3 shows that our best model trained on GLDv2-clean outperforms [53] by a large margin. But the most impor- tant comparison is with SOLAR [28], also based on self- attention, which has trained ResNet101-GeM on GLDv1- noisy. On this training set, our best model clearly outper- forms [28] despite lower dimensionality.\n",
      "METHOD OXF5K PAR6K RMEDIUM RHARD ROxf RPar ROxf RPar 91.9 GLAM baseline 91.3 +local-channel 91.0 +local-spatial +local 91.2 +global-channel 92.5 92.4 +global-spatial 92.3 +global 94.2 +global+local 94.5 95.3 95.1 95.4 94.4 95.1 95.3 95.6 72.8 84.2 49.9 69.7 72.2 85.8 48.3 73.1 72.1 85.3 48.3 71.9 73.7 86.5 52.6 75.0 73.3 84.4 49.8 70.1 73.2 86.3 50.0 72.7 77.2 86.7 57.4 75.0 78.6 88.5 60.2 76.8\n",
      "Table 5: mAP comparison of spatial and channel variants of our local (+local, subsection 3.1) and global (+global, subsection 3.1) attention modules to the baseline.\n",
      "Comparison with state of the art Table 4 shows the performance of four variants of our model, i.e. baseline with or without local/global attention, and compares them against state-of-the-art (SOTA) methods based on global de- scriptors without re-ranking on the complete set of bench- marks, including distractors. Both local and global atten- tion bring signicant gain over the baseline. The effect of global is stronger, while the gain of the two is addi- tive in the combination. The best results are achieved by the global-local attention network (baseline+global+local). With this model, we outperform previous best methods on most benchmarks except mP@10 on RParis (medium) and RParis+R1M (medium), where we are outperformed by [37, 35]. These results demonstrate that our approach is effective for landmark image retrieval. Figure 7 shows some\n",
      "METHOD OXF5K PAR6K RMEDIUM RHARD ROxf RPar ROxf RPar CBAM style GLAM (Ours) 93.8 94.2 95.7 95.6 75.6 78.6 88.4 88.5 53.3 60.2 76.8 76.8\n",
      "Table 6: mAP comparison between CBAM style and our local spatial attention.\n",
      "examples of our ranking results.\n",
      "4.4. Ablation study\n",
      "Our ablation study uses the Google Landmark v2 clean dataset (GLDv2-clean) [53] for training, which is shown to be the most effective in Table 3.\n",
      "7\n",
      "METHOD OXF5K PAR6K RMEDIUM RHARD ROxf RPar ROxf RPar Concatenate Sum (Ours) 89.5 94.2 95.1 95.6 73.6 78.6 86.5 88.5 54.0 60.2 73.7 76.8\n",
      "Table 7: mAP comparison between weighted concatenation and weighted average for feature fusion.\n",
      "METHOD OXF5K PAR6K RMEDIUM RHARD ROxf RPar ROxf RPar Fixed-size Group-size (Ours) 76.1 94.2 82.6 95.6 55.7 78.6 68.4 88.5 29.2 60.2 47.5 76.8\n",
      "Table 8: mAP comparison between xed-size (224  224) and group-size sampling methods.\n",
      "QUERY DATABASE OXF5K PAR6K RMEDIUM RHARD ROxf RPar ROxf RPar Single Multi Single Multi Single Single Multi Multi 93.3 93.9 93.6 94.2 95.2 95.4 95.6 95.6 76.9 87.1 58.6 74.7 78.0 87.7 59.0 75.5 77.0 87.8 57.1 76.0 78.6 88.5 60.2 76.8\n",
      "Table 9: mAP comparison of using multiresolution repre- sentation (Multi) or not (Single) on query or database.\n",
      "Effect of attention modules We ablate the effect of our local and global attention networks as well as their com- bination. Table 5 shows the results, which are more ne- grained than those of Table 4. In particular, it shows the ef- fect of the channel and spatial variants of both local and global attention. We observe that, when used alone, the channel and spatial variants of local attention are harmful in most cases. Even the combination, baseline+local, is not always effective. By contrast, when used alone, the channel and spatial variants of global attention are mostly benecial, especially the latter. Their combination, baseline+global, is impressive, bringing gain of up to 7.5%. Importantly, the combination baseline+global+local improves further by up to another 2.8%. This result shows the necessity of local attention in the nal model.\n",
      "CBAM vs. our local spatial attention We experiment with the local spatial attention of CBAM [54]. CBAM ap- plies average and max-pooling to input features and con- catenates the two for spatial attention. We apply this vari- ant to our local spatial attention module for comparison. For the CBAM style module, we keep the overall design of our module as shown in Figure 3, but apply average and max-pooling to each of the four convolutional layer outputs before concatenation. Table 6 shows that the CBAM style\n",
      "8\n",
      "module is considerably worse than ours on all benchmarks except Paris6k, where it is only slightly better.\n",
      "Concatenation vs. sum for feature fusion We use a softmax-based weighted average of local and global atten- tion feature maps with the original feature map (7). Here, we compare this weighted average with weighted concate- nation, where concatenation replaces the sum operation in (7). As shown in Table 7, the weighted average outper- forms the weighted concatenation.\n",
      "Fixed-size vs. group-size sampling Numerous studies have proposed methods for constructing batches according to image size for efcient training. For instance, Gordo et al. [16], DELF [29], and Yokoo et al. [56] employed dif- ferent image sizes per batch for training instead of a single xed size. We adopt the method of Yokoo et al., which con- structs a batch with images of similar aspect ratio, so that the images can be resized to a size with an aspect ratio that is similar to their own. We call this method group-size sam- pling. Table 8 compares xed-size (224  224) with group- size sampling. We observe that maintaining aspect ratios by using dynamic input sizes is much more effective.\n",
      "Multi-resolution We use the multi-resolution representa- tion [16] for the nal feature of an image at inference time. This method: (1) resizes an image into multiple scales; (2) extracts features from the resized images; and (3) averages the features to obtain the nal feature of the image. The method is applied to both query and database images to en- hance ranking results, especially for small target objects. Table 9 compares the four cases of applying this method or not to query or database images.\n",
      "5. Conclusion\n",
      "We have introduced a novel approach that extracts global and local contextual information using attention mecha- nisms for instance-level image retrieval. It is manifested as a network architecture consisting of global and local atten- tion components, each operating on both spatial and chan- nel dimensions. This constitutes a comprehensive study and empirical evaluation of all four forms of attention that have previously been studied only in isolation. Our ndings indi- cate that the gain (or loss) brought by one form of attention alone strongly depends on the presence of the others, with the maximum gain appearing when all forms are present. The output is a modied feature tensor that can be used in any way, for instance with local feature detection instead of spatial pooling for image retrieval.\n",
      "With the advent of vision transformers [12, 58] and their recent application to image retrieval [13], attention is ex- pected to play a more and more signicant role in vi- sion. According to our classication, transformers perform global spatial attention alone. It is of great interest to in- vestigate the role of the other forms of attention, where our\n",
      "approach may yield a basic building block of such archi- tectures. One may even envision an extension to language models, where transformers originate from [50].\n",
      "References\n",
      "[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa- jdla, and Josef Sivic. NetVLAD: CNN architecture for weakly supervised place recognition. In CVPR, 2016. 2 [2] Artem Babenko and Victor Lempitsky. Aggregating Local Deep Features for Image Retrieval. In ICCV, 2015. 1, 2, 7 [3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and Victor Lempitsky. Neural Codes for Image Retrieval. In ECCV, 2014. 1, 2, 5\n",
      "[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. Attention augmented convolutional net- works. In ICCV, 2019. 2, 3\n",
      "[5] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep local and global features for image search. In ECCV, 2020. 2, 3, 7\n",
      "[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. GCNet: Non-Local Networks Meet Squeeze-Excitation Net- works and Beyond. In ICCV, 2019. 2\n",
      "[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for seman- tic image segmentation. arXiv preprint arXiv:1706.05587, 2017. 4\n",
      "[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 1\n",
      "[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi Feng. A2-nets: Double attention networks. In NeurIPS, 2018. 1, 2, 3, 4\n",
      "[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. ArcFace: Additive Angular Margin Loss for Deep Face Recognition. In CVPR, 2019. 6\n",
      "[11] Michael Donoser and Horst Bischof. Diffusion Processes for Retrieval Revisited. In CVPR, 2013. 1, 2, 6\n",
      "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- arXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 8\n",
      "[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Herve Jegou. Training vision transformers for image re- trieval. Technical report, 2021. 8\n",
      "[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global In CVPR, second-order pooling convolutional networks. 2019. 2, 3, 4\n",
      "[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar- lus. Deep image retrieval: Learning global representations for image search. In ECCV, 2016. 2\n",
      "[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar- lus. End-to-end learning of deep visual representations for image retrieval. IJCV, 2017. 1, 2, 5, 6, 7, 8\n",
      "9\n",
      "[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie. Attention- aware generalized mean pooling for image retrieval. arXiv preprint arXiv:1811.00202, 2018. 2, 3, 7\n",
      "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. 2016. 6\n",
      "[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea Vedaldi. Gather-excite: Exploiting feature context in con- volutional neural networks. In NeurIPS, 2018. 2\n",
      "[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. Squeeze-and-Excitation Networks. In CVPR, 2018. 1, 2 [21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon, and Ondrej Chum. Efcient diffusion on region manifolds: Recovering small objects with compact cnn representations. In CVPR, 2017. 2, 6\n",
      "[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid. Aggregating local image descriptors into com- pact codes. PAMI, (99):11, 2011. 2\n",
      "[23] Albert Jimenez, Jose M. Alvarez, and Xavier Giro-i-Nieto. Class weighted convolutional features for visual instance search. In BMVC, 2017. 2, 3\n",
      "[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero. Crossdimensional weighting for aggregated deep convolu- tional features. In ECCV, 2016. 1, 2, 3, 7\n",
      "[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm. Learned Contextual Feature Reweighting for Image Geo- Localization. In CVPR, 2017. 1, 2, 3\n",
      "[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric learning. In CVPR, 2020. 1\n",
      "[27] David G. Lowe. Distinctive image features from scale- invariant keypoints. In IJCV, 2004. 1, 2\n",
      "[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian Mikolajczyk. SOLAR: Second-Order Loss and Attention for Image Retrieval. In ECCV, 2020. 1, 2, 3, 5, 6, 7\n",
      "[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, and Bohyung Han. Large Scale Image Retrieval with Atten- tive Deep Local Features. In ICCV, 2017. 1, 2, 3, 5, 6, 8 [30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In CVPR, 2016. 1\n",
      "[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An im- perative style, high-performance deep learning. In NeurIPS, 2019. 5\n",
      "[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Object retrieval with large vocabularies and fast spatial matching. In CVPR, 2007. 2, 5\n",
      "[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Lost in quantization:Improving particu- lar object retrieval in large scale image databases. In CVPR, 2008. 5\n",
      "[34] Tobias Plotz and Stefan Roth. Neural nearest neighbors net-\n",
      "works. In NeurIPS, 2018. 2, 3\n",
      "[35] Filip Radenovic, Ahmet\n",
      "Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking. In CVPR, 2018. 5, 6, 7\n",
      "[36] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. CNN image retrieval learns from BoW: Unsupervised ne-tuning with hard examples. In ECCV, 2016. 2, 7\n",
      "[37] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. Fine- Tuning CNN Image Retrieval with No Human Annotation. In TPAMI, 2019. 1, 2, 5, 6, 7\n",
      "[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson, and Atsuto Maki. Visual Instance Retrieval with Deep Con- volutional Networks. In CoRR, 2015. 2\n",
      "[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. In International booktitle of Computer Vision, 2015. 5\n",
      "[40] Oriane Simeoni, Yannis Avrithis, and Ondrej Chum. Local features and visual words emerge in activations. In CVPR, 2019. 2, 6\n",
      "[41] O. Simeoni, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum. Graph-based particular object discovery. Machine Vision and Applications, 30(2):243254, 3 2019. 3\n",
      "[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp- ical networks for few-shot learning. In NeurIPS, 2017. 1 [43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015. 3\n",
      "[44] Mingxing Tan, Ruoming Pang, and Quoc V. Le. EfcientDet: Scalable and Efcient Object Detection. In CVPR, 2020. 5\n",
      "[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack Sim. Detect-to-retrieve: Efcient regional aggregation for image search. In CVPR, 2019. 2\n",
      "[46] Giorgios Tolias, Yannis Avrithis, and Herve Jegou. To aggre- gate or not to aggregate: Selective match kernels for image search. In ICCV, 2013. 2\n",
      "[47] Giorgos Tolias, Tomas Jenicek, and Ondrej Chum. Learn- ing and aggregating deep local descriptors for instance-level recognition. In ECCV, 2020. 2, 3\n",
      "[48] Giorgos Tolias, Ronan Sicre, and Herve Jegou. Particular ob- ject retrieval with integral max-pooling of CNN activations. In ICLR, 2016. 2\n",
      "[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 4 [50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 9 [51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang- meng Zuo, and Qinghua Hu. ECA-Net: Efcient Chan- nel Attention for Deep Convolutional Neural Networks. In CVPR, 2020. 2, 3, 4\n",
      "10\n",
      "[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim- ing He. Non-local Neural Networks. In CVPR, 2018. 1, 2, 3, 4\n",
      "[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google Landmarks Dataset v2 - A Large-Scale Benchmark In CVPR, for Instance-Level Recognition and Retrieval. 2020. 1, 2, 5, 6, 7\n",
      "[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. CBAM: Convolutional Block Attention Module. In ECCV, 2018. 1, 2, 4, 8\n",
      "[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and Shinichi Satoh. Efcient image retrieval via decoupling dif- fusion into online and ofine processing. In AAAI, 2019. 2, 6\n",
      "[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi Iizuka. Two-stage Discriminative Re-ranking for Large-scale Landmark Retrieval. In arXiv:2003.11211, 2020. 6, 8 [57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\n",
      "residual networks. In CVPR, 2017. 4\n",
      "[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens- to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021. 8 [59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\n",
      "self-attention for image recognition. In CVPR, 2020. 2, 3\n"
     ]
    }
   ],
   "source": [
    "for el in doc:\n",
    "    print(el.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexNewArxivPapers:\n",
    "    \"\"\"\n",
    "    This tool indexes new papers from arxiv using the following steps:\n",
    "    \n",
    "    1. Get the paper ids from google search\n",
    "    2. Download the papers\n",
    "    3. Process the papers with GROBID\n",
    "    4. Parse the TEI files\n",
    "    5. Add the papers to the vectorstore\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                vectordb: VectorStore,\n",
    "                n_search_results: int = 2,\n",
    "                pdf_parser: Literal[\"grobid\", \"pymupdf\"] = \"grobid\",\n",
    "                chunk_size: int = 1024,\n",
    "                chunk_overlap: int = 100,\n",
    "                ):\n",
    "    \n",
    "        self.google_api = GoogleSearchAPIWrapper()\n",
    "        self.arxiv_client = arxiv.Client(delay_seconds=0)\n",
    "        self.vectordb = vectordb\n",
    "        self.chromadb_client = chromadb.PersistentClient(\"arxiv_vdb\").get_collection(\"arxiv\")\n",
    "        self.n_search_results = n_search_results\n",
    "        self.splitter = SpacyTextSplitter(\n",
    "            chunk_size = chunk_size,\n",
    "            chunk_overlap = chunk_overlap,\n",
    "            separator=\"\\n\\n\")\n",
    "        self.pdf_parser = pdf_parser\n",
    "    \n",
    "    def _get_paper_ids(self, query: str) -> List[str]:\n",
    "        ARXIV_ID_REGEX =  r\"\\d{4}\\.\\d{4,5}\"\n",
    "        \n",
    "        ids = list(\n",
    "            {re.findall(ARXIV_ID_REGEX, result['link'])[0] \n",
    "             for result in self.google_api.results(query, self.n_search_results)}\n",
    "            )\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def _run(self, query):\n",
    "        with cl.Step():\n",
    "            self.ids = self._get_paper_ids(query)\n",
    "            \n",
    "            os.makedirs(f\"./output/{query}\", exist_ok=True)\n",
    "            os.makedirs(f\"./pdfs/{query}\", exist_ok=True)\n",
    "            \n",
    "            for id in self.ids:\n",
    "                if len(self.chromadb_client.get(where={'paper_id': id})['ids']) > 0:\n",
    "                    self.ids.remove(id)\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            papers = list(self.arxiv_client.results(arxiv.Search(id_list=self.ids)))\n",
    "            for paper in papers:\n",
    "                id = paper.entry_id.split('/')[-1]\n",
    "                paper.download_pdf(dirpath=f\"./pdfs/{query}/\", filename=f\"{paper.entry_id.split('/')[-1]}.pdf\")\n",
    "            \n",
    "            while not all([os.path.exists(f\"./pdfs/{query}/{paper.entry_id.split('/')[-1]}.pdf\") for paper in papers]):\n",
    "                time.sleep(1)\n",
    "            \n",
    "            docs = []\n",
    "            if self.pdf_parser == \"grobid\":\n",
    "                self.grobid_client = GrobidClient(config_path=\"./grobid_client_python/config.json\")\n",
    "                self.grobid_client.process(\"processFulltextDocument\", f\"./pdfs/{query}/\", output=f\"./output/{query}\", force=True)            \n",
    "                \n",
    "                while not all([os.path.exists(f\"./output/{query}/{paper.entry_id.split('/')[-1]}.grobid.tei.xml\") for paper in papers]):\n",
    "                    time.sleep(1)\n",
    "            \n",
    "                for paper in papers:\n",
    "                    tei_object = TEIFile(f\"./output/{query}/{paper.entry_id.split('/')[-1]}.grobid.tei.xml\")\n",
    "                    chunks = self.splitter.split_text(tei_object.text)\n",
    "                    docs.extend([\n",
    "                            Document(\n",
    "                                    page_content=chunk, \n",
    "                                    metadata={\n",
    "                                        'paper_id': paper.entry_id.split(\"/\")[-1],\n",
    "                                        'authors': \", \".join([author.name for author in paper.authors]),\n",
    "                                        'title': paper.title,\n",
    "                                        'chunk_id': f\"{paper.entry_id.split('/')[-1]}-{i}\"\n",
    "                                        }\n",
    "                                    )\n",
    "                                    for i, chunk in enumerate(chunks)\n",
    "                                ]\n",
    "                    )\n",
    "                    \n",
    "            if self.pdf_parser == \"pymupdf\":\n",
    "                for paper in papers:\n",
    "                    loader = PyMuPDFLoader(f\"pdfs/{query}/{paper.entry_id.split('/')[-1]}.pdf\")\n",
    "                    chunks = loader.load_and_split(self.splitter)                    \n",
    "                    docs.extend([\n",
    "                            Document(\n",
    "                                    page_content=chunk.page_content, \n",
    "                                    metadata={\n",
    "                                        'paper_id': paper.entry_id.split(\"/\")[-1],\n",
    "                                        'authors': \", \".join([author.name for author in paper.authors]),\n",
    "                                        'title': paper.title,\n",
    "                                        'chunk_id': f\"{paper.entry_id.split('/')[-1]}-{i}\"\n",
    "                                        }\n",
    "                                    ) \n",
    "                                    for i, chunk in enumerate(chunks)\n",
    "                                ]\n",
    "                        )\n",
    "            self.vectordb.add_documents(\n",
    "                docs\n",
    "            )\n",
    "\n",
    "\n",
    "    async def _arun(self, query: str):\n",
    "        async with cl.Step():\n",
    "            self._run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key to a Transformer model is the self-attention mechanism, which allows the model to analyze an entire sequence in a computationally efficient manner. Recent work has suggested the possibility that general attention mechanisms used by RNNs could be replaced by active-memory mechanisms. In this work, we evaluate whether various activememory mechanisms could replace self-attention in a Transformer. Our experiments suggest that active-memory alone achieves comparable results to the self-attention mechanism for language modelling, but optimal results are mostly achieved by using both active-memory and self-attention mechanisms together. We also note that, for some specific algorithmic tasks, active-memory mechanisms alone outperform both the self attention and a combination of the two.\n",
      "\n",
      "Introduction: The previous state-of-the-art sequence model, the recurrent neural network, has been largely supplanted by the Transformer model: [Vaswani et al., 2017]: , which is primarily built atop a self-attention mechanism. Given a task to train upon, the self-attention mechanism focuses on one token per attention head within the entire sequence at each time-step; the key to the self-attention mechanism's success is the mechanism's ability to learn which token within the entire sequence to focus on in order to achieve the best results.: The self-attention mechanism has proven successful on a variety of natural language processing tasks, but has not achieved ubiquitous success. The authors of: [Kaiser and Bengio, 2016]: pointed out that an attention mechanism would likely struggle to solve a task which required a model to focus on multiple tokens at a given time-step. Further, the authors of: [Kaiser and Sutskever, 2015]: recommended that an attention mechanism could be replaced by active-memory to alleviate these concerns.: Unlike attention, active-memory allows a model to access and change any and all elements of its memory at each timestep. The active-memory mechanism can access more than one element at each time step. In: [Kaiser and Bengio, 2016]: ,: Figure: 1: : The active memory mechanism. In this case, the activememory is implemented in a unidirectional manner, with a kernel size 3.: the authors used an active-memory system to translate English to French, and was capable of outperforming an RNN model, both with and without an attention mechanism.: Motivated by the success of attention mechanism: [Vaswani et al., 2017]: and active-memory: [Kaiser and Bengio, 2016]: , in this paper we investigate the Transformer's self-attention mechanism in comparison to a variety of active-memory mechanisms. We experiment on two types of tasks: the language modeling task and a set of algorithmic tasks.: For the language modelling task, the self-attention mechanism out-performs an active-memory mechanism used alone by a slim margin. However, a combination of both selfattention and active-memory reliably outperform both mechanisms used alone.: We also evaluated the self-attention mechanism and various active-memory mechanisms on a variety of algorithmic tasks, which can also be expressed as a sequence modeling task. Across most of the algorithmic tasks tested, the activememory mechanisms achieve equal, or superior, results to a traditional self-attention mechanism. This would appear to vindicate the hypothesis stated by: [Kaiser and Bengio, 2016]: , suggesting that the nature of the attention mechanism does indeed limit the effectiveness and accuracy of the model. Finally, we note that, for several algorithmic tasks, the mere addition of the self-attention mechanism hinders results; the active-memory mechanism alone outperforms a combination of the two separate mechanisms. This raises an unsolved problem; it would appear that, for deep learning sequence models, there is still no unambiguous model that can optimally solve all possible problems.\n",
      "\n",
      "Related Work: The Transformer model: [Vaswani et al., 2017]: is built with two separate modules, the self-attention mechanism and the feedforward mechanism, which are stacked atop each other for multiple layers. The feedforward mechanism is an intrasequence analysis, where the output for each token in the sequence is dependent only on the token at the same timestep, and independent of all other time-steps. On the other hand, the self-attention mechanism is an inter-sequence analysis, where the output for each time-step is dependent upon the entire sequence. The self-attention mechanism is defined, mathematically, as:: Q t , K t , V t = x t y t = concat(head 1,t , head 2,t , . . . , head n,t )W o head i = Attention(Q t W Q i , K t W K i , V t W V i ) Attention(Q, K, V ) = sof tmax( Q t K T t  d k )V t W o R d k * k,d , W K,Q,V i R d,d k * k: The feed-forward module is defined as:: y t = W l,2 (max(W l,1 x t + b l,1 , 0)) + b l,2 W l,2 R d,d * 4 , W l,1 R d * 4,d: The Transformer model, and its variants: [Dai et al., 2019]: , have achieved remarkable results across a variety of natural language processing tasks since its inception: [Zhenzhong et al., 2019: ] [Delvin et al., 2018: ] [Yang et al., 2019b]: , and are currently investigated heavily by both academia and industry.: The Neural GPU: [Freivalds and Liepins, 2017: ] [Kaiser and Bengio, 2016: ] [Kaiser and Sutskever, 2015]: , which introduced an active-memory model, achieved impressive algorithmic results in: [Kaiser and Sutskever, 2015]: , and also achieved impressive machine translation results in: [Kaiser and Bengio, 2016]: . A Neural GPU contains a CGRU (Convolution Gated Recurrent Unit) module which is iterated repeatedly. This allows the entire sequence to be analyzed in a parallelizable and computationally efficient manner. The CGRU module is defined as:: u = sigmoid(U 1 * x + B 1 ) r = sigmoid(U 2 * x + B 2 ) y = u  x + (1 -u)  tanh(U 0 * (r  x) + B 0 )): where U * x refers to applying a convolutional operator over x, using U as a trainable kernel bank and B is a trainable bias vector. The CGRU has, since its introduction, been used in other models: [Resende et al., 2016]: .: Convolutional operators are traditionally used for image processing: [Alom et al., 2018]: , and have also been used in relation to sequential analysis in previous papers: [Yang et al., 2019a: ] [Wu et al., 2019: ] [Gehring et al., 2017: ] [Dauphin et al., 2016]: . To the best of our knowledge they have not been used explicitly to replace, or augment, the self-attention mechanism. The first sequence-to-sequence model, based on convolutional operators, was, to the best of our knowledge, introduced in: [Gehring et al., 2017]: , which replaced the thentraditional LSTM block with a series of convolutions and gated convolutional networks: [13]: , and outperformed RNNbased models in terms of both speed and accuracy. However, the model introduced in: [Gehring et al., 2017]: was followed shortly afterwards by the Transformer model, which outperformed the convolutional-based model.: The convolutional self-attention network: [Yang et al., 2019a]: was recently introduced, and bares a passing similarity to the traditional convolutional operator described in this paper. The layer of the convolutional self-attention is similar to a traditional self-attention mechanism, but where the key and value tensors are calculated as:: K h = (K h i-M/2 , ..., K h i , ..., K h i+M/2 ) V h = (V h i-M/2 , ..., V h i , ..., V h i+M/2: ) From this point, the convolutional self-attention mechanism acts in an identical manner to the traditional selfattention mechanism. This is in direct comparison to the convolutional operator described in this paper, which explicitly avoids the use of the self-attention mechanism and relies entirely on a purely convolutional operator.\n",
      "\n",
      "Approach: In this paper, we investigate whether various active-memory mechanisms could replace self-attention in a Transformer. We also evaluate the combination of self-attention and activememory mechanisms for language modelling tasks. All the active-memory mechanisms introduced in this paper were inspired by the Neural GPU, as introduced in: [Kaiser and Sutskever, 2015]: . The key allure of the Neural GPU is that the inputs of each time-step can be analyzed and altered, and we were inspired to apply a similar form of sequence modelling alongside a self-attention mechanism. We describe various convolution-based active-memory mechanisms in this section.\n",
      "\n",
      "The Convolutional Operators: The Traditional Convolutional Operator The first, and most simple, active-memory mechanism is the simple convolutional operator. The traditional convolutional operator was formally defined in: [Bai et al., 2019]: . If the task requires the sequence to be analyzed in a unidirectional manner, such as the case for language modelling, then a zerosvector of size k -1 is concatenated to the left of the input tensor so that, for the nth output token, the model only has access to the first n input tokens. This feature is crucial to avoid allowing the model 'seeing' forward through the sequence and having access to information that the model, in practice, would not yet have. This has an identical function to the masking operation of the self-attention mechanism.: If the task can be analyzed in a bidirectional manner, then the model uses a convolutional filter using the SAMEpadding, which allows for the vector to maintain its shape throughout the convolutional operator. However, when the convolutional operator is performed in this manner, the token at time-step t is dependent on the input tokens h: [t-k/2,t+k/2] ,: where k is the kernel size.: The primary flaw of a convolutional operator, in comparison to a self-attention mechanism, is that, given n layers where each kernel has a k kernel size, each token can only see k * n -n + 1 or k/2 * n -n + 1 time-steps across for unidirectional and bidirectional tasks respectively. For example, in our experiments on language modeling (Section 4), the kernel size was set to 20 and was iterated over 8 layers. Therefore, at each time-step t, the final output is capable of analyzing the input from 153 previous time-steps, well above the average sequence-size (90 tokens) in the dataset. The self-attention mechanism, in comparison, can see across a theoretically infinite context size, even using only a single layer. Given this information, the self-attention mechanism is capable of handling theoretically greater long-term dependencies than the active-memory mechanism. However, in practice, the ability of an active-memory mechanism to access and change its entire memory could overcome this limitation.: The convolutional operator is assisted further by the fact that the convolutional operator's complexity grows linearly with the sequence size, while the self-attention mechanism's complexity grows quadratically.: Numerous papers have noted that, while Transformers are parallelizable and capable of capturing long-range dependencies, the Transformer network suffers from the inability of model tokens in a recurrent manner: [Wang et al., 2019: ] [Hao et al., 2019]: . This is in direct comparison to traditional RNN models, which can capture long-range dependencies, but can struggle to capture long-range dependencies. The use of active-memory, in theory, would accomplish this task, given that the output at time-step t h t is dependent of the inputs x: [t-k,t]: where k is the kernel size. Therefore, this operation can, in theory, model recurrence. We did not explicitly test whether this does model recurrence in practice, but will focus on this in future work.: The convolutional operator is followed by the ReLU activation function.\n",
      "\n",
      "The Persistent-Convolutional Operator: The Persistent-Convolutional operator is similar to the traditional convolutional operator described above, except that the zeros vector is replaced by the a trainable vector of identical shape to the zeros vector. This allows the operator to, identical to the traditional convolutional operator, maintain an identical shape across the convolution. To keep parameterization to a minimum, the same persistent vector is used across all convolution operators in the entire model. The persistentconvolutional operator is defined as:: p  W kernel size-1,hiddensize x = [p, x], y = W * x + b: where [.,.] denotes the concatenation function and p is the trainable persistent vector. Persistent vectors have been used previously in language modelling tasks: [Sukhbaatar et al., 2019]: , but never as an augmentation for convolutional operators, as far as we know.: If the model is to be analyzed in a bidirectional manner, rather then a unidirectional manner, then the persistentconvolutional operator can be redefined as: The use of a persistent vector allows for the model to have a permanent memory that, given the fact the vector is trainable, can be expressed in an optimal manner for the model. This is the equivalent of a permanent memory for the deep learning model.: p 1 , p 2  W (kernel size-1)//2,hiddensize x = [p 1 , x, p 2 ]\n",
      "\n",
      "The Highway-Convolutional Operator: The Highway-Convolutional operator is based on the highway network architecture: [5]: , which can be defined as:: a = U 0 * x + B 0 b = sigmoid(U 1 * x + B 1 ) y = a  b + x  (1 -b): The key allure of the highway network, as described in: [Srivastava et al., 2015]: , is the fact that a highway network can be trained for a large number of layers, even hundreds of layers, because information can pass, unimpeded, across each layer. The authors of: [Srivastava et al., 2015]: described these paths as 'information highways'. The use of these 'information highways' allows information to pass through the self-attention mechanism in an equally efficient manner.: In this paper, we use the hard-sigmoid function: [Kaiser and Bengio, 2016]: to stabilize gradients, which is defined as:: y = max(0, min(1, 1.2 * sigmoid(x) -0.1))\n",
      "\n",
      "Self-Attention + Convolutional Operators: The operator calculates the results of the self-attention mechanism and results of the convolutional operator independently, and then adds them together to produce the final output of the operator. This operator would allow the model to analyze the input using both the self-attention mechanism and active-memory mechanism and decide which features from both mechanisms would be most optimal. This approach has the obvious advantage of being able to take the 'best of both worlds', where the optimal features that can only be detected  The loss-per-token of the self-attention mechanism and the active-memory mechanisms on the WT3 dataset, and the difference of loss between the self-attention and the active-memory mechanisms. The lower the loss, the better the model performed. With the exception of the CGRU, all purely active-memory operators achieve a test loss less then 1.2% higher then the self-attention mechanism.: The optimal models combined the self-attention mechanism and an active-memory mechanism, and achieved a lower test loss than the self-attention mechanism and active-memory mechanisms alone.: by the self-attention mechanism, and the optimal features that can only be detected by the convolutional operator, are both available to the model. The architecture of a single layer of the \"self-attention + convolution\" operator is shown in Figure: 2: . This architecture, without the convolutional operator, is a simple Transformer layer. The output of the convolutional operator is added, element-wise, to the output of the self-attention mechanism. This allows, hypothetically, for the best-of-bothworlds, where the model has access to the self-attention mechanism and the active-memory mechanism.: Similarly, we also add the self-attention mechanism to the persistent-convolutional operator and the highwayconvolutional operator, respectively.\n",
      "\n",
      "Experiments: To evaluate the effectiveness of the various convolution-based active-memory mechanisms, we used two separate experiments; a language modelling task that is traditionally associated with attention-based mechanisms: [Shoeybi et al., 2019]: , and algorithmic tasks that are associated with active-memory models: [Kaiser and Sutskever, 2015]: . The active-memory mechanisms are experimented both independently and alongside a self-attention mechanism.\n",
      "\n",
      "Language Modelling Experimental Setup: The first task that the operators were tested with was a unidirectional language modelling task; the WikiText-3 (WT3) dataset: [Merity et al., 2016]: , tokenized using BPEtokenization: [Sennrich et al., 2016]: . The WikiText-3 dataset was sourced entirely from Wikipedia articles, contains over 3.6 millions lines of text, and is split into a training dataset, valid dataset and test dataset. The train dataset contains 103M tokens, while the valid and test dataset contain 250K tokens each.: The models used were all 8-layer models, with a hidden size of 256 and a filter size of 1024, a vocab size of 32,000, (x, y) 1 0 1 1 + 0 0 1 1 (x + y) 0 0 0 0 0 1 1 1 0 Table 2: The binary addition task. Given two numbers (in this case, the two numbers are 11 and 3), the final output is the binary version of the addition of the two input numbers (in this case, 14). kernel size of 20 and a dropout rate of 0.9. No further regularization was used. The optimizer was the Adam Optimizer: [Kingma and Ba, 2014]: and a warmup-learning rate was used, as specified in: [Vaswani et al., 2017]: . All models were implemented using Tensorflow, version 1.07, on a V100 GPU card.: Notable preprocessing was used for analyzing the WikiText-3 dataset; every character was explicitly denoted as lower-case, each hyphenwas replaced by @-@ and punctuation marks, such as fullstops and commas, were seperated by white-space. This was done to discourage the BPE to tokenize sets of characters that included punctuation marks, forcing the model to tokenize sets of characters that were only letters, therefore tokenizing a greater set of words.\n",
      "\n",
      "Experimental Results: With the exception of the CGRU operator, all active-memory mechanisms, when combined with the self-attention mechanism, outperformed the self-attention mechanism alone, achieving a lower loss-per-token. This would appear to vindicate the proposition of both this paper and [2], suggesting that, indeed, active-memory mechanisms and self-attention are comparable. However, no model that purely used an active-memory mechanism outperformed the self-attention mechanism for language modelling.: We note that, if the dropout rate was decreased to 0.7, all operators, with the exception of CGRU, all models achieved superior results to the self-attention mechanism at a the same dropout rate. However, these models did not achieve superior results to the self-attention model with a dropout-rate of 0.9. This would imply that self-attention mechanisms are more sensitive to dropout rates compared to active-memory mechanisms.: Further, each operator, except for the CGRU operator, benefited from combining it with self-attention, allowing both operators to operate independently and concurrently. The model with the lowest loss-per-token had a self-attention mechanism and a highway-convolutional operator. It is further worth noting that the highway-convolutional operator outperformed both other convolutional operators, both with and without the addition of the self-attention mechanism.\n",
      "\n",
      "Algorithmic Tasks Experimental Setup: The second experiment for evaluating the active-memory mechanisms was on various algorithmic tasks::  Reverse: Given an array X of size L, the model is trained to return the array Y, where Y[0] = X[-1]. In order to effectively perform this task, the model must be capable of analyzing the start of the input vector at the very end, and vice-versa.: (x, y): 1 0 1 0 1  0 1 1 0 0 (x  y) 0 0 0 1 1 1 1 1 1 0 0  Sort: Given an array of randomly order integers, the model is trained to return an array that accurately order the input integers. The entire vector must be remembered and analyzed at each time-step.:  Addition: Given two binary numbers, the model is trained to return an array that represents the addition in the form of a third binary number. An example of the addition task is shown in Table: 2: .:  Multiply: Multiplies two binary numbers, as shown in Table: 3: .:  Not: If the input is 1, then not returns 0. Else, the not function returns 1. The output relies only on the input at the current time-step.:  Remember: Given a series of random numbers of sequence size N , followed by a sequence of zeros of identical size, the model is trained to output a series of zeros of size N , followed by the random numbers. In order for this task to be performed, the model must be able to remember tokens over an increasingly long sequence.: All data for the algorithmic tasks were generated in an online manner. For three of the tasks, Sort, Addition and Multiply, the model must focus on multiple tokens at every timestep. In comparison, the Reverse task, the Not task and the Remember task only require the model to focus on a single token at every time-step.: The model that was used for algorithmic tasks contains 4 layers, with a hidden size of 128 units, a filter size of 512 and a kernel size of 20. Each model was trained for a maximum of 100 epochs, where each epoch contains 100 iterations. At the end of each epoch, the model was exposed to an online batch, containing 32 test cases. If the model achieved an accuracy of 100% on the online test batch, the sequence-size of the data is increased, therefore increasing its complexity.: For the Reverse, Sort, Not and Remember task, when the model achieved a 100% accuracy, the sequence was increased by 1. For the Addition and Multiply task, the sequence was increased by 2.: The model was initially trained only for sequences that are 5 tokens long and was not introduced to a larger sequence until the model was capable of achieving 100% accuracy on this sequence-size. We found that this form of curriculum learning was essential: if a model was initially trained on a sequence of several dozen tokens, each operator was incapable of achieving a reasonable accuracy.: The vocabulary size was different for each task. The Reverse task had a vocabulary size of 100, while the Sort task and the Remember task had a vocabulary size of 20. We noted that whenever the vocab size was increased the model would achieve less accurate results. Because all tokens in the Addition, Multiply and Not tasks are either 0, 1, or the separator, the vocabulary size is set to 3.\n",
      "\n",
      "Experimental Results: Each model was tested for each task, and the highest sequence that the model could achieve within 100 epochs was recorded. Each experiment was performed three times, and the average sequence size is presented in Table: 4: . For example, the selfattention mechanism managed to achieve a 100% accuracy for a sequence of 41 tokens for the Reverse task, but could not achieve a 100% accuracy for both the Sort task and the Addition task for a sequence size of 20 (the Sort task achieved a maximum sequence size of 14, while the Addition task achieved a maximum size of 7). Of the six algorithmic tasks tested, active-memory mechanisms were used, either solely or in combination with the self-attention mechanism, in the best-performing model of five of these tasks. For example, the self-attention mechanism achieved an average sequence size of 41.0 for the Reverse Task and 14.0 for the Sort Task, which are lower than those achieved by the \"self-attention + persistent-convolution\" mechanism: (43.7 and 23.3, respectively): . Furthermore, for the Addition and Multiply Tasks, the active-memory mechanisms across the board outperformed both the self-attention mechanism and the combination of the self-attention mechanism and the active-memory mechanism. For example, the traditional convolution operator, for the Addition Task, outperformed the self-attention mechanism and the \"self-attention + convolutional\" mechanism by 34.0 and 4.7 respectively. The results show that the active-memory mechanisms achieve equal, or superior, results to a traditional self-attention mechanism.: Self-attention, used alone, only performed optimally on the Remember task, and equally well on the Not task. Interestingly, across all models for the Addition and Multiply tasks, the self-attention mechanism reliably led to poor results; not only does the self-attention mechanism, alone, achieve the poorest results, but the combination of the self-attention mechanism and any active-memory system performed worse then the active-memory system alone. This is in direct contrast to the Sort task and the Reverse task, where the combination of self-attention mechanism and the active-memory achieve the best results.: The self-attention mechanism would, in theory, outperform active-memory mechanisms for the Remember task. This is because, in order to adequately perform the Remember task, the model must be capable of calculating an output based on long-range dependencies, which active-memory cannot match at a large enough sequence length. Other tasks do require a long-range dependency in order to operate well at large sequence sizes, but are dependent on the model performing other tasks as well. For example, the addition task requires to model long range-dependencies and perform binary addition. The self-attention mechanism, although it can learn these long-range dependencies, cannot access all necessary tokens at a given time to adequately perform binary addition. This is vindicated by the experimental results. In Table: 4: , the self-attention mechanism achieved the highest results on the Remember task. This would suggest that, if the algorithmic task only requires a long-range dependency, then the selfattention mechanism will outperform active-memory mech- The average sequence length that each operator was capable of gaining 100% accuracy within 100 epochs over 3 runs. The higher the sequence size, the better the model learned. For the Reverse and Sort tasks, the combination of self-attention mechanism and persistent-convolution achieved the best results. For the Sort, Addition, and Multiply tasks, the self-attention mechanism was beaten by the active-memory mechanisms. For the Addition and Multiply tasks, the mere use of a self-attention mechanism alongside an active-memory mechanism actively decreased results. The highest possible sequence that can be learned over these epochs is 104 in the Not task. The self-attention mechanism achieved the best result only for the Remember task.: anisms when used alone. In comparison, the self-attention mechanism is incapable of matching the results of activememory for all other tasks. These findings appear to vindicate the statement made by: Kaiser et. al [2]: ; whenever the sequential task requires the model to focus on multiple tokens at every time-step, using an attention mechanism will lead to extremely poor results, especially in comparison to active-memory models.: It is worth noting that, for each of the active-memory mechanisms operating alone, none of the three achieved a 100% accuracy for any sequence over a size of 37 for the Remember task. This is because, given the kernel size of 20 and 4 layers, the model is only capable of seeing 37 time-steps across. Therefore, the model cannot see 37 time-steps across and, therefore, cannot perform the Remember task at this sequence size or any larger sequence size. This displays the importance of utilizing both a self-attention mechanism, which can be utilized for analyzing long-range dependencies, and an active-memory mechanism, which can extract features that the self-attention mechanism cannot.\n",
      "\n",
      "Discussion of Results: The experiments above suggest that, across most tasks, a combination of a self-attention mechanism and an activememory mechanism, at worst, perform comparably to a purely attention-based model, and at best surpass an attention model, with the exception of the Remember task. However, for some algorithmic tasks, we note that the mere inclusion of a self-attention mechanism actively hinders performance.: Models that combine both the attention mechanism and active-memory mechanisms outperformed both attentiononly and active-memory-only models for language modelling. This suggests that, for language modelling tasks, both active-memory mechanisms and attention mechanisms are capable of extracting features that the other mechanism is not capable of extracting, and that both mechanisms operate optimally when used alongside each other.: The findings are further abstracted by studying the effect of various algorithmic tasks; in cases where only a single token needs to be focused on, the self-attention mechanism matches the most ardent active-memory, while active-memory mecha-nisms radically outperform self-attention for other tasks. This would imply that various time-dependencies that cannot be analyzed by a self-attention mechanism can be analyzed by active-memory.: It is worth noting that, for the Not function, all models learn optimally. This is likely due to the fact that the output of each time-step depends only on the input at this time-step, and each model can analyze this dependency equally efficiently. Also, based on the results of the Remember task, the self-attention mechanism can attain greater long-range dependency in comparison to the active-memory mechanisms.: Finally, we note that, for the Remember function, both mechanisms, when used alone, outperform the two mechanisms used together. For every other task, a combination of the self-attention and active-memory would improve upon at least one of the mechanisms when used alone. We are unsure exactly what has led to this result. This will require further investigation in the future.\n",
      "\n",
      "Conclusion: In this paper we investigate the Transformer's self-attention mechanism in comparison to a variety of active-memory mechanisms. We experiment on two types of tasks: the language modeling task and the algorithmic task. Our results show that the self-attention mechanism can be improved by an active-memory mechanism alone or by a combination of the two. Our results have implications for wider sequence modeling tasks, which are currently dominated by self-attention based models.: Our code and models used in experiments are available at: https://github.com/Anon-111/Active-Memory.: In the future, we will further explore the use of activememory for sequence-to-sequence tasks, such as machine translation. We will also analyze the empirical differences between the studied algorithmic tasks, and investigate why the self-attention mechanism may assist one task but harm another.\n",
      "===================\n",
      "We address representation learning for large-scale instance-level image retrieval. Apart from backbone, training pipelines and loss functions, popular approaches have focused on different spatial pooling and attention mechanisms, which are at the core of learning a powerful global image representation. There are different forms of attention according to the interaction of elements of the feature tensor (local and global) and the dimensions where it is applied (spatial and channel). Unfortunately, each study addresses only one or two forms of attention and applies it to different problems like classification, detection or retrieval.: We present global-local attention module (GLAM), which is attached at the end of a backbone network and incorporates all four forms of attention: local and global, spatial and channel. We obtain a new feature tensor and, by spatial pooling, we learn a powerful embedding for image retrieval. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.\n",
      "\n",
      "Introduction: Instance-level image retrieval is at the core of visual representation learning and is connected with many problems of visual recognition and machine learning, for instance metric learning: [30,: 26]: , few-shot learning: [42]: and unsupervised learning: [8]: . Many large-scale open datasets: [3,: 37,: 16,: 29,: 53]: , and competitions 1 have accelerated progress in instance-level image retrieval, which has been transformed by deep learning: [3]: .: Many studies on instance-level image retrieval focus on learning features from convolutional neural networks (CNN), while others focus on re-ranking, for instance by graph-based methods: [11]: . The former can be distinguished according to feature types: local descriptors, reminiscent of SIFT: [27]: , where an image is mapped to a few hundred vectors; and global descriptors, where an image is mapped to a 1 https://www.kaggle.com/c/landmark-retrieval-2020 single vector. In fact, deep learning has brought global descriptors with astounding performance, while allowing efficient search. Our study belongs to this type.: Studies on global descriptors have focused on spatial pooling: [2,: 37]: . The need for compact, discriminative representations that are resistant to clutter has naturally given rise to spatial attention methods: [24,: 28]: . Different kinds of attention have been studied in many areas of computer vision research. There is also channel attention: [20,: 9]: ; local attention, applied independently to elements of the representation (feature map): [54,: 25]: ; global attention, based on interaction between elements: [52,: 9]: ; and combinations thereof. Unfortunately, each study has been limited to one or two kinds of attention only; attention is not always learned; and applications vary.: It is the objective of our work to perform a comprehensive study of all forms of attention above, apply them to instance-level image retrieval and provide a detailed account of their interaction and impact on performance. As shown in Figure: 1: , we collect contextual information from images with both local and global attention, giving rise to two parallel network streams. Importantly, each operates on both spatial locations and feature channels. Local attention is about individual locations and channels; global is about interaction between locations and between channels. The extracted information is separately embedded in local and global attention feature maps, which are combined in a global-local attention feature map before pooling.: Our contributions can be summarized as follows:: 1. We propose a novel network that consists of both global and local attention for image retrieval. This is the first study that employs both mechanisms.\n",
      "\n",
      "Each of the global and local attention mechanisms: comprises both spatial and channel attention. 3. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.: A l c c  1  1  + F l c A l s 1  h  w  + F l  c  h  w F  + c  h  w F gl A g c c  c  F g c A g s hw  hw  + F g\n",
      "\n",
      "Related work: Instance-level image retrieval Studies on instance-level image retrieval can be roughly, but not exclusively, divided into three types: (1) studies on global descriptors: [3,: 16,: 24,: 53,: 2,: 37]: ; (2) studies on local descriptors and geometry-based re-ranking: [29,: 45,: 40,: 53]: ; (3) re-ranking by graph-based methods: [11,: 21,: 55]: . The first two types of studies focus on the feature representation, while the last type focuses on re-ranking extracted features.: Studies on global descriptors focus on spatial pooling of CNN feature maps into vectors, including MAC: [38]: , SPoC: [2]: , CroW: [24]: , R-MAC: [48,: 15,: 16]: , GeM: [37]: , and NetVLAD: [1,: 25]: , as well as learning the representation: [3,: 15,: 16,: 36,: 37]: . Studies before deep learning dominated image retrieval were mostly based on local descriptors like SIFT: [27]: and bag-of-words representation: [32]: or aggregated descriptors like VLAD: [22]: or ASMK: [46]: . Local descriptors have been revived in deep learning, e.g. with DELF: [29]: , DELG: [5]: and ASMK extensions: [45,: 47]: .: We focus on learning a global descriptor in this work, because it is the most efficient in terms of storage and search. However, our generic attention mechanism produces a feature tensor and could be applicable to local descriptors as well, if global pooling were replaced by local feature detection. Re-ranking methods are complementary to the representation and we do not consider them in this work.: Attention Attention mechanisms have been first proposed in image classification studies focusing on channel at-: METHOD LOCAL GLOBAL LRN RET Spatial Channel Spatial Channel: SENet: [20]: ECA-Net: [51]: GCNet: [6]: CBAM: [54]: GE: [19]: NL-Net: [52]: AA-Net: [4]: SAN: [59]: N 3 Net [34] A 2 -Net: [9]: GSoP: [14]: OnA: [23]: AGeM: [17]: CroW: [24]: CRN: [25]: DELF: [29]: DELG: [5]: Tolias et al.: [47]: SOLAR: [28]: Ours Table: 1: : Related work on attention. LRN: learned; RET: applied to instance-level image retrieval.: tention: [20,: 51,: 6]: , spatial attention: [19]: or both, like CBAM: [54]: . In image retrieval, CroW: [24]: also employs  both spatial and channel attention and can be seen as a precursor of CBAM, but, like other studies of spatial attention on retrieval: [41,: 23,: 17]: , it is not learned. CRN: [25]: applies spatial attention for feature reweighting and is learned. Learned spatial attention mechanisms are common for local descriptors: [29,: 5,: 47]: .: c  h  w c  1  1 c  1  1 F A l c: We call the above methods local attention, in the sense that elements of the feature tensor (channels / spatial locations), are weighted independently, based on contextual information obtained by pooling or learned. By constrast, by global attention we refer to mechanisms that model interaction between elements of the feature tensor, for example between channels or between locations.: In image classification, non-local neural network (NL-Net): [52]: is maybe the first global attention mechanism, followed by similar studies: [4,: 59,: 34]: . It is global spatial attention, allowing interaction between any pair of spatial locations. Similarly, there are studies of global channel attention, allowing interaction between channels: [9,: 14]: . Global attention has focused mostly on image recognition and has been applied to either spatial or channel attention so far, not both. In image retrieval, SOLAR: [28]: is a direct application of the global spatial attention mechanism of: [52]: .: Table: 1: attempts to categorize related work on attention according to whether attention is local or global, spatial or channel, whether it is learned and whether it is applied to instance-level image retrieval. We observe that all methods limit to one or two forms of attention only. Of those studies that focus on image retrieval, many are not learned: [23,: 17,: 24]: , and of those that are, some are designed for local descriptors: [29,: 47]: .: By contrast, we provide a comprehensive study of all forms of attention, global and local, spatial and channel, to obtain a learned representation in the form of a tensor that can be used in any way. We spatially pool it into a global descriptor and we study the relative gain of different forms of attention in image retrieval.: feature map conv 1  1 conv 3  3 conv 5  5 conv 7  7 concat conv 1  1 attention map c  h  w 4c  h  w 1  h  w c  h  w dilated conv F F A l s\n",
      "\n",
      "Global-local attention: We design a global-local attention module (GLAM), which is attached at the end of a backbone network. Figure: 1: illustrates its main components. We are given a c  h  w feature tensor F, where c is the number of channels, and h  w is the spatial resolution. Local attention collects context from the image and applies pooling to obtain a c  1  1 local channel attention map A l c and a 1  h  w local spatial attention map A l s . Global attention allows interaction between channels, resulting in a c  c global channel attention map A g c , and between spatial locations, resulting in a hw  hw global spatial attention map A g s . The feature maps produced by the two attention streams are combined with the original one by a learned fusion mechanism into the global-local attention feature map F gl before being spatially pooled into a global image descriptor.\n",
      "\n",
      "Local attention: We extract an 1D channel and a 2D spatial attention map to weigh the feature map in the corresponding dimensions.: Local channel attention Following ECA-Net: [51]: , this attention captures local channel information. As shown in Figure: 2: , we are given a c  h  w feature tensor F from our backbone. We first reduce it to a c  1  1 tensor by global average pooling (GAP). Channel attention is then captured by a 1D convolution of kernel size k along the channel dimension, where k controls the extent of cross-channel interaction. This is followed by a sigmoid function, resulting in the c  1  1 local channel attention map A l c . Local spatial attention Inspired by the inception module: [43]: and similar to: [25]: , this attention map captures local spatial information at different scales. As shown in Figure: 3: , given the same c  h  w feature tensor F from our backbone, we obtain a new tensor F with channels reduced to c , using a 1  1 convolution. We then extract local spatial contextual information using convolutional filters of kernel size 3  3, 5  5, and 7  7, which are efficiently implemented by 3  3 dilated convolutions: [7,: 57]: with dilation parameter 1, 2, and 3 respectively. The resulting features, along with one obtained by 1  1 convolution on F , are concatenated into a 4c  h  w tensor. Finally, we obtain the 1  h  w local spatial attention map A l s by a 1  1 convolution that reduces the channel dimension to 1.: feature map GAP conv1d(k) conv1d(k) sigmoid sigmoid   softmax attention feature map 1  c 1  c 1  c Qc c  c hw  c Vc A g c c  h  w 1  c 1  c Kc F Gc: The middle column of Figure: 6: shows heat maps of local spatial attention, localizing target objects in images.: Local attention feature map We use the local channel attention map A l c to weigh F in the channel dimension: F l c := F A l c + F.: (1): We then use local spatial attention map A l s to weigh F l c in the spatial dimensions, resulting in the c  h  w local attention feature map: F l = F l c A l s + F l c .: (2): Here, A B denotes an element-wise multiplication of tensors A and B, with broadcasting when one tensor is smaller. We adopt the choice of applying channel followed by spatial attention from convolutional block attention module CBAM: [54]: . However, apart from computing A l s at different scales, both attention maps are obtained from the original tensor F rather than sequentially. In addition, both (1) and (2) include residual connections, while CBAM includes a single residual connection over both steps.\n",
      "\n",
      "Global attention: We extract two matrices capturing global pairwise channel and spatial interaction to weigh the feature map. Global channel attention We introduce a global channel attention mechanism that captures global channel interaction. This mechanism is based on the non-local neural network: [52]: , but with the idea of 1D convolution from ECA-Net: [51]: . As shown in Figure: 4: , we are given the c  h  w feature tensor F from our backbone. We apply GAP and squeeze spatial dimensions, followed by a 1D convolution of kernel size k and a sigmoid function, to obtain 1c query Q c and key K c tensors. The value tensor V c is obtained by mere reshaping of F to hwc, without GAP. Next, we form the outer product of K c and Q c , followed by softmax over channels to obtain a c  c global channel attention map: feature map conv 1  1 conv 1  1 conv 1  1   softmax conv 1  1 attention feature map c  hw Qs hw  hw c  h  w c  hw Vs c  h  w A g s c  h  w c  hw Kc F Gs: A g c = softmax(K c Q c ).: (3): Finally, this attention map is multiplied with V c and the matrix product V c A g c is reshaped back to c  h  w to give the global channel attention feature map G c . In GSoP: [14]: and A 2 -Net: [9]: , a c  c global channel attention map is obtained by multiplication of hw  c matrices; (3) is more efficient, using only an outer product of 1  c vectors.: Global spatial attention Since ordinary convolution applies only a local neighborhood at a time, it cannot capture global contextual information. Thus, we apply non-local filtering: [52]: , which is a form of self-attention: [49]: in the spatial dimensions. As shown in Figure: 5: , we are given the same c  h  w feature tensor F from our backbone. By using three 1  1 convolutions, which reduce channels to c , and flattening spatial dimensions to hw, we obtain c  hw query Q s , key K s , and value V s tensors, where each column is a feature vector corresponding to a particular spatial location. We capture pairwise similarities of these vectors by matrix multiplication of K s and Q s , followed by softmax over locations to obtain a hw  hw global spatial attention map:: A g s = softmax(K s Q s ).: (4): This attention map is multiplied with V s and the matrix product V s A g s is reshaped back to c  h  w by expanding the spatial dimensions. Finally, using a 1  1 convolution, which increases channels back to c, we obtain the c  h  w global spatial attention feature map G s .: The right column of Figure: 6: shows heat maps for global spatial attention, localizing target objects in images.: Global attention feature map We use the global channel attention feature map F c to weigh F element-wise: F g c = F G c .: (5): We then use global spatial attention feature map G s to weigh F g c element-wise, resulting in the c  h  w global attention feature map: F g = F g c G s + F g c . (: 6: ): Similarly to F l in (: 1: ) and (: 2: ), we apply channel attention first, followed by spatial attention. However, unlike (1), there is no residual connection in (: 5: ). This choice is supported by early experiments.\n",
      "\n",
      "Global-local attention: Feature fusion As shown in Figure: 1: , we combine the local and global attention feature maps, F l and F g , with the original feature F. While concatenation and summation are common operations for feature combination, we use a weighted average with weights w l , w g , w respectively, obtained by softmax over three learnable scalar parameters, to obtain a c  h  w global-local attention feature map: F gl = w l F l + w g F l + wF.: (7): EfficientDet: [44]: has shown that this is the most effective, among a number of choices, for fusion of features across different scales.: Pooling We apply GeM: [37]: , a learnable spatial pooling mechanism, to feature map F gl (7), followed by a fullyconnected (FC) layer with dropout and batch normalization. The final embedding is obtained by 2 -normalization.\n",
      "\n",
      "Experiments\n",
      "\n",
      "Datasets: Training set There are a number of open landmark datasets commonly used for training in image retrieval studies, including neural code (NC): [3]: , neural code clean (NCclean): [16]: , as well as Google Landmarks v1 (GLDv1): [29]: and v2 (GLDv2): [53]: . Table: 2: shows relevant statistics. These datasets can be categorized into noisy and clean. The clean sets were obtained from the original noisy sets for more effective training: [16,: 53]: . The original noisy datasets are much larger, but they have high intra-class variability.  Evaluation set and metrics We use four common evaluation datasets for landmark image retrieval: Oxford5k (Ox5k): [32]: , Paris6k (Par6k): [33]: , as well as Revisited Oxford (ROxford or ROxf) and Paris (RParis or RPar): [35]: .: ROxford and RParis are used with and without one million distractors (R1M): [28]: and evaluated using the Medium and Hard protocols: [35]: . We evaluate using mean Average Precision (mAP) and mean precision at 10 (mP@10).\n",
      "\n",
      "Implementation details: We train on 8 TITAN RTX 2080Ti GPUs. All models are pre-trained on ImageNet: [39]: and implemented in PyTorch: [31]: . For fair comparisons, we set a training environment similar to the those of compared studies: [56,: 53,: 28,: 35]: . We employ ResNet101: [18]: as a backbone model. The kernel size k of ECANet in subsection 3.1 is set to 3. The parameter p of GeM in subsection 3.3 is set to 3 and the dimension d of final embeddings to 512. We adopt ArcFace: [10]: , a cosine-softmax based loss, with a margin of 0.3. We use stochastic gradient descent with initial learning rate 10 -3 , momentum 0.9 and weight decay 10 -5 .: We adopt the batch sampling of Yokoo et al.: [56]: where mini-batch samples with similar aspect ratios are resized to a particular size. Here, we use a batch size of 64. For image augmentation, we apply scaling, random cropping, and varied illumination. At inference, we apply a multi-resolution representation: [16]: to query and database images.: Our method is denoted as GLAM (global-local attention module). Using the backbone model alone is referred to as baseline. It is compatible with recent models based on ResNet101-GeM trained with ArcFace: [53,: 28]: . Adding our local attention (subsection 3.1) to the baseline model is denoted +local, while adding our global attention (subsection 3.2) is denoted +global. Since we focus on representation learning, we do not consider post-processing methods like geometry-based re-ranking: [29,: 40,: 53]: or graph-based re-ranking: [11,: 21,: 55]: [53,: 28]: . All models use ResNet101-GeM. Red: best results. Blue: GLAM higher than SOLAR: [28]: on GLDv1-noisy.: GLDv2-noisy has 2.6 times more images than GLDv2clean, the latter is superior by a large margin. This shows that, in training, a cleaner dataset can be more important than a larger one. By contrast, NC-clean has the worst performance despite being clean, aparently because it is: [53]: is the only model other than ours trained on GLDv2-clean, while: [28]: is trained on GLDv1-noisy and compared in Table: 3: .: too small. To achieve best possible performance, we use GLDv2-clean as a training set in the remaining experiments.\n",
      "\n",
      "Comparisons on same training set: It is common to compare methods regardless of training sets as more become available, e.g.,: [35,: 28]: . Since GLDv2-clean is relatively new, Weyand et al.: [53]: , which introduced the dataset, is the only study that has trained the same backbone with the same settings (ResNet101-GeM with ArcFace) on GLDv2-clean.: Our baseline is lower than: [53]: , because our dimensinality is 512, while other models based on ResNet101 use 2048. Yet, Table: 3: shows that our best model trained on GLDv2-clean outperforms: [53]: by a large margin. But the most important comparison is with SOLAR: [28]: , also based on selfattention, which has trained ResNet101-GeM on GLDv1noisy. On this training set, our best model clearly outperforms: [28]: despite lower dimensionality. With this model, we outperform previous best methods on most benchmarks except mP@10 on RParis (medium) and RParis+R1M (medium), where we are outperformed by: [37,: 35]: . These results demonstrate that our approach is effective for landmark image retrieval. Figure: 7: shows some\n",
      "\n",
      "Comparison with state of the art\n",
      "\n",
      "Ablation study: Our ablation study uses the Google Landmark v2 clean dataset (GLDv2-clean): [53]: for training, which is shown to be the most effective in Table: 3: Table: 9: : mAP comparison of using multiresolution representation (Multi) or not (Single) on query or database.\n",
      "\n",
      "Effect of attention modules: We ablate the effect of our local and global attention networks as well as their combination. Table: 5: shows the results, which are more finegrained than those of Table: 4: . In particular, it shows the effect of the channel and spatial variants of both local and global attention. We observe that, when used alone, the channel and spatial variants of local attention are harmful in most cases. Even the combination, baseline+local, is not always effective. By contrast, when used alone, the channel and spatial variants of global attention are mostly beneficial, especially the latter. Their combination, baseline+global, is impressive, bringing gain of up to 7.5%. Importantly, the combination baseline+global+local improves further by up to another 2.8%. This result shows the necessity of local attention in the final model.\n",
      "\n",
      "CBAM vs. our local spatial attention: We experiment with the local spatial attention of CBAM: [54]: . CBAM applies average and max-pooling to input features and concatenates the two for spatial attention. We apply this variant to our local spatial attention module for comparison.: For the CBAM style module, we keep the overall design of our module as shown in Figure: 3: , but apply average and max-pooling to each of the four convolutional layer outputs before concatenation. Table: 6: shows that the CBAM style module is considerably worse than ours on all benchmarks except Paris6k, where it is only slightly better.: Concatenation vs. sum for feature fusion We use a softmax-based weighted average of local and global attention feature maps with the original feature map: (7): . Here, we compare this weighted average with weighted concatenation, where concatenation replaces the sum operation in: (7): . As shown in Table: 7: , the weighted average outperforms the weighted concatenation.: Fixed-size vs. group-size sampling Numerous studies have proposed methods for constructing batches according to image size for efficient training. For instance, Gordo et al.: [16]: , DELF: [29]: , and Yokoo et al.: [56]: employed different image sizes per batch for training instead of a single fixed size. We adopt the method of Yokoo et al., which constructs a batch with images of similar aspect ratio, so that the images can be resized to a size with an aspect ratio that is similar to their own. We call this method group-size sampling. Table: 8: compares fixed-size (224  224) with groupsize sampling. We observe that maintaining aspect ratios by using dynamic input sizes is much more effective.\n",
      "\n",
      "Multi-resolution: We use the multi-resolution representation: [16]: for the final feature of an image at inference time. This method: (1) resizes an image into multiple scales; (2) extracts features from the resized images; and (3) averages the features to obtain the final feature of the image. The method is applied to both query and database images to enhance ranking results, especially for small target objects. Table: 9: compares the four cases of applying this method or not to query or database images.\n",
      "\n",
      "Conclusion: We have introduced a novel approach that extracts global and local contextual information using attention mechanisms for instance-level image retrieval. It is manifested as a network architecture consisting of global and local attention components, each operating on both spatial and channel dimensions. This constitutes a comprehensive study and empirical evaluation of all four forms of attention that have previously been studied only in isolation. Our findings indicate that the gain (or loss) brought by one form of attention alone strongly depends on the presence of the others, with the maximum gain appearing when all forms are present. The output is a modified feature tensor that can be used in any way, for instance with local feature detection instead of spatial pooling for image retrieval.: With the advent of vision transformers: [12,: 58]: and their recent application to image retrieval: [13]: , attention is expected to play a more and more significant role in vision. According to our classification, transformers perform global spatial attention alone. It is of great interest to investigate the role of the other forms of attention, where our approach may yield a basic building block of such architectures. One may even envision an extension to language models, where transformers originate from: [50]: .\n",
      "===================\n",
      "This paper aims to tackle the challenging task of oneshot object counting. Given an image containing novel, previously unseen category objects, the goal of the task is to count all instances in the desired category with only one supporting bounding box example. To this end, we propose a counting model by which you only need to Look At One instance (LaoNet). First, a feature correlation module combines the Self-Attention and Correlative-Attention modules to learn both inner-relations and inter-relations. It enables the network to be robust to the inconsistency of rotations and sizes among different instances. Second, a Scale Aggregation mechanism is designed to help extract features with different scale information. Compared with existing few-shot counting methods, LaoNet achieves state-of-the-art results while learning with a high convergence speed. The code will be available soon.\n",
      "\n",
      "INTRODUCTION: Object counting has become increasingly important due to its wide range of applications such as crowd surveillance, traffic monitoring, wildlife conservation and inventory management. Most of the existing counting methods: [1,: 2,: 3]: focus on a particular, single category. However, when applying them into new categories, their performances will drop catastrophically. Meanwhile, it is extremely difficult and costly to collect all categories and label them for training.: For humans, the generalization ability allows them to learn and deal with various vision tasks without much prior knowledge and experience. We are amazed by this remarkable ability and in this work, we focus on this learning paradigm and design a network to efficiently recognize and count new categories given only one example. We follow the few-shot setting in: [4]: and modify it to one-shot object counting. That is, the model takes an image with unseen novel categories and a supporting bounding box containing an example instance of desired category as input, and then predicts the object count in the image.: However, there are two main challenges. First, the object counting task includes many different categories, and even several categories exist within a same image. Moreover in few-shot setting, these categories will not overlap between training and inference. This means that the model needs to have a strong distinguishing ability between features of different categories, and meanwhile, an effective associating ability among instances sharing the same category. Second, in one-shot counting, the model learns from only one supporting instance. Much of the difficulty results from the fact that the supporting sample may differ from other instances in, for example, sizes and poses. Hence, the model is required to be invariant towards these variations without seeing the commonalities across different instances.: Therefore, in this paper, we propose an effective network named LaoNet for one-shot object counting. It consists of three main parts: feature extraction, feature correlation and the density regressor, as shown in Figure: 1: . The feature correlation model and the feature extraction model are elaborately designed to address the above two challenges.: We propose the feature correlation based on Self-Attention and Correlative-Attention modules to learn innerrelations and inter-relations respectively. The Self-Attention encourages the model to focus more on important features and their correlations, improving the efficiency of information refinement. Previous few-shot counting methods: [4,: 5]: usually leverage on a convolution operation to match the similarities between image features and supporting features. However, as the kernel is derived from supporting features with the default size and rotation angle, the convolution operation will greatly depend on the quality of supporting features and the consistency of physical properties among different instances. Instead, our designed feature correlation model benefits from two kinds of attention modules and addresses the above problem by considering all correlations.: We further propose a Scale Aggregation mechanism in scale extraction to deal with scale variations among different categories and different instances. By learning features from multi-subspace, the model aggregates various scale information while maintaining a spatial consistency.: To summarize, our contribution is threefold.:  We design a novel network named LaoNet (A network by which you only need to Look At One instance) for one-shot object counting. By combining Self-Attention and Correlative-Attention modules, LaoNet exploits the correlation among novel category objects with high accuracy and efficiency.:  We propose a Scale Aggregation mechanism to extract more comprehensive features and fuse multi-scale information from the supporting box.:  The experimental results show that our model achieves state-of-the-art results with significant improvements on FSC-147: [4]: and COCO: [6]: datasets under the oneshot setting without fine-tuning.\n",
      "\n",
      "RELATED WORKS: Object counting methods can be briefly divided into two types. Detection based methods: [7]: count the number of objects by exhaustively detecting every target in images. But they rely on the complex labels such as bounding boxes. Regression based methods: [1,: 2]: learn to count by predicting a density map, in which each value represents the density of target objects at the corresponding location. The count prediction equals to the total sum of density map. Nevertheless, most of the counting methods are category specifically, e.g. for human crowd: [1,: 2,: 8,: 9,: 10,: 11]: , for cars: [3,: 12]: , for plants: [13]: or for cells: [14,: 15]: . They focus on only one category and will loss the original satisfied performance when transferring to other categories. Moreover, most traditional approaches usually rely on tens of thousands of instances to train a counting model: [2,: 8,: 9,: 11,: 3,: 12]: .: To reduce considerably the number of samples needed to train a counting model for a particular category, recently, fewshot counting task has been developed. The key lies in the generalization ability of the model to deal with novel categories from few labeled examples. The study: [16]: proposes a Generic Matching Network (GMN) for class-agnostic counting. However it still needs several dozens to hundreds examples of a novel category for adaptation and good performance. CFOCNet is introduced to match and utilize the similarity between objects within the same category: [5]: . The work: [4]: presents a Few Shot Adaptation and Matching Network (Fam-Net) to learn feature correlations and few-shot adaptation and also introduces a few-shot counting dataset named FSC-147.: When the number of labeled example decreases to one, the task evolves into one-shot counting. In other visual tasks, researchers develop methods for one-shot segmentation: [17]: and one-shot object detection: [18,: 19]: . Compared to the fewshot setting which usually uses at least three instances for each object: [4]: , the one-shot setting, where only one instance is available, is clearly more challenging.: It is worth mentioning that detection based approaches: [20,: 21,: 22]: are inferior for the tasks of few-shot and one-shot counting. One main reason is that it requires extra and costly bounding-box annotations of all instances in the training stage while one-shot counting approach which we focus on depends on dot annotations and only one supporting box. To illustrate this point further, we perform experiments in Section 4.3 to compare with detection based approaches and validate the proposed network for one-shot counting.\n",
      "\n",
      "APPROACH\n",
      "\n",
      "Problem Definition: One-shot object counting consists of a training set (I t , s t , y t )  T and a query set (I q , s q )  Q, in which categories are mutually exclusive. Each input for the model contains an image I and a supporting bounding box s annotating one object of the desired category. In training set, abundant point annotations y t are available to supervise the model. In inference stage, we aim the model to learn to count the novel objects in I q with a supporting category instance sampled by s q .\n",
      "\n",
      "Feature Correlation: As the model is required to learn to count from only one supporting object, seizing the correlation between features with high efficiency is quite important. Therefore, we build the feature correlation model in our one-shot network based on Self-Attention and Correlative-Attention modules, for learning the inner-relations and inter-relations respectively.: As illustrated in Figure: 1: (violet block), our Self-Attention module consists of a Multi-head Attention (MA) and a layer normalization (LN). We first introduce the definition of attention: [23]: , given the query Q, key K and value vector V :: A(Q, K, V | W ) = S( (QW Q )(KW K ) T  d + P E)(V W V ),: (1): where S is the softmax function and 1  d is a scaling factor based on the vector dimension d. W : W Q , W K , W V  R dd are weight matrices for projections and P E is the position embedding.: To leverage on more representation subspaces, we adopt the extending form with multi attention heads:: M A(Q, K, V ) = Concat(head 1 , .., head h )W O where head i = A(Q, K, V | W i ).: (: ): 2: The representation dimensions are divided by parallel attention heads, where parameter matrices: W i : W Q i , W K i , W V i  R dd/h and W O  R dd .: One challenging problem in counting task is the existence of many complex interfering things. To efficiently weaken the negative influence by those irrelevant background, we apply Multi-head Self-Attention in image features to learn innerrelations and encourage the model to focus more on repetitive objects that can be counted.: We denote the feature sequences of the query image and the supporting box region as X and S, with sizes X  R HW C and S  R hwC . And the refined query feature is calculated by:: X = LN (M A(X Q , X K , X V ) + X).: (3): A layer normalization (LN) is adopted to balance the value scales.: Meanwhile, as there is only one supporting object in oneshot counting problem, refining the salient features within the object is necessary and helpful for counting efficiency and accuracy. Therefore we apply another Self-Attention module to supporting feature and get refined S.: Previous few-shot counting methods: [4,: 5]: usually adopt a convolution operation where the supporting features act as kernels to match the similarities for target category. However, the results will greatly depend on the quality of supporting features and the consistency of objects' properties, including rotations and scales.: To this end, we propose a Correlative-Attention module to learn inter-relations between query and supporting features and alleviate the constraints of irrelevant properties.: Specifically, we extend the MA by learning correlations between different feature sequences and add a feed-forward network (FFN) to fuse the features, i.e.,: X * = Corr( X, S) = G(M A( XQ , SK , SV ) + X). (4): G includes two LNs and a FFN in the form of residual (light blue block in Figure: 1: ). Finally, X * and S will be fed into the cycle as new feature sequences where each cycle consists of two Self-Attention modules and a Correlative-Attention module.\n",
      "\n",
      "Feature Extraction and Scale Aggregation: To extract feature sequences from images, we use VGG-19 as our backbone. For query image, the output of the final level is directly flattened and transmitted into Self-Attention module. For the supporting box, as there are uncontrollable scale variations among instances due to the perspective, we propose a Scale Aggregation mechanism to fuse different scale information.: Given l as the number of layers in CNN, we aggregate the feature maps among different scales:: S = Concat(F l (s), F l-1 (s), ..., F l+1- (s)),: (5): where F i represents a feature map at i th level and   [1, l] decides the number of layers taken for aggregation. Meanwhile, we leverage on identifying position embedding to help the model distinguish the integrated scale information in attention model. By adopting the fixed sinusoidal absolute position embedding: [23]: , feature sequences from different scales can still maintain the consistency between positions, i.e., P E (posj ,2i) = sin(pos j /10000 2i/d ), P E (posj ,2i+1) = cos(pos j /10000 2i/d ).: (: ): 6: i is the dimension and pos j is the position for j th feature map.\n",
      "\n",
      "Training Loss: We use Euclidean distance to measure the difference between estimated density map and ground truth density map, which is generated based on annotated points following: [1]: . The loss is defined as follows:: L E = ||D gt -D|| 2 2 , (: 7: ): where D is the estimated density map and D gt is the ground truth density map. To improve the local pattern consistency, we also adopt a SSIM loss followed the calculation in: [8]: . By integrating the above two loss functions, we have: L = L E + L SSIM ,: (8): where  is the balanced weight.\n",
      "\n",
      "EXPERIMENTS\n",
      "\n",
      "Implement Details and Evaluation Metrics: We design the density regressor by an upsampling layer and three convolution layers with ReLU activation. The kernel sizes of first two layers are 3  3 and that of last is 1  1. Random scaling and flipping are adopted for each training image. Adam: [24]: with a learning rate 0.5  10 -5 is used to optimize the parameters. We set the number of attention heads h as 4, the correlation cycle T as 2, the number of aggregated layers  as 2, and the loss balanced parameter  as 10 -4 . Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are used to measure the performance of our methods. They are defined by:: M AE = 1 M M i=1 N gt i -N i , RM SE = 1 M M i=1 (N gt i -N i ) 2 ),: (9): where M and N gt are the number of images and the groundtruth count, respectively. The predicted count N is calculated by integrating the estimated density map D. MS-COCO: [6]: is a large dataset widely used in object detection and instance segmentation. In val2017 set, there are 80 common object categories with 5,000 images in complex everyday scenes. We follow: [17]: to generate four train/test splits which each contains 60 training and 20 testing categories.\n",
      "\n",
      "Comparison with Few-Shot Approaches: We hold experiments on above two few-shot counting datasets to evaluate the proposed network. As there are few existing  methods specifically designed for one-shot counting, for comprehensive evaluation, we modify FamNet: [4]: and CFOC-Net: [5]: for this setting and also compare with other few-shot counting approaches: [25,: 26,: 16,: 27,: 17]: . First, quantitative results on FSC-147 are shown in Table 1. We list seven results of previous few-shot detection and counting methods in 3-shot setting and two results of stateof-the-art counting methods in 1-shot setting for comparison. The result of FamNet: [4]: uses the adaptation strategy during testing.: It is worth noticing that our one-shot LaoNet outperforms all of previous few-shot methods, even those in 3 shot set- Second, Table: 2: shows the results on each of four folds of COCO val2017. Methods with  in the upper part of the table follow the experiment setting in: [5]: . That is, the supporting examples are chosen from all instances in the dataset during training and testing, which is laborious and costly under the need of all instances annotated by bounding boxes. While our setting allows only one fixed instance for each image, we reconduct the experiment of CFOCNet: [5]: . As the result shows, our method maintains a great performance on COCO dataset.\n",
      "\n",
      "Discussions: Contribution of Different Terms. We study the accuracy contributions of different terms in FSC-147. The result is shown in Table: 3: , each row whereof reports the results after removing one component or one term from LaoNet. The Self-Attention modules for the two feature sequences to learn inner-relations increase the accuracy in testing set by 19.9% and 15.7% for MAE, 9.5% and 13.1% for RMSE, respectively. Compared to other two terms, the Self-Attention modules contribute most to the performance of our model.: The Scale Aggregation mechanism helps more on RMSE. The result demonstrates a robustness contribution under the multi-scale aggregation. Finally, the SSIM loss further improves the counting accuracy by both lower MAE and RMSE. Convergence Speed. We hold experiments to measure the convergence speed and the performance stability. We pick FamNet: [4]: as the baseline for LaoNet with a pre-trained CNN backbone and an Adam optimizer. We train both two models on FSC-147 and report the validation MAE for 100 epochs.: As shown in Figure: 3: , our model has faster convergence speed and better stability than FamNet. With just 2 epoches, our method achieves a low counting error which FamNet has to reach after 40 epochs. Meanwhile, the convergence of our method is smooth and stable, while that of Famet is jagged, with multiple sharp peaks and the highest error of 70. Comparison with Object Detectors. Object detectors can be used for counting task with the number of predicted detections. However, even these detectors work with categories which they are trained on instead of one-shot setting, their counting performances are still limited. We select images of FSC-147-COCO subset from FSC147 Val and Test sets which share categories with MS-COCO dataset and conduct quantitative experiments.: As the results shown in Table: 4: , we compare LaoNet with several object detectors which are well pre-trained with thou-\n",
      "\n",
      "CONCLUSION: This paper targets one-shot object counting, which requires the counting model to count objects of new categories by looking at only one instance. We propose an efficient network named LaoNet to address this challenge. LaoNet includes a feature correlation module to learn both inner-relations and inter-relations and a scale aggregation module to extract multi-scale information for improving robustness. Without any fine-tuning in inference, our LaoNet outperforms previous state-of-the-art few-shot counting methods with a high convergence speed. In the future, we consider applying our model to a wider range of one-shot vision tasks.\n",
      "===================\n",
      "Attention mechanisms are a central property of cognitive systems allowing them to selectively deploy cognitive resources in a flexible manner. Attention has been long studied in the neurosciences and there are numerous phenomenological models that try to capture its core properties. Recently attentional mechanisms have become a dominating architectural choice of machine learning and are the central innovation of Transformers. The dominant intuition and formalism underlying their development has drawn on ideas of keys and queries in database management systems. In this work, we propose an alternative Bayesian foundation for attentional mechanisms and show how this unifies different attentional architectures in machine learning. This formulation allows to to identify commonality across different attention ML architectures as well as suggest a bridge to those developed in neuroscience. We hope this work will guide more sophisticated intuitions into the key properties of attention architectures as well suggest new ones.\n",
      "\n",
      "Introduction: Designing neural network architectures with favourable inductive biases lies behind many recent successes in Deep Learning: (Baxter, 2000): . In particular, the attention mechanism has allowed language models to achieve human like generation abilities previously thought impossible: (Vaswani et al., 2017): . The success of the attention mechanism as a domain agnostic architecture has prompted it to be adopted across a huge range of tasks and domains notably reaching state-of-the-art performance in visual reasoning and segmentation tasks: (Dosovitskiy et al., 2021;: Wang et al., 2022): . Despite it's success, the role of the attention mechanism remains poorly understood. Indeed, it is unclear to what extent it relates to theories of cognitive attention which inspired it: (Lindsay, 2020): . Here, we aim to provide a parsimonious description grounded in principles of probabilistic inference. This Bayesian perspective provides both a principled method for specifying prior beliefs and reasoning explicitly about the role of the attention variables. Further, understanding the fundamental computation permits us a unified description of different attention mechanisms in the literature. This proceeds in two parts.: First, we show that 'soft' attention mechanisms (e.g. selfattention, cross-attention, graph attention, which we call transformer attention herafter) can be understood probabilistically as taking an expectation over possible connectivity structures, providing an interesting link between softmax-based attention and marginal likelihood.: Second, we extend the uncertainty over connectivity to a bayesian setting which, in turn, provides a theoretical grounding for iterative attention mechanisms (slot-attention, perciever and block-slot attention): (Locatello et al., 2020;: Singh et al., 2022;: Jaegle et al., 2021): and Modern Continuous Hopfield Networks: (Ramsauer et al., 2021): .: Additionally, we apply iterative attention to Predictive Coding Networks, an influential theory in computational neuroscience, creating a new theoretical bridge between machine learning and cognitive science.: Attention(Q, K, V ) = p(E | Q, K) sof tmax( QW Q W T K K T  d k ) V = E p(E|Q,K) [V ]: A key observation is that the attention matrix can be seen as the posterior distribution over an adjacency structure, E, and the full mechanism as computing an expectation of the value function V (X) over the posterior beliefs about the possible relationships that exist between key and query.: This formalism provides an alternate Bayesian theoretical framing within which to understand attention models, which contrasts with the original framing in terms of database management systems and data retrieval, providing a unifying framework to describe different attention architectures. Describing their difference only in terms of their edge relationships supporting more effective analysis and development of new architectures. Additionally providing a principled understanding of the difference between hard and soft attention models.\n",
      "\n",
      "Contributions:  A unifying probabilistic framework for understanding attention mechanisms.:  We show self-attention and cross-attention can be seen as computing a marginal likelihood over possible network structures.:  We show that slot-attention, block-slot-attention and modern continuous hopfield networks can all be seen as collapsed variational inference, where the possible network structures form the collapsed variables.:  Provide a bridge to Bayesian conceptions of attention from computational neuroscience, through the lens of Predictive Coding Networks.:  Provide a framework for reasoning about hard attention, and efficient approximations to the attention mechanism.\n",
      "\n",
      "Related Work: Attention as bi-level optimisation Mapping feed-forward architecture to a minimisation step on a related energy function has been called unfolded optimisation: (Frecon et al., 2022): . Taking this perspective can lead to insights about the inductive biases involved for each architecture. It has been shown that the cross-attention mechanism can be viewed as an optimisation step on the energy function of a form of Hopfield Network: (Ramsauer et al., 2021): , providing a link between attention and associative memory. Whilst: (Yang et al., 2022): extend this view to account for self-attention. Our framework distinguishes hopfield attention, which does not allow an arbritary value matrix, from the standard attention mechanisms. Whilst there remains a strong theoretical connection, it places the Hopfield Energy as an instance of variational free energy, aligning more closely with iterative attention mechanisms such as slotattention.: Relationship to gaussian mixture model Previous works that have taken a probabilistic perspective on the attention mechanism note the connection to inference in a gaussian mixture model: (Gabbur et al., 2021;: Nguyen et al., 2022;: Ding et al., 2020): . Indeed: (Annabi et al., 2022): directly show the connection between the Hopfield energy and the variational free energy of a gaussian mixture model. Although gaussian mixture models, a special case of the framework we present here, are enough to explain cross attention they do not capture slot or self-attention. Further our framework allows us to extend the structural inductive biases beyond what can be expressed in a gaussian mixture model and capture the relationship to hard attention.: Latent alignment and hard attention Several attempts have been made to combine the benefits of soft (differentiability) and hard attention. Most approaches proceed by sampling, e.g., using the REINFORCE estimator: (Deng et al., 2018): or a topK approximation: (Shankar et al., 2018): . The one most similar to ours embeds the full forward-backward algorithm within a forward pass: (Kim et al., 2017): , our approach differs by offering a parsimonious description in terms of marginalisation over an implicit graphical model.: Collapsed Inference Collapsed variational inference has most notably been employed in topic modelling: (Teh et al., 2006): . To our knowledge, linking collapsed inference to attention in deep learning is completely novel.\n",
      "\n",
      "Transformer Attention\n",
      "\n",
      "ATTENTION AS EXPECTATION: We begin by demonstrating transformer attention is best seen as an expectation over latent variables. In the case of self and cross-attention, the expectation of a neural network with respect to possible adjacency structures.: Let x = (x 1 , .., x n ) be observed variables,  be some set of latent variables, and y a variable we need to predict. Given a latent variable model p(y, x, ) = p(y | x, )p(x, ), where p(y | x, ) is parameterised by some function v(y, x, ) e.g. a neural network.: Our goal is to find p(y | x), however  are unobserved so we calculate the marginal likelihood.: p(y | x) =  p( | x)v(y, x, ): Importantly, the softmax function is a natural representation for the posterior: p( | x) = p(x, )  p(x, ) p( | x) = sof tmax(ln p(x, )): Hence, transformer attention can be seen as weighting v(x, ) by the posterior distribution p( | x).: p(y | x) =  sof tmax(ln p(x, ))v(y, x, ) = E p(|x) [v(y, x, )]: (1): We claim (: 1: ) is exactly the equation underlying self and cross-attention. To make a more direct connection, we present the specific generative models corresponding to them. The latent variables  are identified as possible relationships, or edges, between each of the observed variables x (keys and queries).: A natural formalism for modelling these graphical relationships is Markov Random Fields.\n",
      "\n",
      "PAIRWISE MARKOV RANDOM FIELDS: Given a set of random variables X = (X v ) vV with probability distribution [p] and a graph G = (V, E). The variables form a pairwise Markov random field (MRF) with respect to G if the joint density function P (X = x) = p(x) factorises as follows: p(x) = 1 Z exp vV  v + eE  e: where Z is the partition function  v (x v ) and  e =  u,v (x u , x v ) are known as the node and edge potentials respectively 1 .: Beyond the typical set-up, we add a structural prior p(E) over the adjacency structure of the underlying graph.: p(x, E) = P (x | E)P (E) = 1 Z p(E) exp vV  v + eE  e: We briefly remark that (1) respects factorisation of [p] in the following sense; if the distribution admits a factorisation with respect to the latent variables p(x, ) = i f i (x,  i ) and v(x, ) = i v i (x,  i ) then (applying the linearity of expectation) we may write: E p(|x) [v(x, )] = i E p(i|x) [v i ]: (2): Permitting each factor to be marginalised independently.: In the case of an MRF, such a factorisation is natural. If the distibution over edges factorises into local distributions: p(E) = i p(E i ) (using independence properties of the MRF) we can write p(x, E) = 1 Z i f i (x, E i ) where each f i = P (E i ) exp vV  v eEi  e is itself an unnor- malised MRF.: To recover cross-attention and self-attention are such models with we need only specify a structural prior and potential functions.\n",
      "\n",
      "CROSS ATTENTION:  Key nodes K = (x 1 , .., x n ): 1 See: (Shah et al., 2021): for a precise definition.  Query nodes Q: = (x  1 , ..., x  m )  Structural prior p(E) = m i=1 p(E i ), where E i  U nif orm{(x 1 , x  i ), .., (x n , x  i )}: , such that each query node is uniformly likely to connect to each key node.:  Edge potentials (x j , x  i ) = x T i W T Q W K x j: , in effect measuring the similarity of x j and x  i under a certain transformation.:  Value function V i (K, Q, E i ) = W V x s(Ei): , a linear transformation applied to the node, x s(Ei) , the start of the edge E i .: Taking the posterior expectation in each of the factors defined in two (2) gives the standard cross-attention mechanism: E p(Ei|Q,K) [V i ] = j sof tmax j (x T i W T Q W K x j )W V x j E p(E|Q,K) [V ] = sof tmax(Q T W T Q Q K K)W V K 3.0.4. SELF ATTENTION  Nodes K = Q = (x 1 , .., x n )  Structural prior p(E) = n i=1 p(E  i ): , where E  i  U nif orm{(x 1 , x i ), .., (x n , x i )}, such that each node is uniformly likely to connect to every other node.:  Edge potentials (k j , k i ) = x T i W T Q W K x j , in effect measuring the similarity of x j and x  i under a certain transformation.\n",
      "\n",
      " Value function: V i (K, Q, E i ) = W V x s(Ei): , a linear transformation applied to the node, x s(Ei) , the start of the edge E i .: Again, taking the posterior expectation in each of the factors defined in two (2) gives the standard self-attention mechanism: E p(Ei|Q,K) [V i ] = j sof tmax j (x T i W T Q W K x j )W V x j E p(E|Q,K) [V ] = sof tmax(K T W T Q W K K)W V K\n",
      "\n",
      "Iterative Attention: We continue by extending attention to full Bayesian inference. In essence applying the attention trick, marginalisation of attention variables, to the variational free energy (a.k.a the ELBO).: Modern Continuous Hopfield Networks can be seen as a particular instance of this class of system, allowing us to reproduce the 'hopfield attention' updates of: (Ramsauer et al., 2021): within a probabilistic context. Under different structural priors we recover other iterative attention models; slot-attention: (Locatello et al., 2020): , block-slot attention: (Singh et al., 2022): and Perciever: (Jaegle et al., 2021): . Further, we showcase a specific advantage of bayesian attention, hard attention.\n",
      "\n",
      "COLLAPSED INFERENCE: We present a version of collapsed variational inference: (Teh et al., 2006): showing how this results in a bayesian attention mechanism. The term attention mechanism is apt due to the surprising similarity in form between the variational updates (6) and neural attention mechanism (1).: Our setting is the latent variable model p(x, z, ), where x are observed variables, and z, , are latent variables. Typically we wish to infer z given x.: Collapsed inference proceeds by marginalising out the extraneous latent variables  p(x, z): =  p(x, z, ): (3): We define a recognition density q(z)  N (z; ) and optimise the variational free energy with respect to the parameters, , of this distribution.: min  F (x, ) = E q [ln q  (z) -ln p(x, z)]: Under a typical Laplace approximation, we can write the variational free energy as F  -ln p(x, ): 2: . Substituting in (3) and taking the derivative with respect to the variational parameters yields,: F (x, ) = -ln  p(x, , ) F  = - 1  p(x, , )    p(x, , ): (4): Which connects bayesian attention with the standard attention (1). To clarify this, we employ the log-derivative trick, substituting p  = e ln p  and re-express (4) in two ways:: F  = -  sof tmax  (ln p(x, , ))   ln p(x, , ): (5): F  = E p(|x,) [-   ln p(x, , )]: (6): The first form reveals the softmax which is ubiquitous in all attention models. The second, suggests the variational update should be evaluated as the expectation of the typical variational gradient (the term within the square brackets) with respect to the posterior over the parameters represented by the random variable .: In other words, bayesian attention is exactly the nueral attention mechanism applied iteratively, where the value function is the variational free energy gradient. We derive updates for a general MRF before again recovering (iterative) attention models in the literature by specifying particular distributions.\n",
      "\n",
      "FREE ENERGY OF A MARGINALISED MRF: Recall the factorised MRF, p(E): = i p(E i ). p(x, E) = 1 Z i f i (x, E i ) with each f i = P (E i ) exp vV  v eEi  e .: Independence properties mean the marginalisation necessary for collapsed inference can be simplified: E p(x, E) = 1 Z i Ei f i (x, E i ): In an inference setting the nodes are partitioned into observed nodes, x, and latent nodes, z. The variational free energy (4) and the associated forms of it's derivative can be expressed: F (x, , ) = - i ln Ei f i (x, , E i ) F  j = - i Ei sof tmax(f i (x, , E i )) f i  j: Similar to hard attention approaches, the random variable E is an explicit alignment variable. However, unlike hard attention, we avoid inferring E explicitly using the collapsed inference approach outlined above.\n",
      "\n",
      "QUADRATIC POTENTIALS AND THE CONVEX CONCAVE PROCEDURE: We follow: (Ramsauer et al., 2021): in using the CCCP to derive a fixed point equation, which necessarily reduces the free energy.: Assuming the node potentials are quadratic (x i ) = -1 2 x 2 i and the edge potentials have the form (x i , x j ) = x i W x j .:  * j = i Ei sof tmax(g i (x, , E i )) g i  j (7): Where: g i = eEi  e .: By way of the CCCP: (Yuille & Rangarajan, 2001): , this fixed point equation has the property F (x,  * j , )  F (x,  j , ) with equality if and only if  * j is a stationary point of F .: We follow the 3 in specifying specific structural priors and potential functions to recover different iterative attention mechanisms.\n",
      "\n",
      "HOPFIELD-STYLE CROSS ATTENTION: Let the observed x = (x 1 , .., x n ) and latent nodes z = (z 1 , .., z m ) have the following structural prior p(E) = m i=1 p(E i ), where E i  U nif orm{(x 1 , z i ), .., (x n , z i )}. And define edge potentials (x j , z i ) = z i Q T Kx j , Application of (: 7: ):  * i = j sof tmax j ( i W T Q W K x j )W T Q W K x j: When  i is initialised to some query  the system: (Ramsauer et al., 2021): the fixed point update is given by:  * i () = E p(Ei|x,) [W T Q W K x t(Ei) ]. When the patterns x are well separated,  * i ()  W T Q W K x j: , where W T Q W K x j is the closest vector and hence can be used as an associative memory. 4.0.5. SLOT ATTENTION Slot attention: (Locatello et al., 2020): is an object centric learning module built on top of an iterative attention mechanism. Here we show this is a simple adjustment of the prior beliefs on our edge set.: With the same set of nodes and potentials, replace the prior over edges with p(E) = n j=1 p(E j ), E j  U nif orm{(x j , z 1 ), .., (x j , z m )}:  * i = j sof tmax i ( i Q T Kx j )Q T Kx j: Whilst the original slot attention employed an RNN to aid the basic update shown here, the important feature is that the softmax is taken over the 'slots', . This forces competition between slots to account for the observed variables, forcing object centric representations. For example, if the observed variables x are image patches, the slots are forced to cluster similar patches together in order increase the overall likelihood of said patches. The word cluster is accurate, in fact there is an exact equivalence between this mechanism and a step of EM on a gaussian mixture model. 4.0.6. BLOCK SLOT ATTENTION: (Singh et al., 2022): suggest combining an associative memory ability with an object-centric slot-like ability and provide an iterative scheme for doing so, alternating between slot-attention and hopfield updates.   U nif orm{(x j , z 1 ), .., (x j , z m )}, k  U nif orm{(z 1 , m k ), .., (z m , m k )}, with edge potentials between X and Z given by (x j , z i ) = z i Q T Kx j and between Z and M , (z i , m k ) = z i  m k applying (7) gives:  * i = j sof tmax i ( i Q T Kx j )Q T Kx j + k sof tmax k ( i  m k )m k: In the original block-slot attention each slot z i is broken into blocks, where each block can access block-specific memories i.e. z k } kl . Allowing objects to be represented by slots which in turn disentangle features of each object in different blocks. We presented a single block version above, however it is easy to see that the update extends to the multiple block version applying (7) gives:  * i = j sof tmax i ( i Q T Kx j )Q T Kx j + k,b sof tmax k ( (b) i  m (b) k )m (b) k\n",
      "\n",
      "Predictive Coding Networks: Predictive Coding Networks (PCN) have emerged as an influential theory in computational neuroscience: (Rao & Ballard, 1999;: Friston & Kiebel, 2009;: Buckley et al., 2017): . Building on theories of perception as inference and the Bayesian brain, PCNs perform approximate Bayesian inference by minimising the variational free energy which is manifested in the minimisation of local prediction errors. The continuous time dynamics at an individual neuron are given by: F  i = -  - k    +  + k    w : Where  are prediction errors, w represent synaptic strength and k are node specific precisions representing uncertainty in the generative model: (Millidge et al., 2022): .: A natural extension is to apply collapsed inference over the set of incoming and out going connection, i.e. a locally factorised prior over possible connectivity. In the notation of the previous section, we have an MRF with a hierarchical structure Z = {Z (0) , ..., Z (l) , ..., Z (N ) } where the prior on edges factorises into layerwise p(E: (l) ) = {(z i , z j ) : (z i , z j )  Z (l-1)  Z (l) } and potential func- tions (z i , z j ) =  2 i,j = k j (z j -w i,j z i ) 2 . F  i = -  - sof tmax(-  2 )k    +  + sof tmax(-  2 )k    w : The resulting dynamics induce a \"normalisation\" across prediction errors received by a neuron through the softmax function. This dovetails nicely with theories of attention as normalisation in psychology and neuroscience. In contrast previous predictive coding based theories of attention have focused on the precision terms, k, due to their ability to up and down regulate the impact of prediction errors: (Feldman & Friston, 2010: ). Here we see the softmax term can also perform this regulation, while also exhibiting the fast winner-takes-all dynamics that are associated with cognitive attention.\n",
      "\n",
      "Discussion: In this section we will briefly discuss what can be gained from looking at the attention mechanism as a problem of inference.\n",
      "\n",
      "HARD ATTENTION: Recall (1) neural attention may be viewed as calculating an expectation over latent variables E p(|x): [v(x, ): ]. Here the mechanism is 'soft' because we weight multiple possibilities of attention variable . Hard attention, on the other hand, proceeds with a single sample from p( | x). It has been argued this is more biological, more interpretable and has lower computational complexity. Previously the inferior performance of hard-attention has been attributed to it's hard to train, stochastic nature. However, our framing of soft attention as exact marginalisation offers an alternate explanation. Stochastic approximations (hard attention) will always suffer compared with exact marginalisation (soft attention). Further our framework provides a method for seamlessly interchanging hard and soft-attention. Since the distribution p( | x) a the categorical distribution, at any point (during training or inference) it is possible to implement hard attention by taking a single sample  * from p( | x) yielding v(x,  * ).: There are two issues with this approach to collapsing the attention distribution. First, the single sample will collapse any uncertainty, secondly calculation of p( | x), in order to sample, still incurs a quadratic penalty O(n 2 ). However we can employ tools from probability theory to help us analyse the cost of sampling, and linear approximations to the attention distribution.\n",
      "\n",
      "EFFICIENT TRANSFORMERS: Consider some distribution q attempting to approximate p( | x) we can quantify the information loss with the relative entropy L[p, q] D KL [q() || p( | x)] = H[q] + E q [p( | x)]: In the hard attention approximation a single sample from p is used as an approximation L[p, q] = -ln p( * | x) and perhaps intuitively E[L] = H[p] i.e. hard attention is a good approximation when the attention distribution is low-entropy which can be controlled by the temperature parameter (Appendix ??).: Many of the efficient alternatives to attention, such as lowrank and linear approximations, can be cast as approximating p( | x) with q( | x) where calculating q is less expensive than exact marginalisation. Estimating L could be used to quantify the relative information loss when using these alternatives. Another direction taken to reduce computational complexity of the attention mechanism is sparsification the attention matrix, which in our framework reduces to adjustments to the prior over edges (Appendix ??).\n",
      "\n",
      "NEW DESIGNS: The main difference between the description presented and previous probabilistic descriptions is to view soft attention as a principled, exact, probabilistic calculation, with respect to an implicit probabilistic model, as opposed to an impoverished approximation. This leads to possibility of designing new attention mechanisms by altering the distribution that the mechanism marginalises over, either by adjusting the structural prior, or the potential functions. We hope this will enable new architectures to be designed in a principled manner.\n",
      "===================\n",
      "Timeseries analytics is of great importance in many real-world applications. Recently, the Transformer model, popular in natural language processing, has been leveraged to learn high quality feature embeddings from timeseries, core to the performance of various timeseries analytics tasks. However, the quadratic time and space complexities limit Transformers' scalability, especially for long timeseries. To address these issues, we develop a timeseries analytics tool, RITA, which uses a novel attention mechanism, named group attention, to address this scalability issue. Group attention dynamically clusters the objects based on their similarity into a small number of groups and approximately computes the attention at the coarse group granularity. It thus significantly reduces the time and space complexity, yet provides a theoretical guarantee on the quality of the computed attention. The dynamic scheduler of RITA continuously adapts the number of groups and the batch size in the training process, ensuring group attention always uses the fewest groups needed to meet the approximation quality requirement. Extensive experiments on various timeseries datasets and analytics tasks demonstrate that RITA outperforms the state-of-the-art in accuracy and is significantly faster -with speedups of up to 63X.\n",
      "\n",
      "INTRODUCTION: Motivation. Many data driven applications involve processing massive timeseries data, including IoT: [11]: , medical AI: [14]: , stock market: [27]: , and so on. As such, there is a great need for timeseries analytics, such as forecasting: [8]: , classification: [20]: , clustering: [31]: , similarity search: [39]: , and anomaly detection: [50]: , with applications ranging from automatically diagnosing diseases: [5]: , recognizing human activities: [29]: , to stopping financial fraud: [59]: .: Effective feature extraction: [40]: lies at the core of almost all these timeseries analytics tasks. Recently researchers: [61]: have started leveraging the self-supervised pre-training methodology of Transformers: [4,: 16,: 52]: , which have proven remarkably successful in natural language processing (NLP), to automatically learn high quality feature embeddings from timeseries. In NLP, self-supervised pre-training exploits the sequential patterns (correlations) among the words in sentences to produce contextualized feature embeddings. Timeseries bear similarity to natural language, because in timeseries data the sequential order among the values (stock price, volume, etc.) over time matters. That is, each value is highly correlated with other values observed before or after it. Therefore, * Corresponding Author pre-training a Transformer model which takes the correlations among different observations into account is a natural idea to learn feature embeddings from timeseries. Indeed, the experiments in: [61]: confirm that Transformer-based methods outperform traditional timeseries analytics techniques.: However, existing work: [61]: that directly applies Transformers to learn features from timeseries data have been shown not to be scalable to long timeseries: [30]: . The idea of self-attention: [52]: is central to pre-training methods in NLP: It computes pairwise correlations among different semantic units in a sequence (in NLP, a sentence); as such, it has quadratic time and space complexity in the length of the input sequence. Such an approach places limits on the model's scalability, especially when handling large sequences, which are common in real-world timeseries applications such as IoT, medical AI, and finance: [6,: 34,: 62]: . Predictions about timeseries may need to look at months or years of historical data to make accurate predictions, spanning hundreds of thousands of samples. As an example, in collaboration with a research hospital we have been developing a seizure classifier that automatically detects seizures based on EEG signals (timeseries) collected during the clinical observation of patients. As seizures last only a few seconds, we chunk long EEG data into many 2 second segments and detect seizures at a segment level. However, the classification of a particular segment depends on up to 12 hours of prior signal to determine if one 2 second segment indicates seizure or not, because seizure diagnosis needs to consider long-term trends in the EEG data: [6]: . The number of segments in 12 hours is more than 21k. This is far larger than the number of semantic units the typical NLP tasks expect. For example, BERT: [16]: limits the number of units to 512 and even massive models like GPT-3: [4]: limit the number of units to 2048.: Although in NLP some lower-complexity methods have been proposed to approximately compute self-attention: [10,: 26,: 54]: , their performance degrades dramatically when used on timeseries, due to the gap between natural language and timeseries, as we will show in our experiments. Proposed Approach. To tackle the aforementioned problem, we develop RITA, a Transformer-based timeseries analytics tool, which uses a novel attention mechanism, called group attention, to scale to long timeseries.: Leveraging the periodicity of timeseries, RITA chunks the input timeseries into segments and dynamically clusters the segments into a small number (denoted as  ) of groups. Segments in the same group possess similar feature embeddings during the current training iteration, thus enabling them to approximately share the computation of attention. As the timeseries increases in length, more sharing opportunities become available. RITA then computes the self-attention at a group level and produces a compressed group attention matrix. In this way, group attention eliminates both computation and memory bottlenecks in Transformer-style models and thus more scalable to long timeseries.: However, making this idea effective and efficient in Transformer architectures is challenging for several reasons::  Efficiently Producing High Quality Feature Embeddings. Although RITA computes the attention matrix at a group level, to preserve the quality of the feature embeddings, it still has to produce different embeddings for different segments. This is because even if some segments share the attention score temporally, it does not mean they should have the same feature embedding. However, using the group attention matrix, the existing self-attention mechanism will only produce a single feature vector for each group. A naive solution would be to restore the original attention matrix from the group attention matrix. However, in this case we again get an attention matrix with quadratic space complexity. Because GPUs have limited memory, GPU memory will remain a bottleneck in group attention.:  The Number of Groups N. In RITA, the number of groups  is a crucial factor that balances the speed up and the quality of attention approximation. A small  will lead to a large speedup, but the approximation errors can also be significant. On the other hand, although a large  tends to produce high-quality approximations, it inevitably slows down the training process. Therefore, an appropriate  is essential to the performance of group attention. However,  depends on the distributional properties of the dataset. Furthermore, like the classical transformer models, RITA stacks multiple attention layers to produce better embeddings. Ideally, different layers should also use different values of  . In addition, during the model training phrase, group attention should use different values of  at different iterations to adapt to the varying feature embeddings. This makes manually setting appropriate  almost impossible.:  Batch Size. Moreover, as we want to dynamically adjust  during training, a fixed batch size is sub-optimal: as  decreases, the memory usage of a single sample decreases. This allows a larger batch size which is beneficial, because: (1) it makes full use of GPU memory; (2) high-parallelism across the samples in a big batch brings better performance. Our experimental study shows that doubling the batch size reduces the training time by 30%, while still preserving the quality of the model. Thus, RITA should dynamically adjust batch size as  changes.: To address the above problems, we first propose an embedding aggregation strategy and a customized group softmax function to replace the classical softmax function: [52]: . Together they ensure RITA is able to directly use the compressed attention matrix to produce different feature embeddings for different segments. We theoretically show the embeddings RITA produces in this way are identical to those produced by first re-storing the original large attention matrix. Thus RITA is able to produce high quality embeddings without introducing extra overhead. Further, we design a GPU friendly algorithm to group the segments in parallel, effectively minimizing the grouping cost.   Second, we design an adaptive scheduler which dynamically decides an appropriate  for each group attention layer during the training process. It starts with a large  and iteratively merges groups that are similar to each other. Guided by an error bound on the approximated self-attention that users can tolerate, it automatically determines if two groups are mergeable, performing merging efficiently in a GPU-friendly way.\n",
      "\n",
      "RITA Encoder\n",
      "\n",
      "Scale & Input: Moreover, we propose a learning-based method to model the correlation between the number of groups  and the batch size . This model is used to predict  for a given  when training RITA. Specifically, we first sample some  values in a reasonable range. For each sampled  , we find a batch size that consumes up to a certain percentage of GPU memory in a cost-efficient way. Using a small set of mathematical functions as a prior, RITA learns a model with only a few <N, B> pairs as ground truth labels.: Our experiments on public timeseries benchmarks and the MGH EEG data: [6]: confirm that RITA outperforms state-of-the-art methods in accuracy on various timeseries analytics tasks, while our group attention mechanism achieves a 63X speedup with much less memory required, compared to existing self-attention mechanisms: [10,: 52,: 54]: . Contributions. The key contributions of this work include::  Our group attention mechanism leverages the periodicity of timeseries, reducing the time and space complexity of the selfattention mechanism with accuracy guarantees, allowing RITA to scale to long timeseries data.:  Guided by an approximation error bound, our adaptive scheduler dynamically adapts the number of groups and the batch size to the distribution properties of the evolving feature embeddings, making group attention efficient and easily tunable.:  We conduct experiments on various datasets and different analytics tasks, demonstrating that RITA is 4 to 63 times faster than the state-of-the-art while achieving better accuracy when handling long timeseries (length  2000).\n",
      "\n",
      "BACKGROUND: We provide some background on the canonical self-attention module in the Transformer: [52]: . A self-attention module takes  hidden embedding vectors   R  *   as input, then projects them to queries (), keys () and values ( ) and performs Scaled-dot Product Attention, which given input hidden state  , is computed by::  =   ,  =   ,  =    =  =    (      ): (1): Where Given a matrix   R  *  , the softmax function normalizes  to ensure the sum of each row equals to 1, as shown below.:    R   *   ,   R   *   ,   R   *:    ( , ) =  ( , ) -1 =0  ( , ): (2): Note the attention matrix A is an  matrix, where  represents the number of elements in the input sequence (e.g. words in NLP).\n",
      "\n",
      "RITA OVERVIEW: Given a collection of unlabeled timeseries, RITA first pre-trains a Transformer-style model to produce high quality feature embeddings for timeseries data. This pre-trained model is then used to support various downstream tasks, similar to BERT: [16]: . Next, we overview the model architecture of RITA. We show how RITA supports various downstream tasks in Appendix A.7.: As shown in Fig.: 1: , RITA is consist of two components: (1) Timeaware Convolution Layer (2) RITA Encoder. Time-aware Convolution Layer fills the gap between timeseries and natural language. Despite their high-level similarity, there is a big gap between timeseries and natural language. First, in natural language each word, as a discrete semantic unit, has an independent meaning, while each element in a timeseries is a continuous, numerical value and does not necessarily constitute an independent event. Furthermore, the input sequences are single-channeled in NLP, but often multi-channeled in timeseries (i.e., sensor data often consists of several related channels).: RITA leverages the classical convolution: [28]: strategy to solve this problem. Convolution is widely used to capture the local structures of an image. We use convolution to chunk one input timeseries into a sequence of windows and learn the local structure of each window, similar to the discrete semantic units in natural language. It also discovers the correlations across different channels, thus naturally solving the multi-channel problem.: More specifically, treating a multi-variate timeseries of length  and with  variables as an n  m matrix  , RITA uses  convolution kernels to chunk  into n windows and produce one d-dimensional embedding per window using the convolution operation: [28]: . Each convolution kernel corresponds to a w  m matrix, where  defines the number of timestamps that each convolution kernel covers, identical to the window size in sliding window. RITA Encoder functions as Transformer Encoder as described in the original Transformer work: [52]: . It takes the embeddings of  semantic units  1 ,  2 , ...,   (     ) as input (e.g. embeddings of  windows for a timeseries), then models the correlations between the semantic units and outputs  1 , ...,   (     ) as the contextaware embedding of each unit.: What makes RITA Encoder different from Transformer Encoder is that: at the core of Transformer Encoder lies self-attention mechanism which incurs a  ( 2 ) time complexity and memory usage. This quadratic cost becomes prohibitive for long timeseries and limits the scalablity of Transformer-based models. To make the attention computation efficient yet high-quality, we replace the canonical self-attention with our proposed group attention. Self-supervised Pretraining. Inspired by the \"cloze text\" pretraining task in NLP, we designed a mask-and-predict task as the pretraining task for our model. The timeseries is randomly masked and the model should recover the masked values based on corresponding contextual information.: To be specific, we generate masks on time-stamps, with a mask rate . The timeseries is scaled to be non-negative and the values across all the channels on the masked timestamps are set to be -1, an impossible value on normal timestamps. Then the masked timeseries is fed into RITA and the output representation is translated to the recovered timeseries by a Transpose Convolution layer.\n",
      "\n",
      "GROUP ATTENTION MECHANISM: Group attention, a novel and efficient approximate attention mechanism, addresses the performance bottleneck of self-attention in the vanilla Transformer. In this section, we first introduce the framework of group attention and then theoretically establish the bound of its approximation error.\n",
      "\n",
      "The Idea of Group Attention: As periodicity is a natural property of timeseries: [56]: , similar windows frequently occur. Similar windows result in similar queries/keys for attention computation, bringing opportunities for saving computation.: As discussed in Sec. 2,    , the attention score of window  onto window , is determined by the inner product between the query vector of window  and the key vector of window , that is,      . Given another window , if window  has the similar key vector to window , that is,:      , then            . In other words,       when      .: This observation inspires our group attention mechanism. That is, we group the windows by their similarity in keys. Assuming all windows in the same group have the same attention score onto another window , we then only compute the attention once by using one single key to represent this group, for example the centroid of the group of keys. This thus saves significant computation cost.: Better yet, after grouping  windows into  groups, group attention compresses the attention matrix from an  matrix to an  matrix. Because  (number of groups) tends to be much smaller than  (number of windows) due to the periodicity of timeseries, group attention consumes much less memory than the original self-attention mechanism, successfully eliminating the memory bottleneck. Note that it also doesn't hurt quality all that much, as confirmed in our experiments (Sec. 6.2). We now discuss how to efficiently compute the output feature embeddings using the small compressed group attention matrix.\n",
      "\n",
      "Problem: Producing Embeddings w/ Group Attention Matrix: As described in the Background, once we have acquired the attention matrix , canonical self-attention computes the output embedding  as O = AV . Because  is an    matrix and  is an     matrix, the matrix product operation still produces an     matrix . That is, it produces a   dimensional feature vector for each window. However, our group attention will produce an    attention matrix  , where  corresponds to the number of groups. In this case the matrix product will produce a    matrix . That is, it produces a feature vector for each group. However, our goal is to produce different embeddings for different windows, because even if some windows share the attention score temporally, it does not mean they should have the same feature embedding. A Naive Solution. A naive solution would be to restore the full attention matrix  from the group attention matrix . For example, given one group composed of   and   , we map its group attention vector in  into two rows that correspond to   and   in . However, in this case we again get a    attention matrix; and GPU memory remains a bottleneck in group attention.\n",
      "\n",
      "Solution: Embedding Aggregation and Group SoftMax: Using an embedding aggregation operation and a group softmax function, RITA produces  embeddings without restoring the full attention matrix. Fig.: 2: shows the workflow of group attention. Embedding Aggregation. The idea is inspired by the observation on the matrix product operation O = AV conducted on the fully restored attention matrix .: Given an element  , of  corresponding to the   dimension of   's feature vector,  , =     , where vector a i  R n denotes the   row of the attention matrix  and vector v j  R n denotes the   dimension of all the  feature vectors. Given: a i =< a 1 i , a 2 i ,    , a n i > and v j =< v 1 j , v 2 j ,    , v n j >,  , = n k=1 a k i v k j .: As an example, assume  1 and  2 belong to the same group:  1 . Then  1  =  2  =  1  , where  1    corresponds to the attention of group  1 onto   . Therefore,  1   1  +  2   2  =  1  ( 1  +  2  ).: As an immediate generalization of the above analysis, if we aggregate up the windows that belong to the same group and convert the n-dimensional feature vector   into a  -dimensional group feature vector   beforehand, we could directly use the group attention vector   and the group feature vector   to compute  , .: Using embedding aggregation, RITA is able to produce the feature embedding  that is identical to the embedding  produced by using the full attention matrix  and the embedding matrix  . Group Softmax Function. In canonical self-attention the atten-: tion matrix  is computed as  = SoftMax ( QK T  d k: ). To compute , we have to first compute   (denoted as ) which is an    matrix. Then normalizing the  matrix with softmax produces the attention matrix .: Group attention follows the same procedure. But after grouping keys into ,    produces an    matrix . Due to the nonlinearity of the softmax function, applying softmax directly on  will result in a group attention matrix  from which we are not able to recover a full attention matrix that is identical to first restoring  to  and then applying softmax on . The  matrix produced by the latter is desirable, as we want to approximate the original attention matrix as accurately as possible. However, restoring the small     matrix is not memory efficient, as it will end up with a full    matrix .: To solve the above problems, we introduce a new group softmax function to replace the original softmax function (Eq. 2).\n",
      "\n",
      "  : (  , ) =  ( , )  -1 =0    ( , ): (3): In Eq. 3,   represents the number of windows that Group   contains. Compared to the original softmax, our group softmax considers each group   as   elements and counts it   times when summing up the exponential of each group's  , . In this way, the group softmax function operating on the small  matrix will produce exactly the same result to the softmax function operating on the full  matrix. Theoretical Guarantee. In Appendix A.4, we prove that the group softmax function and the embedding aggregation operation produce the same output feature embedding with the naive method that has to first restore the big full attention matrix.: We show an efficient implementation of the embedding aggregation operation and group softmax function in Appendix A.2, Alg. 1. Time Complexity. The time complexity of Alg. 1 is  () and the space complexity is  ( ), while the time and space complexity of the original self-attention mechanism are  ( 2 ) and  ( 2 ).\n",
      "\n",
      "Error Bound: Group attention produces a group attention matrix  which approximates the attention matrix  produced by the classical self-attention with a bounded error, as shown in Lemma 1.: Lemma 1. Let  be the radius of the ball where all key vectors live;   be the representative of the group that contains key   . Let  denote the full attention matrix restored from . Suppose the distance between   and :  (|| k  -k  ||) satisfies: || k  -k  ||  d. Then   > 1, if d  ln( ) 2R , 1   A i,j A i,j  : Lemma 1 shows that the error bound  of the group attention is determined by the distance . As discussed in Sec. 5.1, it inspires us to design a strategy to dynamically determine the number of groups  -the most critical parameter of group attention. Please refer to Appendix A.5 for the proof.\n",
      "\n",
      "GPU Friendly Grouping Method: In this section, we discuss the implementation of a grouping method. To make group attention efficient and effective, the grouping method has to satisfy the following requirements:: (1) Tight distance bound: to ensure the approximation quality, the distance between each key and its group representative should be minimized according to Lemma 1.: (2) Lightweight: to ensure the performance gain, the grouping method must be lightweight, at worst not exceeding the complexity of group attention itself ( ()).: (3) GPU friendly: to take advantage of GPUs, we prefer a grouping method that mainly consists of matrix operations, which can be efficiently executed on a GPU.: To satisfy the above requirements, after thorough investigation on various clustering algorithms, we design a GPU friendly Kmeans: [35]: as the grouping method.: First, K-means minimizes the overall distance between any object and its cluster center, hence naturally satisfying Requirement 1.: Second, given  centers, in each iteration the time and space complexity of K-means is  ( ). Usually, the iteration goes until convergence. However, we observe that rather than seeking a perfect K-means clustering, training a few iterations is sufficient to get a good grouping for group attention, because typically the later iterations only slightly update the clustering and group attention is robust to such imperfection.: Third, we design a GPU-friendly implementation of K-means. The performance bottleneck of K-means comes from the distance computation between each vector and its center, that is,: |v i -c j | =  (v i -c j ) 2 , i  [1, n], j  [1, N ]. The performance bot- tleneck is   -  .: We instead use a different formulation: |  -:   | = |v i -c j | =  |v i | 2 + |c j | 2 -2v i  c j , i  [1, n], j  [1, N ]: . This is because in this formulation, the performance bottleneck is      , which could be implemented as a matrix product operation. Although the complexity of the two formulations is the same, in GPUs matrix product is much more efficient than pairwise difference.\n",
      "\n",
      "ADAPTIVE SCHEDULER: Next, we present the adaptive scheduler of RITA which addresses the challenges of determining an appropriate number of groups  and accordingly the batch size , as described in Introduction.: Using a dynamic scheduling method we propose, the scheduler automatically determines and adjusts  and  based on the distributional properties of the feature embeddings produced over the iterative training process, while guaranteed to produce high quality attention approximation that meets the requirement of users. In Sec. 5.1 we show how RITA automatically determines  . Then we introduce in Sec. 5.2 the learning-based method which given an  , immediately predicts a good batch size.\n",
      "\n",
      "Dynamically Determining the Number of Groups N: Without loss of generality, we use one group attention module as an example to show how RITA automatically gets an appropriate  . The adaptive scheduler of RITA starts with a large  and decreases it dynamically. This is because in the training process of RITA, the feature embeddings produced epoch by epoch tend to get stabler and stabler and gradually converge, thus no need to increase  . RITA reduces the number of groups by merging similar groups. Intuitively, given two groups, we could measure their similarity based on the distance of their centers. If the distance between their centers is smaller than a distance threshold, then the two groups could be merged. However, setting an appropriate distance threshold seems hard -as difficult as setting an appropriate  .: To solve this problem, RITA leverages the error bound of group attention introduced in Sec.: 4:      |   -   | + | -   |  , ,   [1, ]: (4): merging them into one cluster still meets the error bound .: Please refer to Appendix A.6 for the proof. Finding the Mergable Clusters. We formulate the problem of finding mergeable clusters using graph theory:: (1) each cluster is a node in the graph;: (2) if   and   satisfy::     |  -  |+| -  |  , and     |  -  |+| -  |  : there is an undirected edge between   and   ; In this scenario, finding the maximum number of mergeable clusters is equivalent to finding the minimal clique cover in the corresponding graph, which is an NP-hard problem: [24]: . Such heavy computation overhead is not acceptable for RITA. We thus offer a simplified solution:: (1) Halve the clusters into two sets  1 ,  2 ;: (2) If     1 and     2 satisfy::     |  -  | + | -  |  ,     |  -  | + | -  |   2 (: 5: ):   is marked.: (3) Decrease the number of clusters by counting the masks in  2 . In this solution, clusters in  1 can be regarded as transfer nodes. If (5) holds for: (    1 ,   1   2 ) and (    1 ,   2   2 ): , respectively, we have,:     1 |  1 -  2 | + | -  1 |      1 |  1 -  | + |  -  2 | + | -  1 |      1 |  1 -  | + |  -  2 | + | -  1 | + | -  2 |   (6): Thus (4) holds when merging several clusters in  2 with one cluster in  1 . As a result, we can greedily merge clusters in  2 , as illustrated in step: (3): .: Assume the number of clusters decreases by  after merging, we apply a momentum update: [42]: on the number of clusters  , as is commonly used in machine learning to smooth the changing of  and avoid sample selection bias. To be specific:   =  ( -) + (1 -) , where  is a hyper-parameter for momentum.\n",
      "\n",
      "Dynamically Determining the Batch Size: Because of the dynamic grouping operation, the computational graph in deep learning training: [1]: varies from sample to sample. As a result, it is impossible to precisely compute a batch's GPU memory usage without indeed feeding it into the model. To overcome this problem, RITA learns a batch size prediction function offline; then at the RITA training time, given a number of groups  , RITA uses this function to predict a proper batch size.: When the model architecture and hardware are fixed, the batch size depends on the length of the timeseries  and the average group number among all attention module  . So RITA samples several (  ,   ) pairs and estimate a proper batch size for each pair.: More specifically, given a user-defined timeseries maximal length   , we randomly sample integral points (  ,   ) from plane {1      , 1    }. Then we use a binary search based algorithm to find the maximal batch size   that consumes less than 90% available GPU memory, aiming to avoid wasting GPU memory and the risks of out of memory (OOM).: Treating these pairs as ground truth labels, we use function fitting: [18]: to learn the batch size predicting function B = f (L, N ), where B is a function of two variables  and  . Learning the Prediction Function. We apply curve fit from SciPy: [53]: as the function fitting tool to fit the two-variable function:   =  (  ,   ) on plane {1      , 1    }.: We observe that applying one function to the whole plane incurs a huge estimation error. So we develop a dynamic-programming (DP) method to divide the plane into several sub-planes and apply a distinct function to each sub-plane respectively. It is optimal in minimizing the total estimation error on all sub-planes With the learned prediction function  , we can estimate a proper batch size for any (,  ) during training, even if it is not seen in the sampled (  ,   ) pairs. The Algorithms and Optimality Proof. Please refer to Appendix A.3 for the pseudo code of the binary search-based algorithm and the description of the DP method for plane-division and the proof for its optimality.\n",
      "\n",
      "EVALUATION: Our experimental study focuses on the following questions:: 1. Effectiveness and efficiency of RITA: How does RITA compare with other Transformer-based methods and traditional timeseries representation learning methods in accuracy and efficiency?: 2. Ablation Study: How do the key techniques of RITA work?\n",
      "\n",
      "Experimental Setup: Datasets. We evaluate RITA on classification and imputation tasks using 5 multi-variate and 3 uni-variate timeseries datasets.:  WISDM: [55]: is a popular multivariate timeseries dataset generated from the accelerometer in the mobile phone. The subjects performed 18 daily activities (e.g. walking, jogging). The dataset was collected from 51 subjects and the sampling rate is 20 Hz.:  HHAR dataset: [46]: contains sensing data of accelerometer collected from 9 users performing 5 activities with 12 different smartphones (varying in sampling rate). This increases the complexity of the task and thus can test the model's robustness.:  RWHAR RealWorld HAR dataset: [48]: covers 15 subjects performing 8 locomotion-style activities. Each subject wears the sensors for approximately ten minutes. The sampling rate is 50 Hz.:  ECG dataset: [34]: consists of 10,000 EEG recordings for arrhythmia classification. Each recording has an uncertain length ranging from 6 to 60 seconds sampled at 500 Hz. The ECG recordings correspond to 9 types of heart problems such as atrial fibrillation (AF) and premature atrial contraction (PAC), etc.:  MGH: [6]: is a EEG dataset collected by Mass. General Hospital. Each timeseries corresponds to the EEG data observed from one patient during their stay in ICU for a couple of days. The EEG monitoring produced data with 20 channels. The sampling rate is 200 HZ. So it produces very long timeseries.:  WISDM*/HHAR*/RWHAR* are three uni-variate datasets derived by picking one channel from WISDM/HHAR/RWHAR. Training/Validation Data Generation. We apply a sliding window on the raw timeseries to get training/validation samples. The size of the sliding window is set as 200 on small datasets (WISDM, HHAR, RWHAR), 2000 on medium size dataset (ECG), and 10,000 on the large dataset (MGH). Table: 1: shows the statics of the generated datasets. They are randomly split into training/validation set in a proportion of 0.9/0.1. In \"pretraining + few-label finetuning\" scenario, we use 100 labeled data per class for finetuning. We guarantee that training set does not overlap with the validation set. To evaluate our group attention (referred to as Group Attn.), we develop three baselines by replacing the group attention component in RITA with the classic vanilla Self-Attention: [52]: (referred to as Vanilla) and two SOTA methods that reduce the complexity of self-attention by approximation in NLP, namely, Performer: [10]: (referred to as Performer) and Linformer: [54]: (referred to as Linformer). Similar to our proposed Group Attn., Vanilla, Performer, Linformer all use RITA's time-aware convolution operation (Sec. 3) to turn timeseries segments into input feature vectors. We also compare Group Attn. against GRAIL: [40]: , which is the SOTA of the non-deep learning methods for timeseries representation learning. GRAIL supports classification tasks by feeding the learned representations into a Support-Vector Machine: [12]: or K-Nearest Neighbor: [17]: classifier. Note GRAIL only targets uni-variate timeseries and cannot support imputation tasks. Methodology. We mainly focus on two downstream tasks:: (1) Classification. First, we train Group Attn. and the baselines with full labels from scratch to test the effectiveness of RITA framework and the approximation quality of our group attention.: Second, to measure the effectiveness of self-supervised pretraining, we evaluate the accuracy of training on few labeled timeseries with/without pretraining on large scales of unlabeled timeseries. To be specific, we split the training set into a pretraining set and a finetuning set, with very few data in the latter (100 labeled samples per class in our experiment). We train the model on the cloze pretraining task with a mask rate  = 0.2. Then we train two classification models using the finetuning set, either based on the pretrained version or from scratch. We repeat the experiment 5 times with random data splits and report the median accuracy.: (2) Imputation. We run the imputation task on the datasets used in classification as well as the large unlabeled MGH dataset, and measure the mean square error and absolute imputation error. To get timeseries with missing values, we randomly mask the values with an expected mask rate of  = 0.2. The masked values are replaced with a special value.: Finally, to evaluate Group Attn. 's benefit on efficiency, the total time of forward computation, backward propagation, and grouping are measured for all methods in all the experiments.: To save space, we only report the average training time per epoch here and refer readers to Appendix A.8 for the inference time.: We first compare against the Transformer-based methods on multi-variate datasets (sec. 6.2, 6.3), then compare against the nondeep learning method GRAIL on uni-variate datasets (sec. 6.4). Configuration. Please refer to Appendix A.1 for the experiment configuration and hyper-parameter settings.\n",
      "\n",
      "Effectiveness: Transformer-Based Methods: We first evaluate the quality of the models trained with full labels from scratch. We then show how the pretraining of RITA increases the accuracy of the downstream tasks.\n",
      "\n",
      "full-label training (Multi-variate classification): Results shown in Figure: 3: (a) get us the following observations:: (1) RITA's advantage over TST. On all four datasets for the classification tasks, Group Attn. and the other three baselines that use RITA architecture (Vanilla, Performer, and Linformer) outperform TST. In particular, Group Attn. outperforms TST by 49 percentage points on the ECG dataset (88.48% vs 39.93%) with long timeseries. Two deficiencies in TST may cause its poor performance on the long timeseries. Firstly, TST concatenates the output embedding vector of each time stamp, then uses a linear classifier to do classification on the concatenated vector. When the timeseries is long, the linear classifier has so many parameters that it tends to overfit easily. Secondly, TST replaces Layer Normalization in vanilla Transformer with Batch Normalization. When the timeseries is long, it can only accommodate a small number of timeseries in each batch, leading to bias in Batch Normalization.: (2) Group-attention's advantage over other attention mechanisms. Group Attn. is better than Performer and Linformer on 3 out of 4 datasets for classification. Although Linformer works slightly better than Group Attn. on the ECG dataset (90.37% vs 88.84%), its performance is the worst in all other cases compared to any other RITA-based methods. Vanilla computes the attention scores precisely. Thus it is expected to work well. However, Group Attn. outperforms Vanilla on WISDM (87.50% vs 86.95%) and is very close to it on other 3 datasets. This suggests that group attention's approximation quality is good.\n",
      "\n",
      "pretraining + few label finetune (Multi-variate classification): The results shown in Table: 3: get us the following observation:: (1) Pretraining is effective. Pretraining always leads to better accuracy than training with a few labels from scratch. In particular, on WISDM data all the methods using RITA architecture increase the accuracy by at least 10%. This is impressive considering we do not have a very large unlabeled pre-training set to use.: (2) RITA's advantage over TST. our Group Attn. and other three baselines using RITA architecture (Vanilla, Performer, and Linformer) significantly outperform TST on all four classification datasets by 25 percentage points.: (3) Group Attention's advantage over other attention mechanisms. Group Attn. is better than Performer and Linformer on 3 out of 4 datasets. When compared to Vanilla, Group Attn. is better on HHAR and ECG, and comparable on the other two, further confirming its high quality on approximation. Further, we notice that Linformer struggles in this setting: in average its accuracy is worse than Vanilla by 8.22% and Group Attn. by 8.01%. This is because the low-rank projection operation introduces extra model parameters, making Linformer more easily overfit, while overfitting is especially harmful when there are only a few labeled training samples.\n",
      "\n",
      "full-dataset training (Multi-variate imputation): Similar to classification tasks, the results of imputation tasks (Table .2) show that Group Attn. consistently outperforms the baselines in training time while achieving comparable/better MSE. Again, on the large dataset MGH (length = 10,000), TST and Vanilla fail due to out of memory (OOM) errors. Methods using RITA framework (Group Attn., Performer, Linformer) all achieve very low MSE (are highly accurate). Among them Linformer is the worst.\n",
      "\n",
      "Efficiency: Transformer-based Methods: We measure the efficiency by the average training time per epoch including the cost of the forward computation + backward propagation and the grouping overhead. We first show the results on all the 5 datasets in Sec. 6.3.1. We then vary the length of the timeseries on the MGH dataset to show group attention's scalability on long timeseries in Sec. 6.3.2.\n",
      "\n",
      "Training Time: All Multi-variate Datasets: The results in Fig.: 3\n",
      "\n",
      "(b) and Table 2 lead to the below observations:: (1) Vanilla Self-Attention is not scalable. In average, it takes 2-3 minutes to train one epoch when the length of the timeseries is only 200 (WISDM, HHAR, RWHAR), takes over 15 minutes when the length increases to 2,000 (ECG), and fails on the long MGH data when the length reaches 10,000 due to out of GPU memory.: (2) Group Attn.'s advantage over all other attention mechanisms. As we have shown in Sec.: 6: than Performer and Linformer in classification and imputation tasks, while Group Attn. is always faster than Performer, Linformer, and all other baselines on all 5 multi-variate datasets, thus a win-win.: (3) The longer the timeseries, the larger the speedup. On the medium sized ECG dataset with a length of 2,000, Group Attn. has a speedup of 3.86/1.36/2.27 compared to Vanilla/Performer/Linformer. When the length increases to 10,000, the speedup on the MGH dataset increases to 6.59/7.48 compared to Performer/Linformer (Vanilla and TST failed in this case) on imputation task (Table . 2). However, even on the short WISDM, HHAR, RWHAR datasets, Group Attn. still consistently outperforms other methods, confirming that it does not introduce much overhead. This is because when the length of the timeseries gets longer, Group Attn. gets more opportunities to find windows with similar properties.\n",
      "\n",
      "Training time: Varying the Length: In this experiment, we truncate the original MGH timseries into sequences with the lengths at 2000/4000/6000/8000/10000, and compare Group Attn. against Vanilla and other attention mechanisms. Vanilla cannot handle sequences longer than 8000.: The results in Fig.: 4: again show that the longer the timeseries, the larger the speed up. With comparable MSE, Group Attn. outperforms Vanilla by 63X. Moreover, as the length increases from 2000 to 10000, the training time of Group Attn. only increases from 31.2 seconds to 54.4 seconds per epoch. The reason is that as the timeseires becomes longer, there are more grouping opportunities because of the similarity of the timeseries segments.\n",
      "\n",
      "Comparison to Non-deep Learning Methods: We compare against GRAIL, the SOTA of non-deep learning timeseries representation learning. We use the three uni-variate datasets, because GRAIL only targets uni-variate timeseries. Results in Fig.: 5: show that on all 3 datasets RITA significantly outperforms GRAIL in accuracy by 45, 16, and 21 percentage points because of the expressive power of Transformer. Moreover, thanks to the GPU-friendly design of RITA, it is at least 2 faster than GRAIL in training time.\n",
      "\n",
      "Ablation Study\n",
      "\n",
      "Adaptive Scheduler: To evaluate the effectiveness of RITA's adaptive scheduler (Sec. 5), we compare it against a baseline using a fixed group number  . We vary  and the error bound threshold  used by RITA.: From the results in Table: 4: we get the following observations:: (1) Adaptive Scheduler is better than fixed  . Training with Adaptive Scheduler already achieves better or comparable performance compared to the best performing  . More specifically, on the MGH dataset, dynamic scheduler always achieves better accuracy and is much faster compared to fixed  . On the ECG dataset, although fixed  is slightly better than adaptive scheduler in accuracy when setting the N as 512, it runs much slower than adaptive scheduler. Of course, finding the best  that balances the accuracy and running time requires careful tuning.: (2) Adaptive Scheduler is tuning free. It is robust on both accuracy and running time when  varies, while the results of fixed  vary significantly when the value of  changes. Therefore, Adaptive Scheduler frees the users from tuning the  threshold, while it is hard to find an appropriate  for a given dataset.  Table: 5: : RITA Pretraining: increasing sizes of pretrain set.\n",
      "\n",
      "The Sizes of the Pretraining Data: Next, we evaluate how the number of unlabeled data influences the effectiveness of pretraining. To get empirical results, we pretrain RITA on WISDM dataset with 20%/40%/60%/80% of the pretraining data and finetune each pretrained model with 100 labels per class. The results in Table: 5: show that: (1) The more pretraining data, the larger the improvement. The accuracy increases with the sizes of the pretraining data; (2) Marginal utility diminishing.: The first 20% pretraining data gives a 10.38% improvement accuracy (72.94% vs 62.56%), while the remaining 80% pretraining data only gives an additional improvement of 2.12% (75.06% vs 72.94%).\n",
      "\n",
      "RELATED WORK 7.1 Timeseries Analytics: There is a great deal of prior work on timeseries analytics methods. This work can be divided into three categories: (1) non-deep learning methods; (2) CNN/RNN-based deep learning methods; and (3) Transformer-based deep learning methods. Traditional Methods. These methods, such as TS-CHIEF: [45]: , HIVE-COTE: [33]: , ROCKET: [15]: have achieved notable performance on public datasets. Despite that, traditional methods suffer from one or more issues: they (1) rely on expert knowledge for feature extraction;: (2) incur heavy computation cost and are inappropriate for GPU devices; (3) support only uni-variate timeseries; (4) perform classification solely. Some work: [61]: shows that the transformedbased methods outperform these traditional methods especially on multi-variate timeseries.: In particular, as the SOTA of timeseries representation learning, GRAIL: [40]: extracts landmarks from data and computes the representations with the combination of the landmarks. However, GRAIL only supports uni-variate timeseries. Our experiments (Sec. 6.4) show that RITA significantly outperforms GRAIL in both effectiveness and efficiency on uni-variate timeseries.: CNN/RNN-based Deep Learning Methods. CNN-based methods, such as InceptionTime: [21]: and Resnet: [19]: , are good at classification tasks, but can not handle generative tasks such as forecasting because of the inductive bias of convolution networks. RNN-based methods, such as Brit: [7]: and deepAR: [44]: , are capable for classification, regression and generation. However, the recurrent structure brings a lot of problems: (1) limiting the model's ability in capturing long-range correlation; (2) notoriously difficult to train: [41]: because of gradient vanishing and exploding problem. As a result, such methods can hardly scale to very long timeseries. Transformer-based Deep Learning Methods. Given that Transformer is the best choice for backbone in almost all sequence modeling tasks, some effort has been made to apply Transformer to timeseries analytics. Targeting forecasting of uni-variate timeseries, LogTrans: [30]: introduced a log sparsity assumption to attention computation. Informer: [62]: pushes LogTrans a step further and scales forecasting to multi-variate timeseries. Autoformer: [57]: performs forecasting by decomposing timeseries into two parts, i.e. the trend part and the seasonal part.: For imputation tasks, CDSA: [37]: outperforms statistical methods and the SOTA of RNN-based method Brit: [7]: on 3 public and 2 competition datasets. For timeseries classification, AutoTransformer: [43]: performs architecture search to adapt to the tasks in different domains. For timeseries anomaly detection, Anomaly Transformer: [58]: outperforms many widely-used methods such as OmniAnomaly: [47]: , assuming the attention score maps show Gaussian distribution.: All of these works are designed for specific tasks, rather than functioning as a representation learning framework to serve different downstream tasks. To fill this gap, some researchers proposed a Transformer-based architecture, called TST: [61]: . Like RITA, TST supports regression, classification, and unsupervised learning through the \"cloze test\" pretraining task on timeseries. However, TST directly uses the classical Vanilla self-attention, thus not scalable to long timeseries as shown in our experiments (Sec. 6.3.2).\n",
      "\n",
      "Efficient Transformers: The need of improving the scalability of Transformers has led to more efficient variations of Transformers, especially for accommodating long text data in NLP: [49]: .: Introducing fixed/random patterns to self-attention mechanism is an intuitive idea. Sparse Transformer: [9]: and Longformer: [3]: only compute attention at fixed intervals. ETC: [2]: and BigBird: [60]: use global-local attention: the attention computation is limited within a fixed radius, while some auxiliary tokens are added to attend/get attended globally. The deficiencies of fixed attention patterns are obvious: it heavily depends on users to give an optimal setting.: To decrease the reliance on human labor, some works seek to introduce learnable/adaptive attention patterns instead of fixed patterns. Reformer: [26]: proposed only computing the dominant attention terms based on their observation of sparsity in attention matrix from language/image data. Such sparsity is intuitive in language data, in which a word's attention mainly focuses on the nearby sentences. However, attention in timeseries data shows strong seasonal patterns rather than sparse patterns, mainly as result of the periodicity of timeseries data. Therefore, such works do not work well for timeseries.: Apart from introducing attention patterns, some works seek to solve this problem with applied mathematics techniques. Linformer: [54]: performs a projection to decrease the size of query, key and value matrices before attention computation, because the attention matrix tends to be low-ranked. Performer: [10]: uses linear functions to approximate the kernel function softmax, making attention computation commutative. When the sequence length is far greater than the dimension of embedding vectors, Performer benefits from changing the order of matrix multiplication. Linformer and Performer do not depend on the unique properties of language data, thus potentially fitting timeseries better than other techniques, which is why we compared against them in our experiments. However as shown in Sec. 6, our group attention significantly outperforms them in both accuracy and efficiency (training time), because group attention fully leverages the periodicity of timeseries.\n",
      "\n",
      "CONCLUSION: In this work, we presented RITA, an automatic, self-supervised, and scalable timeseries analytics tool. RITA effectively adapts Transformer, popular in NLP, into timeseries analytics. As the key component of RITA, group attention eliminates the performance bottleneck of the classical self-attention mechanisms, thus successfully scaling RITA to highly complex, long timeseries data. Our experiments confirm that RITA significantly speeds up the state-of-the-art by 63X with a better accuracy.\n",
      "\n",
      "A APPENDIX: SUPPLEMENTARY MATERIAL A.1 Experiment Configuration and: Hyper-parameter Settings: Configuration. All models were trained on an NVIDIA Tesla V100 16GB GPU. All the methods are optimized with AdamW: [36]: of which the starting learning rate and weight decay parameter are both 1 -4 . In full-label training scenario, we train the models for 100 epochs. In \"pretraining + few-label finetuning scenario\", as the pretrained models require fewer epochs to converge: [61]: , we train the model for 50 epochs. For a fair comparison, the baselines use a maximal batch size within GPU's capacity during training.: As for model hyper-parameter setting, RITA and the baselines use a Transformer structure balancing Vanilla 's accuracy and efficiency: 8-layer stack of 2-head attention with hidden vectors in dimension of 64. Convolution kernel size is set to 5 by default. We set the error bound threshold (, Sec. 5.1) of Group Attention to 2, as it balances the accuracy and the efficiency in general on all datasets. Because Linformer requires the users to set the sizes of projection matrix, in different settings we choose an accuracyefficiency balancing one among {64,128,256,512}.\n",
      "\n",
      "A.2 Efficient Computation of Group Attention Algorithm 1 Efficient Computation of Group Attention: Require: ,  , ,   ,   Ensure: ,   R  *  ,  R  *  ,   N  ,   N  1: function group_attention(,  , ): 2:: for  = 0   -1 do 3::    -1  =0 (   ==  )  4::     5:: for  = 0   -1 do 6:: for  = 0   -1 do 7::  ,   (  , )   8:: for  = 0   -1 do 9::     -1  =0  ,: 10: :: for  = 0   -1 do 11::     -1  =0  (  , )     12:: return : In Alg. 1, we denote    to be the size of the   group,  to be the number of groups, r  to be the representative key of the   group and R to be the matrix consisting of all r  ,   to be the group that k  belongs to. ,  are the packing matrices of query vectors and value vectors as described in Sec.2. Alg. 1 outputs the packing matrix  for new feature emebddings { 1 , ...,   }, where   corresponds to the feature embedding of   . Lines 2-3 implement the embedding aggregation operation, while   We describe Alg. 3 and intuitively show its optimality. We assume that Scipy: [53]: learns an optimal function in Line 4 so that function COST gives the optimal estimation error when fitting the points in set . When fitting very few points, we assign an infinite cost to prevent a biased fitting function (Line 2). () denotes the minimal estimation error for points in sub-plane { 2     1 ,   }. In Lines 11-13, we enumerate all possible ways of cutting { 2     1 ,   } horizontally into two sub-plane { 2     1 ,   } and { 2     1 ,     } by iterating  from 1 to n. Choosing the cutting strategy that minimizes estimation error gets us a ( 1 ) with minimal estimation error for sub-plane { 2     1 ,    1 }, which is recorded as   1 , 2 in Line 14.  () denotes the minimal estimation error for sub-plane {   }. We enumerate all the possible ways of cutting {   } vertically into two sub-plane {  } and {     } by iterating  from 1 to  (Line 17: -19): . Finally, we have the minimal estimation error for the whole plane as  (  ). Based on the above discussion, this algorithm guarantees to not miss any better solution, hence optimal.: A.: 4: The Correctness of Group Attention Lemma 3. Assuming the windows belonging to the same group   have the same key vector, i.e.   =   (     ), then the feature embedding  produced by the original self-attention mechanism is identical to the output of our group attention mechanism implemented in Algorithm 1.: Proof. Denote   to be the representative vectors of   , i.e.   =   =   (     ). Algorithm 1 gives that:   = -1   =0 (   ==  )v  ,  , = q   r    =  -1   =0  (  , )   ,   =  -1   =0  ,    : (7): By the canonical self-attention mechanism introduced in Sec. 2, we get::  , = q   k j ,  , =  ( , ) -1 =0  ( , ) , o  = -1   =0  , v : (8): With 7 and 8, we have: -1   =0  ( , ) = -1   =0  (q   k  ) =  -1   =0 -1   =0 (   ==  ) (q   k  ) =  -1   =0  (q   r  ) -1   =0 (   ==  ) =  -1   =0  (q   r  )   =  -1   =0  (  , )   =  : (9): Further,: o  = -1   =0  , v j =  -1   =0 -1   =0 (   ==  ) , v  =  -1   =0 -1   =0 (   ==  )  ( , ) -1 =0  ( , ) v  =  -1   =0 -1   =0 (   ==  )  (q   k  ) -1 =0  ( , ) v  =  -1   =0 -1   =0 (   ==  )  (q   r j ) -1 =0  ( , ) v  =  -1   =0  (q   r j ) -1 =0  ( , ) -1   =0 (   ==  )v  =  -1   =0  (q   r j ) -1 =0  ( , )  : (10): Combining (: 7: ), (9) (10), we have o i = N -1 j=0 P i,j s i v j = o i . This concludes that the output of our group attention is identical to vanilla self-attention's. : A.5 The Proof of Error Bound (Lemma 1): Proof. We have:  ( , )  ( , ) =  (q   k  )  (q   k  ) =  (q   ( k  -k  )) =  (||q  ||  || k  -k  ||   (q  , k  -k  )): (11): So:  (-)   ( , )  ( , )   (): (12): Then we have: A i,j  . This proves Lemma 1.: A.: 6: The Proof of Merge Operation (Lemma 2): Proof. Denote the cluster size of   to be   .After mergeing, the new center will be::   =  =1\n",
      "\n",
      "A.7 Downstream Tasks: RITA supports a variety of downstream tasks. In this section, we show that with minimal modification RITA can effectively support classification, imputation and forecasting tasks. Other unsupervised tasks such as similarity search or clustering are naturally supported by extracting feature embeddings from RITA.\n",
      "\n",
      "A.7.1 Classification: To classify timeseries, we input timeseries to the model as described in Sec.\n",
      "\n",
      "A.7.3 Forecasting: Forecasting can be regarded as a special case of imputation, in which all missing values are at the end of timeseries.: So like in imputation task, we scale the timeseries to nonnegative and use a special value (-1) to indicate the values to be predicted::    (, ) =   (, )      -1 : (18): Where    is the observed timestamp. Then the output representations are fed into a Transpose Convolution layer using Mean Squared Error as loss function, as described above.: A.7.4 Other Unsupervised Tasks RITA naturally supports other unsupervised tasks, such as similarity search and clustering: [25,: 31,: 32]: , by producing the embedding of one timeseries (output representation of the special token [CLS]).: Clustering can be performed on the embeddings with flexible choice of distance metrics. Similarly, a high dimensional similarity search system: [22,: 23,: 38]: can be built on the embeddings.\n",
      "\n",
      "A.8 Inference Time: Dataset Length TST: [61]: In this section, we present the average inference time on validation sets. The results in Table. 6 and 7 correspond to the average inference time on validation sets of classification and imputation tasks, respectively. Consistent with the results in Section. 6.3, our method Group Attn. outperforms the baselines on both classification and imputation tasks, particularly on the datasets comprising long timeseries (ECG and MGH).\n",
      "===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muraf\\Courses\\arxiv_llm\\venv\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from arxiv_bot.search import TEIFile\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index import Document, VectorStoreIndex\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "splitter = SpacyTextSplitter(chunk_size=1024, chunk_overlap=100, separator=\"\\n\\n\")\n",
    "for paper in os.listdir(\"./output/\"):\n",
    "    tei_object = TEIFile(f\"./output/{paper}\")\n",
    "    print(tei_object.text)\n",
    "    print(\"===================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='5890981b-8362-4a80-9cd6-7cf661effa8b', embedding=None, metadata={'page_label': '1', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='a970d4d7e2cfc588d7ecbb820b81062b422baf9760e2fc006ef01d191ed6009d', text='Is Attention All What You Need ? - An Empirical Investigation on\\nConvolution-Based Active Memory and Self-Attention\\nThomas Dowdell ,Hongyu Zhang\\nThe University of Newcastle, NSW, Australia\\ntomjamesdowdell@gmail.com, hongyu.zhang@newcastle.edu.au\\nAbstract\\nThe key to a Transformer model is the self-attention\\nmechanism, which allows the model to analyze an\\nentire sequence in a computationally efcient man-\\nner. Recent work has suggested the possibility\\nthat general attention mechanisms used by RNNs\\ncould be replaced by active-memory mechanisms.\\nIn this work, we evaluate whether various active-\\nmemory mechanisms could replace self-attention\\nin a Transformer. Our experiments suggest that\\nactive-memory alone achieves comparable results\\nto the self-attention mechanism for language mod-\\nelling, but optimal results are mostly achieved by\\nusing both active-memory and self-attention mech-\\nanisms together. We also note that, for some spe-\\ncic algorithmic tasks, active-memory mechanisms\\nalone outperform both the self attention and a com-\\nbination of the two.\\n1 Introduction\\nThe previous state-of-the-art sequence model, the recurrent\\nneural network, has been largely supplanted by the Trans-\\nformer model [Vaswani et al. , 2017 ], which is primarily built\\natop a self-attention mechanism. Given a task to train upon,\\nthe self-attention mechanism focuses on one token per atten-\\ntion head within the entire sequence at each time-step; the\\nkey to the self-attention mechanisms success is the mecha-\\nnisms ability to learn which token within the entire sequence\\nto focus on in order to achieve the best results.\\nThe self-attention mechanism has proven successful on\\na variety of natural language processing tasks, but has not\\nachieved ubiquitous success. The authors of [Kaiser and Ben-\\ngio, 2016 ]pointed out that an attention mechanism would\\nlikely struggle to solve a task which required a model to fo-\\ncus on multiple tokens at a given time-step. Further, the au-\\nthors of [Kaiser and Sutskever, 2015 ]recommended that an\\nattention mechanism could be replaced by active-memory to\\nalleviate these concerns.\\nUnlike attention, active-memory allows a model to access\\nand change any and all elements of its memory at each time-\\nstep. The active-memory mechanism can access more than\\none element at each time step. In [Kaiser and Bengio, 2016 ],\\nFigure 1: The active memory mechanism. In this case, the active-\\nmemory is implemented in a unidirectional manner, with a kernel\\nsize 3.\\nthe authors used an active-memory system to translate En-\\nglish to French, and was capable of outperforming an RNN\\nmodel, both with and without an attention mechanism.\\nMotivated by the success of attention mechanism [Vaswani\\net al. , 2017 ]and active-memory [Kaiser and Bengio, 2016 ],\\nin this paper we investigate the Transformers self-attention\\nmechanism in comparison to a variety of active-memory\\nmechanisms. We experiment on two types of tasks: the lan-\\nguage modeling task and a set of algorithmic tasks.\\nFor the language modelling task, the self-attention mecha-\\nnism out-performs an active-memory mechanism used alone\\nby a slim margin. However, a combination of both self-\\nattention and active-memory reliably outperform both mech-\\nanisms used alone.\\nWe also evaluated the self-attention mechanism and vari-\\nous active-memory mechanisms on a variety of algorithmic\\ntasks, which can also be expressed as a sequence modeling\\ntask. Across most of the algorithmic tasks tested, the active-\\nmemory mechanisms achieve equal, or superior, results to a\\ntraditional self-attention mechanism. This would appear to\\nvindicate the hypothesis stated by [Kaiser and Bengio, 2016 ],\\nsuggesting that the nature of the attention mechanism does\\nindeed limit the effectiveness and accuracy of the model. Fi-\\nnally, we note that, for several algorithmic tasks, the mere\\naddition of the self-attention mechanism hinders results; the\\nactive-memory mechanism alone outperforms a combination\\nof the two separate mechanisms. This raises an unsolved\\nproblem; it would appear that, for deep learning sequence\\nmodels, there is still no unambiguous model that can opti-\\nmally solve all possible problems.\\n2 Related Work\\nThe Transformer model [Vaswani et al. , 2017 ]is built with\\ntwo separate modules, the self-attention mechanism and thearXiv:1912.11959v2  [cs.LG]  30 Dec 2019', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5430d76f-6244-48c6-87be-690624578208', embedding=None, metadata={'page_label': '2', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='1b0a79a8af54ac7820591d7216cccc782348d2bace6711cf8650ad6100826795', text='feedforward mechanism, which are stacked atop each other\\nfor multiple layers. The feedforward mechanism is an intra-\\nsequence analysis, where the output for each token in the\\nsequence is dependent only on the token at the same time-\\nstep, and independent of all other time-steps. On the other\\nhand, the self-attention mechanism is an inter-sequence anal-\\nysis, where the output for each time-step is dependent upon\\nthe entire sequence. The self-attention mechanism is dened,\\nmathematically, as:\\nQt,Kt,Vt=xt\\nyt=concat (head 1,t,head 2,t,...,head n,t)Wo\\nhead i=Attention (QtWQ\\ni,KtWK\\ni,VtWV\\ni)\\nAttention (Q,K,V ) =softmax (QtKT\\nt\\ndk)Vt\\nWoRdkk,d,WK,Q,V\\niRd,dkk\\nThe feed-forward module is dened as:\\nyt=Wl,2(max(Wl,1xt+bl,1,0)) +bl,2\\nWl,2Rd,d4,Wl,1Rd4,d\\nThe Transformer model, and its variants [Daiet al. , 2019 ],\\nhave achieved remarkable results across a variety of natural\\nlanguage processing tasks since its inception [Zhenzhong et\\nal., 2019 ] [Delvin et al. , 2018 ] [Yang et al. , 2019b ], and are\\ncurrently investigated heavily by both academia and industry.\\nThe Neural GPU [Freivalds and Liepins, 2017 ] [Kaiser\\nand Bengio, 2016 ] [Kaiser and Sutskever, 2015 ], which in-\\ntroduced an active-memory model, achieved impressive al-\\ngorithmic results in [Kaiser and Sutskever, 2015 ], and also\\nachieved impressive machine translation results in [Kaiser\\nand Bengio, 2016 ]. A Neural GPU contains a CGRU (Con-\\nvolution Gated Recurrent Unit) module which is iterated re-\\npeatedly. This allows the entire sequence to be analyzed in\\na parallelizable and computationally efcient manner. The\\nCGRU module is dened as:\\nu=sigmoid (U1x+B1)\\nr=sigmoid (U2x+B2)\\ny=ux+ (1u)tanh(U0(rx) +B0))\\nwhere U * x refers to applying a convolutional operator over\\nx, using Uas a trainable kernel bank and Bis a trainable bias\\nvector. The CGRU has, since its introduction, been used in\\nother models [Resende et al. , 2016 ].\\nConvolutional operators are traditionally used for image\\nprocessing [Alom et al. , 2018 ], and have also been used in\\nrelation to sequential analysis in previous papers [Yang et al. ,\\n2019a ] [Wuet al. , 2019 ] [Gehring et al. , 2017 ] [Dauphin\\net al. , 2016 ]. To the best of our knowledge they have not\\nbeen used explicitly to replace, or augment, the self-attention\\nmechanism. The rst sequence-to-sequence model, based on\\nconvolutional operators, was, to the best of our knowledge,\\nintroduced in [Gehring et al. , 2017 ], which replaced the then-\\ntraditional LSTM block with a series of convolutions and\\ngated convolutional networks [13], and outperformed RNN-\\nbased models in terms of both speed and accuracy. However,the model introduced in [Gehring et al. , 2017 ]was followed\\nshortly afterwards by the Transformer model, which outper-\\nformed the convolutional-based model.\\nThe convolutional self-attention network [Yang et al. ,\\n2019a ]was recently introduced, and bares a passing similar-\\nity to the traditional convolutional operator described in this\\npaper. The layer of the convolutional self-attention is similar\\nto a traditional self-attention mechanism, but where the key\\nand value tensors are calculated as:\\nKh= (Kh\\niM/2,...,Kh\\ni,...,Kh\\ni+M/2)\\nVh= (Vh\\niM/2,...,Vh\\ni,...,Vh\\ni+M/2)\\nFrom this point, the convolutional self-attention mech-\\nanism acts in an identical manner to the traditional self-\\nattention mechanism. This is in direct comparison to the con-\\nvolutional operator described in this paper, which explicitly\\navoids the use of the self-attention mechanism and relies en-\\ntirely on a purely convolutional operator.\\n3 Approach\\nIn this paper, we investigate whether various active-memory\\nmechanisms could replace self-attention in a Transformer.\\nWe also evaluate the combination of self-attention and active-\\nmemory mechanisms for language modelling tasks. All the\\nactive-memory mechanisms introduced in this paper were\\ninspired by the Neural GPU, as introduced in [Kaiser and\\nSutskever, 2015 ]. The key allure of the Neural GPU is that\\nthe inputs of each time-step can be analyzed and altered, and\\nwe were inspired to apply a similar form of sequence mod-\\nelling alongside a self-attention mechanism. We describe\\nvarious convolution-based active-memory mechanisms in this\\nsection.\\n3.1 The Convolutional Operators\\nThe Traditional Convolutional Operator\\nThe rst, and most simple, active-memory mechanism is the\\nsimple convolutional operator. The traditional convolutional\\noperator was formally dened in [Baiet al. , 2019 ]. If the task\\nrequires the sequence to be analyzed in a unidirectional man-\\nner, such as the case for language modelling, then a zeros-\\nvector of size k  1 is concatenated to the left of the input\\ntensor so that, for the nthoutput token, the model only has\\naccess to the rst ninput tokens. This feature is crucial to\\navoid allowing the model seeing forward through the se-\\nquence and having access to information that the model, in\\npractice, would not yet have. This has an identical function\\nto the masking operation of the self-attention mechanism.\\nIf the task can be analyzed in a bidirectional manner,\\nthen the model uses a convolutional lter using the SAME-\\npadding, which allows for the vector to maintain its shape\\nthroughout the convolutional operator. However, when the\\nconvolutional operator is performed in this manner, the to-\\nken at time-step tis dependent on the input tokens h[t-k/2,t+k/2] ,\\nwhere kis the kernel size.\\nThe primary aw of a convolutional operator, in compar-\\nison to a self-attention mechanism, is that, given nlayers\\nwhere each kernel has a kkernel size, each token can only', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='606d1ad4-bedf-482e-8698-24aaaadd7d7f', embedding=None, metadata={'page_label': '3', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='26f80ec3533febd5c4ef46562cd11e30111018e3518c92d90f9fef72b9c28c55', text='seek * n  n + 1 ork/2 * n - n + 1 time-steps across for unidi-\\nrectional and bidirectional tasks respectively. For example, in\\nour experiments on language modeling (Section 4), the kernel\\nsize was set to 20 and was iterated over 8 layers. Therefore, at\\neach time-step t, the nal output is capable of analyzing the\\ninput from 153 previous time-steps, well above the average\\nsequence-size (90 tokens) in the dataset. The self-attention\\nmechanism, in comparison, can see across a theoretically in-\\nnite context size, even using only a single layer. Given this\\ninformation, the self-attention mechanism is capable of han-\\ndling theoretically greater long-term dependencies than the\\nactive-memory mechanism. However, in practice, the abil-\\nity of an active-memory mechanism to access and change its\\nentire memory could overcome this limitation.\\nThe convolutional operator is assisted further by the fact\\nthat the convolutional operators complexity grows linearly\\nwith the sequence size, while the self-attention mechanisms\\ncomplexity grows quadratically.\\nNumerous papers have noted that, while Transformers are\\nparallelizable and capable of capturing long-range dependen-\\ncies, the Transformer network suffers from the inability of\\nmodel tokens in a recurrent manner [Wang et al. , 2019 ] [Hao\\net al. , 2019 ]. This is in direct comparison to traditional RNN\\nmodels, which can capture long-range dependencies, but can\\nstruggle to capture long-range dependencies. The use of\\nactive-memory, in theory, would accomplish this task, given\\nthat the output at time-step t htis dependent of the inputs\\nx[t-k,t] where k is the kernel size. Therefore, this operation\\ncan, in theory, model recurrence. We did not explicitly test\\nwhether this does model recurrence in practice, but will focus\\non this in future work.\\nThe convolutional operator is followed by the ReLU acti-\\nvation function.\\nThe Persistent-Convolutional Operator\\nThe Persistent-Convolutional operator is similar to the tradi-\\ntional convolutional operator described above, except that the\\nzeros vector is replaced by the a trainable vector of identi-\\ncal shape to the zeros vector. This allows the operator to,\\nidentical to the traditional convolutional operator, maintain an\\nidentical shape across the convolution. To keep parameteriza-\\ntion to a minimum, the same persistent vector is used across\\nall convolution operators in the entire model. The persistent-\\nconvolutional operator is dened as:\\npWkernel size1,hidden size\\nx= [p,x],y=Wx+b\\nwhere [.,.] denotes the concatenation function and pis the\\ntrainable persistent vector. Persistent vectors have been used\\npreviously in language modelling tasks [Sukhbaatar et al. ,\\n2019 ], but never as an augmentation for convolutional opera-\\ntors, as far as we know.\\nIf the model is to be analyzed in a bidirectional man-\\nner, rather then a unidirectional manner, then the persistent-\\nconvolutional operator can be redened as:\\np1,p2W(kernel size1)//2,hidden size\\nx= [p1,x,p 2]\\nFigure 2: The Self-Attention + Convolutional Operator Transformer.\\nThe use of a persistent vector allows for the model to have a\\npermanent memory that, given the fact the vector is trainable,\\ncan be expressed in an optimal manner for the model. This is\\nthe equivalent of a permanent memory for the deep learning\\nmodel.\\nThe Highway-Convolutional Operator\\nThe Highway-Convolutional operator is based on the high-\\nway network architecture [5], which can be dened as:\\na=U0x+B0\\nb=sigmoid (U1x+B1)\\ny=ab+x(1b)\\nThe key allure of the highway network, as described in\\n[Srivastava et al. , 2015 ], is the fact that a highway network\\ncan be trained for a large number of layers, even hundreds\\nof layers, because information can pass, unimpeded, across\\neach layer. The authors of [Srivastava et al. , 2015 ]described\\nthese paths as information highways. The use of these in-\\nformation highways allows information to pass through the\\nself-attention mechanism in an equally efcient manner.\\nIn this paper, we use the hard-sigmoid function [Kaiser and\\nBengio, 2016 ]to stabilize gradients, which is dened as:\\ny=max(0,min (1,1.2sigmoid (x)0.1))\\n3.2 Self-Attention + Convolutional Operators\\nThe operator calculates the results of the self-attention mech-\\nanism and results of the convolutional operator indepen-\\ndently, and then adds them together to produce the nal out-\\nput of the operator. This operator would allow the model to\\nanalyze the input using both the self-attention mechanism and\\nactive-memory mechanism and decide which features from\\nboth mechanisms would be most optimal. This approach has\\nthe obvious advantage of being able to take the best of both\\nworlds, where the optimal features that can only be detected', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f90639a2-3329-4abc-94b4-a1b295006c4c', embedding=None, metadata={'page_label': '4', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='20643145356be3933ca7df23337aee52a907e2ebac9ecb11c89466d3455634cb', text='Model Loss per Token\\nCGRU 1.6834 (+0.1645)\\nConvolution 1.5358 (+0.0169)\\nPersistent-Convolution 1.5341 (+0.0152)\\nHighway-Convolution 1.5327 (+0.0138)\\nSelf-Attention 1.5189 (+0.0)\\nSelf-Attention + Convolution 1.4912 (-0.0277)\\nSelf-Attention + Persistent-Convolution 1.4905 (-0.0284)\\nSelf-Attention + Highway-Convolution 1.4869 (-0.032)\\nTable 1: The loss-per-token of the self-attention mechanism and the\\nactive-memory mechanisms on the WT3 dataset, and the difference\\nof loss between the self-attention and the active-memory mecha-\\nnisms. The lower the loss, the better the model performed. With the\\nexception of the CGRU, all purely active-memory operators achieve\\na test loss less then 1.2% higher then the self-attention mechanism.\\nThe optimal models combined the self-attention mechanism and an\\nactive-memory mechanism, and achieved a lower test loss than the\\nself-attention mechanism and active-memory mechanisms alone.\\nby the self-attention mechanism, and the optimal features that\\ncan only be detected by the convolutional operator, are both\\navailable to the model.\\nThe architecture of a single layer of the self-attention +\\nconvolution operator is shown in Figure 2. This architec-\\nture, without the convolutional operator, is a simple Trans-\\nformer layer. The output of the convolutional operator is\\nadded, element-wise, to the output of the self-attention mech-\\nanism. This allows, hypothetically, for the best-of-both-\\nworlds, where the model has access to the self-attention\\nmechanism and the active-memory mechanism.\\nSimilarly, we also add the self-attention mechanism\\nto the persistent-convolutional operator and the highway-\\nconvolutional operator, respectively.\\n4 Experiments\\nTo evaluate the effectiveness of the various convolution-based\\nactive-memory mechanisms, we used two separate experi-\\nments; a language modelling task that is traditionally associ-\\nated with attention-based mechanisms [Shoeybi et al. , 2019 ],\\nand algorithmic tasks that are associated with active-memory\\nmodels [Kaiser and Sutskever, 2015 ]. The active-memory\\nmechanisms are experimented both independently and along-\\nside a self-attention mechanism.\\n4.1 Language Modelling\\nExperimental Setup\\nThe rst task that the operators were tested with was\\na unidirectional language modelling task; the WikiText-3\\n(WT3) dataset [Merity et al. , 2016 ], tokenized using BPE-\\ntokenization [Sennrich et al. , 2016 ]. The WikiText-3 dataset\\nwas sourced entirely from Wikipedia articles, contains over\\n3.6 millions lines of text, and is split into a training dataset,\\nvalid dataset and test dataset. The train dataset contains 103M\\ntokens, while the valid and test dataset contain 250K tokens\\neach.\\nThe models used were all 8-layer models, with a hidden\\nsize of 256 and a lter size of 1024, a vocab size of 32,000,(x, y) 1 0 1 1 + 0 0 1 1\\n(x + y) 0 0 0 0 0 1 1 1 0\\nTable 2: The binary addition task. Given two numbers (in this case,\\nthe two numbers are 11 and 3), the nal output is the binary version\\nof the addition of the two input numbers (in this case, 14).\\nkernel size of 20 and a dropout rate of 0.9. No further regu-\\nlarization was used. The optimizer was the Adam Optimizer\\n[Kingma and Ba, 2014 ]and a warmup-learning rate was used,\\nas specied in [Vaswani et al. , 2017 ]. All models were im-\\nplemented using Tensorow, version 1.07, on a V100 GPU\\ncard.\\nNotable preprocessing was used for analyzing the\\nWikiText-3 dataset; every character was explicitly denoted as\\nlower-case, each hyphen -was replaced by @-@ and punc-\\ntuation marks, such as fullstops and commas, were seperated\\nby white-space. This was done to discourage the BPE to tok-\\nenize sets of characters that included punctuation marks, forc-\\ning the model to tokenize sets of characters that were only\\nletters, therefore tokenizing a greater set of words.\\nExperimental Results\\nWith the exception of the CGRU operator, all active-memory\\nmechanisms, when combined with the self-attention mech-\\nanism, outperformed the self-attention mechanism alone,\\nachieving a lower loss-per-token. This would appear to vin-\\ndicate the proposition of both this paper and [2], suggesting\\nthat, indeed, active-memory mechanisms and self-attention\\nare comparable. However, no model that purely used an\\nactive-memory mechanism outperformed the self-attention\\nmechanism for language modelling.\\nWe note that, if the dropout rate was decreased to 0.7, all\\noperators, with the exception of CGRU, all models achieved\\nsuperior results to the self-attention mechanism at a the same\\ndropout rate. However, these models did not achieve superior\\nresults to the self-attention model with a dropout-rate of 0.9.\\nThis would imply that self-attention mechanisms are more\\nsensitive to dropout rates compared to active-memory mech-\\nanisms.\\nFurther, each operator, except for the CGRU operator, ben-\\neted from combining it with self-attention, allowing both\\noperators to operate independently and concurrently. The\\nmodel with the lowest loss-per-token had a self-attention\\nmechanism and a highway-convolutional operator. It is fur-\\nther worth noting that the highway-convolutional operator\\noutperformed both other convolutional operators, both with\\nand without the addition of the self-attention mechanism.\\n4.2 Algorithmic Tasks\\nExperimental Setup\\nThe second experiment for evaluating the active-memory\\nmechanisms was on various algorithmic tasks:\\nReverse: Given an array Xof size L, the model is trained\\nto return the array Y, where Y[0] = X[-1] . In order to\\neffectively perform this task, the model must be capable\\nof analyzing the start of the input vector at the very end,\\nand vice-versa.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1afd121e-8966-465c-9bfc-65cc6aa81201', embedding=None, metadata={'page_label': '5', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='3486dac79454a1634092a67a8d2328c0af4d6bdd4704870aff69ea15d232f7d7', text='(x, y) 1 0 1 0 1  0 1 1 0 0\\n(xy) 0 0 0 1 1 1 1 1 1 0 0\\nTable 3: The binary multiplication task. The above example contains\\ntwo input numbers, 12 and 21, and the output number 252.\\nSort: Given an array of randomly order integers, the\\nmodel is trained to return an array that accurately order\\nthe input integers. The entire vector must be remem-\\nbered and analyzed at each time-step.\\nAddition: Given two binary numbers, the model is\\ntrained to return an array that represents the addition in\\nthe form of a third binary number. An example of the\\naddition task is shown in Table 2.\\nMultiply: Multiplies two binary numbers, as shown in\\nTable 3.\\nNot: If the input is 1, then not returns 0. Else, the not\\nfunction returns 1. The output relies only on the input at\\nthe current time-step.\\nRemember: Given a series of random numbers of se-\\nquence sizeN, followed by a sequence of zeros of iden-\\ntical size, the model is trained to output a series of zeros\\nof sizeN, followed by the random numbers. In order\\nfor this task to be performed, the model must be able to\\nremember tokens over an increasingly long sequence.\\nAll data for the algorithmic tasks were generated in an on-\\nline manner. For three of the tasks, Sort, Addition and Mul-\\ntiply, the model must focus on multiple tokens at every time-\\nstep. In comparison, the Reverse task, the Not task and the\\nRemember task only require the model to focus on a single\\ntoken at every time-step.\\nThe model that was used for algorithmic tasks contains 4\\nlayers, with a hidden size of 128 units, a lter size of 512 and\\na kernel size of 20. Each model was trained for a maximum of\\n100 epochs, where each epoch contains 100 iterations. At the\\nend of each epoch, the model was exposed to an online batch,\\ncontaining 32 test cases. If the model achieved an accuracy of\\n100% on the online test batch, the sequence-size of the data\\nis increased, therefore increasing its complexity.\\nFor the Reverse, Sort, Not and Remember task, when the\\nmodel achieved a 100% accuracy, the sequence was increased\\nby 1. For the Addition and Multiply task, the sequence was\\nincreased by 2.\\nThe model was initially trained only for sequences that are\\n5 tokens long and was not introduced to a larger sequence un-\\ntil the model was capable of achieving 100% accuracy on this\\nsequence-size. We found that this form of curriculum learn-\\ning was essential: if a model was initially trained on a se-\\nquence of several dozen tokens, each operator was incapable\\nof achieving a reasonable accuracy.\\nThe vocabulary size was different for each task. The Re-\\nverse task had a vocabulary size of 100, while the Sort task\\nand the Remember task had a vocabulary size of 20. We noted\\nthat whenever the vocab size was increased the model would\\nachieve less accurate results. Because all tokens in the Addi-\\ntion, Multiply and Not tasks are either 0, 1, or the separator,the vocabulary size is set to 3.\\nExperimental Results\\nEach model was tested for each task, and the highest sequence\\nthat the model could achieve within 100 epochs was recorded.\\nEach experiment was performed three times, and the average\\nsequence size is presented in Table 4. For example, the self-\\nattention mechanism managed to achieve a 100% accuracy\\nfor a sequence of 41 tokens for the Reverse task, but could not\\nachieve a 100% accuracy for both the Sort task and the Addi-\\ntion task for a sequence size of 20 (the Sort task achieved\\na maximum sequence size of 14, while the Addition task\\nachieved a maximum size of 7). Of the six algorithmic tasks\\ntested, active-memory mechanisms were used, either solely\\nor in combination with the self-attention mechanism, in the\\nbest-performing model of ve of these tasks. For example,\\nthe self-attention mechanism achieved an average sequence\\nsize of 41.0 for the Reverse Task and 14.0 for the Sort Task,\\nwhich are lower than those achieved by the self-attention\\n+ persistent-convolution mechanism (43.7 and 23.3, respec-\\ntively). Furthermore, for the Addition and Multiply Tasks, the\\nactive-memory mechanisms across the board outperformed\\nboth the self-attention mechanism and the combination of the\\nself-attention mechanism and the active-memory mechanism.\\nFor example, the traditional convolution operator, for the Ad-\\ndition Task, outperformed the self-attention mechanism and\\nthe self-attention + convolutional mechanism by 34.0 and\\n4.7 respectively. The results show that the active-memory\\nmechanisms achieve equal, or superior, results to a traditional\\nself-attention mechanism.\\nSelf-attention, used alone, only performed optimally on the\\nRemember task, and equally well on the Not task. Interest-\\ningly, across all models for the Addition and Multiply tasks,\\nthe self-attention mechanism reliably led to poor results;\\nnot only does the self-attention mechanism, alone, achieve\\nthe poorest results, but the combination of the self-attention\\nmechanism and any active-memory system performed worse\\nthen the active-memory system alone. This is in direct con-\\ntrast to the Sort task and the Reverse task, where the com-\\nbination of self-attention mechanism and the active-memory\\nachieve the best results.\\nThe self-attention mechanism would, in theory, outperform\\nactive-memory mechanisms for the Remember task. This is\\nbecause, in order to adequately perform the Remember task,\\nthe model must be capable of calculating an output based\\non long-range dependencies, which active-memory cannot\\nmatch at a large enough sequence length. Other tasks do\\nrequire a long-range dependency in order to operate well at\\nlarge sequence sizes, but are dependent on the model per-\\nforming other tasks as well. For example, the addition task re-\\nquires to model long range-dependencies and perform binary\\naddition. The self-attention mechanism, although it can learn\\nthese long-range dependencies, cannot access all necessary\\ntokens at a given time to adequately perform binary addition.\\nThis is vindicated by the experimental results. In Table 4, the\\nself-attention mechanism achieved the highest results on the\\nRemember task. This would suggest that, if the algorithmic\\ntask only requires a long-range dependency, then the self-\\nattention mechanism will outperform active-memory mech-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d63dc621-f44c-4d74-b0d8-ef87ac5bb0dc', embedding=None, metadata={'page_label': '6', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='a315150ba7609afab285b5b5e7133ed6eeaee8fa3d9827c9d12485ea3e2d23f7', text='Model Reverse Sort Addition Multiply Not Remember\\nCGRU 7.7 5.7 16.3 9.0 104.0 9.0\\nSelf-Attention 41.0 14.0 7.0 7.0 104.0 57.0\\nConvolution 17.7 20.7 41.0 17.0 104.0 36.0\\nPersistent-Convolution 25.0 20.3 38.3 16.3 104.0 35.0\\nHighway-Convolution 19.7 16.7 35.7 13.7 104.0 33.0\\nSelf-Attention + Convolution 41.0 23.3 36.3 12.3 104.0 26.0\\nSelf-Attention + Persistent-Convolution 43.7 23.3 35.0 12.3 104.0 27.0\\nSelf-Attention + Highway-Convolution 41.0 20.0 34.3 11.7 104.0 24.7\\nTable 4: The average sequence length that each operator was capable of gaining 100% accuracy within 100 epochs over 3 runs. The\\nhigher the sequence size, the better the model learned. For the Reverse and Sort tasks, the combination of self-attention mechanism and\\npersistent-convolution achieved the best results. For the Sort, Addition, and Multiply tasks, the self-attention mechanism was beaten by the\\nactive-memory mechanisms. For the Addition and Multiply tasks, the mere use of a self-attention mechanism alongside an active-memory\\nmechanism actively decreased results. The highest possible sequence that can be learned over these epochs is 104 in the Not task. The\\nself-attention mechanism achieved the best result only for the Remember task.\\nanisms when used alone. In comparison, the self-attention\\nmechanism is incapable of matching the results of active-\\nmemory for all other tasks. These ndings appear to vindi-\\ncate the statement made by Kaiser et. al [2]; whenever the\\nsequential task requires the model to focus on multiple to-\\nkens at every time-step, using an attention mechanism will\\nlead to extremely poor results, especially in comparison to\\nactive-memory models.\\nIt is worth noting that, for each of the active-memory mech-\\nanisms operating alone, none of the three achieved a 100%\\naccuracy for any sequence over a size of 37 for the Remem-\\nber task. This is because, given the kernel size of 20 and\\n4 layers, the model is only capable of seeing 37 time-steps\\nacross. Therefore, the model cannot see 37 time-steps across\\nand, therefore, cannot perform the Remember task at this se-\\nquence size or any larger sequence size. This displays the im-\\nportance of utilizing both a self-attention mechanism, which\\ncan be utilized for analyzing long-range dependencies, and\\nan active-memory mechanism, which can extract features that\\nthe self-attention mechanism cannot.\\n4.3 Discussion of Results\\nThe experiments above suggest that, across most tasks, a\\ncombination of a self-attention mechanism and an active-\\nmemory mechanism, at worst, perform comparably to a\\npurely attention-based model, and at best surpass an attention\\nmodel, with the exception of the Remember task. However,\\nfor some algorithmic tasks, we note that the mere inclusion\\nof a self-attention mechanism actively hinders performance.\\nModels that combine both the attention mechanism and\\nactive-memory mechanisms outperformed both attention-\\nonly and active-memory-only models for language mod-\\nelling. This suggests that, for language modelling tasks,\\nboth active-memory mechanisms and attention mechanisms\\nare capable of extracting features that the other mechanism is\\nnot capable of extracting, and that both mechanisms operate\\noptimally when used alongside each other.\\nThe ndings are further abstracted by studying the effect of\\nvarious algorithmic tasks; in cases where only a single token\\nneeds to be focused on, the self-attention mechanism matches\\nthe most ardent active-memory, while active-memory mecha-nisms radically outperform self-attention for other tasks. This\\nwould imply that various time-dependencies that cannot be\\nanalyzed by a self-attention mechanism can be analyzed by\\nactive-memory.\\nIt is worth noting that, for the Not function, all models learn\\noptimally. This is likely due to the fact that the output of each\\ntime-step depends only on the input at this time-step, and each\\nmodel can analyze this dependency equally efciently. Also,\\nbased on the results of the Remember task, the self-attention\\nmechanism can attain greater long-range dependency in com-\\nparison to the active-memory mechanisms.\\nFinally, we note that, for the Remember function, both\\nmechanisms, when used alone, outperform the two mecha-\\nnisms used together. For every other task, a combination of\\nthe self-attention and active-memory would improve upon at\\nleast one of the mechanisms when used alone. We are unsure\\nexactly what has led to this result. This will require further\\ninvestigation in the future.\\n5 Conclusion\\nIn this paper we investigate the Transformers self-attention\\nmechanism in comparison to a variety of active-memory\\nmechanisms. We experiment on two types of tasks: the\\nlanguage modeling task and the algorithmic task. Our re-\\nsults show that the self-attention mechanism can be improved\\nby an active-memory mechanism alone or by a combina-\\ntion of the two. Our results have implications for wider se-\\nquence modeling tasks, which are currently dominated by\\nself-attention based models.\\nOur code and models used in experiments are available at:\\nhttps://github.com/Anon-111/Active-Memory .\\nIn the future, we will further explore the use of active-\\nmemory for sequence-to-sequence tasks, such as machine\\ntranslation. We will also analyze the empirical differences\\nbetween the studied algorithmic tasks, and investigate why\\nthe self-attention mechanism may assist one task but harm\\nanother.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0f729b8e-76e1-4caf-a3df-e1e0c748ac73', embedding=None, metadata={'page_label': '7', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='d41a1e17554070c02784e7f184c5c00bac6bca6dc197488e6249af6267337eaa', text='References\\n[Alom et al. , 2018 ]Md Zahangir Alom, Tarek M. Taha,\\nChristopher Yakopcic, Stefan Westbery, Paheding Sidike,\\nMst Sharmina Nasrin, Brian C Van Esesn, Abdul A S.\\nAwwal, and Vijayan K. Asari. The history began from\\nalexnet: A comprehensive survey on deep learning ap-\\nproaches. arXiv preprint arXiv:1803.01164 , 2018.\\n[Baiet al. , 2019 ]Shaojie Bai, J. Zico Kolter, and Vladlen\\nKoltun. An empirical evaluation of generic convolutional\\nand recurrent networks for sequence modeling. arXiv\\npreprint arXiv:1803.01271 , 2019.\\n[Daiet al. , 2019 ]Zihang Dai, Zhilin Yang, Yiming Yang,\\nJaime Carbonell, Quoc V . Le, and Ruslan Salakhutdi-\\nnov. Transformer-XL: Attentive language models beyond\\na xed-length context. arXiv preprint arXiv:1901.02860 ,\\n2019.\\n[Dauphin et al. , 2016 ]Yann N. Dauphin, Angela Fan,\\nMichael Auli, and David Grangier. Language model-\\ning with gated convolutional networks. arXiv preprint\\narXiv:1612.08083 , 2016.\\n[Delvin et al. , 2018 ]Jacob Delvin, Ming-Wei Chang, Ken-\\nton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understand-\\ning.arXiv preprint arXiv:1810.04805 , 2018.\\n[Freivalds and Liepins, 2017 ]Karlis Freivalds and Renars\\nLiepins. Improving the neural gpu architecture for algo-\\nrithm learning. arXiv preprint arXiv:1702.08727 , 2017.\\n[Gehring et al. , 2017 ]Jonas Gehring, Michael Auli, David\\nGrangier, Denis Yarats, and Yann Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint\\narXiv:1705.03122 , 2017.\\n[Haoet al. , 2019 ]Jie Hao, Xing Wang, Baosong Yang,\\nLongyue Wang, Jinfeng Zhang, and Zhaopeng Tu.\\nModeling recurrence for transformer. arXiv preprint\\narXiv:1904.03092 , 2019.\\n[Kaiser and Bengio, 2016 ]Lukasz Kaiser and Samy Bengio.\\nCan active memory replace attention. arXiv preprint\\narXiv:1610.08613 , 2016.\\n[Kaiser and Sutskever, 2015 ]Lukasz Kaiser and Ilya\\nSutskever. Neural gpus learn algorithms. arXiv preprint\\narXiv:1511.08228 , 2015.\\n[Kingma and Ba, 2014 ]Diederik P. Kingma and Jimmy Ba.\\nAdam: A method for stochastic optimization. arXiv\\npreprint arXiv:1412.6980 , 2014.\\n[Merity et al. , 2016 ]Stephen Merity, Caiming Xiong, James\\nBradbury, and Richard Socher. Pointer sentinel mixture\\nmodels. arXiv preprint arXiv:1609.07843 , 2016.[Resende et al. , 2016 ]Danilo J. Resende, Shakir Mohamed,\\nIvo Danihelka, Karol Gregor, and Daan Wiestra. One shot\\ngeneralization in deep generative models. arXiv preprint\\narXiv:1603.05106 , 2016.\\n[Sennrich et al. , 2016 ]Rico Sennrich, Barry Haddow, and\\nAlexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 ,\\n2016.\\n[Shoeybi et al. , 2019 ]Mohammad Shoeybi, Mostofa Pat-\\nwary, Raul Puri, Patrick LeGresley, Jared Casper, and\\nBryan Catanzaro. Megatron-lm: Training multi-billion pa-\\nrameter language models using model parallelism. arXiv\\npreprint arXiv:1909.08053 , 2019.\\n[Srivastava et al. , 2015 ]Rupseh Kumar Srivastava, Klaus\\nGreff, and J urgen Schmidhuber. Highway networks. arXiv\\npreprint arXiv:1505.00387 , 2015.\\n[Sukhbaatar et al. , 2019 ]Sainbayar Sukhbaatar, Edouard\\nGrave, Guillaume Lample, Herve Jegou, and Armand\\nJoulin. Augmenting self-attention with persistent memory.\\narXiv preprint arXiv:1907.01470 , 2019.\\n[Vaswani et al. , 2017 ]Ashish Vaswani, Noam Shazeer, Niki\\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\\nukasz Kaiser, and Illia Polosukhin. Attention is all you\\nneed. arXiv preprint arXiv:1706.03762 , 2017.\\n[Wang et al. , 2019 ]Zhiwei Wang, Yao Ma, Zitao Liu, and\\nJiliang Tang. R-transformer: Recurrent neural network\\nenhanced transformer. arXiv preprint arXiv:1907.05572 ,\\n2019.\\n[Wuet al. , 2019 ]Felix Wu, Angela Fan, Alexei Baevski,\\nYann N. Dauphin, and Michael Auli. Pay less attention\\nwith lightweight and dynamic attention. arXiv preprint\\narXiv:1901.10430 , 2019.\\n[Yang et al. , 2019a ]Baosong Yang, Longyue Wang,\\nDerek F. Wong, Lidia S. Chao, and Zhaopeng Tu.\\nConvolutional self-attention networks. arXix preprint\\narXiv:1904.03107 , 2019.\\n[Yang et al. , 2019b ]Zhilin Yang, Zihang Dai, Yiming Yang,\\nJaime Carbonell, Ruslan Salakhutdinov, and Quoc V . Le.\\nXlnet: Generalized autoregressive pretraining for lan-\\nguage understanding. arXiv preprint arXiv:1906.08237 ,\\n2019.\\n[Zhenzhong et al. , 2019 ]Lan Zhenzhong, Mingda Chen, Se-\\nbastian Goodman, Kevin Gimpel, Piyush Sharma, and\\nRadu Soricut. Albert: A lite bert for self-supervised\\nlearning of language representations. arXiv preprint\\narXiv:1909.11942 , 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='530922f8-65e5-4e48-8980-8cefb72c792b', embedding=None, metadata={'page_label': '1', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='b624fc55dbf835eb3678e85c5dbad12ae47f08e2a6011bdd592e3b82626554e9', text='All the attention you need:\\nGlobal-local, spatial-channel attention for image retrieval\\nChull Hwan Song\\nOdd ConceptsHye Joo Han\\nOdd ConceptsYannis Avrithis\\nInria, Univ Rennes, CNRS, IRISA\\nAbstract\\nWe address representation learning for large-scale\\ninstance-level image retrieval. Apart from backbone, train-\\ning pipelines and loss functions, popular approaches have\\nfocused on different spatial pooling and attention mecha-\\nnisms, which are at the core of learning a powerful global\\nimage representation. There are different forms of attention\\naccording to the interaction of elements of the feature tensor\\n(local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses\\nonly one or two forms of attention and applies it to different\\nproblems like classication, detection or retrieval.\\nWe present global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network and\\nincorporates all four forms of attention: local and global,\\nspatial and channel. We obtain a new feature tensor and, by\\nspatial pooling, we learn a powerful embedding for image\\nretrieval. Focusing on global descriptors, we provide em-\\npirical evidence of the interaction of all forms of attention\\nand improve the state of the art on standard benchmarks.\\n1. Introduction\\nInstance-level image retrieval is at the core of visual rep-\\nresentation learning and is connected with many problems\\nof visual recognition and machine learning, for instance\\nmetric learning [30, 26], few-shot learning [42] and unsu-\\npervised learning [8]. Many large-scale open datasets [3,\\n37, 16, 29, 53], and competitions1have accelerated progress\\nin instance-level image retrieval, which has been trans-\\nformed by deep learning [3].\\nMany studies on instance-level image retrieval focus\\non learning features from convolutional neural networks\\n(CNN), while others focus on re-ranking , for instance by\\ngraph-based methods [11]. The former can be distinguished\\naccording to feature types: local descriptors , reminiscent of\\nSIFT [27], where an image is mapped to a few hundred vec-\\ntors; and global descriptors , where an image is mapped to a\\n1https://www.kaggle.com/c/landmark-retrieval-2020single vector. In fact, deep learning has brought global de-\\nscriptors with astounding performance, while allowing ef-\\ncient search. Our study belongs to this type.\\nStudies on global descriptors have focused on spatial\\npooling [2, 37]. The need for compact, discriminative rep-\\nresentations that are resistant to clutter has naturally given\\nrise to spatial attention methods [24, 28]. Different kinds\\nof attention have been studied in many areas of computer\\nvision research. There is also channel attention [20, 9]; lo-\\ncal attention , applied independently to elements of the rep-\\nresentation (feature map) [54, 25]; global attention , based\\non interaction between elements [52, 9]; and combinations\\nthereof. Unfortunately, each study has been limited to one or\\ntwo kinds of attention only; attention is not always learned;\\nand applications vary.\\nIt is the objective of our work to perform a compre-\\nhensive study of all forms of attention above, apply them\\nto instance-level image retrieval and provide a detailed ac-\\ncount of their interaction and impact on performance. As\\nshown in Figure 1, we collect contextual information from\\nimages with both local andglobal attention, giving rise to\\ntwo parallel network streams. Importantly, each operates\\non both spatial locations andfeature channels . Local at-\\ntention is about individual locations and channels; global is\\nabout interaction between locations and between channels.\\nThe extracted information is separately embedded in local\\nand global attention feature maps, which are combined in a\\nglobal-local attention feature map before pooling.\\nOur contributions can be summarized as follows:\\n1. We propose a novel network that consists of both\\nglobal and local attention for image retrieval. This is\\nthe rst study that employs both mechanisms.\\n2. Each of the global and local attention mechanisms\\ncomprises both spatial and channel attention.\\n3. Focusing on global descriptors, we provide empirical\\nevidence of the interaction of all forms of attention and\\nimprove the state of the art on standard benchmarks.\\n1arXiv:2107.08000v1  [cs.CV]  16 Jul 2021', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3330ec81-ab0d-484d-81df-7b4de04312ea', embedding=None, metadata={'page_label': '2', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='947525f560903fc4e631aa81372e24c733ddd6c401d5a258ba532d250b93b908', text='Al\\nc\\nc11 +Fl\\nc Al\\ns\\n1hw +Fl\\n\\nchwF\\n +\\nchwFgl\\nAg\\nc\\nccFg\\ncAg\\ns\\nhwhw +Fg\\nwl\\nw\\nwg\\nchannel attention spatial attentionfusionlocal attention\\nglobal attention\\nFigure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\\ntion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\\n(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\\nc),\\nlocal spatial (Al\\ns),global channel (Ag\\nc) and global spatial (Ag\\ns). The input feature map Fis weighted into local ( Fl) and\\nglobal ( Fg) attention feature maps, which are fused with Fto yield the global-local attention feature map Fgl. The diagram\\nis abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\\n2. Related work\\nInstance-level image retrieval Studies on instance-level\\nimage retrieval can be roughly, but not exclusively, di-\\nvided into three types: (1) studies on global descriptors\\n[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\\ngeometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\\nby graph-based methods [11, 21, 55]. The rst two types\\nof studies focus on the feature representation, while the last\\ntype focuses on re-ranking extracted features.\\nStudies on global descriptors focus on spatial pooling\\nof CNN feature maps into vectors, including MAC [38],\\nSPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\\nand NetVLAD [1, 25], as well as learning the representa-\\ntion[3, 15, 16, 36, 37]. Studies before deep learning dom-\\ninated image retrieval were mostly based on local descrip-\\ntorslike SIFT [27] and bag-of-words representation [32] or\\naggregated descriptors like VLAD [22] or ASMK [46]. Lo-\\ncal descriptors have been revived in deep learning, e.g. with\\nDELF [29], DELG [5] and ASMK extensions [45, 47].\\nWe focus on learning a global descriptor in this work, be-\\ncause it is the most efcient in terms of storage and search.\\nHowever, our generic attention mechanism produces a fea-\\nture tensor and could be applicable to local descriptors as\\nwell, if global pooling were replaced by local feature detec-\\ntion. Re-ranking methods are complementary to the repre-\\nsentation and we do not consider them in this work.\\nAttention Attention mechanisms have been rst proposed\\ninimage classication studies focusing on channel at-METHODLOCAL GLOBALLRNRET\\nSpatial Channel Spatial Channel\\nSENet [20]  \\nECA-Net [51]  \\nGCNet [6]  \\nCBAM [54]   \\nGE [19]  \\nNL-Net [52]  \\nAA-Net [4]  \\nSAN [59]  \\nN3Net [34]  \\nA2-Net [9]  \\nGSoP [14]  \\nOnA [23]  \\nAGeM [17]  \\nCroW [24]   \\nCRN [25]   \\nDELF [29]   \\nDELG [5]   \\nTolias et al. [47]   \\nSOLAR [28]   \\nOurs      \\nTable 1: Related work on attention. LRN: learned; RET: ap-\\nplied to instance-level image retrieval.\\ntention [20, 51, 6], spatial attention [19] or both, like\\nCBAM [54]. In image retrieval , CroW [24] also employs\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e73e0a23-1343-4d73-9ff3-29d676927b37', embedding=None, metadata={'page_label': '3', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='ad3da50b8030ddbab40d085b8831e59c8801e6eebe7dec86b0e0afc9edc8f3ac', text='feature map\\nGAP\\nconv1d( k)\\nsigmoid\\nattention mapchw\\nc11\\nc11F\\nAl\\nc\\nFigure 2: Local channel attention.\\nboth spatial and channel attention and can be seen as a pre-\\ncursor of CBAM, but, like other studies of spatial attention\\non retrieval [41, 23, 17], it is not learned. CRN [25] ap-\\nplies spatial attention for feature reweighting and is learned.\\nLearned spatial attention mechanisms are common for local\\ndescriptors [29, 5, 47].\\nWe call the above methods local attention , in the sense\\nthat elements of the feature tensor (channels / spatial loca-\\ntions), are weighted independently, based on contextual in-\\nformation obtained by pooling or learned. By constrast, by\\nglobal attention we refer to mechanisms that model inter-\\naction between elements of the feature tensor, for example\\nbetween channels or between locations.\\nInimage classication ,non-local neural network (NL-\\nNet) [52] is maybe the rst global attention mechanism, fol-\\nlowed by similar studies [4, 59, 34]. It is global spatial at-\\ntention , allowing interaction between any pair of spatial lo-\\ncations. Similarly, there are studies of global channel atten-\\ntion, allowing interaction between channels [9, 14]. Global\\nattention has focused mostly on image recognition and has\\nbeen applied to either spatial or channel attention so far, not\\nboth. In image retrieval , SOLAR [28] is a direct application\\nof the global spatial attention mechanism of [52].\\nTable 1 attempts to categorize related work on atten-\\ntion according to whether attention is local or global, spa-\\ntial or channel, whether it is learned and whether it is ap-\\nplied to instance-level image retrieval. We observe that all\\nmethods limit to one or two forms of attention only. Of\\nthose studies that focus on image retrieval, many are not\\nlearned [23, 17, 24], and of those that are, some are de-\\nsigned for local descriptors [29, 47].\\nBy contrast, we provide a comprehensive study of all\\nforms of attention, global and local, spatial and channel, to\\nobtain a learned representation in the form of a tensor that\\ncan be used in any way. We spatially pool it into a global\\ndescriptor and we study the relative gain of different forms\\nof attention in image retrieval.feature map\\nconv11\\nconv33 conv55 conv77\\nconcat\\nconv11\\nattention mapchw\\n4chw\\n1hwchw\\ndilated\\nconvF\\nF\\nAl\\ns\\nFigure 3: Local spatial attention. Convolutional layers in\\nblue implemented by dilated convolutions with kernel size\\n33and dilation factors 1,3,5.\\n3. Global-local attention\\nWe design a global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network. Figure 1\\nillustrates its main components. We are given a chw\\nfeature tensor F, where cis the number of channels, and\\nhwis the spatial resolution. Local attention collects con-\\ntext from the image and applies pooling to obtain a c11\\nlocal channel attention map Al\\ncand a 1hwlocal spa-\\ntial attention map Al\\ns. Global attention allows interaction\\nbetween channels, resulting in a ccglobal channel at-\\ntention map Ag\\nc, and between spatial locations, resulting in\\nahwhwglobal spatial attention map Ag\\ns. The feature\\nmaps produced by the two attention streams are combined\\nwith the original one by a learned fusion mechanism into\\ntheglobal-local attention feature map Fglbefore being spa-\\ntially pooled into a global image descriptor.\\n3.1. Local attention\\nWe extract an 1D channel and a 2D spatial attention map\\nto weigh the feature map in the corresponding dimensions.\\nLocal channel attention Following ECA-Net [51], this\\nattention captures local channel information. As shown in\\nFigure 2, we are given a chwfeature tensor Ffrom our\\nbackbone. We rst reduce it to a c11tensor by global\\naverage pooling (GAP). Channel attention is then captured\\nby a 1D convolution of kernel size kalong the channel di-\\nmension, where kcontrols the extent of cross-channel inter-\\naction. This is followed by a sigmoid function, resulting in\\nthec11local channel attention map Al\\nc.\\nLocal spatial attention Inspired by the inception mod-\\nule [43] and similar to [25], this attention map captures local\\nspatial information at different scales. As shown in Figure 3,\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aeb904bc-80ca-467c-8758-e075cfb5c3c2', embedding=None, metadata={'page_label': '4', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='b4c4c24bb228e289f05db71639bdd907366bfe6372ef45e0e1ade4715c69c5c7', text='feature map GAP\\nconv1d( k) conv1d( k)\\nsigmoid sigmoid\\n\\n softmax\\nattention feature map1c 1c\\n1c Qc\\ncchwc Vc\\nAg\\nc\\nchw1c\\n1c KcF\\nGc\\nFigure 4: Global channel attention.\\ngiven the same chwfeature tensor Ffrom our back-\\nbone, we obtain a new tensor Fwith channels reduced to\\nc, using a 11convolution. We then extract local spatial\\ncontextual information using convolutional lters of kernel\\nsize33,55, and 77, which are efciently imple-\\nmented by 33dilated convolutions [7, 57] with dilation\\nparameter 1, 2, and 3 respectively. The resulting features,\\nalong with one obtained by 11convolution on F, are\\nconcatenated into a 4chwtensor. Finally, we obtain\\nthe1hwlocal spatial attention map Al\\nsby a 11\\nconvolution that reduces the channel dimension to 1.\\nThe middle column of Figure 6 shows heat maps of local\\nspatial attention, localizing target objects in images.\\nLocal attention feature map We use the local channel\\nattention map Al\\ncto weigh Fin the channel dimension\\nFl\\nc:=FAl\\nc+F. (1)\\nWe then use local spatial attention map Al\\nsto weigh Fl\\nc\\nin the spatial dimensions, resulting in the chwlocal\\nattention feature map\\nFl=Fl\\ncAl\\ns+Fl\\nc. (2)\\nHere,ABdenotes an element-wise multiplication of ten-\\nsorsAandB, with broadcasting when one tensor is smaller.\\nWe adopt the choice of applying channel followed by spa-\\ntial attention from convolutional block attention module\\nCBAM [54]. However, apart from computing Al\\nsat differ-\\nent scales, both attention maps are obtained from the orig-\\ninal tensor Frather than sequentially. In addition, both (1)\\nand (2) include residual connections, while CBAM includes\\na single residual connection over both steps.\\n3.2. Global attention\\nWe extract two matrices capturing global pairwise chan-\\nnel and spatial interaction to weigh the feature map.feature map\\nconv11 conv11 conv11\\n\\n softmax\\nconv11\\nattention feature mapchw Qs\\nhwhw\\nchwchw Vs\\nchwAg\\nschw\\nchw KcF\\nGs\\nFigure 5: Global spatial attention.\\nGlobal channel attention We introduce a global channel\\nattention mechanism that captures global channel interac-\\ntion. This mechanism is based on the non-local neural net-\\nwork [52], but with the idea of 1D convolution from ECA-\\nNet [51]. As shown in Figure 4, we are given the chw\\nfeature tensor Ffrom our backbone. We apply GAP and\\nsqueeze spatial dimensions, followed by a 1D convolution\\nof kernel size kand a sigmoid function, to obtain 1cquery\\nQcandkeyKctensors. The value tensorVcis obtained by\\nmere reshaping of Ftohwc, without GAP. Next, we form\\nthe outer product of KcandQc, followed by softmax over\\nchannels to obtain a ccglobal channel attention map\\nAg\\nc= softmax( KcQc). (3)\\nFinally, this attention map is multiplied with Vcand the ma-\\ntrix product VcAg\\ncis reshaped back to chwto give the\\nglobal channel attention feature map Gc. In GSoP [14] and\\nA2-Net [9], a ccglobal channel attention map is obtained\\nby multiplication of hwcmatrices; (3) is more efcient,\\nusing only an outer product of 1cvectors.\\nGlobal spatial attention Since ordinary convolution ap-\\nplies only a local neighborhood at a time, it cannot capture\\nglobal contextual information. Thus, we apply non-local l-\\ntering [52], which is a form of self-attention [49] in the spa-\\ntial dimensions. As shown in Figure 5, we are given the\\nsame chwfeature tensor Ffrom our backbone. By\\nusing three 11convolutions, which reduce channels to c,\\nand attening spatial dimensions to hw, we obtain chw\\nqueryQs,keyKs, and valueVstensors, where each col-\\numn is a feature vector corresponding to a particular spatial\\nlocation. We capture pairwise similarities of these vectors\\nby matrix multiplication of KsandQs, followed by soft-\\nmax over locations to obtain a hwhwglobal spatial at-\\ntention map :\\nAg\\ns= softmax( K\\nsQs). (4)\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3f1e825b-6f63-4fad-a422-da353909ef9c', embedding=None, metadata={'page_label': '5', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='f3b5c3aed2a4779ebaa4f097fa401a46883112875b71c0e541c37b3252f5b173', text='This attention map is multiplied with Vsand the matrix\\nproduct VsAg\\nsis reshaped back to chwby expanding\\nthe spatial dimensions. Finally, using a 11convolution,\\nwhich increases channels back to c, we obtain the chw\\nglobal spatial attention feature map Gs.\\nThe right column of Figure 6 shows heat maps for global\\nspatial attention, localizing target objects in images.\\nGlobal attention feature map We use the global channel\\nattention feature map Fcto weigh Felement-wise\\nFg\\nc=FGc. (5)\\nWe then use global spatial attention feature map Gsto\\nweighFg\\ncelement-wise, resulting in the chwglobal\\nattention feature map\\nFg=Fg\\ncGs+Fg\\nc. (6)\\nSimilarly to Flin (1) and (2), we apply channel attention\\nrst, followed by spatial attention. However, unlike (1),\\nthere is no residual connection in (5). This choice is sup-\\nported by early experiments.\\n3.3. Global-local attention\\nFeature fusion As shown in Figure 1, we combine the\\nlocal and global attention feature maps, FlandFg, with\\nthe original feature F. While concatenation and summation\\nare common operations for feature combination, we use a\\nweighted average with weights wl,wg,wrespectively, ob-\\ntained by softmax over three learnable scalar parameters, to\\nobtain a chwglobal-local attention feature map\\nFgl=wlFl+wgFl+wF. (7)\\nEfcientDet [44] has shown that this is the most effective,\\namong a number of choices, for fusion of features across\\ndifferent scales.\\nPooling We apply GeM [37], a learnable spatial pooling\\nmechanism, to feature map Fgl(7), followed by a fully-\\nconnected (FC) layer with dropout and batch normalization.\\nThe nal embedding is obtained by 2-normalization.\\n4. Experiments\\n4.1. Datasets\\nTraining set There are a number of open landmark\\ndatasets commonly used for training in image retrieval stud-\\nies, including neural code (NC) [3], neural code clean (NC-\\nclean) [16], as well as Google Landmarks v1 (GLDv1) [29]\\nand v2 (GLDv2) [53]. Table 2 shows relevant statistics.\\nThese datasets can be categorized into noisy and clean. The\\nclean sets were obtained from the original noisy sets for\\nmore effective training [16, 53]. The original noisy datasets\\nare much larger, but they have high intra-class variability.\\n(a) input (b) local (c) global\\nFigure 6: Local and global spatial attention . Left: input\\nimages. Middle: local spatial attention heat maps. Right:\\nglobal spatial attention heat maps. Red (blue) means higher\\n(lower) attention weight.\\nEach class can include visually dissimilar images such as\\nexterior and interior views of a building or landmark, in-\\ncluding oor plans and paintings inside. The clean datasets\\nfocus on views directly relevant to landmark recognition but\\nhave a much smaller number of images.\\nEvaluation set and metrics We use four common eval-\\nuation datasets for landmark image retrieval: Oxford5k\\n(Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox-\\nford (ROxford orROxf) and Paris (RParis orRPar) [35].\\nROxford andRParis are used with and without one million\\ndistractors (R1M) [28] and evaluated using the Medium and\\nHard protocols [35]. We evaluate using mean Average Pre-\\ncision (mAP) and mean precision at 10 (mP@10).\\n4.2. Implementation details\\nWe train on 8 TITAN RTX 2080Ti GPUs. All models are\\npre-trained on ImageNet [39] and implemented in PyTorch\\n[31]. For fair comparisons, we set a training environment\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='adc89e72-4033-44fa-82b7-aa2b1f0885e4', embedding=None, metadata={'page_label': '6', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='515990f84628399fa7a265e73dfcd82cdd898ac29c5ad47dd58c833e141ba991', text='Figure 7: Examples of our ranking results. In each row, the rst image on the left (pink dotted outline) is a query image with a\\ntarget object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images\\nfor the query; red solid outline: negative.\\nsimilar to the those of compared studies [56, 53, 28, 35]. We\\nemploy ResNet101 [18] as a backbone model. The kernel\\nsizekof ECANet in subsection 3.1 is set to 3. The param-\\neterpof GeM in subsection 3.3 is set to 3 and the dimen-\\nsiondof nal embeddings to 512. We adopt ArcFace [10],\\na cosine-softmax based loss, with a margin of 0.3. We use\\nstochastic gradient descent with initial learning rate 103,\\nmomentum 0.9 and weight decay 105.\\nWe adopt the batch sampling of Yokoo et al. [56] where\\nmini-batch samples with similar aspect ratios are resized to\\na particular size. Here, we use a batch size of 64. For image\\naugmentation, we apply scaling, random cropping, and var-\\nied illumination. At inference, we apply a multi-resolution\\nrepresentation [16] to query and database images.\\nOur method is denoted as GLAM ( global-local atten-\\ntion module ). Using the backbone model alone is referred\\nto as baseline . It is compatible with recent models based\\non ResNet101-GeM trained with ArcFace [53, 28]. Adding\\nour local attention (subsection 3.1) to the baseline model is\\ndenoted +local , while adding our global attention (subsec-\\ntion 3.2) is denoted +global . Since we focus on representa-\\ntion learning, we do not consider post-processing methods\\nlike geometry-based re-ranking [29, 40, 53] or graph-based\\nre-ranking [11, 21, 55].\\n4.3. Benchmarking\\nNoisy vs. clean training sets We begin by training our\\nbest model (baseline+local+global) on all training sets of\\nTable 2, except NC-noisy because some images are cur-\\nrently unavailable. As shown in Table 3, even thoughTRAIN SET #IMAGES #CLASSES\\nNC-noisy 213,678 672\\nNC-clean 27,965 581\\nSfM-120k 117,369 713\\nGLDv1-noisy 1,225,029 14, 951\\nGLDv2-noisy 4,132,914 203,094\\nGLDv2-clean 1,580,470 81,313\\nTable 2: Statistics of different training sets.\\nMETHOD TRAIN SET DIM OXF5KPAR6KRMEDIUMRHARD\\nROxfRParROxfRPar\\nGeM-Siamese [37, 35] SfM-120k 2048 87.8 92.7 64.7 77.2 38.5 56.3\\nSOLAR [28] GLDv1-noisy 2048   69.9 81.6 47.9 64.5\\nGLDv2 [53] GLDv2-clean 2048   74.2 84.9 51.6 70.3\\nGLAM (Ours) NC-clean 512 77.8 85.8 51.6 68.1 20.9 44.7\\nGLDv1-noisy 512 92.8 95.0 73.7 83.5 49.8 69.4\\nGLDv2-noisy 512 93.3 95.3 75.7 86.0 53.1 73.8\\nGLDv2-clean 512 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 3: mAP comparison of our best model (base-\\nline+local+global) trained on different training sets against\\n[53, 28]. All models use ResNet101-GeM. Red: best results.\\nBlue: GLAM higher than SOLAR [28] on GLDv1-noisy.\\nGLDv2-noisy has 2.6 times more images than GLDv2-\\nclean, the latter is superior by a large margin. This shows\\nthat, in training, a cleaner dataset can be more important\\nthan a larger one. By contrast, NC-clean has the worst\\nperformance despite being clean, aparently because it is\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ff21aef7-f3ab-4e3a-a66e-e287be801912', embedding=None, metadata={'page_label': '7', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='84fff119eb186832bfb6e7159c22c2e0684e3988253fd5fb4ba5c278baa1f6fb', text='METHOD TRAIN SET DIMBASE MEDIUM HARD\\nOx5k Par6kROxf +R1MRPar +R1MROxf +R1MRPar +R1M\\nmAP mAP mAP mP mAP mP mAP mP mAP mP mAP mP mAP mP mAP mP mAP mP\\nSPoC-V16 [2, 35] [O] 512 53.1 38.0 54.6 17.1 33.3 59.8 93.0 30.3 83.0 11.4 20.9 0.9 2.9 32.4 69.7 7.6 30.6\\nSPoC-R101 [35] [O] 2048   39.8 61.0 21.5 40.4 69.2 96.7 41.6 92.0 12.4 23.8 2.8 5.6 44.7 78.0 15.3 54.4\\nCroW-V16 [24, 35] [O] 512 70.8 79.7 41.4 58.8 22.5 40.5 62.9 94.4 34.1 87.1 13.9 25.7 3.0 6.6 36.9 77.9 10.3 45.1\\nCroW-R101 [35] [O] 2048   42.4 61.9 21.2 39.4 70.4 97.1 42.7 92.9 13.3 27.7 3.3 9.3 47.2 83.6 16.3 61.6\\nMAC-V16-Siamese [36, 35] [O] 512 80.0 82.9 37.8 57.8 21.8 39.7 59.2 93.3 33.6 87.1 14.6 27.0 7.4 11.9 35.9 78.4 13.2 54.7\\nMAC-R101-Siamese [35] [O] 2048   41.7 65.0 24.2 43.7 66.2 96.4 40.8 93.0 18.0 32.9 5.7 14.4 44.1 86.3 18.2 67.7\\nRMAC-V16-Siamese [36, 35] [O] 512 80.1 85.0 42.5 62.8 21.7 40.3 66.2 95.4 39.9 88.9 12.0 26.1 1.7 5.8 40.9 77.1 14.8 54.0\\nRMAC-R101-Siamese [35] [O] 2048   49.8 68.9 29.2 48.9 74.0 97.7 49.3 93.7 18.5 32.2 4.5 13.0 52.1 87.1 21.3 67.4\\nRMAC-R101-Triplet [16, 35] NC-clean 2048 86.1 94.5 60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0\\nGeM-R101-Siamese [37, 35] SfM-120k 2048 87.8 92.7 64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3\\nAGeM-R101-Siamese [17] SfM-120k 2048   67.0    78.1    40.7    57.3   \\nSOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048   69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6\\nDELG-GeM-R101-ArcFace [5] GLDv1-noisy 2048   73.2  54.8  82.4  61.8  51.2  30.3  64.7  35.5 \\nGeM-R101-ArcFace [53] GLDv2-clean 2048   74.2    84.9    51.6    70.3   \\nGLAM-GeM-R101-ArcFace baseline GLDv2-clean 512 91.9 94.5 72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7\\n+local GLDv2-clean 512 91.2 95.4 73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1\\n+global GLDv2-clean 512 92.3 95.3 77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0\\n+global+local GLDv2-clean 512 94.2 95.6 78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0\\nTable 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16:\\nVGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet).: dimension d= 256 [2]. mP: mP@10. Red:\\nbest results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only\\nmodel other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\\ntoo small. To achieve best possible performance, we use\\nGLDv2-clean as a training set in the remaining experiments.\\nComparisons on same training set It is common to com-\\npare methods regardless of training sets as more become\\navailable, e.g., [35, 28]. Since GLDv2-clean is relatively\\nnew, Weyand et al. [53], which introduced the dataset, is the\\nonly study that has trained the same backbone with the same\\nsettings (ResNet101-GeM with ArcFace) on GLDv2-clean.\\nOur baseline is lower than [53], because our dimensinality is\\n512, while other models based on ResNet101 use 2048. Yet,\\nTable 3 shows that our best model trained on GLDv2-clean\\noutperforms [53] by a large margin. But the most impor-\\ntant comparison is with SOLAR [28], also based on self-\\nattention, which has trained ResNet101-GeM on GLDv1-\\nnoisy. On this training set, our best model clearly outper-\\nforms [28] despite lower dimensionality.\\nComparison with state of the art Table 4 shows the\\nperformance of four variants of our model, i.e. baseline\\nwith or without local/global attention, and compares them\\nagainst state-of-the-art (SOTA) methods based on global de-\\nscriptors without re-ranking on the complete set of bench-\\nmarks, including distractors. Both local and global atten-\\ntion bring signicant gain over the baseline. The effect\\nof global is stronger, while the gain of the two is addi-\\ntive in the combination. The best results are achieved by\\nthe global-local attention network (baseline+global+local).\\nWith this model, we outperform previous best methods\\non most benchmarks except mP@10 on RParis (medium)\\nandRParis+R1M (medium), where we are outperformed\\nby [37, 35]. These results demonstrate that our approach is\\neffective for landmark image retrieval. Figure 7 shows someMETHOD OXF5KPAR6KRMEDIUM RHARD\\nROxfRParROxfRPar\\nGLAM baseline 91.9 94.5 72.8 84.2 49.9 69.7\\n+local-channel 91.3 95.3 72.2 85.8 48.3 73.1\\n+local-spatial 91.0 95.1 72.1 85.3 48.3 71.9\\n+local 91.2 95.4 73.7 86.5 52.6 75.0\\n+global-channel 92.5 94.4 73.3 84.4 49.8 70.1\\n+global-spatial 92.4 95.1 73.2 86.3 50.0 72.7\\n+global 92.3 95.3 77.2 86.7 57.4 75.0\\n+global+local 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 5: mAP comparison of spatial and channel variants\\nof our local (+local, subsection 3.1) and global (+global,\\nsubsection 3.1) attention modules to the baseline.\\nMETHOD OXF5KPAR6KRMEDIUM RHARD\\nROxfRParROxfRPar\\nCBAM style 93.8 95.7 75.6 88.4 53.3 76.8\\nGLAM (Ours) 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 6: mAP comparison between CBAM style and our\\nlocal spatial attention.\\nexamples of our ranking results.\\n4.4. Ablation study\\nOur ablation study uses the Google Landmark v2 clean\\ndataset (GLDv2-clean) [53] for training, which is shown to\\nbe the most effective in Table 3.\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d6414640-a245-4e09-98fe-1ae714bd982b', embedding=None, metadata={'page_label': '8', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='45d0acb29e345bbd285918e3d9358b91be9b78e2272755f7a73f248d8b749e2c', text='METHOD OXF5KPAR6KRMEDIUM RHARD\\nROxfRParROxfRPar\\nConcatenate 89.5 95.1 73.6 86.5 54.0 73.7\\nSum (Ours) 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 7: mAP comparison between weighted concatenation\\nand weighted average for feature fusion.\\nMETHOD OXF5KPAR6KRMEDIUM RHARD\\nROxfRParROxfRPar\\nFixed-size 76.1 82.6 55.7 68.4 29.2 47.5\\nGroup-size (Ours) 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 8: mAP comparison between xed-size ( 224224)\\nand group-size sampling methods.\\nQUERY DATABASE OXF5KPAR6KRMEDIUM RHARD\\nROxfRParROxfRPar\\nSingle Single 93.3 95.2 76.9 87.1 58.6 74.7\\nMulti Single 93.9 95.4 78.0 87.7 59.0 75.5\\nSingle Multi 93.6 95.6 77.0 87.8 57.1 76.0\\nMulti Multi 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 9: mAP comparison of using multiresolution repre-\\nsentation (Multi) or not (Single) on query or database.\\nEffect of attention modules We ablate the effect of our\\nlocal and global attention networks as well as their com-\\nbination. Table 5 shows the results, which are more ne-\\ngrained than those of Table 4. In particular, it shows the ef-\\nfect of the channel and spatial variants of both local and\\nglobal attention. We observe that, when used alone, the\\nchannel and spatial variants of local attention are harmful\\nin most cases. Even the combination, baseline+local, is not\\nalways effective. By contrast, when used alone, the channel\\nand spatial variants of global attention are mostly benecial,\\nespecially the latter. Their combination, baseline+global, is\\nimpressive, bringing gain of up to 7.5%. Importantly, the\\ncombination baseline+global+local improves further by up\\nto another 2.8%. This result shows the necessity of local\\nattention in the nal model.\\nCBAM vs. our local spatial attention We experiment\\nwith the local spatial attention of CBAM [54]. CBAM ap-\\nplies average and max-pooling to input features and con-\\ncatenates the two for spatial attention. We apply this vari-\\nant to our local spatial attention module for comparison.\\nFor the CBAM style module, we keep the overall design\\nof our module as shown in Figure 3, but apply average and\\nmax-pooling to each of the four convolutional layer outputs\\nbefore concatenation. Table 6 shows that the CBAM stylemodule is considerably worse than ours on all benchmarks\\nexcept Paris6k, where it is only slightly better.\\nConcatenation vs. sum for feature fusion We use a\\nsoftmax-based weighted average of local and global atten-\\ntion feature maps with the original feature map (7). Here,\\nwe compare this weighted average with weighted concate-\\nnation, where concatenation replaces the sum operation\\nin (7). As shown in Table 7, the weighted average outper-\\nforms the weighted concatenation.\\nFixed-size vs. group-size sampling Numerous studies\\nhave proposed methods for constructing batches according\\nto image size for efcient training. For instance, Gordo et\\nal. [16], DELF [29], and Yokoo et al. [56] employed dif-\\nferent image sizes per batch for training instead of a single\\nxed size. We adopt the method of Yokoo et al., which con-\\nstructs a batch with images of similar aspect ratio, so that\\nthe images can be resized to a size with an aspect ratio that\\nis similar to their own. We call this method group-size sam-\\npling . Table 8 compares xed-size ( 224224) with group-\\nsize sampling. We observe that maintaining aspect ratios by\\nusing dynamic input sizes is much more effective.\\nMulti-resolution We use the multi-resolution representa-\\ntion [16] for the nal feature of an image at inference time.\\nThis method: (1) resizes an image into multiple scales; (2)\\nextracts features from the resized images; and (3) averages\\nthe features to obtain the nal feature of the image. The\\nmethod is applied to both query and database images to en-\\nhance ranking results, especially for small target objects.\\nTable 9 compares the four cases of applying this method or\\nnot to query or database images.\\n5. Conclusion\\nWe have introduced a novel approach that extracts global\\nand local contextual information using attention mecha-\\nnisms for instance-level image retrieval. It is manifested as\\na network architecture consisting of global and local atten-\\ntion components, each operating on both spatial and chan-\\nnel dimensions. This constitutes a comprehensive study and\\nempirical evaluation of all four forms of attention that have\\npreviously been studied only in isolation. Our ndings indi-\\ncate that the gain (or loss) brought by one form of attention\\nalone strongly depends on the presence of the others, with\\nthe maximum gain appearing when all forms are present.\\nThe output is a modied feature tensor that can be used in\\nany way, for instance with local feature detection instead of\\nspatial pooling for image retrieval.\\nWith the advent of vision transformers [12, 58] and their\\nrecent application to image retrieval [13], attention is ex-\\npected to play a more and more signicant role in vi-\\nsion. According to our classication, transformers perform\\nglobal spatial attention alone. It is of great interest to in-\\nvestigate the role of the other forms of attention, where our\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='213dea60-e4c4-4ac8-bd94-cd3360c4ba9a', embedding=None, metadata={'page_label': '9', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='325ca1cfae7c16f17deb140ce20f9e5cbdde948b602697534115c2aa64ac0872', text='approach may yield a basic building block of such archi-\\ntectures. One may even envision an extension to language\\nmodels, where transformers originate from [50].\\nReferences\\n[1] Relja Arandjelovi c, Petr Gronat, Akihiko Torii, Tomas Pa-\\njdla, and Josef Sivic. NetVLAD: CNN architecture for\\nweakly supervised place recognition. In CVPR , 2016. 2\\n[2] Artem Babenko and Victor Lempitsky. Aggregating Local\\nDeep Features for Image Retrieval. In ICCV , 2015. 1, 2, 7\\n[3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and\\nVictor Lempitsky. Neural Codes for Image Retrieval. In\\nECCV , 2014. 1, 2, 5\\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\\nand Quoc V . Le. Attention augmented convolutional net-\\nworks. In ICCV , 2019. 2, 3\\n[5] Bingyi Cao, Andr e Araujo, and Jack Sim. Unifying deep\\nlocal and global features for image search. In ECCV , 2020.\\n2, 3, 7\\n[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.\\nGCNet: Non-Local Networks Meet Squeeze-Excitation Net-\\nworks and Beyond. In ICCV , 2019. 2\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for seman-\\ntic image segmentation. arXiv preprint arXiv:1706.05587 ,\\n2017. 4\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In ICML , 2020. 1\\n[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. A2-nets: Double attention networks.\\nInNeurIPS , 2018. 1, 2, 3, 4\\n[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. ArcFace: Additive Angular Margin Loss for Deep\\nFace Recognition. In CVPR , 2019. 6\\n[11] Michael Donoser and Horst Bischof. Diffusion Processes for\\nRetrieval Revisited. In CVPR , 2013. 1, 2, 6\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 , 2020. 8\\n[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\\nHerv e Jegou. Training vision transformers for image re-\\ntrieval. Technical report, 2021. 8\\n[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\\nsecond-order pooling convolutional networks. In CVPR ,\\n2019. 2, 3, 4\\n[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. Deep image retrieval: Learning global representations\\nfor image search. In ECCV , 2016. 2\\n[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. End-to-end learning of deep visual representations for\\nimage retrieval. IJCV , 2017. 1, 2, 5, 6, 7, 8[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie. Attention-\\naware generalized mean pooling for image retrieval. arXiv\\npreprint arXiv:1811.00202 , 2018. 2, 3, 7\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In CVPR ,\\n2016. 6\\n[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in con-\\nvolutional neural networks. In NeurIPS , 2018. 2\\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\\nSqueeze-and-Excitation Networks. In CVPR , 2018. 1, 2\\n[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,\\nand Ondrej Chum. Efcient diffusion on region manifolds:\\nRecovering small objects with compact cnn representations.\\nInCVPR , 2017. 2, 6\\n[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into com-\\npact codes. PAMI , (99):11, 2011. 2\\n[23] Albert Jimenez, Jose M. Alvarez, and Xavier Gir o-i-Nieto.\\nClass weighted convolutional features for visual instance\\nsearch. In BMVC , 2017. 2, 3\\n[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.\\nCrossdimensional weighting for aggregated deep convolu-\\ntional features. In ECCV , 2016. 1, 2, 3, 7\\n[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.\\nLearned Contextual Feature Reweighting for Image Geo-\\nLocalization. In CVPR , 2017. 1, 2, 3\\n[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\\nProxy anchor loss for deep metric learning. In CVPR , 2020.\\n1\\n[27] David G. Lowe. Distinctive image features from scale-\\ninvariant keypoints. In IJCV , 2004. 1, 2\\n[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian\\nMikolajczyk. SOLAR: Second-Order Loss and Attention for\\nImage Retrieval. In ECCV , 2020. 1, 2, 3, 5, 6, 7\\n[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,\\nand Bohyung Han. Large Scale Image Retrieval with Atten-\\ntive Deep Local Features. In ICCV , 2017. 1, 2, 3, 5, 6, 8\\n[30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\\nSavarese. Deep metric learning via lifted structured feature\\nembedding. In CVPR , 2016. 1\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas K opf, Edward Yang, Zach DeVito, Martin Raison,\\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\\nperative style, high-performance deep learning. In NeurIPS ,\\n2019. 5\\n[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Object retrieval with large vocabularies\\nand fast spatial matching. In CVPR , 2007. 2, 5\\n[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Lost in quantization:Improving particu-\\nlar object retrieval in large scale image databases. In CVPR ,\\n2008. 5\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f220815a-8b89-44c9-b18d-8143c4fcb46c', embedding=None, metadata={'page_label': '10', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='ec19fa8f62a85d22e9c18627d56cc67ac015b1d45a9806d68b0d54579c3f7d89', text='[34] Tobias Pl otz and Stefan Roth. Neural nearest neighbors net-\\nworks. In NeurIPS , 2018. 2, 3\\n[35] Filip Radenovi c, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ond rej Chum. Revisiting Oxford and Paris:\\nLarge-Scale Image Retrieval Benchmarking. In CVPR , 2018.\\n5, 6, 7\\n[36] Filip Radenovi c, Giorgos Tolias, and Ond rej Chum. CNN\\nimage retrieval learns from BoW: Unsupervised ne-tuning\\nwith hard examples. In ECCV , 2016. 2, 7\\n[37] Filip Radenovi c, Giorgos Tolias, and Ond rej Chum. Fine-\\nTuning CNN Image Retrieval with No Human Annotation.\\nInTPAMI , 2019. 1, 2, 5, 6, 7\\n[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson,\\nand Atsuto Maki. Visual Instance Retrieval with Deep Con-\\nvolutional Networks. In CoRR , 2015. 2\\n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge. In International booktitle of Computer Vision , 2015.\\n5\\n[40] Oriane Sim eoni, Yannis Avrithis, and Ondrej Chum. Local\\nfeatures and visual words emerge in activations. In CVPR ,\\n2019. 2, 6\\n[41] O. Sim eoni, A. Iscen, G. Tolias, Y . Avrithis, and O. Chum.\\nGraph-based particular object discovery. Machine Vision and\\nApplications , 30(2):243254, 3 2019. 3\\n[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\\nical networks for few-shot learning. In NeurIPS , 2017. 1\\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\nconvolutions. In CVPR , 2015. 3\\n[44] Mingxing Tan, Ruoming Pang, and Quoc V . Le. EfcientDet:\\nScalable and Efcient Object Detection. In CVPR , 2020. 5\\n[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack\\nSim. Detect-to-retrieve: Efcient regional aggregation for\\nimage search. In CVPR , 2019. 2\\n[46] Giorgios Tolias, Yannis Avrithis, and Herv e Jegou. To aggre-\\ngate or not to aggregate: Selective match kernels for image\\nsearch. In ICCV , 2013. 2\\n[47] Giorgos Tolias, Tomas Jenicek, and Ond rej Chum. Learn-\\ning and aggregating deep local descriptors for instance-level\\nrecognition. In ECCV , 2020. 2, 3\\n[48] Giorgos Tolias, Ronan Sicre, and Herv e Jegou. Particular ob-\\nject retrieval with integral max-pooling of CNN activations.\\nInICLR , 2016. 2\\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 4\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 9\\n[51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\\nmeng Zuo, and Qinghua Hu. ECA-Net: Efcient Chan-\\nnel Attention for Deep Convolutional Neural Networks. In\\nCVPR , 2020. 2, 3, 4[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local Neural Networks. In CVPR , 2018. 1, 2,\\n3, 4\\n[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle Landmarks Dataset v2 - A Large-Scale Benchmark\\nfor Instance-Level Recognition and Retrieval. In CVPR ,\\n2020. 1, 2, 5, 6, 7\\n[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\\nKweon. CBAM: Convolutional Block Attention Module. In\\nECCV , 2018. 1, 2, 4, 8\\n[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and\\nShinichi Satoh. Efcient image retrieval via decoupling dif-\\nfusion into online and ofine processing. In AAAI , 2019. 2,\\n6\\n[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi\\nIizuka. Two-stage Discriminative Re-ranking for Large-scale\\nLandmark Retrieval. In arXiv:2003.11211 , 2020. 6, 8\\n[57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\\nresidual networks. In CVPR , 2017. 4\\n[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\\nto-token vit: Training vision transformers from scratch on\\nimagenet. arXiv preprint arXiv:2101.11986 , 2021. 8\\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\\nself-attention for image recognition. In CVPR , 2020. 2, 3\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c39b0faa-c5d9-4b07-aaa5-d5a284f48fff', embedding=None, metadata={'page_label': '1', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='800b98e3b638fb0634e559bd57b0c3d345db10a45e0cdc0c4113ebdcbd2a3f0e', text='OBJECT COUNTING: YOU ONLY NEED TO LOOK AT ONE\\nHui LIN, Xiaopeng HONG, Yabin WANG\\nSchool of Cyber Science and Engineering, Xian Jiaotong University, China\\nEmails: linhuixjtu@gmail.com; hongxiaopeng@ieee.org; iamwangyabin@stu.xjtu.edu.cn\\nABSTRACT\\nThis paper aims to tackle the challenging task of one-\\nshot object counting. Given an image containing novel, pre-\\nviously unseen category objects, the goal of the task is to\\ncount all instances in the desired category with only one sup-\\nporting bounding box example. To this end, we propose a\\ncounting model by which you only need to Look At One in-\\nstance (LaoNet). First, a feature correlation module combines\\nthe Self-Attention and Correlative-Attention modules to learn\\nboth inner-relations and inter-relations. It enables the network\\nto be robust to the inconsistency of rotations and sizes among\\ndifferent instances. Second, a Scale Aggregation mechanism\\nis designed to help extract features with different scale infor-\\nmation. Compared with existing few-shot counting methods,\\nLaoNet achieves state-of-the-art results while learning with a\\nhigh convergence speed. The code will be available soon.\\nIndex Terms Object Counting, One-Shot Learning, At-\\ntention Mechanism\\n1. INTRODUCTION\\nObject counting has become increasingly important due to its\\nwide range of applications such as crowd surveillance, trafc\\nmonitoring, wildlife conservation and inventory management.\\nMost of the existing counting methods [1, 2, 3] focus on a par-\\nticular, single category. However, when applying them into\\nnew categories, their performances will drop catastrophically.\\nMeanwhile, it is extremely difcult and costly to collect all\\ncategories and label them for training.\\nFor humans, the generalization ability allows them to\\nlearn and deal with various vision tasks without much prior\\nknowledge and experience. We are amazed by this remark-\\nable ability and in this work, we focus on this learning\\nparadigm and design a network to efciently recognize and\\ncount new categories given only one example. We follow the\\nfew-shot setting in [4] and modify it to one-shot object count-\\ning. That is, the model takes an image with unseen novel cate-\\ngories and a supporting bounding box containing an example\\ninstance of desired category as input, and then predicts the\\nobject count in the image.\\nHowever, there are two main challenges. First , the object\\ncounting task includes many different categories, and evenseveral categories exist within a same image. Moreover in\\nfew-shot setting, these categories will not overlap between\\ntraining and inference. This means that the model needs to\\nhave a strong distinguishing ability between features of differ-\\nent categories, and meanwhile, an effective associating abil-\\nity among instances sharing the same category. Second , in\\none-shot counting, the model learns from only one support-\\ning instance. Much of the difculty results from the fact that\\nthe supporting sample may differ from other instances in, for\\nexample, sizes and poses. Hence, the model is required to\\nbe invariant towards these variations without seeing the com-\\nmonalities across different instances.\\nTherefore, in this paper, we propose an effective network\\nnamed LaoNet for one-shot object counting. It consists of\\nthree main parts: feature extraction, feature correlation and\\nthe density regressor, as shown in Figure 1. The feature corre-\\nlation model and the feature extraction model are elaborately\\ndesigned to address the above two challenges.\\nWe propose the feature correlation based on Self-\\nAttention and Correlative-Attention modules to learn inner-\\nrelations and inter-relations respectively. The Self-Attention\\nencourages the model to focus more on important features and\\ntheir correlations, improving the efciency of information re-\\nnement. Previous few-shot counting methods [4, 5] usually\\nleverage on a convolution operation to match the similarities\\nbetween image features and supporting features. However,\\nas the kernel is derived from supporting features with the de-\\nfault size and rotation angle, the convolution operation will\\ngreatly depend on the quality of supporting features and the\\nconsistency of physical properties among different instances.\\nInstead, our designed feature correlation model benets from\\ntwo kinds of attention modules and addresses the above prob-\\nlem by considering all correlations.\\nWe further propose a Scale Aggregation mechanism in\\nscale extraction to deal with scale variations among different\\ncategories and different instances. By learning features from\\nmulti-subspace, the model aggregates various scale informa-\\ntion while maintaining a spatial consistency.\\nTo summarize, our contribution is threefold.\\n We design a novel network named LaoNet (A network\\nby which you only need to Look At One instance) for\\none-shot object counting. By combining Self-AttentionarXiv:2112.05993v1  [cs.CV]  11 Dec 2021', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='27ca9c89-1fd0-4d5d-a74f-63b83a2c985f', embedding=None, metadata={'page_label': '2', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='cc265564b7d4159a6ccec79248598cc0f4897fcc89d9056d57fa6e43b039350c', text='Fig. 1 . The overall architecture of the proposed LaoNet for one-shot object counting. Both the query image and the supporting\\nbox are fed into CNN to extract features. Supporting features are aggregated among scales. Then the atten features with unique\\nposition embedding are transmitted into feature correlation model with Self-Attentions and Correlative Attentions. Finally, a\\ndensity regressor is adopted to predict the nal density map.\\nand Correlative-Attention modules, LaoNet exploits\\nthe correlation among novel category objects with high\\naccuracy and efciency.\\n We propose a Scale Aggregation mechanism to extract\\nmore comprehensive features and fuse multi-scale in-\\nformation from the supporting box.\\n The experimental results show that our model achieves\\nstate-of-the-art results with signicant improvements\\non FSC-147 [4] and COCO [6] datasets under the one-\\nshot setting without ne-tuning.\\n2. RELATED WORKS\\nObject counting methods can be briey divided into two\\ntypes. Detection based methods [7] count the number of ob-\\njects by exhaustively detecting every target in images. But\\nthey rely on the complex labels such as bounding boxes. Re-\\ngression based methods [1, 2] learn to count by predicting a\\ndensity map, in which each value represents the density of\\ntarget objects at the corresponding location. The count pre-\\ndiction equals to the total sum of density map.\\nNevertheless, most of the counting methods are category\\nspecically, e.g. for human crowd [1, 2, 8, 9, 10, 11], for\\ncars [3, 12], for plants [13] or for cells [14, 15]. They focus\\non only one category and will loss the original satised per-\\nformance when transferring to other categories. Moreover,\\nmost traditional approaches usually rely on tens of thousands\\nof instances to train a counting model [2, 8, 9, 11, 3, 12].\\nTo reduce considerably the number of samples needed to\\ntrain a counting model for a particular category, recently, few-\\nshot counting task has been developed. The key lies in the\\ngeneralization ability of the model to deal with novel cate-\\ngories from few labeled examples. The study [16] proposes aGeneric Matching Network (GMN) for class-agnostic count-\\ning. However it still needs several dozens to hundreds exam-\\nples of a novel category for adaptation and good performance.\\nCFOCNet is introduced to match and utilize the similarity be-\\ntween objects within the same category [5]. The work [4]\\npresents a Few Shot Adaptation and Matching Network (Fam-\\nNet) to learn feature correlations and few-shot adaptation and\\nalso introduces a few-shot counting dataset named FSC-147.\\nWhen the number of labeled example decreases to one,\\nthe task evolves into one-shot counting. In other visual tasks,\\nresearchers develop methods for one-shot segmentation [17]\\nand one-shot object detection [18, 19]. Compared to the few-\\nshot setting which usually uses at least three instances for\\neach object [4], the one-shot setting, where only one instance\\nis available, is clearly more challenging.\\nIt is worth mentioning that detection based approaches\\n[20, 21, 22] are inferior for the tasks of few-shot and one-shot\\ncounting. One main reason is that it requires extra and costly\\nbounding-box annotations of allinstances in the training stage\\nwhile one-shot counting approach which we focus on depends\\non dot annotations and only onesupporting box. To illus-\\ntrate this point further, we perform experiments in Section 4.3\\nto compare with detection based approaches and validate the\\nproposed network for one-shot counting.\\n3. APPROACH\\n3.1. Problem Denition\\nOne-shot object counting consists of a training set\\n(It,st,yt)T and a query set (Iq,sq)Q, in which cate-\\ngories are mutually exclusive. Each input for the model con-\\ntains an image Iand a supporting bounding box sannotating\\none object of the desired category. In training set, abundant', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a224d8e-6d3b-41ec-8423-3f197920648e', embedding=None, metadata={'page_label': '3', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='a43ec105cbd109c9956806cf124fa18dff18d8dfbb22c38a3bc9ca82ba16b7a1', text='point annotations ytare available to supervise the model. In\\ninference stage, we aim the model to learn to count the novel\\nobjects inIqwith a supporting category instance sampled by\\nsq.\\n3.2. Feature Correlation\\nAs the model is required to learn to count from only one sup-\\nporting object, seizing the correlation between features with\\nhigh efciency is quite important. Therefore, we build the\\nfeature correlation model in our one-shot network based on\\nSelf-Attention and Correlative-Attention modules, for learn-\\ning the inner-relations and inter-relations respectively.\\nAs illustrated in Figure 1 (violet block), our Self-\\nAttention module consists of a Multi-head Attention (MA)\\nand a layer normalization (LN). We rst introduce the de-\\nnition of attention [23], given the query Q, keyKand value\\nvectorV:\\nA(Q,K,V|W) =S((QWQ)(KWK)T\\n\\nd+PE)(VWV),\\n(1)\\nwhereSis the softmax function and1\\ndis a scaling factor\\nbased on the vector dimension d.W:WQ,WK,WV\\nRddare weight matrices for projections and PEis the posi-\\ntion embedding.\\nTo leverage on more representation subspaces, we adopt\\nthe extending form with multi attention heads:\\nMA(Q,K,V ) =Concat (head 1,..,headh)WO\\nwherehead i=A(Q,K,V|Wi).(2)\\nThe representation dimensions are divided by parallel atten-\\ntion heads, where parameter matrices Wi:WQ\\ni,WK\\ni,WV\\ni\\nRdd/handWORdd.\\nOne challenging problem in counting task is the existence\\nof many complex interfering things. To efciently weaken the\\nnegative inuence by those irrelevant background, we apply\\nMulti-head Self-Attention in image features to learn inner-\\nrelations and encourage the model to focus more on repetitive\\nobjects that can be counted.\\nWe denote the feature sequences of the query image and\\nthe supporting box region as XandS, with sizes X\\nRHWCandSRhwC. And the rened query feature\\nis calculated by:\\nX=LN(MA(XQ,XK,XV) +X). (3)\\nA layer normalization (LN) is adopted to balance the value\\nscales.\\nMeanwhile, as there is only one supporting object in one-\\nshot counting problem, rening the salient features within the\\nobject is necessary and helpful for counting efciency and\\naccuracy. Therefore we apply another Self-Attention module\\nto supporting feature and get rened S.Previous few-shot counting methods [4, 5] usually adopt\\na convolution operation where the supporting features act as\\nkernels to match the similarities for target category. However,\\nthe results will greatly depend on the quality of supporting\\nfeatures and the consistency of objects properties, including\\nrotations and scales.\\nTo this end, we propose a Correlative-Attention module\\nto learn inter-relations between query and supporting features\\nand alleviate the constraints of irrelevant properties.\\nSpecically, we extend the MA by learning correlations\\nbetween different feature sequences and add a feed-forward\\nnetwork (FFN) to fuse the features, i.e.,\\nX=Corr (X,S) =G(MA(XQ,SK,SV) +X).(4)\\nGincludes two LNs and a FFN in the form of residual (light\\nblue block in Figure 1). Finally, XandSwill be fed into the\\ncycle as new feature sequences where each cycle consists of\\ntwo Self-Attention modules and a Correlative-Attention mod-\\nule.\\n3.3. Feature Extraction and Scale Aggregation\\nTo extract feature sequences from images, we use VGG-19 as\\nour backbone. For query image, the output of the nal level\\nis directly attened and transmitted into Self-Attention mod-\\nule. For the supporting box, as there are uncontrollable scale\\nvariations among instances due to the perspective, we pro-\\npose a Scale Aggregation mechanism to fuse different scale\\ninformation.\\nGivenlas the number of layers in CNN, we aggregate the\\nfeature maps among different scales:\\nS=Concat (Fl(s),Fl1(s),...,Fl+1(s)), (5)\\nwhereFirepresents a feature map at ithlevel and[1,l]\\ndecides the number of layers taken for aggregation.\\nMeanwhile, we leverage on identifying position embed-\\nding to help the model distinguish the integrated scale in-\\nformation in attention model. By adopting the xed sinu-\\nsoidal absolute position embedding [23], feature sequences\\nfrom different scales can still maintain the consistency be-\\ntween positions, i.e.,\\nPE (pos j,2i)=sin(posj/100002i/d),\\nPE (pos j,2i+1)=cos(posj/100002i/d).(6)\\niis the dimension and posjis the position for jthfeature map.\\n3.4. Training Loss\\nWe use Euclidean distance to measure the difference between\\nestimated density map and ground truth density map, which is\\ngenerated based on annotated points following [1]. The loss\\nis dened as follows:\\nLE=||DgtD||2\\n2, (7)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a5dac28-c952-428b-afa9-1f93d8e7a5cf', embedding=None, metadata={'page_label': '4', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='39347736f6f8c90b74a8c090e3e7a546812ac3957ed93bd326703ca01cd52c82', text='whereDis the estimated density map and Dgtis the ground\\ntruth density map. To improve the local pattern consistency,\\nwe also adopt a SSIM loss followed the calculation in [8]. By\\nintegrating the above two loss functions, we have\\nL=LE+LSSIM, (8)\\nwhereis the balanced weight.\\n4. EXPERIMENTS\\n4.1. Implement Details and Evaluation Metrics\\nWe design the density regressor by an upsampling layer and\\nthree convolution layers with ReLU activation. The kernel\\nsizes of rst two layers are 3  3 and that of last is 1  1. Ran-\\ndom scaling and ipping are adopted for each training image.\\nAdam [24] with a learning rate 0.5105is used to optimize\\nthe parameters. We set the number of attention heads has 4,\\nthe correlation cycle Tas 2, the number of aggregated layers\\nas 2, and the loss balanced parameter as104.\\nMean Absolute Error (MAE) and Root Mean Squared Er-\\nror (RMSE) are used to measure the performance of our meth-\\nods. They are dened by:\\nMAE =1\\nMM\\ni=1Ngt\\niNi,\\nRMSE =\\ued6a\\ued6b\\ued6b1\\nMM\\ni=1(Ngt\\niNi)2),(9)\\nwhereMandNgtare the number of images and the ground-\\ntruth count, respectively. The predicted count Nis calculated\\nby integrating the estimated density map D.\\n4.2. Datesets\\nFSC-147 [4] contains a total of 6135 images collected for\\nfew-shot counting problem. In each image, three randomly\\nselected object instances are annotated by bounding boxes\\nwhile other instances are annotated by points. 89 object cat-\\negories with 3,659 images are divided for training set. Each\\n29 categories with 1,286 and 1,190 images respectively are\\ndivided for validation and testing sets.\\nMS-COCO [6] is a large dataset widely used in object detec-\\ntion and instance segmentation. In val2017 set, there are 80\\ncommon object categories with 5,000 images in complex ev-\\neryday scenes. We follow [17] to generate four train/test splits\\nwhich each contains 60 training and 20 testing categories.\\n4.3. Comparison with Few-Shot Approaches\\nWe hold experiments on above two few-shot counting datasets\\nto evaluate the proposed network. As there are few existingMethodsVal Test\\nMAE RMSE MAE RMSE\\n3-shot\\nMean 53.38 124.53 47.55 147.67\\nMedian 48.68 129.70 47.73 152.46\\nFR detector [25] 45.45 112.53 41.64 141.04\\nFSOD detector [26] 36.36 115.00 32.53 140.65\\nGMN [16] 29.66 89.81 26.52 124.57\\nMAML [27] 25.54 79.44 24.90 112.68\\nFamNet [4] 23.75 69.07 22.08 99.54\\n1-shot\\nCFOCNet [5] 27.82 71.99 28.60 123.96\\nFamNet [4] 26.55 77.01 26.76 110.95\\nLaoNet (Ours) 17.11 56.81 15.78 97.15\\nTable 1 . Comparisons with previous state-of-the-art few-shot\\nmethods on FSC-147. The upper part of the table presents the\\nresults in 3-shot setting while the lower part presents 1-shot\\nresults. FamNet [4] uses the adaptation strategy during test-\\ning. It is worth noticing that our one-shot LaoNet outperforms\\nall of previous methods, even those in 3-shot setting, without\\nany ne-tuning strategy.\\nGT: 33 GT: 14 GT: 35\\nPre: 35 Pre: 14 Pre: 37\\nFig. 2 . Visualizations of one-shot counting inputs and cor-\\nresponding predicted density maps. The model can perform\\ngreat counting accuracy even it has never seen strawberry, hot\\nair balloon or cashew before.\\nmethods specically designed for one-shot counting, for com-\\nprehensive evaluation, we modify FamNet [4] and CFOC-\\nNet [5] for this setting and also compare with other few-shot\\ncounting approaches [25, 26, 16, 27, 17].\\nFirst, quantitative results on FSC-147 are shown in Ta-\\nble 1. We list seven results of previous few-shot detection and\\ncounting methods in 3-shot setting and two results of state-\\nof-the-art counting methods in 1-shot setting for comparison.\\nThe result of FamNet [4] uses the adaptation strategy during\\ntesting.\\nIt is worth noticing that our one-shot LaoNet outperforms\\nall of previous few-shot methods, even those in 3 shot set-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e703a8ed-feed-4b6f-b85d-76b926e7d7dc', embedding=None, metadata={'page_label': '5', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='e3243bba93030f51dbffa36c129b57f4ecff096719b5b982f4e42a27b7172e75', text='MethodsFold 0 Fold 1 Fold 2 Fold 3 Average\\nMAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE\\nSegment [17]2.91 4.20 2.47 3.67 2.64 3.79 2.82 4.09 2.71 3.94\\nGMN [16]2.97 4.02 3.39 4.56 3.00 3.94 3.30 4.40 3.17 4.23\\nCFOCNet [5]2.24 3.50 1.78 2.90 2.66 3.82 2.16 3.27 2.21 3.37\\nFamNet [4] 2.34 3.78 1.41 2.85 2.40 2.75 2.27 3.66 2.11 3.26\\nCFOCNet [5] 2.23 4.04 1.62 2.72 1.83 3.02 2.13 3.03 1.95 3.20\\nLaoNet (Ours) 2.20 3.78 1.32 2.66 1.58 2.19 1.84 2.90 1.73 2.93\\nTable 2 . Results on each of four folds of COCO val2017. Methods with follow the experiment setting in [5]. Our method\\nachieves great accuracy without any ne-tuning on testing categories.\\nMethodsVal Test\\nMAE RMSE MAE RMSE\\nLaoNet 17.11 56.81 15.78 97.15\\nSelf-Attention ( X) 19.83 64.84 19.71 107.32\\nSelf-Attention ( S) 19.67 63.79 18.71 111.83\\nScale Aggregation 18.82 63.74 17.16 106.40\\nSSIM 17.82 57.66 16.11 100.59\\nTable 3 . Ablation study for different terms. Xstands for\\nfeature sequences of query image and Sstands for that of\\nsupporting box region. Experiments are performed in FSC-\\n147 val and test.\\nting, without any ne-tuning strategy. We have generated new\\nrecords by reducing the error of FamNet from 26.55 to 17.11\\nfor MAE and from 77.01 to 56.81 for RMSE in validation set,\\nfrom 26.76 to 15.78 for MAE and from 110.95 to 97.15 for\\nRMSE in testing set.\\nSecond, Table 2 shows the results on each of four folds of\\nCOCO val2017. Methods with in the upper part of the table\\nfollow the experiment setting in [5]. That is, the supporting\\nexamples are chosen from all instances in the dataset during\\ntraining and testing, which is laborious and costly under the\\nneed of all instances annotated by bounding boxes. While our\\nsetting allows only one xed instance for each image, we re-\\nconduct the experiment of CFOCNet [5]. As the result shows,\\nour method maintains a great performance on COCO dataset.\\n4.4. Discussions\\nContribution of Different Terms. We study the accuracy\\ncontributions of different terms in FSC-147. The result is\\nshown in Table 3, each row whereof reports the results af-\\nter removing one component or one term from LaoNet. The\\nSelf-Attention modules for the two feature sequences to learn\\ninner-relations increase the accuracy in testing set by 19.9%\\nand15.7%for MAE, 9.5%and13.1%for RMSE, respec-\\ntively. Compared to other two terms, the Self-Attention mod-\\nules contribute most to the performance of our model.\\nThe Scale Aggregation mechanism helps more on RMSE.MethodsFSC147-COCO Val FSC147-COCO Test\\nMAE RMSE MAE RMSE\\nRetinaNet [20] 63.57 174.36 52.67 85.86\\nFaster R-CNN [21] 52.79 172.46 36.20 79.59\\nMask R-CNN [22] 52.51 172.21 35.56 80.00\\nFamNet [4] 39.82 108.13 22.76 45.92\\nLaoNet (Ours) 31.12 97.15 12.89 26.64\\nTable 4 . Comparisons with pre-trained object detectors on\\nFSC147-COCO splits of FSC147 which contain images with\\nCOCO categories. Even pre-trained with thousands of anno-\\ntated examples on MS-COCO dataset, these object detectors\\nstill perform unsatised accuracy on counting task.\\nThe result demonstrates a robustness contribution under the\\nmulti-scale aggregation. Finally, the SSIM loss further im-\\nproves the counting accuracy by both lower MAE and RMSE.\\nConvergence Speed. We hold experiments to measure the\\nconvergence speed and the performance stability. We pick\\nFamNet [4] as the baseline for LaoNet with a pre-trained\\nCNN backbone and an Adam optimizer. We train both two\\nmodels on FSC-147 and report the validation MAE for 100\\nepochs.\\nAs shown in Figure 3, our model has faster convergence\\nspeed and better stability than FamNet. With just 2 epoches,\\nour method achieves a low counting error which FamNet has\\nto reach after 40 epochs. Meanwhile, the convergence of our\\nmethod is smooth and stable, while that of Famet is jagged,\\nwith multiple sharp peaks and the highest error of 70.\\nComparison with Object Detectors. Object detectors can\\nbe used for counting task with the number of predicted de-\\ntections. However, even these detectors work with categories\\nwhich they are trained on instead of one-shot setting, their\\ncounting performances are still limited. We select images of\\nFSC-147-COCO subset from FSC147 Val and Test sets which\\nshare categories with MS-COCO dataset and conduct quanti-\\ntative experiments.\\nAs the results shown in Table 4, we compare LaoNet with\\nseveral object detectors which are well pre-trained with thou-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eee45b64-364a-4304-9de7-921db594d457', embedding=None, metadata={'page_label': '6', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='2298814f552b4cdda2a846e34c33463c17d631eadac6ef9ea8c8a6a4ef27eb7c', text='Fig. 3 . Comparisons of validation MAE during training. The\\nblue line represents our proposed LaoNet. With just one\\nepoch, it can perform a great accuracy which FamNet needs\\nto train for about 20 epochs.\\nsands of annotated examples on MS-COCO. Nevertheless,\\nour method, which counts unseen categories, still outperforms\\nthe detection based methods which have met those categories\\nin training, by a large margin.\\n5. CONCLUSION\\nThis paper targets one-shot object counting, which requires\\nthe counting model to count objects of new categories by\\nlooking at only one instance. We propose an efcient network\\nnamed LaoNet to address this challenge. LaoNet includes\\na feature correlation module to learn both inner-relations\\nand inter-relations and a scale aggregation module to extract\\nmulti-scale information for improving robustness. Without\\nany ne-tuning in inference, our LaoNet outperforms previ-\\nous state-of-the-art few-shot counting methods with a high\\nconvergence speed. In the future, we consider applying our\\nmodel to a wider range of one-shot vision tasks.\\n6. REFERENCES\\n[1] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua\\nGao, and Yi Ma, Single-image crowd counting via\\nmulti-column convolutional neural network, in CVPR ,\\n2016.\\n[2] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong\\nGong, Bayesian loss for crowd count estimation with\\npoint supervision, in ICCV , 2019.\\n[3] Debojit Biswas, Hongbo Su, Chengyi Wang, Jason\\nBlankenship, and Aleksandar Stevanovic, An auto-\\nmatic car counting system using overfeat framework,\\nSensors (Basel) , 2017.\\n[4] Viresh Ranjan, Udbhav Sharma, Thu Nguyen, and Minh\\nHoai, Learning to count everything, in CVPR , 2021.\\n[5] Shuo-Diao Yang, Hung-Ting Su, Winston H Hsu, and\\nWen-Chin Chen, Class-agnostic few-shot object count-\\ning, in WACV , 2021.\\n[6] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ar, and\\nC Lawrence Zitnick, Microsoft coco: Common objects\\nin context, in ECCV . Springer, 2014.[7] Prithvijit Chattopadhyay, Ramakrishna Vedantam,\\nRamprasaath R Selvaraju, Dhruv Batra, and Devi\\nParikh, Counting everyday objects in everyday\\nscenes, in CVPR , 2017, pp. 11351144.\\n[8] Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su,\\nScale aggregation network for accurate and efcient\\ncrowd counting, in ECCV , 2018.\\n[9] Weizhe Liu, Mathieu Salzmann, and Pascal Fua,\\nContext-aware crowd counting, in CVPR , 2019.\\n[10] Boyu Wang, Huidong Liu, Dimitris Samaras, and\\nMinh Hoai Nguyen, Distribution matching for crowd\\ncounting, Advances in Neural Information Processing\\nSystems , vol. 33, 2020.\\n[11] Hui Lin, Xiaopeng Hong, Zhiheng Ma, Xing Wei, Yun-\\nfeng Qiu, Yaowei Wang, and Yihong Gong, Direct\\nmeasure matching for crowd counting, in IJCAI , 2021.\\n[12] Thomas Moranduzzo and Farid Melgani, Automatic\\ncar counting method for unmanned aerial vehicle im-\\nages, TGRS , 2013.\\n[13] M elissande Machefer, Franc ois Lemarchand, Vir-\\nginie Bonnefond, Alasdair Hitchins, and Panagiotis\\nSidiropoulos, Mask r-cnn retting strategy for plant\\ncounting and sizing in uav imagery, Remote Sensing ,\\n2020.\\n[14] Thorsten Falk, Dominic Mai, Robert Bensch, Ozgun\\nC  ic ek, Ahmed Abdulkadir, Yassine Marrakchi, Anton\\nBohm, Jan Deubner, Zoe J ackel, Katharina Seiwald,\\net al., U-net: deep learning for cell counting, detec-\\ntion, and morphometry, Nature methods , 2019.\\n[15] Weidi Xie, J Alison Noble, and Andrew Zisserman,\\nMicroscopy cell counting and detection with fully con-\\nvolutional regression networks, Computer methods in\\nbiomechanics and biomedical engineering: Imaging &\\nVisualization , 2018.\\n[16] Erika Lu, Weidi Xie, and Andrew Zisserman, Class-\\nagnostic counting, in ACCV , 2018.\\n[17] Claudio Michaelis, Ivan Ustyuzhaninov, Matthias\\nBethge, and Alexander S Ecker, One-shot instance seg-\\nmentation, arXiv preprint , 2018.\\n[18] Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, and\\nTyng-Luh Liu, One-shot object detection with co-\\nattention and co-excitation, in NIPS , 2019.\\n[19] Xiang Li, Lin Zhang, Yau Pun Chen, Yu-Wing Tai, and\\nChi-Keung Tang, One-shot object detection without\\nne-tuning, arXiv preprint , 2020.\\n[20] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,\\nand Piotr Doll ar, Focal loss for dense object detection,\\ninICCV , 2017.\\n[21] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian\\nSun, Faster r-cnn: Towards real-time object detection\\nwith region proposal networks, NIPS , 2015.\\n[22] Kaiming He, Georgia Gkioxari, Piotr Doll ar, and Ross\\nGirshick, Mask r-cnn, in ICCV , 2017.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ceaead4-159e-4cd1-8f41-8aab9baeca3e', embedding=None, metadata={'page_label': '7', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='e644a54df419ad506321de15524d4ff8f6ab2464502fe679a159f5f435ce2148', text='[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,\\nand Illia Polosukhin, Attention is all you need, in\\nNIPS , 2017.\\n[24] Diederik P Kingma and Jimmy Lei Ba, Adam:\\nAmethod for stochastic optimization, .\\n[25] Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi\\nFeng, and Trevor Darrell, Few-shot object detection\\nvia feature reweighting, in ICCV , 2019.\\n[26] Qi Fan, Wei Zhuo, Chi-Keung Tang, and Yu-Wing Tai,\\nFew-shot object detection with attention-rpn and multi-\\nrelation detector, in CVPR , 2020.\\n[27] Chelsea Finn, Pieter Abbeel, and Sergey Levine,\\nModel-agnostic meta-learning for fast adaptation of\\ndeep networks, in ICML , 2017.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='380e26da-ba2b-4051-b6f0-9ab584b9ca37', embedding=None, metadata={'page_label': '1', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='99b6e20b26a1d13f7bfe53f2f1cee25b4a9a16bd46213319384e3e5ffa962ff1', text='arXiv:2304.04556v1  [cs.LG]  7 Apr 2023Attention: Marginal Probability is All You Need?\\nRyan Singh1Christopher L. Buckley1\\nAbstract\\nAttention mechanisms are a central property of\\ncognitive systems allowing them to selectively\\ndeploy cognitive resources in a exible manner.\\nAttention has been long studied in the neuro-\\nsciences and there are numerous phenomenolog-\\nical models that try to capture its core proper-\\nties. Recently attentional mechanisms have be-\\ncome a dominating architectural choice of ma-\\nchine learning and are the central innovation of\\nTransformers. The dominant intuition and for-\\nmalism underlying their development has drawn\\non ideas of keys and queries in database man-\\nagement systems. In this work, we propose an\\nalternative Bayesian foundation for attentional\\nmechanisms and show how this unies differ-\\nent attentional architectures in machine learning.\\nThis formulation allows to to identify common-\\nality across different attention ML architectures\\nas well as suggest a bridge to those developed in\\nneuroscience. We hope this work will guide more\\nsophisticated intuitions into the key properties of\\nattention architectures as well suggest new ones.\\n1. Introduction\\nDesigning neural network architectures with favourable\\ninductive biases lies behind many recent successes in\\nDeep Learning ( Baxter ,2000 ). In particular, the attention\\nmechanism has allowed language models to achieve hu-\\nman like generation abilities previously thought impossib le\\n(Vaswani et al. ,2017 ). The success of the attention mech-\\nanism as a domain agnostic architecture has prompted it\\nto be adopted across a huge range of tasks and domains\\nnotably reaching state-of-the-art performance in visual r ea-\\nsoning and segmentation tasks ( Dosovitskiy et al. ,2021 ;\\nWang et al. ,2022 ).\\nDespite its success, the role of the attention mechanism\\nremains poorly understood. Indeed, it is unclear to what\\n1School of Engineering and Informatics, University of Susse x.\\nCorrespondence to: Ryan Singh <rs773@sussex.ac.uk >.extent it relates to theories of cognitive attention which i n-\\nspired it ( Lindsay ,2020 ). Here, we aim to provide a parsi-\\nmonious description grounded in principles of probabilist ic\\ninference. This Bayesian perspective provides both a prin-\\ncipled method for specifying prior beliefs and reasoning\\nexplicitly about the role of the attention variables. Furth er,\\nunderstanding the fundamental computation permits us a\\nunied description of different attention mechanisms in th e\\nliterature. This proceeds in two parts.\\nFirst, we show that soft attention mechanisms (e.g. self-\\nattention, cross-attention, graph attention, which we cal l\\ntransformer attention herafter) can be understood prob-\\nabilistically as taking an expectation over possible con-\\nnectivity structures, providing an interesting link betwe en\\nsoftmax-based attention and marginal likelihood.\\nSecond, we extend the uncertainty over connectiv-\\nity to a bayesian setting which, in turn, provides a\\ntheoretical grounding for iterative attention mecha-\\nnisms (slot-attention, perciever and block-slot attentio n)\\n(Locatello et al. ,2020 ;Singh et al. ,2022 ;Jaegle et al. ,\\n2021 ) and Modern Continuous Hopeld Networks\\n(Ramsauer et al. ,2021 ).\\nAdditionally, we apply iterative attention to Predictive C od-\\ning Networks, an inuential theory in computational neuro-\\nscience, creating a new theoretical bridge between machine\\nlearning and cognitive science.\\nAttention (Q,K,V) =p(E|Q,K)\\ued17\\ued1a\\ued19\\ued18\\nsoftmax (QWQWT\\nKKT\\ndk)V\\n=Ep(E|Q,K)[V]\\nA key observation is that the attention matrix can be seen\\nas the posterior distribution over an adjacency structure, E,\\nand the full mechanism as computing an expectation of the\\nvalue function V(X)over the posterior beliefs about the\\npossible relationships that exist between key and query.\\nThis formalism provides an alternate Bayesian theoreti-\\ncal framing within which to understand attention mod-\\nels, which contrasts with the original framing in terms of\\ndatabase management systems and data retrieval, providing', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='473a389b-a9bc-402d-86f7-16c249dc049e', embedding=None, metadata={'page_label': '2', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='804b094d82f3be1082ec18f85a44ad2856c4e10c5449f7efb8f40b3551647e2d', text='Attention: Marginal Probabiliy is All You Need?\\na unifying framework to describe different attention archi -\\ntectures. Describing their difference only in terms of thei r\\nedge relationships supporting more effective analysis and\\ndevelopment of new architectures. Additionally providing\\na principled understanding of the difference between hard\\nand soft attention models.\\nContributions\\n A unifying probabilistic framework for understanding\\nattention mechanisms.\\n We show self-attention and cross-attention can be seen\\nas computing a marginal likelihood over possible net-\\nwork structures.\\n We show that slot-attention, block-slot-attention and\\nmodern continuous hopeld networks can all be seen\\nas collapsed variational inference, where the possible\\nnetwork structures form the collapsed variables.\\n Provide a bridge to Bayesian conceptions of attention\\nfrom computational neuroscience, through the lens of\\nPredictive Coding Networks.\\n Provide a framework for reasoning about hard at-\\ntention, and efcient approximations to the attention\\nmechanism.\\n2. Related Work\\nAttention as bi-level optimisation Mapping feed-forward\\narchitecture to a minimisation step on a related energy func -\\ntion has been called unfolded optimisation ( Frecon et al. ,\\n2022 ). Taking this perspective can lead to insights about\\nthe inductive biases involved for each architecture. It has\\nbeen shown that the cross-attention mechanism can be\\nviewed as an optimisation step on the energy function of\\na form of Hopeld Network ( Ramsauer et al. ,2021 ), pro-\\nviding a link between attention and associative memory.\\nWhilst ( Yang et al. ,2022 ) extend this view to account for\\nself-attention. Our framework distinguishes hopeld atte n-\\ntion, which does not allow an arbritary value matrix, from\\nthe standard attention mechanisms. Whilst there remains\\na strong theoretical connection, it places the Hopeld En-\\nergy as an instance of variational free energy, aligning mor e\\nclosely with iterative attention mechanisms such as slot-\\nattention.\\nRelationship to gaussian mixture model Previous works\\nthat have taken a probabilistic perspective on the attentio n\\nmechanism note the connection to inference in a gaussian\\nmixture model ( Gabbur et al. ,2021 ;Nguyen et al. ,2022 ;\\nDing et al. ,2020 ). Indeed ( Annabi et al. ,2022 ) directly\\nshow the connection between the Hopeld energy and the\\nvariational free energy of a gaussian mixture model. Al-\\nthough gaussian mixture models, a special case of theframework we present here, are enough to explain cross\\nattention they do not capture slot or self-attention. Furth er\\nour framework allows us to extend the structural inductive\\nbiases beyond what can be expressed in a gaussian mixture\\nmodel and capture the relationship to hard attention.\\nLatent alignment and hard attention Several attempts\\nhave been made to combine the benets of soft (dif-\\nferentiability) and hard attention. Most approaches pro-\\nceed by sampling, e.g., using the REINFORCE es-\\ntimator ( Deng et al. ,2018 ) or atopK approximation\\n(Shankar et al. ,2018 ). The one most similar to ours em-\\nbeds the full forward-backward algorithm within a forward\\npass ( Kim et al. ,2017 ), our approach differs by offering a\\nparsimonious description in terms of marginalisation over\\nan implicit graphical model.\\nCollapsed Inference Collapsed variational inference has\\nmost notably been employed in topic modelling ( Teh et al. ,\\n2006 ). To our knowledge, linking collapsed inference to\\nattention in deep learning is completely novel.\\n3. Transformer Attention\\n3.0.1. A TTENTION AS EXPECTATION\\nWe begin by demonstrating transformer attention is best\\nseen as an expectation over latent variables. In the case\\nof self and cross-attention, the expectation of a neural net -\\nwork with respect to possible adjacency structures.\\nLetx= (x1,..,xn)be observed variables, be some\\nset of latent variables, and ya variable we need to pre-\\ndict. Given a latent variable model p(y,x,) =p(y|\\nx,)p(x,), wherep(y|x,)is parameterised by some\\nfunctionv(y,x,)e.g. a neural network.\\nOur goal is to nd p(y|x), howeverare unobserved so\\nwe calculate the marginal likelihood.\\np(y|x) =\\np(|x)v(y,x,)\\nImportantly, the softmax function is a natural representa-\\ntion for the posterior\\np(|x) =p(x,)\\np(x,)\\np(|x) =softmax (lnp(x,))\\nHence, transformer attention can be seen as weighting\\nv(x,)by the posterior distribution p(|x).\\np(y|x) =\\nsoftmax (lnp(x,))v(y,x,)\\n=Ep(|x)[v(y,x,)](1)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='82cfa96d-ab11-4116-9c05-d792af83eec5', embedding=None, metadata={'page_label': '3', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='6995c82104fd36ee0a58a6e46d335f5ea281a5b887d1dad0878499818d42ffe2', text='Attention: Marginal Probabiliy is All You Need?\\nWe claim ( 1) is exactly the equation underlying self and\\ncross-attention. To make a more direct connection, we\\npresent the specic generative models corresponding to\\nthem. The latent variables are identied as possible rela-\\ntionships , or edges, between each of the observed variables\\nx(keys and queries).\\nA natural formalism for modelling these graphical relation -\\nships is Markov Random Fields.\\n3.0.2. P AIRWISE MARKOV RANDOM FIELDS\\nGiven a set of random variables X= (Xv)vVwith prob-\\nability distribution [p]and a graph G= (V,E). The vari-\\nables form a pairwise Markov random eld (MRF) with\\nrespect toGif the joint density function P(X=x) =p(x)\\nfactorises as follows\\np(x) =1\\nZexp(\\nvVv+\\neEe)\\nwhereZis the partition function v(xv)ande=\\nu,v(xu,xv)are known as the node and edge potentials\\nrespectively1.\\nBeyond the typical set-up, we add a structural prior p(E)\\nover the adjacency structure of the underlying graph.\\np(x,E) =P(x|E)P(E)\\n=1\\nZp(E)exp(\\nvVv+\\neEe)\\nWe briey remark that ( 1) respects factorisation of [p]in the\\nfollowing sense; if the distribution admits a factorisatio n\\nwith respect to the latent variables p(x,) =\\nifi(x,i)\\nandv(x,) =\\nivi(x,i)then (applying the linearity of\\nexpectation) we may write\\nEp(|x)[v(x,)] =\\niEp(i|x)[vi] (2)\\nPermitting each factor to be marginalised independently.\\nIn the case of an MRF, such a factorisation is natural. If\\nthe distibution over edges factorises into local distribut ions\\np(E) =\\nip(Ei)(using independence properties of the\\nMRF) we can write p(x,E) =1\\nZ\\nifi(x,Ei)where each\\nfi=P(Ei)exp\\nvVv\\neEieis itself an unnor-\\nmalised MRF.\\nTo recover cross-attention and self-attention are such mod -\\nels with we need only specify a structural prior and poten-\\ntial functions.\\n3.0.3. C ROSS ATTENTION\\n Key nodes K= (x1,..,xn)\\n1See ( Shah et al. ,2021 ) for a precise denition.x1\\nx2\\nx3\\nx4\\nxnx\\nmx\\n3x\\n2x\\n1\\n......\\na Cross Attentionx1x2\\nx3\\nx4\\nx5x6xnx1x1x1x1x1x1x1\\nb Self Attention\\nx1\\nx2\\nx3\\nx4\\nxnzmz3z2z1\\n......\\nc Modern Continous Hop-\\neld Networkx1\\nx2\\nx3\\nx4\\nxnzmzmzmzmzmz3z3z3z3z3z2z2z2z2z2z1z1z1z1z1\\n......\\nd Slot Attention\\nFigure 1. Comparison of different attention modules in the liter-\\nature, the highlighted edges is representative of the margi nalisa-\\ntion being performed for the random variable E1, in1aand1b\\nall nodes are observed, as opposed to 1cand1d, where there are\\nlatent nodes (indicated in grey).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='77615da1-6df3-4af4-87d6-21e24aa2b05a', embedding=None, metadata={'page_label': '4', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='b45d545af85277ed9edf6bc079dc1ab42646ea484f050d716410438f060b6735', text='Attention: Marginal Probabiliy is All You Need?\\n Query nodes Q= (x\\n1,...,x\\nm)\\n Structural prior p(E) =m\\ni=1p(Ei), whereEi\\nUniform {(x1,x\\ni),..,(xn,x\\ni)}, such that each query\\nnode is uniformly likely to connect to each key node.\\n Edge potentials (xj,x\\ni) =xT\\niWT\\nQWKxj, in effect\\nmeasuring the similarity of xjandx\\niunder a certain\\ntransformation.\\n Value function Vi(K,Q,E i) =WVxs(Ei), a linear\\ntransformation applied to the node, xs(Ei), the start of\\nthe edgeEi.\\nTaking the posterior expectation in each of the factors de-\\nned in two ( 2) gives the standard cross- attention mecha-\\nnism\\nEp(Ei|Q,K)[Vi] =\\njsoftmax j(xT\\niWT\\nQWKxj)WVxj\\nEp(E|Q,K)[V] =softmax (QTWT\\nQQKK)WVK\\n3.0.4. S ELFATTENTION\\n NodesK=Q= (x1,..,xn)\\n Structural prior p(E) =n\\ni=1p(E\\ni), whereE\\ni\\nUniform {(x1,xi),..,(xn,xi)}, such that each node\\nis uniformly likely to connect to every other node.\\n Edge potentials (kj,ki) =xT\\niWT\\nQWKxj, in effect\\nmeasuring the similarity of xjandx\\niunder a certain\\ntransformation.\\n Value function Vi(K,Q,E i) =WVxs(Ei), a linear\\ntransformation applied to the node, xs(Ei), the start of\\nthe edgeEi.\\nAgain, taking the posterior expectation in each of the fac-\\ntors dened in two ( 2) gives the standard self- attention\\nmechanism\\nEp(Ei|Q,K)[Vi] =\\njsoftmax j(xT\\niWT\\nQWKxj)WVxj\\nEp(E|Q,K)[V] =softmax (KTWT\\nQWKK)WVK\\n4. Iterative Attention\\nWe continue by extending attention to full Bayesian infer-\\nence. In essence applying the attention trick, marginali-\\nsation of attention variables, to the variational free ener gy\\n(a.k.a the ELBO).\\nModern Continuous Hopeld Networks can be seen as\\na particular instance of this class of system, allow-\\ning us to reproduce the hopeld attention updates of(Ramsauer et al. ,2021 ) within a probabilistic context. Un-\\nder different structural priors we recover other iterative\\nattention models; slot-attention ( Locatello et al. ,2020 ),\\nblock-slot attention ( Singh et al. ,2022 ) and Perciever\\n(Jaegle et al. ,2021 ). Further, we showcase a specic ad-\\nvantage of bayesian attention, hard attention.\\n4.0.1. C OLLAPSED INFERENCE\\nWe present a version of collapsed variational inference\\n(Teh et al. ,2006 ) showing how this results in a bayesian\\nattention mechanism. The term attention mechanism is apt\\ndue to the surprising similarity in form between the varia-\\ntional updates ( 6) and neural attention mechanism ( 1).\\nOur setting is the latent variable model p(x,z,), wherex\\nare observed variables, and z,, are latent variables. Typi-\\ncally we wish to infer zgivenx.\\nCollapsed inference proceeds by marginalising out the ex-\\ntraneous latent variables \\np(x,z) =\\np(x,z,) (3)\\nWe dene a recognition density q(z)N(z;)and opti-\\nmise the variational free energy with respect to the parame-\\nters,, of this distribution.\\nmin\\nF(x,) =Eq[lnq(z)lnp(x,z)]\\nUnder a typical Laplace approximation, we can write the\\nvariational free energy as F lnp(x,)2. Substituting\\nin (3) and taking the derivative with respect to the varia-\\ntional parameters yields,\\nF(x,) =ln\\np(x,,)\\nF\\n=1\\np(x,,)\\n\\np(x,,) (4)\\nWhich connects bayesian attention with the standard atten-\\ntion ( 1). To clarify this, we employ the log-derivative trick,\\nsubstituting p=elnpand re-express ( 4) in two ways:\\nF\\n=\\nsoftmax (lnp(x,,))\\nlnp(x,,)\\n(5)\\nF\\n=Ep(|x,)[\\nlnp(x,,)] (6)\\nThe rst form reveals the softmax which is ubiquitous in\\nall attention models. The second, suggests the variational\\n2See appendix for a more principled derivation taking accoun t\\nof higher order terms', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='13815825-6039-482c-871f-33606c6d549a', embedding=None, metadata={'page_label': '5', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='b89cfde33a23bed027cb1e61c07dab506b110ce0a5bcd5c559725b7e047c4913', text='Attention: Marginal Probabiliy is All You Need?\\nupdate should be evaluated as the expectation of the typi-\\ncal variational gradient (the term within the square brack-\\nets) with respect to the posterior over the parameters repre -\\nsented by the random variable .\\nIn other words, bayesian attention is exactly the nueral\\nattention mechanism applied iteratively, where the value\\nfunction is the variational free energy gradient. We derive\\nupdates for a general MRF before again recovering (itera-\\ntive) attention models in the literature by specifying part ic-\\nular distributions.\\n4.0.2. F REE ENERGY OF A MARGINALISED MRF\\nRecall the factorised MRF, p(E) =\\nip(Ei).\\np(x,E) =1\\nZ\\nifi(x,Ei)with each fi=\\nP(Ei)exp\\nvVv\\neEie. Independence prop-\\nerties mean the marginalisation necessary for collapsed\\ninference can be simplied\\n\\nEp(x,E) =1\\nZ\\ni\\nEifi(x,Ei)\\nIn an inference setting the nodes are partitioned into ob-\\nserved nodes, x, and latent nodes, z. The variational free\\nenergy ( 4) and the associated forms of its derivative can be\\nexpressed\\nF(x,,) =\\niln\\nEifi(x,,E i)\\nF\\nj=\\ni\\nEisoftmax (fi(x,,E i))fi\\nj\\nSimilar to hard attention approaches, the random variable\\nEis an explicit alignment variable. However, unlike hard\\nattention, we avoid inferring Eexplicitly using the col-\\nlapsed inference approach outlined above.\\n4.0.3. Q UADRATIC POTENTIALS AND THE CONVEX\\nCONCAVE PROCEDURE\\nWe follow ( Ramsauer et al. ,2021 ) in using the CCCP to\\nderive a xed point equation, which necessarily reduces the\\nfree energy.\\nAssuming the node potentials are quadratic (xi) =1\\n2x2\\ni\\nand the edge potentials have the form (xi,xj) =xiWxj.\\n\\nj=\\ni\\nEisoftmax (gi(x,,E i))gi\\nj(7)\\nWheregi=\\neEie.\\nBy way of the CCCP ( Yuille & Rangarajan ,2001 ), this\\nxed point equation has the property F(x,\\nj,)F(x,j,)with equality if and only if \\njis a stationary\\npoint ofF.\\nWe follow the 3in specifying specic structural priors and\\npotential functions to recover different iterative attent ion\\nmechanisms.\\n4.0.4. H OPFIELD -STYLE CROSS ATTENTION\\nLet the observed x= (x1,..,xn)and latent nodes z=\\n(z1,..,zm)have the following structural prior p(E) =m\\ni=1p(Ei), whereEiUniform {(x1,zi),..,(xn,zi)}.\\nAnd dene edge potentials (xj,zi) =ziQTKxj, Appli-\\ncation of ( 7)\\n\\ni=\\njsoftmax j(iWT\\nQWKxj)WT\\nQWKxj\\nWheniis initialised to some query the system\\n(Ramsauer et al. ,2021 ) the xed point update is given by\\n\\ni() =Ep(Ei|x,)[WT\\nQWKxt(Ei)]. When the patterns x\\nare well separated, \\ni()WT\\nQWKxj, whereWT\\nQWKxj\\nis the closest vector and hence can be used as an associative\\nmemory.\\n4.0.5. S LOT ATTENTION\\nSlot attention ( Locatello et al. ,2020 ) is an object centric\\nlearning module built on top of an iterative attention mech-\\nanism. Here we show this is a simple adjustment of the\\nprior beliefs on our edge set.\\nWith the same set of nodes and potentials, replace the\\nprior over edges with p(E) =n\\nj=1p(Ej),Ej\\nUniform {(xj,z1),..,(xj,zm)}\\n\\ni=\\njsoftmax i(iQTKxj)QTKxj\\nWhilst the original slot attention employed an RNN to aid\\nthe basic update shown here, the important feature is that\\nthe softmax is taken over the slots, . This forces compe-\\ntition between slots to account for the observed variables,\\nforcing object centric representations. For example, if th e\\nobserved variables xare image patches, the slots are forced\\nto cluster similar patches together in order increase the ov er-\\nall likelihood of said patches. The word cluster is accurate ,\\nin fact there is an exact equivalence between this mecha-\\nnism and a step of EM on a gaussian mixture model.\\n4.0.6. B LOCK SLOT ATTENTION\\n(Singh et al. ,2022 ) suggest combining an associative mem-\\nory ability with an object-centric slot-like ability and pr o-\\nvide an iterative scheme for doing so, alternating between\\nslot-attention and hopeld updates.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0f61caae-8a6a-448f-927a-434aabd5bbc4', embedding=None, metadata={'page_label': '6', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='5013d300177af9e4330046692d60b8b86c6aee055d1b7496bf1b49ab006c7897', text='Attention: Marginal Probabiliy is All You Need?\\nx1\\nx2\\nx3\\nx4\\nxnzmzmzmzmzmz3z3z3z3z3z2z2z2z2z2z1z1z1z1z1m1\\nm2\\nm3\\nm4\\nml......\\n...\\nFigure 2. Block Slot Attention\\nOur framework permits us to exibly combine different\\nattention mechanisms through different latent graph struc -\\ntures, allowing us to derive a model informed version of\\nblock-slot attention. In this setting we have three sets of\\nvariablesX, the observations, Zthe latent variables to be\\ninferred and Mwhich are parameters.\\nDene the pairwise MRF X={x1,...,xn},\\nZ={z1,...,zm}andM={m1,...,ml}with a\\nprior over edges p(E) =m\\nj=1p(Ej)l\\nk=1p(Ek),\\nEjUniform {(xj,z1),..,(xj,zm)},Ek\\nUniform {(z1,mk),..,(zm,mk)}, with edge poten-\\ntials between XandZgiven by(xj,zi) =ziQTKxj\\nand between ZandM,(zi,mk) =zimk\\napplying ( 7) gives\\n\\ni=\\njsoftmax i(iQTKxj)QTKxj\\n+\\nksoftmax k(imk)mk\\nIn the original block-slot attention each slot ziis broken\\ninto blocks, where each block can access block-specic\\nmemories i.e. z(b)\\nican has possible connections to mem-\\nory nodes {m(b)\\nk}kl. Allowing objects to be represented\\nby slots which in turn disentangle features of each object\\nin different blocks. We presented a single block version\\nabove, however it is easy to see that the update extends to\\nthe multiple block version applying ( 7) gives\\n\\ni=\\njsoftmax i(iQTKxj)QTKxj\\n+\\nk,bsoftmax k((b)\\nim(b)\\nk)m(b)\\nk5. Predictive Coding Networks\\nPredictive Coding Networks (PCN) have emerged\\nas an inuential theory in computational neuro-\\nscience ( Rao & Ballard ,1999 ;Friston & Kiebel ,2009 ;\\nBuckley et al. ,2017 ). Building on theories of perception\\nas inference and the Bayesian brain, PCNs perform approx-\\nimate Bayesian inference by minimising the variational\\nfree energy which is manifested in the minimisation of\\nlocal prediction errors. The continuous time dynamics at\\nan individual neuron are given by\\nF\\ni=\\nk+\\n+kw\\nWhereare prediction errors, wrepresent synaptic strength\\nandkare node specic precisions representing uncertainty\\nin the generative model ( Millidge et al. ,2022 ).\\nA natural extension is to apply collapsed inference over\\nthe set of incoming and out going connection, i.e. a lo-\\ncally factorised prior over possible connectivity. In the n o-\\ntation of the previous section, we have an MRF with a hi-\\nerarchical structure Z={Z(0),...,Z(l),...,Z(N)}where\\nthe prior on edges factorises into layerwise p(E(l)) =\\n{(zi,zj) : (zi,zj)Z(l1)Z(l)}and potential func-\\ntions(zi,zj) =2\\ni,j=kj(zjwi,jzi)2.\\nF\\ni=\\nsoftmax (2)k\\n+\\n+softmax (2)kw\\nThe resulting dynamics induce a normalisation across\\nprediction errors received by a neuron through the softmax\\nfunction. This dovetails nicely with theories of attention\\nas normalisation in psychology and neuroscience. In con-\\ntrast previous predictive coding based theories of attenti on\\nhave focused on the precision terms, k, due to their abil-\\nity to up and down regulate the impact of prediction errors\\n(Feldman & Friston ,2010 ). Here we see the softmax term\\ncan also perform this regulation, while also exhibiting the\\nfast winner-takes-all dynamics that are associated with co g-\\nnitive attention.\\n5.1. Discussion\\nIn this section we will briey discuss what can be gained\\nfrom looking at the attention mechanism as a problem of\\ninference.\\n5.1.1. H ARD ATTENTION\\nRecall ( 1) neural attention may be viewed as calculating an\\nexpectation over latent variables Ep(|x)[v(x,)]. Here the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='89e90620-d682-47f8-8027-cf30eaa89aa1', embedding=None, metadata={'page_label': '7', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='ad787b96b89cdf6d7b0858ea3882cf5ed41e0f5fd146e0996213bd9939131bb2', text='Attention: Marginal Probabiliy is All You Need?\\nmechanism is soft because we weight multiple possibil-\\nities of attention variable . Hard attention, on the other\\nhand, proceeds with a single sample from p(|x). It has\\nbeen argued this is more biological, more interpretable and\\nhas lower computational complexity. Previously the infe-\\nrior performance of hard-attention has been attributed to\\nits hard to train, stochastic nature. However, our framing\\nof soft attention as exact marginalisation offers an altern ate\\nexplanation. Stochastic approximations (hard attention)\\nwill always suffer compared with exact marginalisation\\n(soft attention). Further our framework provides a method\\nfor seamlessly interchanging hard and soft-attention. Sin ce\\nthe distribution p(|x)a the categorical distribution, at\\nany point (during training or inference) it is possible to im -\\nplement hard attention by taking a single sample from\\np(|x)yieldingv(x,).\\nThere are two issues with this approach to collapsing the\\nattention distribution. First, the single sample will coll apse\\nany uncertainty, secondly calculation of p(|x), in order\\nto sample, still incurs a quadratic penalty O(n2). However\\nwe can employ tools from probability theory to help us anal-\\nyse the cost of sampling, and linear approximations to the\\nattention distribution.\\n5.1.2. E FFICIENT TRANSFORMERS\\nConsider some distribution qattempting to approximate\\np(|x)we can quantify the information loss with the rela-\\ntive entropy\\nL[p,q]DKL[q()||p(|x)] =H[q]+Eq[p(|x)]\\nIn the hard attention approximation a single sample from\\npis used as an approximation L[p,q] =lnp(|x)\\nand perhaps intuitively E[L] =H[p]i.e. hard attention\\nis a good approximation when the attention distribution is\\nlow-entropy which can be controlled by the temperature pa-\\nrameter (Appendix ??).\\nMany of the efcient alternatives to attention, such as low-\\nrank and linear approximations, can be cast as approximat-\\ningp(|x)withq(|x)where calculating qis less expen-\\nsive than exact marginalisation. Estimating Lcould be used\\nto quantify the relative information loss when using these\\nalternatives. Another direction taken to reduce computa-\\ntional complexity of the attention mechanism is sparsica-\\ntion the attention matrix, which in our framework reduces\\nto adjustments to the prior over edges (Appendix ??).\\n5.1.3. N EWDESIGNS\\nThe main difference between the description presented and\\nprevious probabilistic descriptions is to view soft attent ion\\nas a principled, exact, probabilistic calculation, with re -\\nspect to an implicit probabilistic model, as opposed to an\\nimpoverished approximation. This leads to possibility ofdesigning new attention mechanisms by altering the distri-\\nbution that the mechanism marginalises over, either by ad-\\njusting the structural prior, or the potential functions. W e\\nhope this will enable new architectures to be designed in a\\nprincipled manner.\\nReferences\\nAnnabi, L., Pitti, A., and Quoy, M. On the Re-\\nlationship Between Variational Inference and\\nAuto-Associative Memory, October 2022. URL\\nhttp://arxiv.org/abs/2210.08013 .\\narXiv:2210.08013 [cs].\\nBaxter, J. A Model of Inductive Bias Learning. Journal\\nof Articial Intelligence Research , 12:149198, March\\n2000. ISSN 1076-9757. doi: 10.1613/jair.731. URL\\nhttps://www.jair.org/index.php/jair/article/view/10 253.\\nBuckley, C. L., Kim, C. S., McGregor, S., and Seth,\\nA. K. The free energy principle for action and\\nperception: A mathematical review. Journal of\\nMathematical Psychology , 81:5579, December 2017.\\nISSN 0022-2496. doi: 10.1016/j.jmp.2017.09.004. URL\\nhttps://www.sciencedirect.com/science/article/pii/S 0022249617300962 .\\nDeng, Y ., Kim, Y ., Chiu, J., Guo, D., and Rush, A.\\nLatent Alignment and Variational Attention. In\\nAdvances in Neural Information Processing Sys-\\ntems, volume 31. Curran Associates, Inc., 2018. URL\\nhttps://proceedings.neurips.cc/paper/2018/hash/b691 334ccf10d4ab144d672f7783c8a3-Abstract.html .\\nDing, N., Fan, X., Lan, Z., Schuurmans, D., and Soricut, R.\\nAttention that does not Explain Away, September 2020.\\nURLhttp://arxiv.org/abs/2009.14308 .\\narXiv:2009.14308 [cs, stat].\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Min-\\nderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and\\nHoulsby, N. An Image is Worth 16x16 Words: Trans-\\nformers for Image Recognition at Scale, June 2021.\\nURLhttp://arxiv.org/abs/2010.11929 .\\narXiv:2010.11929 [cs] version: 2.\\nFeldman, H. and Friston, K. Attention, Uncer-\\ntainty, and Free-Energy. Frontiers in Human\\nNeuroscience , 4, 2010. ISSN 1662-5161. URL\\nhttps://www.frontiersin.org/articles/10.3389/fnhum. 2010.00215 .\\nFrecon, J., Gasso, G., Pontil, M., and Salzo, S. Breg-\\nman Neural Networks. In Proceedings of the\\n39th International Conference on Machine Learn-\\ning, pp. 67796792. PMLR, June 2022. URL\\nhttps://proceedings.mlr.press/v162/frecon22a.html .\\nISSN: 2640-3498.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='49785aa8-9ab4-4aaa-9ef5-ec2347060597', embedding=None, metadata={'page_label': '8', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='1b9aa55e6544acee57da090ef2524b131e79cf6229008f9baca1f388ad36d751', text='Attention: Marginal Probabiliy is All You Need?\\nFriston, K. and Kiebel, S. Predictive coding under\\nthe free-energy principle. Philosophical Trans-\\nactions of the Royal Society B: Biological Sci-\\nences , 364(1521):12111221, May 2009. ISSN\\n0962-8436. doi: 10.1098/rstb.2008.0300. URL\\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC266670 3/.\\nGabbur, P., Bilkhu, M., and Movellan, J. Probabilistic\\nAttention for Interactive Segmentation, July 2021.\\nURLhttp://arxiv.org/abs/2106.15338 .\\narXiv:2106.15338 [cs].\\nJaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zis-\\nserman, A., and Carreira, J. Perceiver: General\\nPerception with Iterative Attention. In Proceedings\\nof the 38th International Conference on Machine\\nLearning , pp. 46514664. PMLR, July 2021. URL\\nhttps://proceedings.mlr.press/v139/jaegle21a.html .\\nISSN: 2640-3498.\\nKim, Y ., Denton, C., Hoang, L., and Rush, A. M.\\nStructured Attention Networks, February 2017.\\nURLhttp://arxiv.org/abs/1702.00887 .\\narXiv:1702.00887 [cs].\\nLindsay, G. W. Attention in Psychology, Neuroscience,\\nand Machine Learning. Frontiers in Computational\\nNeuroscience , 14, 2020. ISSN 1662-5188. URL\\nhttps://www.frontiersin.org/articles/10.3389/fncom. 2020.00029 .\\nLocatello, F., Weissenborn, D., Unterthiner, T., Ma-\\nhendran, A., Heigold, G., Uszkoreit, J., Doso-\\nvitskiy, A., and Kipf, T. Object-Centric Learn-\\ning with Slot Attention, October 2020. URL\\nhttp://arxiv.org/abs/2006.15055 .\\narXiv:2006.15055 [cs, stat].\\nMillidge, B., Song, Y ., Salvatori, T., Lukasiewicz, T., and\\nBogacz, R. A Theoretical Framework for Inference and\\nLearning in Predictive Coding Networks, August 2022.\\nURLhttp://arxiv.org/abs/2207.12316 .\\narXiv:2207.12316 [cs].\\nNguyen, T. M., Nguyen, T. M., Le, D. D. D., Nguyen,\\nD. K., Tran, V .-A., Baraniuk, R., Ho, N., and\\nOsher, S. Improving Transformers with Proba-\\nbilistic Attention Keys. In Proceedings of the\\n39th International Conference on Machine Learn-\\ning, pp. 1659516621. PMLR, June 2022. URL\\nhttps://proceedings.mlr.press/v162/nguyen22c.html .\\nISSN: 2640-3498.\\nRamsauer, H., Sch a, B., Lehner, J., Seidl, P., Widrich,\\nM., Adler, T., Gruber, L., Holzleitner, M., Pavlovi c,\\nM., Sandve, G. K., Greiff, V ., Kreil, D., Kopp, M.,\\nKlambauer, G., Brandstetter, J., and Hochreiter, S.\\nHopeld Networks is All You Need, April 2021.URLhttp://arxiv.org/abs/2008.02217 .\\narXiv:2008.02217 [cs, stat].\\nRao, R. P. N. and Ballard, D. H. Predictive cod-\\ning in the visual cortex: a functional interpre-\\ntation of some extra-classical receptive-eld ef-\\nfects. Nature Neuroscience , 2(1):7987, January\\n1999. ISSN 1546-1726. doi: 10.1038/4580. URL\\nhttps://www.nature.com/articles/nn0199_79 .\\nNumber: 1 Publisher: Nature Publishing Group.\\nShah, A., Shah, D., and Wornell, G. On Learn-\\ning Continuous Pairwise Markov Random Fields.\\nInProceedings of The 24th International Con-\\nference on Articial Intelligence and Statistics ,\\npp. 11531161. PMLR, March 2021. URL\\nhttps://proceedings.mlr.press/v130/shah21a.html .\\nISSN: 2640-3498.\\nShankar, S., Garg, S., and Sarawagi, S. Surprisingly Easy\\nHard-Attention for Sequence to Sequence Learning. In\\nProceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing , pp. 640645, Brus-\\nsels, Belgium, October 2018. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/D18-1065. URL\\nhttps://aclanthology.org/D18-1065 .\\nSingh, G., Kim, Y ., and Ahn, S. Neural Block-\\nSlot Representations, November 2022. URL\\nhttp://arxiv.org/abs/2211.01177 .\\narXiv:2211.01177 [cs].\\nTeh, Y ., Newman, D., and Welling, M. A Collapsed\\nVariational Bayesian Inference Algorithm for Latent\\nDirichlet Allocation. In Advances in Neural Information\\nProcessing Systems , volume 19. MIT Press, 2006. URL\\nhttps://proceedings.neurips.cc/paper_files/paper/20 06/hash/532b7cbe070a3579f424988a040752f2-Abstract.h tml.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\\nJones, L., Gomez, A. N., Kaiser, L., and Polosukhin,\\nI. Attention Is All You Need, December 2017.\\nURLhttp://arxiv.org/abs/1706.03762 .\\narXiv:1706.03762 [cs].\\nWang, W., Bao, H., Dong, L., Bjorck, J., Peng,\\nZ., Liu, Q., Aggarwal, K., Mohammed, O. K.,\\nSinghal, S., Som, S., and Wei, F. Image as a\\nForeign Language: BEiT Pretraining for All Vi-\\nsion and Vision-Language Tasks, August 2022.\\nURLhttp://arxiv.org/abs/2208.10442 .\\narXiv:2208.10442 [cs].\\nYang, Y ., Huang, Z., and Wipf, D. Transform-\\ners from an Optimization Perspective, May 2022.\\nURLhttp://arxiv.org/abs/2205.13891 .\\narXiv:2205.13891 [cs].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5409fd2-7759-4d4d-b8ed-40503510aa01', embedding=None, metadata={'page_label': '9', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='63ce423cb82f11f4a189b02b5e7fad7c7f980199f9a8f6ae1a879dd89bec4088', text='Attention: Marginal Probabiliy is All You Need?\\nYuille, A. L. and Rangarajan, A. The Concave-Convex\\nProcedure (CCCP). In Advances in Neural Information\\nProcessing Systems , volume 14. MIT Press, 2001. URL\\nhttps://proceedings.neurips.cc/paper/2001/hash/a012 869311d64a44b5a0d567cd20de04-Abstract.html .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0bad7133-fa6b-4a1b-acd0-79d5f5542c8a', embedding=None, metadata={'page_label': '10', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='dc9a60866c57c4542bb31e9995cda0f21c1b58a45b8ee16f228a93f6af29affc', text='This figure \"example_figures.png\" is available in \"png\"\\n format from:\\nhttp://arxiv.org/ps/2304.04556v1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9fa894e9-00fc-4f9f-9dce-fad0b13b3989', embedding=None, metadata={'page_label': '11', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='77a8b9e7d933d129ed710296248691c490592434b799dd625006abe8acee06c6', text='This figure \"example_graphics.PNG\" is available in \"PNG\"\\n format from:\\nhttp://arxiv.org/ps/2304.04556v1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4b3c8227-c217-4f27-adf0-03fa6d8108cb', embedding=None, metadata={'page_label': '1', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='393dbf743b9977d3b79e9d9b3f604f6c15658149c87c1400e7e07daf67ea7797', text='RITA: Group Attention is All You Need for Timeseries Analytics\\nJiaming Liang\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nliangjm@seas.upenn.eduLei Cao\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nlcao@csail.mit.eduSamuel Madden\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nmadden@csail.mit.edu\\nZachary Ives\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nzives@cis.upenn.eduGuoliang Li\\nTsinghua University\\nBeijing, China\\nliguoliang@tsinghua.edu.cn\\nABSTRACT\\nTimeseries analytics is of great importance in many real-world\\napplications. Recently, the Transformer model, popular in natu-\\nral language processing, has been leveraged to learn high quality\\nfeature embeddings from timeseries, core to the performance of\\nvarious timeseries analytics tasks. However, the quadratic time and\\nspace complexities limit Transformers scalability, especially for\\nlong timeseries. To address these issues, we develop a timeseries an-\\nalytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention , to address this scalability issue. Group attention dy-\\nnamically clusters the objects based on their similarity into a small\\nnumber of groups and approximately computes the attention at\\nthe coarse group granularity. It thus significantly reduces the time\\nand space complexity, yet provides a theoretical guarantee on the\\nquality of the computed attention. The dynamic scheduler of RITA\\ncontinuously adapts the number of groups and the batch size in the\\ntraining process, ensuring group attention always uses the fewest\\ngroups needed to meet the approximation quality requirement. Ex-\\ntensive experiments on various timeseries datasets and analytics\\ntasks demonstrate that RITA outperforms the state-of-the-art in\\naccuracy and is significantly faster  with speedups of up to 63X.\\n1 INTRODUCTION\\nMotivation. Many data driven applications involve processing\\nmassive timeseries data, including IoT [11], medical AI [14], stock\\nmarket [ 27], and so on. As such, there is a great need for timeseries\\nanalytics, such as forecasting [ 8], classification [ 20], clustering [ 31],\\nsimilarity search [ 39], and anomaly detection [ 50], with applications\\nranging from automatically diagnosing diseases [ 5], recognizing\\nhuman activities [29], to stopping financial fraud [59].\\nEffective feature extraction [ 40] lies at the core of almost all\\nthese timeseries analytics tasks. Recently researchers [ 61] have\\nstarted leveraging the self-supervised pre-training methodology of\\nTransformers [ 4,16,52], which have proven remarkably successful\\nin natural language processing (NLP), to automatically learn high\\nquality feature embeddings from timeseries. In NLP, self-supervised\\npre-training exploits the sequential patterns (correlations) among\\nthe words in sentences to produce contextualized feature embed-\\ndings. Timeseries bear similarity to natural language, because in\\ntimeseries data the sequential order among the values (stock price,\\nvolume, etc.) over time matters. That is, each value is highly cor-\\nrelated with other values observed before or after it. Therefore,\\nCorresponding Authorpre-training a Transformer model which takes the correlations\\namong different observations into account is a natural idea to learn\\nfeature embeddings from timeseries. Indeed, the experiments in [ 61]\\nconfirm that Transformer-based methods outperform traditional\\ntimeseries analytics techniques.\\nHowever, existing work [ 61] that directly applies Transformers\\nto learn features from timeseries data have been shown not to be\\nscalable to long timeseries [ 30]. The idea of self-attention [ 52] is\\ncentral to pre-training methods in NLP: It computes pairwise cor-\\nrelations among different semantic units in a sequence (in NLP, a\\nsentence); as such, it has quadratic time and space complexity in\\nthe length of the input sequence. Such an approach places limits on\\nthe models scalability, especially when handling large sequences,\\nwhich are common in real-world timeseries applications such as\\nIoT, medical AI, and finance [ 6,34,62]. Predictions about timeseries\\nmay need to look at months or years of historical data to make ac-\\ncurate predictions, spanning hundreds of thousands of samples. As\\nan example, in collaboration with a research hospital we have been\\ndeveloping a seizure classifier that automatically detects seizures\\nbased on EEG signals (timeseries) collected during the clinical ob-\\nservation of patients. As seizures last only a few seconds, we chunk\\nlong EEG data into many 2 second segments and detect seizures at\\na segment level. However, the classification of a particular segment\\ndepends on up to 12 hours of prior signal to determine if one 2\\nsecond segment indicates seizure or not, because seizure diagnosis\\nneeds to consider long-term trends in the EEG data [ 6]. The number\\nof segments in 12 hours is more than 21k. This is far larger than\\nthe number of semantic units the typical NLP tasks expect. For\\nexample, BERT [ 16] limits the number of units to 512 and even\\nmassive models like GPT-3 [4] limit the number of units to 2048.\\nAlthough in NLP some lower-complexity methods have been\\nproposed to approximately compute self-attention [ 10,26,54], their\\nperformance degrades dramatically when used on timeseries, due\\nto the gap between natural language and timeseries, as we will\\nshow in our experiments.\\nProposed Approach. To tackle the aforementioned problem, we\\ndevelop RITA , a Transformer-based timeseries analytics tool, which\\nuses a novel attention mechanism, called group attention , to scale\\nto long timeseries.\\nLeveraging the periodicity of timeseries, RITA chunks the input\\ntimeseries into segments and dynamically clusters the segments\\ninto a small number (denoted as ) of groups. Segments in the\\nsame group possess similar feature embeddings during the current\\ntraining iteration, thus enabling them to approximately share the\\n1arXiv:2306.01926v1  [cs.LG]  2 Jun 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='212f47a2-adf9-4142-a14e-8b893efedfa1', embedding=None, metadata={'page_label': '2', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='92cc0b55e42fee4ecbaf5eccfff11820800de8f1600e6c8055c0422cd7688161', text='computation of attention. As the timeseries increases in length,\\nmore sharing opportunities become available. RITA then computes\\nthe self-attention at a group level and produces a compressed group\\nattention matrix . In this way, group attention eliminates both com-\\nputation and memory bottlenecks in Transformer-style models and\\nthus more scalable to long timeseries.\\nHowever, making this idea effective and efficient in Transformer\\narchitectures is challenging for several reasons:\\nEfficiently Producing High Quality Feature Embeddings.\\nAlthough RITA computes the attention matrix at a group level, to\\npreserve the quality of the feature embeddings, it still has to pro-\\nduce different embeddings for different segments. This is because\\neven if some segments share the attention score temporally, it does\\nnot mean they should have the same feature embedding. However,\\nusing the group attention matrix, the existing self-attention mech-\\nanism will only produce a single feature vector for each group. A\\nnaive solution would be to restore the original attention matrix\\nfrom the group attention matrix. However, in this case we again\\nget an attention matrix with quadratic space complexity. Because\\nGPUs have limited memory, GPU memory will remain a bottleneck\\nin group attention.\\nThe Number of Groups N. In RITA, the number of groups\\nis a crucial factor that balances the speed up and the quality of\\nattention approximation. A small will lead to a large speedup,\\nbut the approximation errors can also be significant. On the other\\nhand, although a large tends to produce high-quality approxima-\\ntions, it inevitably slows down the training process. Therefore, an\\nappropriate is essential to the performance of group attention.\\nHowever,depends on the distributional properties of the dataset.\\nFurthermore, like the classical transformer models, RITA stacks\\nmultiple attention layers to produce better embeddings. Ideally,\\ndifferent layers should also use different values of . In addition,\\nduring the model training phrase, group attention should use dif-\\nferent values of at different iterations to adapt to the varying\\nfeature embeddings. This makes manually setting appropriate \\nalmost impossible.\\nBatch Size. Moreover, as we want to dynamically adjust \\nduring training, a fixed batch size is sub-optimal: as decreases,\\nthe memory usage of a single sample decreases. This allows a larger\\nbatch size which is beneficial, because: (1) it makes full use of GPU\\nmemory; (2) high-parallelism across the samples in a big batch\\nbrings better performance. Our experimental study shows that\\ndoubling the batch size reduces the training time by 30%, while still\\npreserving the quality of the model. Thus, RITA should dynamically\\nadjust batch size as changes.\\nTo address the above problems, we first propose an embedding\\naggregation strategy and a customized group softmax function to\\nreplace the classical softmax function [ 52]. Together they ensure\\nRITA is able to directly use the compressed attention matrix to\\nproduce different feature embeddings for different segments. We\\ntheoretically show the embeddings RITA produces in this way are\\nidentical to those produced by first re-storing the original large\\nattention matrix. Thus RITA is able to produce high quality embed-\\ndings without introducing extra overhead. Further, we design a GPU\\nfriendly algorithm to group the segments in parallel , effectively\\nminimizing the grouping cost.\\nP0PositionEmbeddingW1+++Window Embedding+E0\\nRawTimeseriesTime-aware ConvolutionW[CLS]W2\\n.....WnP1P2.....Pn.....E1E2En.....O0O1O2On.....RITA Encoder\\nScale & InputFigure 1: RITA Architecture\\nSecond, we design an adaptive scheduler which dynamically de-\\ncides an appropriate for each group attention layer during the\\ntraining process. It starts with a large and iteratively merges\\ngroups that are similar to each other. Guided by an error bound on\\nthe approximated self-attention that users can tolerate, it automati-\\ncally determines if two groups are mergeable, performing merging\\nefficiently in a GPU-friendly way.\\nMoreover, we propose a learning-based method to model the\\ncorrelation between the number of groups and the batch size .\\nThis model is used to predict for a givenwhen training RITA.\\nSpecifically, we first sample some values in a reasonable range.\\nFor each sampled , we find a batch size that consumes up to a\\ncertain percentage of GPU memory in a cost-efficient way. Using a\\nsmall set of mathematical functions as a prior, RITA learns a model\\nwith only a few <N, B> pairs as ground truth labels.\\nOur experiments on public timeseries benchmarks and the MGH\\nEEG data [ 6] confirm that RITA outperforms state-of-the-art meth-\\nods in accuracy on various timeseries analytics tasks, while our\\ngroup attention mechanism achieves a 63X speedup with much\\nless memory required, compared to existing self-attention mecha-\\nnisms [10, 52, 54].\\nContributions. The key contributions of this work include:\\nOur group attention mechanism leverages the periodicity of\\ntimeseries, reducing the time and space complexity of the self-\\nattention mechanism with accuracy guarantees, allowing RITA to\\nscale to long timeseries data.\\nGuided by an approximation error bound, our adaptive sched-\\nuler dynamically adapts the number of groups and the batch size\\nto the distribution properties of the evolving feature embeddings,\\nmaking group attention efficient and easily tunable.\\nWe conduct experiments on various datasets and different ana-\\nlytics tasks, demonstrating that RITA is 4 to 63 times faster than\\nthe state-of-the-art while achieving better accuracy when handling\\nlong timeseries (length 2000).\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60ad65f3-ac75-4d6a-aab5-95bc66be7048', embedding=None, metadata={'page_label': '3', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='fcf8cd4e8280b6bc583520b9a875717733ca0d4cb2224b72cb0ba8687649095a', text='2 BACKGROUND\\nWe provide some background on the canonical self-attention mod-\\nule in the Transformer[ 52]. Aself-attention module takes hidden\\nembedding vectors Ras input, then projects them to\\nqueries (), keys () and values ( ) and performs Scaled-dot Prod-\\nuct Attention, which given input hidden state , is computed by:\\n=,=,=\\n==(\\n\\n)(1)\\nWhereR,R,Rare projection\\nmatrices for generating ,, .Ris also regarded as the\\npacking ofquery vectors{1,...,}with dimension into a\\nmatrix.R,Rare regarded as the packing of key\\nvectors{1,...,}and value vectors{1,...,}in the same way.\\nGiven a matrix R, the softmax function normalizes \\nto ensure the sum of each row equals to 1, as shown below.\\n(,)=(,)\\n1\\n=0(,)(2)\\nNote the attention matrix A is an matrix, where represents\\nthe number of elements in the input sequence (e.g. words in NLP).\\n3 RITA OVERVIEW\\nGiven a collection of unlabeled timeseries, RITA first pre-trains\\na Transformer-style model to produce high quality feature em-\\nbeddings for timeseries data. This pre-trained model is then used\\nto support various downstream tasks, similar to BERT [ 16]. Next,\\nwe overview the model architecture of RITA. We show how RITA\\nsupports various downstream tasks in Appendix A.7.\\nAs shown in Fig. 1, RITA is consist of two components: (1) Time-\\naware Convolution Layer (2) RITA Encoder.\\nTime-aware Convolution Layer fills the gap between timeseries\\nand natural language. Despite their high-level similarity, there is a\\nbig gap between timeseries and natural language. First, in natural\\nlanguage each word, as a discrete semantic unit, has an indepen-\\ndent meaning, while each element in a timeseries is a continuous,\\nnumerical value and does not necessarily constitute an independent\\nevent. Furthermore, the input sequences are single-channeled in\\nNLP, but often multi-channeled in timeseries (i.e., sensor data often\\nconsists of several related channels).\\nRITA leverages the classical convolution [ 28] strategy to solve\\nthis problem. Convolution is widely used to capture the local struc-\\ntures of an image. We use convolution to chunk one input timeseries\\ninto a sequence of windows and learn the local structure of each\\nwindow, similar to the discrete semantic units in natural language.\\nIt also discovers the correlations across different channels, thus\\nnaturally solving the multi-channel problem.\\nMore specifically, treating a multi-variate timeseries of length \\nand withvariables as an nmmatrix, RITA usesconvolution\\nkernels to chunk intonwindows and produce one d-dimensional\\nembedding per window using the convolution operation [ 28]. Each\\nconvolution kernel corresponds to a wmmatrix, where defines\\nthe number of timestamps that each convolution kernel covers,\\nidentical to the window size in sliding window.\\nRITA Encoder functions as Transformer Encoder as described in\\nthe original Transformer work[ 52]. It takes the embeddings of semantic units 1,2,...,()as input (e.g. embeddings of\\nwindows for a timeseries), then models the correlations between\\nthe semantic units and outputs 1,...,()as the context-\\naware embedding of each unit.\\nWhat makes RITA Encoder different from Transformer Encoder\\nis that: at the core of Transformer Encoder lies self-attention mech-\\nanism which incurs a (2)time complexity and memory usage.\\nThis quadratic cost becomes prohibitive for long timeseries and\\nlimits the scalablity of Transformer-based models. To make the\\nattention computation efficient yet high-quality, we replace the\\ncanonical self-attention with our proposed group attention .\\nSelf-supervised Pretraining. Inspired by the cloze text pre-\\ntraining task in NLP, we designed a mask-and-predict task as the\\npretraining task for our model. The timeseries is randomly masked\\nand the model should recover the masked values based on corre-\\nsponding contextual information.\\nTo be specific, we generate masks on time-stamps, with a mask\\nrate. The timeseries is scaled to be non-negative and the values\\nacross all the channels on the masked timestamps are set to be -1,\\nan impossible value on normal timestamps. Then the masked time-\\nseries is fed into RITA and the output representation is translated\\nto the recovered timeseries by a Transpose Convolution layer.\\n4 GROUP ATTENTION MECHANISM\\nGroup attention, a novel and efficient approximate attention mecha-\\nnism, addresses the performance bottleneck of self-attention in the\\nvanilla Transformer. In this section, we first introduce the frame-\\nwork of group attention and then theoretically establish the bound\\nof its approximation error.\\n4.1 The Idea of Group Attention\\nAs periodicity is a natural property of timeseries [ 56], similar\\nwindows frequently occur. Similar windows result in similar\\nqueries/keys for attention computation, bringing opportunities for\\nsaving computation.\\nAs discussed in Sec. 2, , the attention score of window onto\\nwindow, is determined by the inner product between the query\\nvector of window and the key vector of window , that is,.\\nGiven another window , if window has the similar key vector\\nto window, that is,, then. In other words,\\nwhen.\\nThis observation inspires our group attention mechanism. That\\nis, we group the windows by their similarity in keys. Assuming\\nall windows in the same group have the same attention score onto\\nanother window , we then only compute the attention once by\\nusing one single key to represent this group, for example the centroid\\nof the group of keys. This thus saves significant computation cost.\\nBetter yet, after grouping windows into groups, group atten-\\ntion compresses the attention matrix from an matrix to an \\nmatrix. Because (number of groups) tends to be much smaller\\nthan(number of windows) due to the periodicity of timeseries,\\ngroup attention consumes much less memory than the original\\nself-attention mechanism, successfully eliminating the memory\\nbottleneck. Note that it also doesnt hurt quality all that much, as\\nconfirmed in our experiments (Sec. 6.2).\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='84a8b73f-1af7-4cb4-806a-fed90c9c5c02', embedding=None, metadata={'page_label': '4', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='456100fe842ce4b180956399fbbe40389a161d2c095de011dd371071ff50e39d', text='GroupingAverageKQMatMulAttentionMatrixWeightedSoftMaxVSumAggregateTransposeMatMulOutputQ K VFigure 2: Group Attention\\n4.2 Computing the Output Feature Embedding\\nWe now discuss how to efficiently compute the output feature\\nembeddings using the small compressed group attention matrix.\\n4.2.1 Problem: Producing Embeddings w/ Group Attention Matrix\\nAs described in the Background, once we have acquired the at-\\ntention matrix , canonical self-attention computes the output\\nembeddingasO=AV. Becauseis anmatrix andis an\\nmatrix, the matrix product operation still produces an \\nmatrix. That is, it produces a dimensional feature vector for\\neach window . However, our group attention will produce an \\nattention matrix e, wherecorresponds to the number of groups.\\nIn this case the matrix product will produce a matrixe. That\\nis, it produces a feature vector for each group . However, our goal\\nis to produce different embeddings for different windows, because\\neven if some windows share the attention score temporally, it does\\nnot mean they should have the same feature embedding.\\nA Naive Solution. A naive solution would be to restore the full\\nattention matrix from the group attention matrix e. For example,\\ngiven one group composed of and, we map its group\\nattention vector in einto two rows that correspond to and\\nin. However, in this case we again get a attention\\nmatrix; and GPU memory remains a bottleneck in group attention.\\n4.2.2 Solution: Embedding Aggregation and Group SoftMax\\nUsing an embedding aggregation operation and a group softmax\\nfunction, RITA produces embeddings without restoring the full\\nattention matrix. Fig. 2 shows the workflow of group attention.\\nEmbedding Aggregation. The idea is inspired by the observation\\non the matrix product operation O=AVconducted on the fully\\nrestored attention matrix .\\nGiven an element ,ofcorresponding to the dimension of\\ns feature vector, ,=, where vector aiRndenotes the\\nrow of the attention matrix and vector vjRndenotes the \\ndimension of all the feature vectors. Given ai=<a1\\ni,a2\\ni,,an\\ni>\\nandvj=<v1\\nj,v2\\nj,,vn\\nj>,,=n\\nk=1ak\\nivk\\nj.\\nAs an example, assume  1and 2belong to the same group\\n1. Then1\\n=2\\n=e1\\n, where e1\\necorresponds to the attention\\nof group1onto. Therefore, 1\\n1\\n+2\\n2\\n=e1\\n(1\\n+2\\n).\\nAs an immediate generalization of the above analysis, if we ag-\\ngregate up the windows that belong to the same group and convert\\nthe n-dimensional feature vector into a-dimensional group fea-\\nture vector ebeforehand, we could directly use the group attention\\nvectoreand the group feature vector eto compute ,.Using embedding aggregation, RITA is able to produce the fea-\\nture embedding ethat is identical to the embedding produced\\nby using the full attention matrix and the embedding matrix .\\nGroup Softmax Function. In canonical self-attention the atten-\\ntion matrix is computed as =SoftMax(QKT\\ndk). To compute ,\\nwe have to first compute (denoted as ) which is an \\nmatrix. Then normalizing the matrix with softmax produces the\\nattention matrix .\\nGroup attention follows the same procedure. But after grouping\\nkeys into e,eproduces an matrix e. Due to the non-\\nlinearity of the softmax function, applying softmax directly on e\\nwill result in a group attention matrix efrom which we are not able\\nto recover a full attention matrix that is identical to first restoring\\netoand then applying softmax on . Thematrix produced\\nby the latter is desirable, as we want to approximate the original\\nattention matrix as accurately as possible. However, restoring the\\nsmallematrix is not memory efficient, as it will end up with\\na fullmatrix.\\nTo solve the above problems, we introduce a new group softmax\\nfunction to replace the original softmax function (Eq. 2).\\n(g,)=(,)\\n1\\n=0(,)(3)\\nIn Eq. 3,represents the number of windows that Group\\ncontains. Compared to the original softmax, our group softmax\\nconsiders each group aselements and counts it \\ntimes when summing up the exponential of each groups ,. In\\nthis way, the group softmax function operating on the small e\\nmatrix will produce exactly the same result to the softmax function\\noperating on the full matrix.\\nTheoretical Guarantee. In Appendix A.4, we prove that the group\\nsoftmax function and the embedding aggregation operation produce\\nthe same output feature embedding with the naive method that has\\nto first restore the big full attention matrix.\\nWe show an efficient implementation of the embedding aggrega-\\ntion operation and group softmax function in Appendix A.2, Alg. 1.\\nTime Complexity. The time complexity of Alg. 1 is ()and\\nthe space complexity is (), while the time and space complexity\\nof the original self-attention mechanism are (2)and(2).\\n4.3 Error Bound\\nGroup attention produces a group attention matrix ewhich approxi-\\nmates the attention matrix produced by the classical self-attention\\nwith a bounded error , as shown in Lemma 1.\\nLemma 1. Letbe the radius of the ball where all key vectors\\nlive;ebe the representative of the group that contains key . Let\\ndenote the full attention matrix restored from e. Suppose the distance\\nbetween eand(||ekk||)satisfies:||ekk||d.\\nThen>1, ifdln()\\n2R,1\\nAi,j\\nAi,j\\nLemma 1 shows that the error bound of the group attention is\\ndetermined by the distance . As discussed in Sec. 5.1, it inspires\\nus to design a strategy to dynamically determine the number of\\ngroups the most critical parameter of group attention. Please\\nrefer to Appendix A.5 for the proof.\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='21f8b7ce-d9c7-44ae-8865-9a063ca1e4bf', embedding=None, metadata={'page_label': '5', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='829194a8202fa185f5bfc40d8bcb6891ebbd0c8a4c382caa9d1358a9f47a87b5', text='4.4 GPU Friendly Grouping Method\\nIn this section, we discuss the implementation of a grouping method.\\nTo make group attention efficient and effective, the grouping\\nmethod has to satisfy the following requirements:\\n(1) Tight distance bound: to ensure the approximation quality,\\nthe distance between each key and its group representative should\\nbe minimized according to Lemma 1.\\n(2) Lightweight: to ensure the performance gain, the grouping\\nmethod must be lightweight, at worst not exceeding the complexity\\nof group attention itself ( ()).\\n(3) GPU friendly: to take advantage of GPUs, we prefer a group-\\ning method that mainly consists of matrix operations, which can\\nbe efficiently executed on a GPU.\\nTo satisfy the above requirements, after thorough investigation\\non various clustering algorithms, we design a GPU friendly K-\\nmeans [35] as the grouping method.\\nFirst, K-means minimizes the overall distance between any object\\nand its cluster center, hence naturally satisfying Requirement 1.\\nSecond, given centers, in each iteration the time and space\\ncomplexity of K-means is (). Usually, the iteration goes until\\nconvergence. However, we observe that rather than seeking a per-\\nfect K-means clustering, training a few iterations is sufficient to\\nget a good grouping for group attention, because typically the later\\niterations only slightly update the clustering and group attention\\nis robust to such imperfection.\\nThird, we design a GPU-friendly implementation of K-means.\\nThe performance bottleneck of K-means comes from the dis-\\ntance computation between each vector and its center, that is,\\n|vicj|=\\n(vicj)2,i[1,n],j[1,N]. The performance bot-\\ntleneck is. We instead use a different formulation: |\\n|=|vicj|=\\n|vi|2+|cj|22vicj,i[1,n],j[1,N]. This is\\nbecause in this formulation, the performance bottleneck is ,\\nwhich could be implemented as a matrix product operation. Al-\\nthough the complexity of the two formulations is the same, in GPUs\\nmatrix product is much more efficient than pairwise difference.\\n5 ADAPTIVE SCHEDULER\\nNext, we present the adaptive scheduler of RITA which addresses\\nthe challenges of determining an appropriate number of groups\\nand accordingly the batch size , as described in Introduction.\\nUsing a dynamic scheduling method we propose, the scheduler\\nautomatically determines and adjusts andbased on the distri-\\nbutional properties of the feature embeddings produced over the\\niterative training process, while guaranteed to produce high quality\\nattention approximation that meets the requirement of users.\\nIn Sec. 5.1 we show how RITA automatically determines . Then\\nwe introduce in Sec. 5.2 the learning-based method which given an\\n, immediately predicts a good batch size.\\n5.1 Dynamically Determining the Number of\\nGroups N\\nWithout loss of generality, we use one group attention module as\\nan example to show how RITA automatically gets an appropriate .\\nThe adaptive scheduler of RITA starts with a large and decreases\\nit dynamically. This is because in the training process of RITA, thefeature embeddings produced epoch by epoch tend to get stabler\\nand stabler and gradually converge, thus no need to increase .\\nRITA reduces the number of groups by merging similar groups.\\nIntuitively, given two groups, we could measure their similarity\\nbased on the distance of their centers. If the distance between\\ntheir centers is smaller than a distance threshold, then the two\\ngroups could be merged. However, setting an appropriate distance\\nthreshold seems hard  as difficult as setting an appropriate .\\nTo solve this problem, RITA leverages the error bound of group\\nattention introduced in Sec. 4.3. It only requires users to set an\\nerror bound , and then uses Lemma 1 to translate to a distance\\nthreshold. RITA then uses Lemma 2 to determine if merging some\\ngiven clusters still meets the error bound threshold .\\nLemma 2. Denoteto be the cluster center of . Assume\\nthe existing grouping satisfies k,max\\nxcluster k|ckx|d, thus satis-\\nfying an error bound by Lemma 1. If there exist clusters, namely,\\n1,2,..., , satisfying that:\\n\\n||+||,,[1,] (4)\\nmerging them into one cluster still meets the error bound .\\nPlease refer to Appendix A.6 for the proof.\\nFinding the Mergable Clusters. We formulate the problem of\\nfinding mergeable clusters using graph theory:\\n(1) each cluster is a node in the graph;\\n(2) ifandsatisfy:\\n\\n||+||, and\\n||+||\\nthere is an undirected edge between and;\\nIn this scenario, finding the maximum number of mergeable\\nclusters is equivalent to finding the minimal clique cover in the\\ncorresponding graph, which is an NP-hard problem [ 24]. Such\\nheavy computation overhead is not acceptable for RITA. We thus\\noffer a simplified solution:\\n(1) Halve the clusters into two sets 1,2;\\n(2) If1and2satisfy:\\n\\n||+||, \\n||+||\\n2\\n(5)\\nis marked.\\n(3) Decrease the number of clusters by counting the masks in 2.\\nIn this solution, clusters in 1can be regarded as transfer nodes.\\nIf(5)holds for(1,12)and(\\n1,22), respectively, we have,\\n\\n1|12|+|1|\\n\\n1|1|+|2|+|1|\\n\\n1|1|+|2|+|1|+|2|(6)\\nThus (4)holds when merging several clusters in 2with one\\ncluster in1. As a result, we can greedily merge clusters in 2, as\\nillustrated in step(3).\\nAssume the number of clusters decreases by after merging,\\nwe apply a momentum update [ 42] on the number of clusters , as\\nis commonly used in machine learning to smooth the changing of\\nand avoid sample selection bias. To be specific: =(\\n)+(1), whereis a hyper-parameter for momentum.\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e0a6b86-7f11-4bfe-9292-71f12d4f98ea', embedding=None, metadata={'page_label': '6', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='e4eed7f598028156094c544e7580dc8c373be8350a2718db0940197e6f87aa95', text='5.2 Dynamically Determining the Batch Size\\nBecause of the dynamic grouping operation, the computational\\ngraph in deep learning training [ 1] varies from sample to sample. As\\na result, it is impossible to precisely compute a batchs GPU memory\\nusage without indeed feeding it into the model. To overcome this\\nproblem, RITA learns a batch size prediction function offline; then\\nat the RITA training time, given a number of groups , RITA uses\\nthis function to predict a proper batch size.\\nWhen the model architecture and hardware are fixed, the batch\\nsize depends on the length of the timeseries and the average\\ngroup number among all attention module . So RITA samples\\nseveral(,)pairs and estimate a proper batch size for each pair.\\nMore specifically, given a user-defined timeseries maximal length\\n, we randomly sample integral points (,)from plane\\n{1,1}. Then we use a binary search based\\nalgorithm to find the maximal batch size that consumes less than\\n90%available GPU memory, aiming to avoid wasting GPU memory\\nand the risks of out of memory (OOM).\\nTreating these pairs as ground truth labels, we use function\\nfitting [18] to learn the batch size predicting function B=f(L,N),\\nwhere B is a function of two variables and.\\nLearning the Prediction Function. We apply curve fit from\\nSciPy [ 53] as the function fitting tool to fit the two-variable function\\n=(,)on plane{1,1}.\\nWe observe that applying one function to the whole plane incurs\\na huge estimation error. So we develop a dynamic-programming\\n(DP) method to divide the plane into several sub-planes and apply\\na distinct function to each sub-plane respectively. It is optimal in\\nminimizing the total estimation error on all sub-planes\\nWith the learned prediction function , we can estimate a proper\\nbatch size for any (,)during training, even if it is not seen in\\nthe sampled(,)pairs.\\nThe Algorithms and Optimality Proof. Please refer to Appen-\\ndix A.3 for the pseudo code of the binary search-based algorithm\\nand the description of the DP method for plane-division and the\\nproof for its optimality.\\n6 EVALUATION\\nOur experimental study focuses on the following questions:\\n1.Effectiveness and efficiency of RITA : How does RITA com-\\npare with other Transformer-based methods and traditional time-\\nseries representation learning methods in accuracy and efficiency?\\n2.Ablation Study : How do the key techniques of RITA work?\\n6.1 Experimental Setup\\nDatasets. We evaluate RITA on classification and imputation tasks\\nusing 5 multi-variate and 3 uni-variate timeseries datasets.\\nWISDM [55] is a popular multivariate timeseries dataset gen-\\nerated from the accelerometer in the mobile phone. The subjects\\nperformed 18 daily activities (e.g. walking, jogging). The dataset\\nwas collected from 51 subjects and the sampling rate is 20 Hz.\\nHHAR dataset [ 46] contains sensing data of accelerometer col-\\nlected from 9 users performing 5 activities with 12 different smart-\\nphones (varying in sampling rate). This increases the complexity\\nof the task and thus can test the models robustness.RWHAR RealWorld HAR dataset [ 48] covers 15 subjects per-\\nforming 8 locomotion-style activities. Each subject wears the sen-\\nsors for approximately ten minutes. The sampling rate is 50 Hz.\\nECG dataset [ 34] consists of 10,000 EEG recordings for arrhyth-\\nmia classification. Each recording has an uncertain length ranging\\nfrom 6 to 60 seconds sampled at 500 Hz. The ECG recordings corre-\\nspond to 9 types of heart problems such as atrial fibrillation (AF)\\nand premature atrial contraction (PAC), etc.\\nMGH [6] is a EEG dataset collected by Mass. General Hospital.\\nEach timeseries corresponds to the EEG data observed from one\\npatient during their stay in ICU for a couple of days. The EEG\\nmonitoring produced data with 20 channels. The sampling rate is\\n200 HZ. So it produces very long timeseries.\\nWISDM*/HHAR*/RWHAR* are three uni-variate datasets de-\\nrived by picking one channel from WISDM/HHAR/RWHAR .\\nTraining/Validation Data Generation. We apply a sliding win-\\ndow on the raw timeseries to get training/validation samples. The\\nsize of the sliding window is set as 200 on small datasets (WISDM,\\nHHAR, RWHAR), 2000 on medium size dataset (ECG), and 10,000\\non the large dataset (MGH). Table 1 shows the statics of the gen-\\nerated datasets. They are randomly split into training/validation\\nset in a proportion of 0.9/0.1. In pretraining + few-label finetun-\\ning scenario, we use 100 labeled data per class for finetuning. We\\nguarantee that training set does not overlap with the validation set.\\nDataset Train. Size Valid. Size Length Channel Classes\\nWISDM 28,280 3,112 200 3 18\\nHHAR 20,484 2,296 200 3 5\\nRWHAR 27,253 3,059 200 3 8\\nECG 31,091 3,551 2000 12 9\\nMGH 8,550 950 10000 21 N/A\\nTable 1: The statistics of the datasets\\nAlternative Methods. We compare RITA against the SOTA Trans-\\nformer based timeseries representation learning method TST [61].\\nTo evaluate our group attention (referred to as Group Attn. ), we\\ndevelop three baselines by replacing the group attention compo-\\nnent in RITA with the classic vanilla Self-Attention [ 52](referred\\nto as Vanilla ) and two SOTA methods that reduce the complexity\\nof self-attention by approximation in NLP, namely, Performer [ 10]\\n(referred to as Performer ) and Linformer [ 54] (referred to as Lin-\\nformer ). Similar to our proposed Group Attn., Vanilla, Performer,\\nLinformer all use RITAs time-aware convolution operation (Sec. 3)\\nto turn timeseries segments into input feature vectors.\\nWe also compare Group Attn. against GRAIL [40], which is\\nthe SOTA of the non-deep learning methods for timeseries repre-\\nsentation learning. GRAIL supports classification tasks by feeding\\nthe learned representations into a Support-Vector Machine [ 12]\\nor K-Nearest Neighbor [ 17] classifier. Note GRAIL only targets\\nuni-variate timeseries and cannot support imputation tasks.\\nMethodology. We mainly focus on two downstream tasks:\\n(1)Classification . First, we train Group Attn. and the base-\\nlines with full labels from scratch to test the effectiveness of RITA\\nframework and the approximation quality of our group attention.\\nSecond, to measure the effectiveness of self-supervised pretrain-\\ning, we evaluate the accuracy of training on few labeled timeseries\\nwith/without pretraining on large scales of unlabeled timeseries. To\\nbe specific, we split the training set into a pretraining set and a fine-\\ntuning set, with very few data in the latter (100 labeled samples per\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f306cdf-a66e-4e99-89b9-4fd467bf65f6', embedding=None, metadata={'page_label': '7', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='5dc692b48ea4f6b9d969cbfadeb713e6c73e4c6a77f5b67fd357992f5dba5b1e', text='(a) Effectiveness (b) Efficiency\\nTrainingTime/secFigure 3: Full-label classification results (multi-variate data).\\nclass in our experiment). We train the model on the cloze pretrain-\\ning task with a mask rate =0.2. Then we train two classification\\nmodels using the finetuning set, either based on the pretrained\\nversion or from scratch. We repeat the experiment 5 times with\\nrandom data splits and report the median accuracy.\\n(2)Imputation . We run the imputation task on the datasets used\\nin classification as well as the large unlabeled MGH dataset, and\\nmeasure the mean square error and absolute imputation error. To\\nget timeseries with missing values, we randomly mask the values\\nwith an expected mask rate of =0.2. The masked values are\\nreplaced with a special value.\\nFinally, to evaluate Group Attn.s benefit on efficiency , the total\\ntime of forward computation, backward propagation, and grouping\\nare measured for all methods in all the experiments.\\nTo save space, we only report the average training time per epoch\\nhere and refer readers to Appendix A.8 for the inference time.\\nWe first compare against the Transformer-based methods on\\nmulti-variate datasets (sec. 6.2, 6.3), then compare against the non-\\ndeep learning method GRAIL on uni-variate datasets (sec. 6.4).\\nConfiguration. Please refer to Appendix A.1 for the experiment\\nconfiguration and hyper-parameter settings.\\n6.2 Effectiveness: Transformer-Based Methods\\nWe first evaluate the quality of the models trained with full labels\\nfrom scratch. We then show how the pretraining of RITA increases\\nthe accuracy of the downstream tasks.\\n6.2.1 full-label training (Multi-variate classification)\\nResults shown in Figure 3(a) get us the following observations:\\n(1) RITAs advantage over TST. On all four datasets for the clas-\\nsification tasks, Group Attn. and the other three baselines that use\\nRITA architecture (Vanilla, Performer, and Linformer) outperform\\nTST. In particular, Group Attn. outperforms TST by 49 percentage\\npoints on the ECG dataset (88.48% vs 39.93%) with long timeseries.\\nTwo deficiencies in TST may cause its poor performance on the long\\ntimeseries. Firstly, TST concatenates the output embedding vector\\nof each time stamp, then uses a linear classifier to do classification\\non the concatenated vector. When the timeseries is long, the linear\\nclassifier has so many parameters that it tends to overfit easily.\\nSecondly, TST replaces Layer Normalization in vanilla Transformer\\nwith Batch Normalization. When the timeseries is long, it can only\\naccommodate a small number of timeseries in each batch, leading\\nto bias in Batch Normalization.\\n(2) Group-attentions advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on3 out of 4 datasets for classification. Although Linformer works\\nslightly better than Group Attn. on the ECG dataset (90.37% vs\\n88.84%), its performance is the worst in all other cases compared\\nto any other RITA-based methods. Vanilla computes the attention\\nscores precisely. Thus it is expected to work well. However, Group\\nAttn. outperforms Vanilla on WISDM (87.50% vs 86.95%) and is very\\nclose to it on other 3 datasets. This suggests that group attentions\\napproximation quality is good.\\n6.2.2 pretraining + few label finetune (Multi-variate classification)\\nThe results shown in Table 3 get us the following observation:\\n(1) Pretraining is effective. Pretraining always leads to better\\naccuracy than training with a few labels from scratch. In particular,\\non WISDM data all the methods using RITA architecture increase\\nthe accuracy by at least 10%. This is impressive considering we do\\nnot have a very large unlabeled pre-training set to use.\\n(2) RITAs advantage over TST. our Group Attn. and other\\nthree baselines using RITA architecture (Vanilla, Performer, and\\nLinformer) significantly outperform TST on all four classification\\ndatasets by 25 percentage points.\\n(3) Group Attentions advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on 3\\nout of 4 datasets. When compared to Vanilla, Group Attn. is better\\non HHAR and ECG, and comparable on the other two, further con-\\nfirming its high quality on approximation. Further, we notice that\\nLinformer struggles in this setting: in average its accuracy is worse\\nthan Vanilla by 8.22% and Group Attn. by 8.01%. This is because the\\nlow-rank projection operation introduces extra model parameters,\\nmaking Linformer more easily overfit, while overfitting is especially\\nharmful when there are only a few labeled training samples.\\n6.2.3 full-dataset training (Multi-variate imputation)\\nSimilar to classification tasks, the results of imputation tasks\\n(Table.2) show that Group Attn. consistently outperforms the base-\\nlines in training time while achieving comparable/better MSE. Again,\\non the large dataset MGH (length = 10,000), TST and Vanilla fail due\\nto out of memory (OOM) errors. Methods using RITA framework\\n(Group Attn., Performer, Linformer) all achieve very low MSE (are\\nhighly accurate). Among them Linformer is the worst.\\n6.3 Efficiency: Transformer-based Methods\\nWe measure the efficiency by the average training time per epoch\\nincluding the cost of the forward computation + backward propaga-\\ntion and the grouping overhead. We first show the results on all the\\n5 datasets in Sec. 6.3.1. We then vary the length of the timeseries\\non the MGH dataset to show group attentions scalability on long\\ntimeseries in Sec. 6.3.2.\\n6.3.1 Training Time: All Multi-variate Datasets\\nThe results in Fig. 3(b) and Table 2 lead to the below observations:\\n(1) Vanilla Self-Attention is not scalable. In average, it takes\\n2-3 minutes to train one epoch when the length of the timeseries is\\nonly 200 (WISDM, HHAR, RWHAR), takes over 15 minutes when\\nthe length increases to 2,000 (ECG), and fails on the long MGH data\\nwhen the length reaches 10,000 due to out of GPU memory.\\n(2) Group Attn.s advantage over all other attention mecha-\\nnisms. As we have shown in Sec. 6.2, Group Attn. is more accurate\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='93dfaadf-09bc-479e-86e9-e0d4a6a233bc', embedding=None, metadata={'page_label': '8', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='1b83ea45707fe77c642ddbcf57959770d21200a4dfe4e254e796ccb72f4357db', text='Dataset LengthTST [61] Vanilla Performer Linformer Group Attn.\\nMSE Time/s MSE Time/s MSE Time/s MSE Time/s MSE Time/s\\nWISDM 200 13.30 150.3 3.240 178.1 3.449 162.6 3.852 141.9 3.277 136.7\\nHHAR 200 1.085 78.2 0.2968 97.4 0.2980 82.6 0.3198 81.1 0.2974 73.3\\nRWHAR 200 0.0882 83.9 0.0478 108.1 0.0489 89.1 0.0572 98.4 0.0478 81.3\\nECG 2000 0.0905 696.3 0.0037 857.9 0.0033 270.2 0.0035 291.38 0.0038 164.36\\nMGH 10000 N/A N/A N/A N/A 0.00014 356.2 0.00088 404.9 0.00042 54.4\\nTable 2: Imputation results (multi-variate data). The best results are marked with bold .\\nDataset Pretrain SizeTST [61] Vanilla Performer Linformer Group Attn.\\nScratch Pre. Scratch Pre. Scratch Pre. Scratch Pre. Scratch Pre.\\nWISDM 62,231 49.13% 50.03% 66.16% 75.89% 66.09% 73.97% 50.12% 67.44% 62.56% 75.06%\\nHHAR 68,294 72.56% 75.30% 75.60% 81.35% 76.52% 80.70% 65.94% 76.52% 76.17% 82.62%\\nRWHAR 63,599 69.46% 80.41% 85.68% 91.14% 87.54% 91.33% 81.03% 86.33% 86.13% 89.63%\\nECG 561,358 20.98% 27.99% 42.05% 46.16% 43.34% 45.58% 27.19% 31.34% 42.58% 46.39%\\nTable 3: Pretrain + few-label finetuning results. The best results are marked with bold .\\nTraining Time/secMSE(a) Effectiveness(b) Efficiency\\nFigure 4: Varying the lengths of timeseries.\\nthan Performer and Linformer in classification and imputation tasks,\\nwhile Group Attn. is always faster than Performer, Linformer, and\\nall other baselines on all 5 multi-variate datasets, thus a win-win .\\n(3) The longer the timeseries, the larger the speedup. On\\nthe medium sized ECG dataset with a length of 2,000, Group Attn.\\nhas a speedup of 3.86/1.36/2.27 compared to Vanilla/Performer/Lin-\\nformer. When the length increases to 10,000, the speedup on the\\nMGH dataset increases to 6.59/7.48 compared to Performer/Lin-\\nformer (Vanilla and TST failed in this case) on imputation task\\n(Table. 2). However, even on the short WISDM, HHAR, RWHAR\\ndatasets, Group Attn. still consistently outperforms other methods,\\nconfirming that it does not introduce much overhead. This is be-\\ncause when the length of the timeseries gets longer, Group Attn.\\ngets more opportunities to find windows with similar properties.\\n6.3.2 Training time: Varying the Length\\nIn this experiment, we truncate the original MGH timseries into\\nsequences with the lengths at 2000/4000/6000/8000/10000, and com-\\npare Group Attn. against Vanilla and other attention mechanisms.\\nVanilla cannot handle sequences longer than 8000.\\nThe results in Fig. 4 again show that the longer the timeseries, the\\nlarger the speed up . With comparable MSE, Group Attn. outperforms\\nVanilla by 63X. Moreover, as the length increases from 2000 to 10000,\\nthe training time of Group Attn. only increases from 31.2 seconds\\nto 54.4 seconds per epoch. The reason is that as the timeseires\\nbecomes longer, there are more grouping opportunities because of\\nthe similarity of the timeseries segments.\\nAccuracy\\nTraining Time/sec\\n(a)(b)Figure 5: Comparison to non-deep learning method (uni-\\nvariate data).\\n6.4 Comparison to Non-deep Learning Methods\\nWe compare against GRAIL, the SOTA of non-deep learning time-\\nseries representation learning. We use the three uni-variate datasets,\\nbecause GRAIL only targets uni-variate timeseries.\\nResults in Fig. 5 show that on all 3 datasets RITA significantly\\noutperforms GRAIL in accuracy by 45, 16, and 21 percentage points\\nbecause of the expressive power of Transformer. Moreover, thanks\\nto the GPU-friendly design of RITA, it is at least 2 faster than\\nGRAIL in training time.\\n6.5 Ablation Study\\n6.5.1 Adaptive Scheduler\\nTo evaluate the effectiveness of RITAs adaptive scheduler (Sec. 5),\\nwe compare it against a baseline using a fixed group number . We\\nvaryand the error bound threshold used by RITA.\\nFrom the results in Table 4 we get the following observations:\\n(1) Adaptive Scheduler is better than fixed .Training with\\nAdaptive Scheduler already achieves better or comparable perfor-\\nmance compared to the best performing . More specifically, on\\nthe MGH dataset, dynamic scheduler always achieves better accu-\\nracy and is much faster compared to fixed . On the ECG dataset,\\nalthough fixed is slightly better than adaptive scheduler in accu-\\nracy when setting the N as 512, it runs much slower than adaptive\\nscheduler. Of course, finding the best that balances the accuracy\\nand running time requires careful tuning.\\n(2) Adaptive Scheduler is tuning free. It is robust on both\\naccuracy and running time when varies, while the results of\\nfixedvary significantly when the value of changes. Therefore,\\nAdaptive Scheduler frees the users from tuning the threshold,\\nwhile it is hard to find an appropriate for a given dataset.\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5bdd0ac6-d5d4-42b8-a60b-8434964bd5d3', embedding=None, metadata={'page_label': '9', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='8a45615ac2e105853fc7c171d7f17d530eb810c3e8f8356281762031dc7ef5ed', text='Dataset Task Scheduler Parameter Metric Time\\nECG Class.Dynamic1.5 88.34% 292.5\\n2 88.48% 236.8\\n3 87.83% 216.8\\nFixed64 87.50% 255.2\\n128 88.96% 297.2\\n256 88.82% 414.1\\n512 90.03% 662.6\\n1024 88.65% 873.7\\nMGH Imput.Dynamic1.5 0.00041 60.7\\n2 0.00040 57.9\\n3 0.00042 54.4\\nFixed128 0.00054 128.6\\n256 0.00053 190.2\\n512 0.00049 240.8\\n1024 0.00046 323.3\\nTable 4: Adaptive Scheduling VS Fixed N.\\nPretrain Data size Few-label Accuracy\\nN/A 62.56%\\n12,446 72.94%\\n24,892 72.78%\\n37,338 74.10%\\n49,784 74.22%\\n62,231 75.06%\\nTable 5: RITA Pretraining: increasing sizes of pretrain set.\\n6.5.2 The Sizes of the Pretraining Data\\nNext, we evaluate how the number of unlabeled data influences the\\neffectiveness of pretraining. To get empirical results, we pretrain\\nRITA on WISDM dataset with 20%/40%/60%/80% of the pretraining\\ndata and finetune each pretrained model with 100 labels per class.\\nThe results in Table 5 show that: (1) The more pretraining data,\\nthe larger the improvement. The accuracy increases with the\\nsizes of the pretraining data; (2) Marginal utility diminishing.\\nThe first 20% pretraining data gives a 10.38% improvement in accu-\\nracy (72.94% vs 62.56%), while the remaining 80% pretraining data\\nonly gives an additional improvement of 2.12% (75.06% vs 72.94%).\\n7 RELATED WORK\\n7.1 Timeseries Analytics\\nThere is a great deal of prior work on timeseries analytics methods.\\nThis work can be divided into three categories: (1) non-deep learn-\\ning methods; (2) CNN/RNN-based deep learning methods; and (3)\\nTransformer-based deep learning methods.\\nTraditional Methods. These methods, such as TS-CHIEF [ 45],\\nHIVE-COTE [ 33], ROCKET [ 15] have achieved notable performance\\non public datasets. Despite that, traditional methods suffer from\\none or more issues: they (1) rely on expert knowledge for feature\\nextraction; (2) incur heavy computation cost and are inappropriate\\nfor GPU devices; (3) support only uni-variate timeseries; (4) perform\\nclassification solely. Some work [ 61] shows that the transformed-\\nbased methods outperform these traditional methods especially on\\nmulti-variate timeseries.\\nIn particular, as the SOTA of timeseries representation learn-\\ning, GRAIL [ 40] extracts landmarks from data and computes the\\nrepresentations with the combination of the landmarks. However,\\nGRAIL only supports uni-variate timeseries. Our experiments (Sec. 6.4)\\nshow that RITA significantly outperforms GRAIL in both effective-\\nness and efficiency on uni-variate timeseries.CNN/RNN-based Deep Learning Methods. CNN-based methods,\\nsuch as InceptionTime [ 21] and Resnet [ 19], are good at classifica-\\ntion tasks, but can not handle generative tasks such as forecasting\\nbecause of the inductive bias of convolution networks. RNN-based\\nmethods, such as Brit [ 7] and deepAR [ 44], are capable for classifi-\\ncation, regression and generation. However, the recurrent structure\\nbrings a lot of problems: (1) limiting the models ability in captur-\\ning long-range correlation; (2) notoriously difficult to train [ 41]\\nbecause of gradient vanishing and exploding problem. As a result,\\nsuch methods can hardly scale to very long timeseries.\\nTransformer-based Deep Learning Methods. Given that Trans-\\nformer is the best choice for backbone in almost all sequence mod-\\neling tasks, some effort has been made to apply Transformer to\\ntimeseries analytics. Targeting forecasting of uni-variate timeseries,\\nLogTrans [ 30] introduced a log sparsity assumption to attention\\ncomputation. Informer [ 62] pushes LogTrans a step further and\\nscales forecasting to multi-variate timeseries. Autoformer [ 57] per-\\nforms forecasting by decomposing timeseries into two parts, i.e.\\nthe trend part and the seasonal part.\\nFor imputation tasks, CDSA [ 37] outperforms statistical meth-\\nods and the SOTA of RNN-based method Brit [ 7] on 3 public and\\n2 competition datasets. For timeseries classification, AutoTrans-\\nformer [ 43] performs architecture search to adapt to the tasks\\nin different domains. For timeseries anomaly detection, Anomaly\\nTransformer [ 58] outperforms many widely-used methods such\\nas OmniAnomaly [ 47], assuming the attention score maps show\\nGaussian distribution.\\nAll of these works are designed for specific tasks, rather than\\nfunctioning as a representation learning framework to serve\\ndifferent downstream tasks. To fill this gap, some researchers pro-\\nposed a Transformer-based architecture, called TST [ 61]. Like RITA,\\nTST supports regression, classification, and unsupervised learning\\nthrough the cloze test pretraining task on timeseries. However,\\nTST directly uses the classical Vanilla self-attention, thus not scal-\\nable to long timeseries as shown in our experiments (Sec. 6.3.2).\\n7.2 Efficient Transformers\\nThe need of improving the scalability of Transformers has led to\\nmore efficient variations of Transformers, especially for accommo-\\ndating long text data in NLP [49].\\nIntroducing fixed/random patterns to self-attention mechanism\\nis an intuitive idea. Sparse Transformer [ 9] and Longformer [ 3] only\\ncompute attention at fixed intervals. ETC [ 2] and BigBird [ 60] use\\nglobal-local attention: the attention computation is limited within\\na fixed radius, while some auxiliary tokens are added to attend/get\\nattended globally. The deficiencies of fixed attention patterns are\\nobvious: it heavily depends on users to give an optimal setting.\\nTo decrease the reliance on human labor, some works seek to\\nintroduce learnable/adaptive attention patterns instead of fixed\\npatterns. Reformer [ 26] proposed only computing the dominant\\nattention terms based on their observation of sparsity in atten-\\ntion matrix from language/image data. Such sparsity is intuitive\\nin language data, in which a words attention mainly focuses on\\nthe nearby sentences. However, attention in timeseries data shows\\nstrong seasonal patterns rather than sparse patterns, mainly as\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5cd203de-86d9-469f-aeff-c5d46af2226e', embedding=None, metadata={'page_label': '10', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='0b3715c694784e862030ed8d9bc7e68bd58f4b0c9412c61bb398d653216bac3b', text='result of the periodicity of timeseries data. Therefore, such works\\ndo not work well for timeseries.\\nApart from introducing attention patterns, some works seek\\nto solve this problem with applied mathematics techniques. Lin-\\nformer [ 54] performs a projection to decrease the size of query,\\nkey and value matrices before attention computation, because the\\nattention matrix tends to be low-ranked. Performer [ 10] uses linear\\nfunctions to approximate the kernel function softmax , making at-\\ntention computation commutative. When the sequence length is far\\ngreater than the dimension of embedding vectors, Performer ben-\\nefits from changing the order of matrix multiplication. Linformer\\nand Performer do not depend on the unique properties of language\\ndata, thus potentially fitting timeseries better than other techniques,\\nwhich is why we compared against them in our experiments. How-\\never as shown in Sec. 6, our group attention significantly outper-\\nforms them in both accuracy and efficiency (training time), because\\ngroup attention fully leverages the periodicity of timeseries.\\n8 CONCLUSION\\nIn this work, we presented RITA, an automatic, self-supervised, and\\nscalable timeseries analytics tool. RITA effectively adapts Trans-\\nformer, popular in NLP, into timeseries analytics. As the key com-\\nponent of RITA, group attention eliminates the performance bottle-\\nneck of the classical self-attention mechanisms, thus successfully\\nscaling RITA to highly complex, long timeseries data. Our experi-\\nments confirm that RITA significantly speeds up the state-of-the-art\\nby 63X with a better accuracy.\\nREFERENCES\\n[1]Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\\nCraig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al .\\n2016. Tensorflow: Large-scale machine learning on heterogeneous distributed\\nsystems. arXiv preprint arXiv:1603.04467 (2016).\\n[2]Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,\\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020.\\nETC: Encoding long and structured inputs in transformers. arXiv preprint\\narXiv:2004.08483 (2020).\\n[3]Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\\ndocument transformer. arXiv preprint arXiv:2004.05150 (2020).\\n[4]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al .2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 18771901.\\n[5]C Bui, N Pham, A Vo, A Tran, A Nguyen, and T Le. 2017. Time series forecasting\\nfor healthcare diagnosis and prognostics with the focus on cardiovascular dis-\\neases. In International conference on the development of biomedical engineering in\\nVietnam . Springer, 809818.\\n[6]Lei Cao, Wenbo Tao, Sungtae An, Jing Jin, Yizhou Yan, Xiaoyu Liu, Wendong\\nGe, Adam Sah, Leilani Battle, Jimeng Sun, Remco Chang, M. Brandon Westover,\\nSamuel Madden, and Michael Stonebraker. 2019. Smile: A System to Support\\nMachine Learning on EEG Data at Scale. Proc. VLDB Endow. 12, 12 (2019), 2230\\n2241.\\n[7]Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018. Brits:\\nBidirectional recurrent imputation for time series. Advances in neural information\\nprocessing systems 31 (2018).\\n[8] Chris Chatfield. 2000. Time-series forecasting . Chapman and Hall/CRC.\\n[9]Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating\\nlong sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).\\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\\nLukasz Kaiser, et al .2020. Rethinking attention with performers. arXiv preprint\\narXiv:2009.14794 (2020).\\n[11] Andrew A Cook, Gksel Msrl, and Zhong Fan. 2019. Anomaly detection for IoT\\ntime-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 64816494.\\n[12] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine\\nlearning 20, 3 (1995), 273297.[13] David R Cox. 1958. The regression analysis of binary sequences. Journal of the\\nRoyal Statistical Society: Series B (Methodological) 20, 2 (1958), 215232.\\n[14] Benjamin F Crabtree, Subhash C Ray, Priscilla M Schmidt, Patrick T OConnor,\\nand David D Schmidt. 1990. The individual over time: time series applications in\\nhealth care research. Journal of clinical epidemiology 43, 3 (1990), 241260.\\n[15] Angus Dempster, Franois Petitjean, and Geoffrey I. Webb. 2020. ROCKET: excep-\\ntionally fast and accurate time series classification using random convolutional\\nkernels. Data Min. Knowl. Discov. 34, 5 (2020), 14541495.\\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\\nProceedings of the 2019 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) . 4171\\n4186.\\n[17] Evelyn Fix and Joseph Lawson Hodges. 1989. Discriminatory analysis. Nonpara-\\nmetric discrimination: Consistency properties. International Statistical Review/Re-\\nvue Internationale de Statistique 57, 3 (1989), 238247.\\n[18] Philip George Guest and Philip George Guest. 2012. Numerical methods of curve\\nfitting . Cambridge University Press.\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\\nlearning for image recognition. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition . 770778.\\n[20] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,\\nand Pierre-Alain Muller. 2019. Deep learning for time series classification: a\\nreview. Data mining and knowledge discovery 33, 4 (2019), 917963.\\n[21] Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier,\\nDaniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar, Pierre-\\nAlain Muller, and Franois Petitjean. 2020. Inceptiontime: Finding alexnet for\\ntime series classification. Data Mining and Knowledge Discovery 34, 6 (2020),\\n19361962.\\n[22] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization\\nfor nearest neighbor search. IEEE transactions on pattern analysis and machine\\nintelligence 33, 1 (2010), 117128.\\n[23] Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019. Billion-scale similarity\\nsearch with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535547.\\n[24] Richard M Karp. 1972. Reducibility among combinatorial problems. In Complexity\\nof computer computations . Springer, 85103.\\n[25] Eamonn Keogh, Kaushik Chakrabarti, Michael Pazzani, and Sharad Mehrotra.\\n2001. Dimensionality reduction for fast similarity search in large time series\\ndatabases. Knowledge and information Systems 3, 3 (2001), 263286.\\n[26] Nikita Kitaev, ukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient\\ntransformer. arXiv preprint arXiv:2001.04451 (2020).\\n[27] John Kraft and Arthur Kraft. 1977. Determinants of common stock prices: A time\\nseries analysis. The journal of finance 32, 2 (1977), 417425.\\n[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-\\nsification with Deep Convolutional Neural Networks. In Advances in Neural\\nInformation Processing Systems , F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-\\nberger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/\\npaper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\\n[29] Oscar D Lara and Miguel A Labrador. 2012. A survey on human activity recog-\\nnition using wearable sensors. IEEE communications surveys & tutorials 15, 3\\n(2012), 11921209.\\n[30] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,\\nand Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-\\nneck of transformer on time series forecasting. Advances in Neural Information\\nProcessing Systems 32 (2019).\\n[31] T Warren Liao. 2005. Clustering of time series dataa survey. Pattern recognition\\n38, 11 (2005), 18571874.\\n[32] Rake& Agrawal King-lp Lin and Harpreet S Sawhney Kyuseok Shim. 1995. Fast\\nsimilarity search in the presence of noise, scaling, and translation in time-series\\ndatabases. In Proceeding of the 21th International Conference on Very Large Data\\nBases . 490501.\\n[33] Jason Lines, Sarah Taylor, and Anthony Bagnall. 2018. Time Series Classification\\nwith HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based\\nEnsembles. ACM Trans. Knowl. Discov. Data 12, 5, Article 52 (jul 2018), 35 pages.\\n[34] Feifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan\\nXu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, et al .2018. An open\\naccess database for evaluating the algorithms of electrocardiogram rhythm and\\nmorphology abnormality detection. Journal of Medical Imaging and Health\\nInformatics 8, 7 (2018), 13681373.\\n[35] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on\\ninformation theory 28, 2 (1982), 129137.\\n[36] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\\narXiv preprint arXiv:1711.05101 (2017).\\n[37] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, and\\nShih-Fu Chang. 2019. CDSA: cross-dimensional self-attention for multivariate,\\ngeo-tagged time series imputation. arXiv preprint arXiv:1905.09904 (2019).\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1cdc68a7-d7db-4131-9934-646534df124a', embedding=None, metadata={'page_label': '11', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='d7d66531b32f0b7867df3a1780e747d09251c04bd15d48576ce40b8b9b9a4202', text='[38] Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate\\nnearest neighbor search using hierarchical navigable small world graphs. IEEE\\ntransactions on pattern analysis and machine intelligence 42, 4 (2018), 824836.\\n[39] Tripti Negi and Veena Bansal. 2005. Time series: Similarity search and its appli-\\ncations. In Proceedings of the International Conference on Systemics, Cybernetics\\nand Informatics: ICSCI-04, Hyderabad, India . 528533.\\n[40] John Paparrizos and Michael J Franklin. 2019. Grail: efficient time-series repre-\\nsentation learning. Proceedings of the VLDB Endowment 12, 11 (2019), 17621777.\\n[41] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty\\nof training recurrent neural networks. In International conference on machine\\nlearning . PMLR, 13101318.\\n[42] Ning Qian. 1999. On the momentum term in gradient descent learning algorithms.\\nNeural networks 12, 1 (1999), 145151.\\n[43] Yankun Ren, Longfei Li, Xinxing Yang, and Jun Zhou. 2022. AutoTransformer:\\nAutomatic Transformer Architecture Design for Time Series Classification. In\\nPacific-Asia Conference on Knowledge Discovery and Data Mining . Springer, 143\\n155.\\n[44] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.\\nDeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-\\nnational Journal of Forecasting 36, 3 (2020), 11811191.\\n[45] Ahmed Shifaz, Charlotte Pelletier, Franois Petitjean, and Geoffrey I. Webb. 2020.\\nTS-CHIEF: a scalable and accurate forest algorithm for time series classification.\\nData Mining and Knowledge Discovery 34 (2020), 742775.\\n[46] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,\\nMikkel Baun Kjrgaard, Anind Dey, Tobias Sonne, and Mads Mller Jensen.\\n2015. Smart devices are different: Assessing and mitigatingmobile sensing het-\\nerogeneities for activity recognition. In Proceedings of the 13th ACM conference\\non embedded networked sensor systems . 127140.\\n[47] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust\\nanomaly detection for multivariate time series through stochastic recurrent\\nneural network. In Proceedings of the 25th ACM SIGKDD international conference\\non knowledge discovery & data mining . 28282837.\\n[48] Timo Sztyler and Heiner Stuckenschmidt. 2016. On-body localization of wearable\\ndevices: An investigation of position-aware activity recognition. In 2016 IEEE\\nInternational Conference on Pervasive Computing and Communications (PerCom) .\\nIEEE, 19.\\n[49] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient\\ntransformers: A survey. ACM Computing Surveys (CSUR) (2020).\\n[50] Mingyan Teng. 2010. Anomaly detection on time series. In 2010 IEEE International\\nConference on Progress in Informatics and Computing , Vol. 1. IEEE, 603608.\\n[51] Patrick A Thompson. 1990. An MSE statistic for comparing forecast accuracy\\nacross series. International Journal of Forecasting 6, 2 (1990), 219227.\\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In Advances in Neural Information Processing Systems 30: Annual Con-\\nference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA . 59986008.\\n[53] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\\nReddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser,\\nJonathan Bright, Stfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar-\\nrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern,\\nEric Larson, C J Carey, lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,\\nDenis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\\nCharles R. Harris, Anne M. Archibald, Antnio H. Ribeiro, Fabian Pedregosa,\\nPaul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Al-\\ngorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261272.\\nhttps://doi.org/10.1038/s41592-019-0686-2\\n[54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin-\\nformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768\\n(2020).\\n[55] Gary M Weiss, Kenichi Yoneda, and Thaier Hayajneh. 2019. Smartphone and\\nsmartwatch-based biometrics using activities of daily living. IEEE Access 7 (2019),\\n133190133202.\\n[56] Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke, and Huan Xu. 2021.\\nRobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection.\\nInProceedings of the 2021 International Conference on Management of Data (Virtual\\nEvent, China) (SIGMOD 21) . Association for Computing Machinery, New York,\\nNY, USA, 23282337. https://doi.org/10.1145/3448016.3452779\\n[57] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-\\ncomposition transformers with auto-correlation for long-term series forecasting.\\nAdvances in Neural Information Processing Systems 34 (2021), 2241922430.\\n[58] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomaly\\nTransformer: Time Series Anomaly Detection with Association Discrepancy.\\narXiv preprint arXiv:2110.02642 (2021).\\n[59] Dianmin Yue, Xiaodan Wu, Yunfeng Wang, Yue Li, and Chao-Hsien Chu. 2007. A\\nreview of data mining-based financial fraud detection research. In 2007 Interna-\\ntional Conference on Wireless Communications, Networking and Mobile Computing .Ieee, 55195522.\\n[60] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\\net al.2020. Big bird: Transformers for longer sequences. Advances in Neural\\nInformation Processing Systems 33 (2020), 1728317297.\\n[61] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and\\nCarsten Eickhoff. 2021. A Transformer-based Framework for Multivariate Time\\nSeries Representation Learning. In KDD 21: The 27th ACM SIGKDD Conference\\non Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,\\n2021. 21142124.\\n[62] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-\\nquence time-series forecasting. In Proceedings of AAAI .\\nA APPENDIX: SUPPLEMENTARY MATERIAL\\nA.1 Experiment Configuration and\\nHyper-parameter Settings\\nConfiguration. All models were trained on an NVIDIA Tesla V100\\n16GB GPU. All the methods are optimized with AdamW [ 36] of\\nwhich the starting learning rate and weight decay parameter are\\nboth 14. In full-label training scenario, we train the models for\\n100 epochs. In pretraining + few-label finetuning scenario, as the\\npretrained models require fewer epochs to converge [ 61], we train\\nthe model for 50 epochs. For a fair comparison, the baselines use a\\nmaximal batch size within GPUs capacity during training.\\nAs for model hyper-parameter setting, RITA and the baselines\\nuse a Transformer structure balancing Vanilla s accuracy and\\nefficiency: 8-layer stack of 2-head attention with hidden vectors\\nin dimension of 64. Convolution kernel size is set to 5 by default.\\nWe set the error bound threshold ( , Sec. 5.1) of Group Attention\\nto 2, as it balances the accuracy and the efficiency in general on\\nall datasets. Because Linformer requires the users to set the sizes\\nof projection matrix, in different settings we choose an accuracy-\\nefficiency balancing one among {64,128,256,512}.\\nA.2 Efficient Computation of Group Attention\\nAlgorithm 1 Efficient Computation of Group Attention\\nRequire:,,,,\\nEnsure:,R,R,N,N\\n1:function group_attention (,, )\\n2: for=01do\\n3: e1\\n=0(==)\\n4:e\\n5: for=01do\\n6: for=01do\\n7: ,(e,)\\n8: for=01do\\n9:1\\n=0,\\n10: for=01do\\n11:1\\n=0(e,)\\ne\\n12: return\\nIn Alg. 1, we denote to be the size of the group,to\\nbe the number of groups, rto be the representative key of the \\ngroup and Rto be the matrix consisting of all r,to be\\nthe group that kbelongs to., are the packing matrices of query\\nvectors and value vectors as described in Sec.2. Alg. 1 outputs the\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4adf015e-55ca-4b9f-b042-63b86be6164d', embedding=None, metadata={'page_label': '12', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='7a5faef424c3f9afbed048e93a2e5931489813863ed98a3c476db81891d007c4', text='packing matrix for new feature emebddings {1,...,}, where\\ncorresponds to the feature embedding of . Lines 2-3 implement\\nthe embedding aggregation operation, while Lines 8-11 implement\\nthe group softmax function.\\nA.3 The Algorithms and Optimality Proof for\\nDynamically Determing Batch Size\\nAlgorithm 2 Binary Search for Batch Size\\nRequire:,\\nEnsure: 1,1\\n1:function binary_search (,)\\n2:1\\n3:\\n4:\\n5:\\n6: whiledo\\n7:\\n8:()\\n9:\\n10:\\n\\n11: if0.9>then\\n12: +1\\n13: \\n14: else\\n15: 1\\n16:+\\n2\\n17: return\\nAlgorithm 3 Dynamic Programming for Plane Division\\nRequire:,,,\\nEnsure: 1,1\\n1:function cost (S)\\n2: if||<then return+\\n3:,,\\n4:(|,)\\nreturn(,,|)\\n5:function dynamic_programming (,, )\\n6: for1=1 do\\n7: for2=11do\\n8: for=11do\\n9: {21,}\\n10: ()()\\n11: for=1do\\n12: {21,}\\n13: ()((),()+())\\n14: 2,1(1)\\n15:\\n16: for=1 do\\n17:()(1,)\\n18: for=1do\\n19: ()((),()+(,))\\nreturn()\\nWe describe Alg. 3 and intuitively show its optimality. We assume\\nthat Scipy [ 53] learns an optimal function in Line 4 so that function\\nCOST gives the optimal estimation error when fitting the points in\\nset. When fitting very few points, we assign an infinite cost to\\nprevent a biased fitting function (Line 2). ()denotes the minimalestimation error for points in sub-plane {21,}. In\\nLines 11-13, we enumerate all possible ways of cutting {2\\n1,}horizontally into two sub-plane {21,}and\\n{21,}by iterating from 1 to n. Choosing the\\ncutting strategy that minimizes estimation error gets us a (1)with\\nminimal estimation error for sub-plane {21,1}, which\\nis recorded as 1,2in Line 14.()denotes the minimal estimation\\nerror for sub-plane {}. We enumerate all the possible ways\\nof cutting{}vertically into two sub-plane {}and{\\n}by iterating from 1 to(Line 17-19). Finally, we have the\\nminimal estimation error for the whole plane as (). Based\\non the above discussion, this algorithm guarantees to not miss any\\nbetter solution, hence optimal.\\nA.4 The Correctness of Group Attention\\nLemma 3. Assuming the windows belonging to the same group \\nhave the same key vector, i.e. =(), then the feature\\nembeddingproduced by the original self-attention mechanism is\\nidentical to the output of our group attention mechanism implemented\\nin Algorithm 1.\\nProof. Denote eto be the representative vectors of , i.e.e=\\n=(). Algorithm 1 gives that\\ne=1\\n=0(==)v,e,=qr\\n=1\\n=0(e,),e=1\\n=0e,\\ne(7)\\nBy the canonical self-attention mechanism introduced in Sec. 2,\\nwe get:\\n,=qkj, ,=(,)\\n1\\n=0(,),o=1\\n=0,v (8)\\nWith 7 and 8, we have\\n1\\n=0(,)=1\\n=0(qk)\\n=1\\n=01\\n=0(==)(qk)\\n=1\\n=0(qr)1\\n=0(==)\\n=1\\n=0(qr)\\n=1\\n=0(e,)\\n=(9)\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97fee91f-7df4-4e4b-9c41-b7c54f7d7e7b', embedding=None, metadata={'page_label': '13', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='5c28c2842948f4b228359ad8efd59aa686e55d804f17e29c35b6e5f7f7a819aa', text='Further,\\no=1\\n=0,vj\\n=1\\n=01\\n=0(==),v\\n=1\\n=01\\n=0(==)(,)\\n1\\n=0(,)v\\n=1\\n=01\\n=0(==)(qk)\\n1\\n=0(,)v\\n=1\\n=01\\n=0(==)(qrj)\\n1\\n=0(,)v\\n=1\\n=0(qrj)\\n1\\n=0(,)1\\n=0(==)v\\n=1\\n=0(qrj)\\n1\\n=0(,)e(10)\\nCombining (7), (9) (10), we have oi=N1\\nj=0ePi,j\\nsievj=eoi.\\nThis concludes that the output of our group attention is identical\\nto vanilla self-attentions. \\nA.5 The Proof of Error Bound (Lemma 1)\\nProof. We have\\n(,)\\n(,)=(qek)\\n(qk)=(q(ekk))\\n=(||q||||ekk||(q,ekk))(11)\\nSo\\n()(,)\\n(,)() (12)\\nThen we have:\\n,\\n,=(,)\\n\\n=1(,)/(,)\\n=1(,)\\n=(,)\\n(,)\\n=1(,)\\n\\n=1(,)(13)\\nCombining (12) (13), the error is bounded by\\n(2),\\n,(2) (14)\\nThus, if dln()\\n2R,1\\nAi,j\\nAi,j. This proves Lemma 1.\\nA.6 The Proof of Merge Operation (Lemma 2)\\nProof. Denote the cluster size of to be.After merge-\\ning, the new center will be:\\n=\\n=1\\n=1For[1,],, it holds that:\\n||||+||( )\\n=||+|\\n=1\\n=1\\n=1\\n=1|\\n=||+|\\n=1()\\n\\n=1|\\n=||+|\\n=1()|\\n\\n=1\\n||+\\n=1||\\n\\n=1\\n=\\n=1(||+||)\\n\\n=1\\n\\n=1\\n\\n=1=(15)\\nA.7 Downstream Tasks\\nRITA supports a variety of downstream tasks. In this section, we\\nshow that with minimal modification RITA can effectively support\\nclassification, imputation and forecasting tasks. Other unsupervised\\ntasks such as similarity search or clustering are naturally supported\\nby extracting feature embeddings from RITA.\\nA.7.1 Classification\\nTo classify timeseries, we input timeseries to the model as described\\nin Sec. 3 and attach a special token [CLS] as the first input em-\\nbedding. [CLS] s embedding acts as the embedding for the entire\\ntimeseries, and the output representation of [CLS] is fed into a\\nclassifier: y=Softmax(WclsZ[CLS]+Bcls), where[]Ris\\nthe output representation of [CLS] , C is the number of classes, and\\nWclsRCd,BclsRCare learnable parameters for classification\\ntask. The result vector Rrepresents the possibility that the\\ninput timeseries belongs to each class.\\nWe apply Cross Entropy Loss as the loss function of the classi-\\nfication task [ 13]:L=1\\nCC\\ni=1y(i)log(y(i)), where is a binary\\nindicator for ground truth label:\\n()=(\\n1is ground truth label\\n0(16)\\nA.7.2 Imputation\\nTimeseries are mainly generated by sensors, a common problem\\nof which is missing values. This becomes a challenge when many\\ndownstream analytics require the missing values to be recovered.\\nThe recovering task is imputation.\\nDenote the real timeseries as R, the observed timeseries\\nwith missing values as R, and the set of missing values\\npositions as . We scale the values of all timeseries to non-negative\\nand use a special value (-1) to indicate missing values:\\n(,)=(\\n1(,)\\n(,) (,)(17)\\nis fed into the RITA as input, and the output representa-\\ntions are concatenated and fed into a Transpose Convolution layer\\nwhich decodes the output embedding vectors from hidden space to\\ntimeseries values, corresponding to the convolution operation in\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c56eaef8-c26f-4b04-af73-ca9333780770', embedding=None, metadata={'page_label': '14', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='86deb4def749667fc21ddf045b2c649b597cbfa946009359f27841b6a59f8060', text='the input stage, i.e., Y=TransposeCNN(Z1+Z2+...+Zn), where\\nRis the recovered timeseries, and Ris the output of\\neach position.\\nHere Mean Square Error is chosen as the loss function [ 51]:\\n=1\\n||\\n(,)((,)(,))2.\\nA.7.3 Forecasting\\nForecasting can be regarded as a special case of imputation, in\\nwhich all missing values are at the end of timeseries.\\nSo like in imputation task, we scale the timeseries to non-\\nnegative and use a special value (-1) to indicate the values to be\\npredicted:\\n(,)=(\\n(,)\\n1(18)\\nWhere is the observed timestamp. Then the output\\nrepresentations are fed into a Transpose Convolution layer using\\nMean Squared Error as loss function, as described above.\\nA.7.4 Other Unsupervised Tasks\\nRITA naturally supports other unsupervised tasks, such as similar-\\nity search and clustering [ 25,31,32], by producing the embedding\\nof one timeseries (output representation of the special token [CLS] ).\\nClustering can be performed on the embeddings with flexible choice\\nof distance metrics. Similarly, a high dimensional similarity search\\nsystem [22, 23, 38] can be built on the embeddings.A.8 Inference Time\\nDataset Length TST[61] Vanilla Performer Linformer Group Attn.\\nWISDM 200 2.18 2.26 2.35 2.22 2.17\\nHHAR 200 1.19 1.23 1.28 1.21 1.18\\nRWHAR 200 1.32 1.37 1.42 1.34 1.31\\nECG 2000 18.44 15.26 5.80 6.08 5.16\\nTable 6: Inference time: Classification on multi-variate data\\n(seconds).\\nDataset Length TST[61] Vanilla Performer Linformer Group Attn.\\nWISDM 200 2.03 2.11 2.19 2.07 2.02\\nHHAR 200 1.11 1.14 1.19 1.12 1.10\\nRWHAR 200 1.23 1.27 1.32 1.25 1.22\\nECG 2000 17.22 14.32 4.73 4.99 4.11\\nMGH 10000 N/A N/A 6.58 6.88 1.35\\nTable 7: Inference time: Imputation on multi-variate data\\n(seconds).\\nIn this section, we present the average inference time on valida-\\ntion sets. The results in Table. 6 and 7 correspond to the average\\ninference time on validation sets of classification and imputation\\ntasks, respectively. Consistent with the results in Section. 6.3, our\\nmethod Group Attn. outperforms the baselines on both classifica-\\ntion and imputation tasks, particularly on the datasets comprising\\nlong timeseries (ECG and MGH).\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GROBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "reader = PyPDF2.PdfReader(\"./pdfs/2107.08000v1.pdf\")\n",
    "writer = PyPDF2.PdfWriter()\n",
    "for i in range(len(reader.pages)):\n",
    "    page = reader.pages[i]\n",
    "    page.annotations.clear()\n",
    "    writer.add_page(page)\n",
    "    \n",
    "    \n",
    "with open('./pdfs/2107.08000v1.pdf', 'wb') as f:\n",
    "    writer.write(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "doc = fitz.open(\"./pdfs/2107.08000v1.pdf\")\n",
    "\n",
    "doc.save(\"./pdfs/2107.08000v1_modified.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fpdf\n",
    "\n",
    "\n",
    "def convert_pdf_to_grayscale(input_pdf_path, output_pdf_path):\n",
    "    # Open the PDF\n",
    "    document = fitz.open(input_pdf_path)\n",
    "\n",
    "    # Create a new PDF to store the grayscale pages\n",
    "    grayscale_pdf = fitz.open()\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page in document:\n",
    "        links = page.get_links()\n",
    "        for link in links:\n",
    "            page.delete_link(link)\n",
    "        # Convert the page to a pixmap (image) with colors converted to grayscale\n",
    "        pixmap = page.get_pixmap(matrix=fitz.Matrix(2, 2), colorspace=\"GRAY\")\n",
    "\n",
    "        # Create a new blank page in the grayscale PDF with the same dimensions\n",
    "        pdf_page = grayscale_pdf.new_page(width=pixmap.width, height=pixmap.height)\n",
    "\n",
    "        # Insert the grayscale pixmap into the new page\n",
    "        pdf_page.insert_image(pdf_page.rect, pixmap=pixmap)\n",
    "\n",
    "    # Save the grayscale PDF\n",
    "    grayscale_pdf.save(output_pdf_path)\n",
    "    grayscale_pdf.close()\n",
    "    document.close()\n",
    "\n",
    "# Usage\n",
    "convert_pdf_to_grayscale('./pdfs/2107.08000v1.pdf', './pdfs/2107.08000v1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    }
   ],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "\n",
    "client = GrobidClient(config_path=\"./grobid_client_python/config.json\")\n",
    "client.process(\n",
    "    \"processFulltextDocument\",\n",
    "    \"./pdfs/\",\n",
    "    output=\"./output/\",\n",
    "    force=True,\n",
    "    n=20\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"./output/2107.08000v1.grobid.tei.xml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"lxml-xml\")\n",
    "chunks = []\n",
    "content = \"\"\n",
    "for div in soup.body.find_all([\"div\", \"figure\"]):\n",
    "    if div.name == \"figure\":\n",
    "        head = div.find(\"head\")\n",
    "        fig_desc = div.find(\"figDesc\")\n",
    "        # if ('Figure' not in head.getText()) and ('Table' not in head.getText()):\n",
    "        if( 'Figure' not in head.get_text()) and ('Table' not in head.get_text()):\n",
    "            content += head.getText().replace(\" \", \"\") + fig_desc.getText() + \"\\n\"\n",
    "    elif div.name == \"div\":\n",
    "        section_num = div.head.get(\"n\")\n",
    "        section_title = div.head.getText(separator=' ', strip=True)\n",
    "        if section_num is None:\n",
    "            section_num = \"\"\n",
    "        content += section_num + \" \" + div.getText(separator= ' ', strip=True) + \"\\n\"\n",
    "    content += \"=======================================\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Introduction Instance-level image retrieval is at the core of visual representation learning and is connected with many problems of visual recognition and machine learning, for instance metric learning [30, 26] , few-shot learning [42] and unsupervised learning [8] . Many large-scale open datasets [3, 37, 16, 29, 53] , and competitions 1 have accelerated progress in instance-level image retrieval, which has been transformed by deep learning [3] . Many studies on instance-level image retrieval focus on learning features from convolutional neural networks (CNN), while others focus on re-ranking, for instance by graph-based methods [11] . The former can be distinguished according to feature types: local descriptors, reminiscent of SIFT [27] , where an image is mapped to a few hundred vectors; and global descriptors, where an image is mapped to a 1 https://www.kaggle.com/c/landmark-retrieval-2020 single vector. In fact, deep learning has brought global descriptors with astounding performance, while allowing efficient search. Our study belongs to this type. Studies on global descriptors have focused on spatial pooling [2, 37] . The need for compact, discriminative representations that are resistant to clutter has naturally given rise to spatial attention methods [24, 28] . Different kinds of attention have been studied in many areas of computer vision research. There is also channel attention [20, 9] ; local attention, applied independently to elements of the representation (feature map) [54, 25] ; global attention, based on interaction between elements [52, 9] ; and combinations thereof. Unfortunately, each study has been limited to one or two kinds of attention only; attention is not always learned; and applications vary. It is the objective of our work to perform a comprehensive study of all forms of attention above, apply them to instance-level image retrieval and provide a detailed account of their interaction and impact on performance. As shown in Figure 1 , we collect contextual information from images with both local and global attention, giving rise to two parallel network streams. Importantly, each operates on both spatial locations and feature channels. Local attention is about individual locations and channels; global is about interaction between locations and between channels. The extracted information is separately embedded in local and global attention feature maps, which are combined in a global-local attention feature map before pooling. Our contributions can be summarized as follows: 1. We propose a novel network that consists of both global and local attention for image retrieval. This is the first study that employs both mechanisms.\n",
      "=======================================\n",
      "2. Each of the global and local attention mechanisms comprises both spatial and channel attention. 3. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks. A l c c  1  1  + F l c A l s 1  h  w  + F l  c  h  w F  + c  h  w F gl A g c c  c  F g c A g s hw  hw  + F g\n",
      "=======================================\n",
      "2. Related work Instance-level image retrieval Studies on instance-level image retrieval can be roughly, but not exclusively, divided into three types: (1) studies on global descriptors [3, 16, 24, 53, 2, 37] ; (2) studies on local descriptors and geometry-based re-ranking [29, 45, 40, 53] ; (3) re-ranking by graph-based methods [11, 21, 55] . The first two types of studies focus on the feature representation, while the last type focuses on re-ranking extracted features. Studies on global descriptors focus on spatial pooling of CNN feature maps into vectors, including MAC [38] , SPoC [2] , CroW [24] , R-MAC [48, 15, 16] , GeM [37] , and NetVLAD [1, 25] , as well as learning the representation [3, 15, 16, 36, 37] . Studies before deep learning dominated image retrieval were mostly based on local descriptors like SIFT [27] and bag-of-words representation [32] or aggregated descriptors like VLAD [22] or ASMK [46] . Local descriptors have been revived in deep learning, e.g. with DELF [29] , DELG [5] and ASMK extensions [45, 47] . We focus on learning a global descriptor in this work, because it is the most efficient in terms of storage and search. However, our generic attention mechanism produces a feature tensor and could be applicable to local descriptors as well, if global pooling were replaced by local feature detection. Re-ranking methods are complementary to the representation and we do not consider them in this work. Attention Attention mechanisms have been first proposed in image classification studies focusing on channel at- METHOD LOCAL GLOBAL LRN RET Spatial Channel Spatial Channel SENet [20] ECA-Net [51] GCNet [6] CBAM [54] GE [19] NL-Net [52] AA-Net [4] SAN [59] N 3 Net [34] A 2 -Net [9] GSoP [14] OnA [23] AGeM [17] CroW [24] CRN [25] DELF [29] DELG [5] Tolias et al. [47] SOLAR [28] Ours Table 1 : Related work on attention. LRN: learned; RET: applied to instance-level image retrieval. tention [20, 51, 6] , spatial attention [19] or both, like CBAM [54] . In image retrieval, CroW [24] also employs  both spatial and channel attention and can be seen as a precursor of CBAM, but, like other studies of spatial attention on retrieval [41, 23, 17] , it is not learned. CRN [25] applies spatial attention for feature reweighting and is learned. Learned spatial attention mechanisms are common for local descriptors [29, 5, 47] . c  h  w c  1  1 c  1  1 F A l c We call the above methods local attention, in the sense that elements of the feature tensor (channels / spatial locations), are weighted independently, based on contextual information obtained by pooling or learned. By constrast, by global attention we refer to mechanisms that model interaction between elements of the feature tensor, for example between channels or between locations. In image classification, non-local neural network (NL-Net) [52] is maybe the first global attention mechanism, followed by similar studies [4, 59, 34] . It is global spatial attention, allowing interaction between any pair of spatial locations. Similarly, there are studies of global channel attention, allowing interaction between channels [9, 14] . Global attention has focused mostly on image recognition and has been applied to either spatial or channel attention so far, not both. In image retrieval, SOLAR [28] is a direct application of the global spatial attention mechanism of [52] . Table 1 attempts to categorize related work on attention according to whether attention is local or global, spatial or channel, whether it is learned and whether it is applied to instance-level image retrieval. We observe that all methods limit to one or two forms of attention only. Of those studies that focus on image retrieval, many are not learned [23, 17, 24] , and of those that are, some are designed for local descriptors [29, 47] . By contrast, we provide a comprehensive study of all forms of attention, global and local, spatial and channel, to obtain a learned representation in the form of a tensor that can be used in any way. We spatially pool it into a global descriptor and we study the relative gain of different forms of attention in image retrieval. feature map conv 1  1 conv 3  3 conv 5  5 conv 7  7 concat conv 1  1 attention map c  h  w 4c  h  w 1  h  w c  h  w dilated conv F F A l s\n",
      "=======================================\n",
      "3. Global-local attention We design a global-local attention module (GLAM), which is attached at the end of a backbone network. Figure 1 illustrates its main components. We are given a c  h  w feature tensor F, where c is the number of channels, and h  w is the spatial resolution. Local attention collects context from the image and applies pooling to obtain a c  1  1 local channel attention map A l c and a 1  h  w local spatial attention map A l s . Global attention allows interaction between channels, resulting in a c  c global channel attention map A g c , and between spatial locations, resulting in a hw  hw global spatial attention map A g s . The feature maps produced by the two attention streams are combined with the original one by a learned fusion mechanism into the global-local attention feature map F gl before being spatially pooled into a global image descriptor.\n",
      "=======================================\n",
      "3.1. Local attention We extract an 1D channel and a 2D spatial attention map to weigh the feature map in the corresponding dimensions. Local channel attention Following ECA-Net [51] , this attention captures local channel information. As shown in Figure 2 , we are given a c  h  w feature tensor F from our backbone. We first reduce it to a c  1  1 tensor by global average pooling (GAP). Channel attention is then captured by a 1D convolution of kernel size k along the channel dimension, where k controls the extent of cross-channel interaction. This is followed by a sigmoid function, resulting in the c  1  1 local channel attention map A l c . Local spatial attention Inspired by the inception module [43] and similar to [25] , this attention map captures local spatial information at different scales. As shown in Figure 3 , given the same c  h  w feature tensor F from our backbone, we obtain a new tensor F with channels reduced to c , using a 1  1 convolution. We then extract local spatial contextual information using convolutional filters of kernel size 3  3, 5  5, and 7  7, which are efficiently implemented by 3  3 dilated convolutions [7, 57] with dilation parameter 1, 2, and 3 respectively. The resulting features, along with one obtained by 1  1 convolution on F , are concatenated into a 4c  h  w tensor. Finally, we obtain the 1  h  w local spatial attention map A l s by a 1  1 convolution that reduces the channel dimension to 1. feature map GAP conv1d(k) conv1d(k) sigmoid sigmoid   softmax attention feature map 1  c 1  c 1  c Qc c  c hw  c Vc A g c c  h  w 1  c 1  c Kc F Gc The middle column of Figure 6 shows heat maps of local spatial attention, localizing target objects in images. Local attention feature map We use the local channel attention map A l c to weigh F in the channel dimension F l c := F A l c + F. (1) We then use local spatial attention map A l s to weigh F l c in the spatial dimensions, resulting in the c  h  w local attention feature map F l = F l c A l s + F l c . (2) Here, A B denotes an element-wise multiplication of tensors A and B, with broadcasting when one tensor is smaller. We adopt the choice of applying channel followed by spatial attention from convolutional block attention module CBAM [54] . However, apart from computing A l s at different scales, both attention maps are obtained from the original tensor F rather than sequentially. In addition, both (1) and (2) include residual connections, while CBAM includes a single residual connection over both steps.\n",
      "=======================================\n",
      "3.2. Global attention We extract two matrices capturing global pairwise channel and spatial interaction to weigh the feature map. Global channel attention We introduce a global channel attention mechanism that captures global channel interaction. This mechanism is based on the non-local neural network [52] , but with the idea of 1D convolution from ECA-Net [51] . As shown in Figure 4 , we are given the c  h  w feature tensor F from our backbone. We apply GAP and squeeze spatial dimensions, followed by a 1D convolution of kernel size k and a sigmoid function, to obtain 1c query Q c and key K c tensors. The value tensor V c is obtained by mere reshaping of F to hwc, without GAP. Next, we form the outer product of K c and Q c , followed by softmax over channels to obtain a c  c global channel attention map feature map conv 1  1 conv 1  1 conv 1  1   softmax conv 1  1 attention feature map c  hw Qs hw  hw c  h  w c  hw Vs c  h  w A g s c  h  w c  hw Kc F Gs A g c = softmax(K c Q c ). (3) Finally, this attention map is multiplied with V c and the matrix product V c A g c is reshaped back to c  h  w to give the global channel attention feature map G c . In GSoP [14] and A 2 -Net [9] , a c  c global channel attention map is obtained by multiplication of hw  c matrices; (3) is more efficient, using only an outer product of 1  c vectors. Global spatial attention Since ordinary convolution applies only a local neighborhood at a time, it cannot capture global contextual information. Thus, we apply non-local filtering [52] , which is a form of self-attention [49] in the spatial dimensions. As shown in Figure 5 , we are given the same c  h  w feature tensor F from our backbone. By using three 1  1 convolutions, which reduce channels to c , and flattening spatial dimensions to hw, we obtain c  hw query Q s , key K s , and value V s tensors, where each column is a feature vector corresponding to a particular spatial location. We capture pairwise similarities of these vectors by matrix multiplication of K s and Q s , followed by softmax over locations to obtain a hw  hw global spatial attention map: A g s = softmax(K s Q s ). (4) This attention map is multiplied with V s and the matrix product V s A g s is reshaped back to c  h  w by expanding the spatial dimensions. Finally, using a 1  1 convolution, which increases channels back to c, we obtain the c  h  w global spatial attention feature map G s . The right column of Figure 6 shows heat maps for global spatial attention, localizing target objects in images. Global attention feature map We use the global channel attention feature map F c to weigh F element-wise F g c = F G c . (5) We then use global spatial attention feature map G s to weigh F g c element-wise, resulting in the c  h  w global attention feature map F g = F g c G s + F g c . ( 6 ) Similarly to F l in ( 1 ) and ( 2 ), we apply channel attention first, followed by spatial attention. However, unlike (1), there is no residual connection in ( 5 ). This choice is supported by early experiments.\n",
      "=======================================\n",
      "3.3. Global-local attention Feature fusion As shown in Figure 1 , we combine the local and global attention feature maps, F l and F g , with the original feature F. While concatenation and summation are common operations for feature combination, we use a weighted average with weights w l , w g , w respectively, obtained by softmax over three learnable scalar parameters, to obtain a c  h  w global-local attention feature map F gl = w l F l + w g F l + wF. (7) EfficientDet [44] has shown that this is the most effective, among a number of choices, for fusion of features across different scales. Pooling We apply GeM [37] , a learnable spatial pooling mechanism, to feature map F gl (7), followed by a fullyconnected (FC) layer with dropout and batch normalization. The final embedding is obtained by 2 -normalization.\n",
      "=======================================\n",
      "4. Experiments\n",
      "=======================================\n",
      "4.1. Datasets Training set There are a number of open landmark datasets commonly used for training in image retrieval studies, including neural code (NC) [3] , neural code clean (NCclean) [16] , as well as Google Landmarks v1 (GLDv1) [29] and v2 (GLDv2) [53] . Table 2 shows relevant statistics. These datasets can be categorized into noisy and clean. The clean sets were obtained from the original noisy sets for more effective training [16, 53] . The original noisy datasets are much larger, but they have high intra-class variability.  Evaluation set and metrics We use four common evaluation datasets for landmark image retrieval: Oxford5k (Ox5k) [32] , Paris6k (Par6k) [33] , as well as Revisited Oxford (ROxford or ROxf) and Paris (RParis or RPar) [35] . ROxford and RParis are used with and without one million distractors (R1M) [28] and evaluated using the Medium and Hard protocols [35] . We evaluate using mean Average Precision (mAP) and mean precision at 10 (mP@10).\n",
      "=======================================\n",
      "4.2. Implementation details We train on 8 TITAN RTX 2080Ti GPUs. All models are pre-trained on ImageNet [39] and implemented in PyTorch [31] . For fair comparisons, we set a training environment similar to the those of compared studies [56, 53, 28, 35] . We employ ResNet101 [18] as a backbone model. The kernel size k of ECANet in subsection 3.1 is set to 3. The parameter p of GeM in subsection 3.3 is set to 3 and the dimension d of final embeddings to 512. We adopt ArcFace [10] , a cosine-softmax based loss, with a margin of 0.3. We use stochastic gradient descent with initial learning rate 10 -3 , momentum 0.9 and weight decay 10 -5 . We adopt the batch sampling of Yokoo et al. [56] where mini-batch samples with similar aspect ratios are resized to a particular size. Here, we use a batch size of 64. For image augmentation, we apply scaling, random cropping, and varied illumination. At inference, we apply a multi-resolution representation [16] to query and database images. Our method is denoted as GLAM (global-local attention module). Using the backbone model alone is referred to as baseline. It is compatible with recent models based on ResNet101-GeM trained with ArcFace [53, 28] . Adding our local attention (subsection 3.1) to the baseline model is denoted +local, while adding our global attention (subsection 3.2) is denoted +global. Since we focus on representation learning, we do not consider post-processing methods like geometry-based re-ranking [29, 40, 53] or graph-based re-ranking [11, 21, 55] [53, 28] . All models use ResNet101-GeM. Red: best results. Blue: GLAM higher than SOLAR [28] on GLDv1-noisy. GLDv2-noisy has 2.6 times more images than GLDv2clean, the latter is superior by a large margin. This shows that, in training, a cleaner dataset can be more important than a larger one. By contrast, NC-clean has the worst performance despite being clean, aparently because it is [53] is the only model other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3 . too small. To achieve best possible performance, we use GLDv2-clean as a training set in the remaining experiments.\n",
      "=======================================\n",
      " Comparisons on same training set It is common to compare methods regardless of training sets as more become available, e.g., [35, 28] . Since GLDv2-clean is relatively new, Weyand et al. [53] , which introduced the dataset, is the only study that has trained the same backbone with the same settings (ResNet101-GeM with ArcFace) on GLDv2-clean. Our baseline is lower than [53] , because our dimensinality is 512, while other models based on ResNet101 use 2048. Yet, Table 3 shows that our best model trained on GLDv2-clean outperforms [53] by a large margin. But the most important comparison is with SOLAR [28] , also based on selfattention, which has trained ResNet101-GeM on GLDv1noisy. On this training set, our best model clearly outperforms [28] despite lower dimensionality. With this model, we outperform previous best methods on most benchmarks except mP@10 on RParis (medium) and RParis+R1M (medium), where we are outperformed by [37, 35] . These results demonstrate that our approach is effective for landmark image retrieval. Figure 7 shows some\n",
      "=======================================\n",
      " Comparison with state of the art\n",
      "=======================================\n",
      "4.4. Ablation study Our ablation study uses the Google Landmark v2 clean dataset (GLDv2-clean) [53] for training, which is shown to be the most effective in Table 3 Table 9 : mAP comparison of using multiresolution representation (Multi) or not (Single) on query or database.\n",
      "=======================================\n",
      " Effect of attention modules We ablate the effect of our local and global attention networks as well as their combination. Table 5 shows the results, which are more finegrained than those of Table 4 . In particular, it shows the effect of the channel and spatial variants of both local and global attention. We observe that, when used alone, the channel and spatial variants of local attention are harmful in most cases. Even the combination, baseline+local, is not always effective. By contrast, when used alone, the channel and spatial variants of global attention are mostly beneficial, especially the latter. Their combination, baseline+global, is impressive, bringing gain of up to 7.5%. Importantly, the combination baseline+global+local improves further by up to another 2.8%. This result shows the necessity of local attention in the final model.\n",
      "=======================================\n",
      " CBAM vs. our local spatial attention We experiment with the local spatial attention of CBAM [54] . CBAM applies average and max-pooling to input features and concatenates the two for spatial attention. We apply this variant to our local spatial attention module for comparison. For the CBAM style module, we keep the overall design of our module as shown in Figure 3 , but apply average and max-pooling to each of the four convolutional layer outputs before concatenation. Table 6 shows that the CBAM style module is considerably worse than ours on all benchmarks except Paris6k, where it is only slightly better. Concatenation vs. sum for feature fusion We use a softmax-based weighted average of local and global attention feature maps with the original feature map (7) . Here, we compare this weighted average with weighted concatenation, where concatenation replaces the sum operation in (7) . As shown in Table 7 , the weighted average outperforms the weighted concatenation. Fixed-size vs. group-size sampling Numerous studies have proposed methods for constructing batches according to image size for efficient training. For instance, Gordo et al. [16] , DELF [29] , and Yokoo et al. [56] employed different image sizes per batch for training instead of a single fixed size. We adopt the method of Yokoo et al., which constructs a batch with images of similar aspect ratio, so that the images can be resized to a size with an aspect ratio that is similar to their own. We call this method group-size sampling. Table 8 compares fixed-size (224  224) with groupsize sampling. We observe that maintaining aspect ratios by using dynamic input sizes is much more effective.\n",
      "=======================================\n",
      " Multi-resolution We use the multi-resolution representation [16] for the final feature of an image at inference time. This method: (1) resizes an image into multiple scales; (2) extracts features from the resized images; and (3) averages the features to obtain the final feature of the image. The method is applied to both query and database images to enhance ranking results, especially for small target objects. Table 9 compares the four cases of applying this method or not to query or database images.\n",
      "=======================================\n",
      "5. Conclusion We have introduced a novel approach that extracts global and local contextual information using attention mechanisms for instance-level image retrieval. It is manifested as a network architecture consisting of global and local attention components, each operating on both spatial and channel dimensions. This constitutes a comprehensive study and empirical evaluation of all four forms of attention that have previously been studied only in isolation. Our findings indicate that the gain (or loss) brought by one form of attention alone strongly depends on the presence of the others, with the maximum gain appearing when all forms are present. The output is a modified feature tensor that can be used in any way, for instance with local feature detection instead of spatial pooling for image retrieval. With the advent of vision transformers [12, 58] and their recent application to image retrieval [13] , attention is expected to play a more and more significant role in vision. According to our classification, transformers perform global spatial attention alone. It is of great interest to investigate the role of the other forms of attention, where our approach may yield a basic building block of such architectures. One may even envision an extension to language models, where transformers originate from [50] .\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      ".\n",
      "=======================================\n",
      "4.3.Benchmarking Noisy vs. clean training sets We begin by training our best model (baseline+local+global) on all training sets of\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
