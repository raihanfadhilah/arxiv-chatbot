{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "import arxiv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=\"attention is all you need\",\n",
    "    max_results=5\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = client.results(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./pdfs/', exist_ok=True)\n",
    "for rs in res:\n",
    "    rs.download_pdf('./pdfs/', filename=rs.entry_id.split('/')[-1]+'.pdf')\n",
    "    # rs.download_source('./pdfs/', filename=rs.entry_id.split('/')[-1]+'.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from typing import Literal, List\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "import chainlit as cl\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-22 23:02:38 - Loaded .env file\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup #type: ignore\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from grobid_client.grobid_client import GrobidClient #type: ignore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from typing import  Literal\n",
    "import arxiv #type: ignore\n",
    "import chainlit as cl\n",
    "import chromadb\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import List\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"IndexNewArxivPapers\").setLevel(logging.INFO)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def read_tei(tei_file):\n",
    "    with open(tei_file, 'r', encoding='utf-8') as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "        return soup\n",
    "    raise RuntimeError('Cannot generate a soup from the input')\n",
    "\n",
    "def elem_to_text(elem, default=''):\n",
    "    if elem:\n",
    "        return elem.getText()\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "class TEIFile(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.soup = read_tei(filename)\n",
    "        self._text = None\n",
    "        self._title = ''\n",
    "        self._abstract = ''\n",
    "\n",
    "    @property\n",
    "    def doi(self):\n",
    "        idno_elem = self.soup.find('idno', type='DOI')\n",
    "        if not idno_elem:\n",
    "            return ''\n",
    "        else:\n",
    "            return idno_elem.getText()\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        if not self._title:\n",
    "            self._title = self.soup.title.getText() #type: ignore\n",
    "        return self._title\n",
    "\n",
    "    @property\n",
    "    def abstract(self):\n",
    "        if not self._abstract:\n",
    "            abstract = self.soup.abstract.getText(separator=' ', strip=True) #type: ignore\n",
    "            self._abstract = abstract\n",
    "        return self._abstract\n",
    "\n",
    "    @property\n",
    "    def authors(self):\n",
    "        authors_in_header = self.soup.analytic.find_all('author') #type: ignore\n",
    "\n",
    "        result = []\n",
    "        for author in authors_in_header:\n",
    "            persname = author.persname\n",
    "            if not persname:\n",
    "                continue\n",
    "            firstname = elem_to_text(persname.find(\"forename\", type=\"first\")).strip()\n",
    "            middlename = elem_to_text(persname.find(\"forename\", type=\"middle\")).strip()\n",
    "            surname = elem_to_text(persname.surname).strip()\n",
    "            if middlename == '':\n",
    "                full_name = f\"{firstname} {surname}\".strip()\n",
    "            else:\n",
    "                full_name = f\"{firstname} {middlename} {surname}\".strip()\n",
    "            result.append(full_name)\n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        if not self._text:\n",
    "            divs_text = []\n",
    "            for div in self.soup.body.find_all(\"div\"): #type: ignore\n",
    "                # div is neither an appendix nor references, just plain text.\n",
    "                # if not div.get(\"type\"):\n",
    "                #     text = div.getText(separator=': ', strip=True).replace(\"\\n\", \"\")\n",
    "                    \n",
    "                #     divs_text.append(text)\n",
    "                print(div)\n",
    "            plain_text = \"\\n\\n\".join(divs_text)\n",
    "            self._text = plain_text\n",
    "        return self._text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    }
   ],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "import os\n",
    "\n",
    "client = GrobidClient(config_path=\"./grobid_client_python/config.json\")\n",
    "\n",
    "os.makedirs(\"./output/\", exist_ok=True)\n",
    "client.process(\n",
    "    \"processFulltextDocument\",\n",
    "    input_path = \"./pdfs/\",\n",
    "    output=\"./output/\",\n",
    "    force=True,\n",
    "    consolidate_citations=True,\n",
    "    include_raw_citations=True,\n",
    "    tei_coordinates=True,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import grobid_tei_xml\n",
    "\n",
    "xml_path = \"./output/2107.08000v1.grobid.tei.xml\"\n",
    "\n",
    "with open(xml_path, 'r') as xml_file:\n",
    "    doc = grobid_tei_xml.parse_document_xml(xml_file.read())\n",
    "\n",
    "with open('./output/2107.08000v1.grobid.tei.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(doc.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_list = \"6,50.11,649.64,88.59,9.85;6,50.11,668.26,236.25,9.03;6,50.11,680.60,236.25,8.64\"\n",
    "\n",
    "new_coords_list = []\n",
    "for coordinates in coordinates_list.split(';'):\n",
    "    page_num = int(coordinates.split(',')[0])\n",
    "    coordinates = [float(x) for x in coordinates.split(',')[1:]]\n",
    "    new_coords_list.append((page_num, coordinates))\n",
    "print(new_coords_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, [50.11, 649.64, 88.59, 9.85]), (6, [50.11, 668.26, 236.25, 9.03]), (6, [50.11, 680.6, 236.25, 8.64])]\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "def crop_pdf(input_pdf, output_pdf, coordinates):\n",
    "    # Create a PDF file reader\n",
    "    reader = PdfReader(input_pdf)\n",
    "    page_num = coordinates[0]\n",
    "    coords = coordinates[1]\n",
    "\n",
    "    # Create a PDF file writer\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    # For each page, crop it and add it to the new PDF\n",
    "    page = reader.pages[page_num - 1]\n",
    "    \n",
    "    page_x, page_y = page.cropbox.get()\n",
    "\n",
    "    # Write the output PDF\n",
    "    with open(output_pdf, \"wb\") as out_f:\n",
    "        writer.write(out_f)\n",
    "\n",
    "    # Convert the cropped PDF to PNG\n",
    "    images = convert_from_path(output_pdf)\n",
    "\n",
    "    # Save the first page as a PNG\n",
    "    images[0].save('output.png', 'PNG')\n",
    "    os.remove(output_pdf)\n",
    "\n",
    "# Parse the coordinates\n",
    "\n",
    "# Crop the PDF\n",
    "crop_pdf(\"pdfs/2107.08000v1.pdf\", \"output.pdf\", coords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction Instance-level image retrieval is at the core of visual representation learning and is connected with many problems of visual recognition and machine learning, for instance metric learning  [30, 26] , few-shot learning  [42]  and unsupervised learning  [8] . Many large-scale open datasets  [3, 37, 16, 29, 53] , and competitions 1 have accelerated progress in instance-level image retrieval, which has been transformed by deep learning  [3] . Many studies on instance-level image retrieval focus on learning features from convolutional neural networks (CNN), while others focus on re-ranking, for instance by graph-based methods  [11] . The former can be distinguished according to feature types: local descriptors, reminiscent of SIFT  [27] , where an image is mapped to a few hundred vectors; and global descriptors, where an image is mapped to a 1 https://www.kaggle.com/c/landmark-retrieval-2020 single vector. In fact, deep learning has brought global descriptors with astounding performance, while allowing efficient search. Our study belongs to this type. Studies on global descriptors have focused on spatial pooling  [2, 37] . The need for compact, discriminative representations that are resistant to clutter has naturally given rise to spatial attention methods  [24, 28] . Different kinds of attention have been studied in many areas of computer vision research. There is also channel attention  [20, 9] ; local attention, applied independently to elements of the representation (feature map)  [54, 25] ; global attention, based on interaction between elements  [52, 9] ; and combinations thereof. Unfortunately, each study has been limited to one or two kinds of attention only; attention is not always learned; and applications vary. It is the objective of our work to perform a comprehensive study of all forms of attention above, apply them to instance-level image retrieval and provide a detailed account of their interaction and impact on performance. As shown in Figure  1 , we collect contextual information from images with both local and global attention, giving rise to two parallel network streams. Importantly, each operates on both spatial locations and feature channels. Local attention is about individual locations and channels; global is about interaction between locations and between channels. The extracted information is separately embedded in local and global attention feature maps, which are combined in a global-local attention feature map before pooling. Our contributions can be summarized as follows: 1. We propose a novel network that consists of both global and local attention for image retrieval. This is the first study that employs both mechanisms. \n",
      " Each of the global and local attention mechanisms comprises both spatial and channel attention. 3. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.  A l c c Ã— 1 Ã— 1 Ã— + F l c A l s 1 Ã— h Ã— w Ã— + F l Ã— c Ã— h Ã— w F Ã— + c Ã— h Ã— w F gl A g c c Ã— c Ã— F g c A g s hw Ã— hw Ã— + F g \n",
      " Related work Instance-level image retrieval Studies on instance-level image retrieval can be roughly, but not exclusively, divided into three types: (1) studies on global descriptors  [3, 16, 24, 53, 2, 37] ; (2) studies on local descriptors and geometry-based re-ranking  [29, 45, 40, 53] ; (3) re-ranking by graph-based methods  [11, 21, 55] . The first two types of studies focus on the feature representation, while the last type focuses on re-ranking extracted features. Studies on global descriptors focus on spatial pooling of CNN feature maps into vectors, including MAC  [38] , SPoC [2], CroW  [24] , R-MAC  [48, 15, 16] , GeM  [37] , and NetVLAD  [1, 25] , as well as learning the representation  [3, 15, 16, 36, 37] . Studies before deep learning dominated image retrieval were mostly based on local descriptors like SIFT [27] and bag-of-words representation  [32]  or aggregated descriptors like VLAD  [22]  or ASMK  [46] . Local descriptors have been revived in deep learning, e.g. with DELF  [29] , DELG  [5]  and ASMK extensions  [45, 47] . We focus on learning a global descriptor in this work, because it is the most efficient in terms of storage and search. However, our generic attention mechanism produces a feature tensor and could be applicable to local descriptors as well, if global pooling were replaced by local feature detection. Re-ranking methods are complementary to the representation and we do not consider them in this work. Attention Attention mechanisms have been first proposed in image classification studies focusing on channel at- METHOD LOCAL GLOBAL LRN RET Spatial Channel Spatial Channel SENet  [20]  ECA-Net  [51]  GCNet  [6]  CBAM  [54]  GE  [19]  NL-Net  [52]  AA-Net  [4]  SAN [59] N 3 Net [34] A 2 -Net  [9]  GSoP  [14]  OnA  [23]  AGeM  [17]  CroW  [24]  CRN  [25]  DELF  [29]  DELG  [5]  Tolias et al.  [47]  SOLAR  [28]  Ours Table  1 : Related work on attention. LRN: learned; RET: applied to instance-level image retrieval. tention  [20, 51, 6] , spatial attention  [19]  or both, like CBAM  [54] . In image retrieval, CroW  [24]  also employs  both spatial and channel attention and can be seen as a precursor of CBAM, but, like other studies of spatial attention on retrieval  [41, 23, 17] , it is not learned. CRN  [25]  applies spatial attention for feature reweighting and is learned. Learned spatial attention mechanisms are common for local descriptors  [29, 5, 47] . c Ã— h Ã— w c Ã— 1 Ã— 1 c Ã— 1 Ã— 1 F A l c We call the above methods local attention, in the sense that elements of the feature tensor (channels / spatial locations), are weighted independently, based on contextual information obtained by pooling or learned. By constrast, by global attention we refer to mechanisms that model interaction between elements of the feature tensor, for example between channels or between locations. In image classification, non-local neural network (NL-Net)  [52]  is maybe the first global attention mechanism, followed by similar studies  [4, 59, 34] . It is global spatial attention, allowing interaction between any pair of spatial locations. Similarly, there are studies of global channel attention, allowing interaction between channels  [9, 14] . Global attention has focused mostly on image recognition and has been applied to either spatial or channel attention so far, not both. In image retrieval, SOLAR  [28]  is a direct application of the global spatial attention mechanism of  [52] . Table  1  attempts to categorize related work on attention according to whether attention is local or global, spatial or channel, whether it is learned and whether it is applied to instance-level image retrieval. We observe that all methods limit to one or two forms of attention only. Of those studies that focus on image retrieval, many are not learned  [23, 17, 24] , and of those that are, some are designed for local descriptors  [29, 47] . By contrast, we provide a comprehensive study of all forms of attention, global and local, spatial and channel, to obtain a learned representation in the form of a tensor that can be used in any way. We spatially pool it into a global descriptor and we study the relative gain of different forms of attention in image retrieval.  feature map conv 1 Ã— 1 conv 3 Ã— 3 conv 5 Ã— 5 conv 7 Ã— 7 concat conv 1 Ã— 1 attention map c Ã— h Ã— w 4c Ã— h Ã— w 1 Ã— h Ã— w c Ã— h Ã— w dilated conv F F A l s \n",
      " Global-local attention We design a global-local attention module (GLAM), which is attached at the end of a backbone network. Figure  1  illustrates its main components. We are given a c Ã— h Ã— w feature tensor F, where c is the number of channels, and h Ã— w is the spatial resolution. Local attention collects context from the image and applies pooling to obtain a c Ã— 1 Ã— 1 local channel attention map A l c and a 1 Ã— h Ã— w local spatial attention map A l s . Global attention allows interaction between channels, resulting in a c Ã— c global channel attention map A g c , and between spatial locations, resulting in a hw Ã— hw global spatial attention map A g s . The feature maps produced by the two attention streams are combined with the original one by a learned fusion mechanism into the global-local attention feature map F gl before being spatially pooled into a global image descriptor. \n",
      " Local attention We extract an 1D channel and a 2D spatial attention map to weigh the feature map in the corresponding dimensions. Local channel attention Following ECA-Net  [51] , this attention captures local channel information. As shown in Figure  2 , we are given a c Ã— h Ã— w feature tensor F from our backbone. We first reduce it to a c Ã— 1 Ã— 1 tensor by global average pooling (GAP). Channel attention is then captured by a 1D convolution of kernel size k along the channel dimension, where k controls the extent of cross-channel interaction. This is followed by a sigmoid function, resulting in the c Ã— 1 Ã— 1 local channel attention map A l c . Local spatial attention Inspired by the inception module  [43]  and similar to  [25] , this attention map captures local spatial information at different scales. As shown in Figure  3 , given the same c Ã— h Ã— w feature tensor F from our backbone, we obtain a new tensor F with channels reduced to c , using a 1 Ã— 1 convolution. We then extract local spatial contextual information using convolutional filters of kernel size 3 Ã— 3, 5 Ã— 5, and 7 Ã— 7, which are efficiently implemented by 3 Ã— 3 dilated convolutions  [7, 57]  with dilation parameter 1, 2, and 3 respectively. The resulting features, along with one obtained by 1 Ã— 1 convolution on F , are concatenated into a 4c Ã— h Ã— w tensor. Finally, we obtain the 1 Ã— h Ã— w local spatial attention map A l s by a 1 Ã— 1 convolution that reduces the channel dimension to 1. feature map GAP conv1d(k) conv1d(k) sigmoid sigmoid Ã— Ã— softmax attention feature map 1 Ã— c 1 Ã— c 1 Ã— c Qc c Ã— c hw Ã— c Vc A g c c Ã— h Ã— w 1 Ã— c 1 Ã— c Kc F Gc The middle column of Figure  6  shows heat maps of local spatial attention, localizing target objects in images. Local attention feature map We use the local channel attention map A l c to weigh F in the channel dimension F l c := F A l c + F. (1) We then use local spatial attention map A l s to weigh F l c in the spatial dimensions, resulting in the c Ã— h Ã— w local attention feature map F l = F l c A l s + F l c . (2) Here, A B denotes an element-wise multiplication of tensors A and B, with broadcasting when one tensor is smaller. We adopt the choice of applying channel followed by spatial attention from convolutional block attention module CBAM  [54] . However, apart from computing A l s at different scales, both attention maps are obtained from the original tensor F rather than sequentially. In addition, both (1) and (2) include residual connections, while CBAM includes a single residual connection over both steps. \n",
      " Global attention We extract two matrices capturing global pairwise channel and spatial interaction to weigh the feature map. Global channel attention We introduce a global channel attention mechanism that captures global channel interaction. This mechanism is based on the non-local neural network  [52] , but with the idea of 1D convolution from ECA-Net  [51] . As shown in Figure  4 , we are given the c Ã— h Ã— w feature tensor F from our backbone. We apply GAP and squeeze spatial dimensions, followed by a 1D convolution of kernel size k and a sigmoid function, to obtain 1Ã—c query Q c and key K c tensors. The value tensor V c is obtained by mere reshaping of F to hwÃ—c, without GAP. Next, we form the outer product of K c and Q c , followed by softmax over channels to obtain a c Ã— c global channel attention map feature map conv 1 Ã— 1 conv 1 Ã— 1 conv 1 Ã— 1 Ã— Ã— softmax conv 1 Ã— 1 attention feature map c Ã— hw Qs hw Ã— hw c Ã— h Ã— w c Ã— hw Vs c Ã— h Ã— w A g s c Ã— h Ã— w c Ã— hw Kc F Gs A g c = softmax(K c Q c ). (3) Finally, this attention map is multiplied with V c and the matrix product V c A g c is reshaped back to c Ã— h Ã— w to give the global channel attention feature map G c . In GSoP  [14]  and A 2 -Net  [9] , a c Ã— c global channel attention map is obtained by multiplication of hw Ã— c matrices; (3) is more efficient, using only an outer product of 1 Ã— c vectors. Global spatial attention Since ordinary convolution applies only a local neighborhood at a time, it cannot capture global contextual information. Thus, we apply non-local filtering  [52] , which is a form of self-attention  [49]  in the spatial dimensions. As shown in Figure  5 , we are given the same c Ã— h Ã— w feature tensor F from our backbone. By using three 1 Ã— 1 convolutions, which reduce channels to c , and flattening spatial dimensions to hw, we obtain c Ã— hw query Q s , key K s , and value V s tensors, where each column is a feature vector corresponding to a particular spatial location. We capture pairwise similarities of these vectors by matrix multiplication of K s and Q s , followed by softmax over locations to obtain a hw Ã— hw global spatial attention map: A g s = softmax(K s Q s ). (4) This attention map is multiplied with V s and the matrix product V s A g s is reshaped back to c Ã— h Ã— w by expanding the spatial dimensions. Finally, using a 1 Ã— 1 convolution, which increases channels back to c, we obtain the c Ã— h Ã— w global spatial attention feature map G s . The right column of Figure  6  shows heat maps for global spatial attention, localizing target objects in images. Global attention feature map We use the global channel attention feature map F c to weigh F element-wise F g c = F G c . (5) We then use global spatial attention feature map G s to weigh F g c element-wise, resulting in the c Ã— h Ã— w global attention feature map F g = F g c G s + F g c . ( 6 ) Similarly to F l in (  1 ) and (2), we apply channel attention first, followed by spatial attention. However, unlike (1), there is no residual connection in (  5 ). This choice is supported by early experiments. \n",
      " Global-local attention Feature fusion As shown in Figure  1 , we combine the local and global attention feature maps, F l and F g , with the original feature F. While concatenation and summation are common operations for feature combination, we use a weighted average with weights w l , w g , w respectively, obtained by softmax over three learnable scalar parameters, to obtain a c Ã— h Ã— w global-local attention feature map F gl = w l F l + w g F l + wF. (7) EfficientDet  [44]  has shown that this is the most effective, among a number of choices, for fusion of features across different scales. Pooling We apply GeM  [37] , a learnable spatial pooling mechanism, to feature map F gl (7), followed by a fullyconnected (FC) layer with dropout and batch normalization. The final embedding is obtained by 2 -normalization. \n",
      " Experiments \n",
      " Datasets Training set There are a number of open landmark datasets commonly used for training in image retrieval studies, including neural code (NC)  [3] , neural code clean (NCclean)  [16] , as well as Google Landmarks v1 (GLDv1)  [29]  and v2 (GLDv2)  [53] . Table  2  shows relevant statistics. These datasets can be categorized into noisy and clean. The clean sets were obtained from the original noisy sets for more effective training  [16, 53] . The original noisy datasets are much larger, but they have high intra-class variability.  Evaluation set and metrics We use four common evaluation datasets for landmark image retrieval: Oxford5k (Ox5k)  [32] , Paris6k (Par6k)  [33] , as well as Revisited Oxford (ROxford or ROxf) and Paris (RParis or RPar)  [35] . ROxford and RParis are used with and without one million distractors (R1M)  [28]  and evaluated using the Medium and Hard protocols  [35] . We evaluate using mean Average Precision (mAP) and mean precision at 10 (mP@10). \n",
      " Implementation details We train on 8 TITAN RTX 2080Ti GPUs. All models are pre-trained on ImageNet  [39]  and implemented in PyTorch  [31] . For fair comparisons, we set a training environment similar to the those of compared studies  [56, 53, 28, 35] . We employ ResNet101  [18]  as a backbone model. The kernel size k of ECANet in subsection 3.1 is set to 3. The parameter p of GeM in subsection 3.3 is set to 3 and the dimension d of final embeddings to 512. We adopt ArcFace  [10] , a cosine-softmax based loss, with a margin of 0.3. We use stochastic gradient descent with initial learning rate 10 -3 , momentum 0.9 and weight decay 10 -5 . We adopt the batch sampling of Yokoo et al.  [56]  where mini-batch samples with similar aspect ratios are resized to a particular size. Here, we use a batch size of 64. For image augmentation, we apply scaling, random cropping, and varied illumination. At inference, we apply a multi-resolution representation  [16]  to query and database images. Our method is denoted as GLAM (global-local attention module). Using the backbone model alone is referred to as baseline. It is compatible with recent models based on ResNet101-GeM trained with ArcFace  [53, 28] . Adding our local attention (subsection 3.1) to the baseline model is denoted +local, while adding our global attention (subsection 3.2) is denoted +global. Since we focus on representation learning, we do not consider post-processing methods like geometry-based re-ranking  [29, 40, 53]  or graph-based re-ranking  [11, 21, 55]       [53, 28] . All models use ResNet101-GeM. Red: best results. Blue: GLAM higher than SOLAR  [28]  on GLDv1-noisy. GLDv2-noisy has 2.6 times more images than GLDv2clean, the latter is superior by a large margin. This shows that, in training, a cleaner dataset can be more important than a larger one. By contrast, NC-clean has the worst performance despite being clean, aparently because it is   [16, 35]  NC-clean 2048 86.1 94.5 60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0 GeM-R101-Siamese  [37, 35]  SfM   [53]  is the only model other than ours trained on GLDv2-clean, while  [28]  is trained on GLDv1-noisy and compared in Table  3 . too small. To achieve best possible performance, we use GLDv2-clean as a training set in the remaining experiments. \n",
      " Comparisons on same training set It is common to compare methods regardless of training sets as more become available, e.g.,  [35, 28] . Since GLDv2-clean is relatively new, Weyand et al.  [53] , which introduced the dataset, is the only study that has trained the same backbone with the same settings (ResNet101-GeM with ArcFace) on GLDv2-clean. Our baseline is lower than  [53] , because our dimensinality is 512, while other models based on ResNet101 use 2048. Yet, Table  3  shows that our best model trained on GLDv2-clean outperforms  [53]  by a large margin. But the most important comparison is with SOLAR  [28] , also based on selfattention, which has trained ResNet101-GeM on GLDv1noisy. On this training set, our best model clearly outperforms  [28]  despite lower dimensionality. With this model, we outperform previous best methods on most benchmarks except mP@10 on RParis (medium) and RParis+R1M (medium), where we are outperformed by  [37, 35] . These results demonstrate that our approach is effective for landmark image retrieval. Figure  7  shows some  \n",
      " Comparison with state of the art \n",
      " Ablation study Our ablation study uses the Google Landmark v2 clean dataset (GLDv2-clean)  [53]  for training, which is shown to be the most effective in Table  3  Table  9 : mAP comparison of using multiresolution representation (Multi) or not (Single) on query or database. \n",
      " Effect of attention modules We ablate the effect of our local and global attention networks as well as their combination. Table  5  shows the results, which are more finegrained than those of Table  4 . In particular, it shows the effect of the channel and spatial variants of both local and global attention. We observe that, when used alone, the channel and spatial variants of local attention are harmful in most cases. Even the combination, baseline+local, is not always effective. By contrast, when used alone, the channel and spatial variants of global attention are mostly beneficial, especially the latter. Their combination, baseline+global, is impressive, bringing gain of up to 7.5%. Importantly, the combination baseline+global+local improves further by up to another 2.8%. This result shows the necessity of local attention in the final model. \n",
      " CBAM vs. our local spatial attention We experiment with the local spatial attention of CBAM  [54] . CBAM applies average and max-pooling to input features and concatenates the two for spatial attention. We apply this variant to our local spatial attention module for comparison. For the CBAM style module, we keep the overall design of our module as shown in Figure  3 , but apply average and max-pooling to each of the four convolutional layer outputs before concatenation. Table  6  shows that the CBAM style module is considerably worse than ours on all benchmarks except Paris6k, where it is only slightly better. Concatenation vs. sum for feature fusion We use a softmax-based weighted average of local and global attention feature maps with the original feature map  (7) . Here, we compare this weighted average with weighted concatenation, where concatenation replaces the sum operation in  (7) . As shown in Table  7 , the weighted average outperforms the weighted concatenation. Fixed-size vs. group-size sampling Numerous studies have proposed methods for constructing batches according to image size for efficient training. For instance, Gordo et al.  [16] , DELF  [29] , and Yokoo et al.  [56]  employed different image sizes per batch for training instead of a single fixed size. We adopt the method of Yokoo et al., which constructs a batch with images of similar aspect ratio, so that the images can be resized to a size with an aspect ratio that is similar to their own. We call this method group-size sampling. Table  8  compares fixed-size (224 Ã— 224) with groupsize sampling. We observe that maintaining aspect ratios by using dynamic input sizes is much more effective. \n",
      " Multi-resolution We use the multi-resolution representation  [16]  for the final feature of an image at inference time. This method: (1) resizes an image into multiple scales; (2) extracts features from the resized images; and (3) averages the features to obtain the final feature of the image. The method is applied to both query and database images to enhance ranking results, especially for small target objects. Table  9  compares the four cases of applying this method or not to query or database images. \n",
      " Conclusion We have introduced a novel approach that extracts global and local contextual information using attention mechanisms for instance-level image retrieval. It is manifested as a network architecture consisting of global and local attention components, each operating on both spatial and channel dimensions. This constitutes a comprehensive study and empirical evaluation of all four forms of attention that have previously been studied only in isolation. Our findings indicate that the gain (or loss) brought by one form of attention alone strongly depends on the presence of the others, with the maximum gain appearing when all forms are present. The output is a modified feature tensor that can be used in any way, for instance with local feature detection instead of spatial pooling for image retrieval. With the advent of vision transformers  [12, 58]  and their recent application to image retrieval  [13] , attention is expected to play a more and more significant role in vision. According to our classification, transformers perform global spatial attention alone. It is of great interest to investigate the role of the other forms of attention, where our approach may yield a basic building block of such architectures. One may even envision an extension to language models, where transformers originate from  [50] . Figure 1 : 1 Figure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local attention (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention (based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (A l c ), local spatial (A l s ), global channel (A g c ) and global spatial (A g s ). The input feature map F is weighted into local (F l ) and global (F g ) attention feature maps, which are fused with F to yield the global-local attention feature map F gl . The diagram is abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5. \n",
      " Figure 2 : 2 Figure 2: Local channel attention. \n",
      " Figure 3 : 3 Figure 3: Local spatial attention. Convolutional layers in blue implemented by dilated convolutions with kernel size 3 Ã— 3 and dilation factors 1, 3, 5. \n",
      " Figure 4 : 4 Figure 4: Global channel attention. \n",
      " Figure 5 : 5 Figure 5: Global spatial attention. \n",
      " Figure 6 : 6 Figure 6: Local and global spatial attention. Left: input images. Middle: local spatial attention heat maps. Right: global spatial attention heat maps. Red (blue) means higher (lower) attention weight. \n",
      " Figure 7 : 7 Figure 7: Examples of our ranking results. In each row, the first image on the left (pink dotted outline) is a query image with a target object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images for the query; red solid outline: negative. \n",
      " . \n",
      " 4. 3 . 3 Benchmarking Noisy vs. clean training sets We begin by training our best model (baseline+local+global) on all training sets of \n",
      " Table 2 , 2 except NC-noisy because some images are currently unavailable. As shown in Table3, even though TRAIN SET #IMAGES #CLASSES NC-noisy 213,678 672 NC-clean 27,965 581 SfM-120k 117,369 713 GLDv1-noisy 1,225,029 14, 951 GLDv2-noisy 4,132,914 203,094 GLDv2-clean 1,580,470 81,313 \n",
      " Table 2 : 2 Statistics of different training sets. METHOD TRAIN SET DIM OXF5K PAR6K RMEDIUM RHARD ROxf RPar ROxf RPar GeM-Siamese [37, 35] SfM-120k 2048 87.8 92.7 64.7 77.2 38.5 56.3 SOLAR [28] GLDv1-noisy 2048 - - 69.9 81.6 47.9 64.5 GLDv2 [53] GLDv2-clean 2048 - - 74.2 84.9 51.6 70.3 GLAM (Ours) NC-clean 512 77.8 85.8 51.6 68.1 20.9 44.7 GLDv1-noisy 512 92.8 95.0 73.7 83.5 49.8 69.4 GLDv2-noisy 512 93.3 95.3 75.7 86.0 53.1 73.8 GLDv2-clean 512 94.2 95.6 78.6 88.5 60.2 76.8 \n",
      " Table 3 : 3 mAP comparison of our best model (base-line+local+global) trained on different training sets against \n",
      " Table 4 : 4 mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16: VGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). -120k 2048 87.8 92.7 64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3 AGeM-R101-Siamese [17] SfM-120k 2048 - - 67.0 - - -78.1 - - -40.7 - - -57.3 - - - SOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048 - - 69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6 DELG-GeM-R101-ArcFace [5] GLDv1-noisy 2048 - - 73.2 -54.8 -82.4 -61.8 -51.2 -30.3 -64.7 -35.5 - GeM-R101-ArcFace [53] GLDv2-clean 2048 - - 74.2 - - -84.9 - - -51.6 - - -70.3 - - - GLAM-GeM-R101-ArcFace baseline GLDv2-clean 512 91.9 94.5 72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7 +local GLDv2-clean 512 91.2 95.4 73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1 +global GLDv2-clean 512 92.3 95.3 77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0 +global+local GLDv2-clean 512 94.2 95.6 78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0 * : dimension d = 256 [2]. mP: mP@10. Red: best results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. \n",
      " Table 4 4 shows the performance of four variants of our model, i.e. baseline with or without local/global attention, and compares them against state-of-the-art (SOTA) methods based on global de- scriptors without re-ranking on the complete set of bench- marks, including distractors. Both local and global atten- tion bring significant gain over the baseline. The effect of global is stronger, while the gain of the two is addi- tive in the combination. The best results are achieved by the global-local attention network (baseline+global+local). \n",
      " . METHOD OXF5K PAR6K RMEDIUM RHARD ROxf RPar ROxf RPar Concatenate 89.5 95.1 73.6 86.5 54.0 73.7 Sum (Ours) 94.2 95.6 78.6 88.5 60.2 76.8 Table 7: mAP comparison between weighted concatenation and weighted average for feature fusion. METHOD OXF5K PAR6K RMEDIUM RHARD ROxf RPar ROxf RPar Fixed-size 76.1 82.6 55.7 68.4 29.2 47.5 Group-size (Ours) 94.2 95.6 78.6 88.5 60.2 76.8 Table 8: mAP comparison between fixed-size (224 Ã— 224) and group-size sampling methods. QUERY DATABASE OXF5K PAR6K RMEDIUM RHARD ROxf RPar ROxf RPar Single Single 93.3 95.2 76.9 87.1 58.6 74.7 Multi Single 93.9 95.4 78.0 87.7 59.0 75.5 Single Multi 93.6 95.6 77.0 87.8 57.1 76.0 Multi Multi 94.2 95.6 78.6 88.5 60.2 76.8\n"
     ]
    }
   ],
   "source": [
    "print(doc.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/2306.01926v1.grobid.tei.xml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"lxml-xml\")\n",
    "chunks = []\n",
    "content = \"\"\n",
    "for div in soup.body.find_all([\"div\", \"figure\"]):\n",
    "    if div.name == \"figure\":\n",
    "        head = div.find(\"head\")\n",
    "        fig_desc = div.find(\"figDesc\")\n",
    "        # if ('Figure' not in head.getText()) and ('Table' not in head.getText()):\n",
    "        content += head.getText().replace(\" \", \"\") + fig_desc.getText() + \"\\n\"\n",
    "    elif div.name == \"div\":\n",
    "        section_num = div.head.get(\"n\")\n",
    "        section_title = div.head.getText(separator=' ', strip=True)\n",
    "        if section_num is None:\n",
    "            section_num = \"\"\n",
    "        content += section_num + \" \" + div.getText(separator=': ', strip=True) + \"\\n\"\n",
    "    content += \"=======================================\\n\"\n",
    "    # for head in div.find_all(\"head\"):\n",
    "    #     section_num = head.get(\"n\")\n",
    "    #     section_title = head.getText(separator=' ', strip=True)\n",
    "    #     print(section_num, section_title)\n",
    "    # content = \"\"\n",
    "    \n",
    "    # for p in div.find_all(\"p\"):\n",
    "    #     content += p.getText(separator=' ', strip=True)\n",
    "    # print(content)\n",
    "    # print(\"=================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IndexNewArxivPapers:\n",
    "    \"\"\"\n",
    "    This tool indexes new papers from arxiv using the following steps:\n",
    "    \n",
    "    1. Get the paper ids from google search\n",
    "    2. Download the papers\n",
    "    3. Process the papers with GROBID\n",
    "    4. Parse the TEI files\n",
    "    5. Add the papers to the vectorstore\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                vectordb: VectorStore,\n",
    "                n_search_results: int = 2,\n",
    "                pdf_parser: Literal[\"grobid\", \"pymupdf\"] = \"grobid\",\n",
    "                chunk_size: int = 1024,\n",
    "                chunk_overlap: int = 100,\n",
    "                ):\n",
    "    \n",
    "        self.google_api = GoogleSearchAPIWrapper()\n",
    "        self.arxiv_client = arxiv.Client(delay_seconds=0)\n",
    "        self.vectordb = vectordb\n",
    "        self.chromadb_client = chromadb.PersistentClient(\"arxiv_vdb\").get_collection(\"arxiv\")\n",
    "        self.n_search_results = n_search_results\n",
    "        self.splitter = SpacyTextSplitter(\n",
    "            chunk_size = chunk_size,\n",
    "            chunk_overlap = chunk_overlap,\n",
    "            separator=\"\\n\\n\")\n",
    "        self.pdf_parser = pdf_parser\n",
    "    \n",
    "    def _get_paper_ids(self, query: str) -> List[str]:\n",
    "        ARXIV_ID_REGEX =  r\"\\d{4}\\.\\d{4,5}\"\n",
    "        \n",
    "        ids = list(\n",
    "            {re.findall(ARXIV_ID_REGEX, result['link'])[0] \n",
    "             for result in self.google_api.results(query, self.n_search_results)}\n",
    "            )\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def _run(self, query):\n",
    "        with cl.Step():\n",
    "            self.ids = self._get_paper_ids(query)\n",
    "            \n",
    "            os.makedirs(f\"./output/{query}\", exist_ok=True)\n",
    "            os.makedirs(f\"./pdfs/{query}\", exist_ok=True)\n",
    "            \n",
    "            for id in self.ids:\n",
    "                if len(self.chromadb_client.get(where={'paper_id': id})['ids']) > 0:\n",
    "                    self.ids.remove(id)\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            papers = list(self.arxiv_client.results(arxiv.Search(id_list=self.ids)))\n",
    "            for paper in papers:\n",
    "                id = paper.entry_id.split('/')[-1]\n",
    "                paper.download_pdf(dirpath=f\"./pdfs/{query}/\", filename=f\"{paper.entry_id.split('/')[-1]}.pdf\")\n",
    "            \n",
    "            while not all([os.path.exists(f\"./pdfs/{query}/{paper.entry_id.split('/')[-1]}.pdf\") for paper in papers]):\n",
    "                time.sleep(1)\n",
    "            \n",
    "            docs = []\n",
    "            if self.pdf_parser == \"grobid\":\n",
    "                self.grobid_client = GrobidClient(config_path=\"./grobid_client_python/config.json\")\n",
    "                self.grobid_client.process(\"processFulltextDocument\", f\"./pdfs/{query}/\", output=f\"./output/{query}\", force=True)            \n",
    "                \n",
    "                while not all([os.path.exists(f\"./output/{query}/{paper.entry_id.split('/')[-1]}.grobid.tei.xml\") for paper in papers]):\n",
    "                    time.sleep(1)\n",
    "            \n",
    "                for paper in papers:\n",
    "                    tei_object = TEIFile(f\"./output/{query}/{paper.entry_id.split('/')[-1]}.grobid.tei.xml\")\n",
    "                    chunks = self.splitter.split_text(tei_object.text)\n",
    "                    docs.extend([\n",
    "                            Document(\n",
    "                                    page_content=chunk, \n",
    "                                    metadata={\n",
    "                                        'paper_id': paper.entry_id.split(\"/\")[-1],\n",
    "                                        'authors': \", \".join([author.name for author in paper.authors]),\n",
    "                                        'title': paper.title,\n",
    "                                        'chunk_id': f\"{paper.entry_id.split('/')[-1]}-{i}\"\n",
    "                                        }\n",
    "                                    )\n",
    "                                    for i, chunk in enumerate(chunks)\n",
    "                                ]\n",
    "                    )\n",
    "                    \n",
    "            if self.pdf_parser == \"pymupdf\":\n",
    "                for paper in papers:\n",
    "                    loader = PyMuPDFLoader(f\"pdfs/{query}/{paper.entry_id.split('/')[-1]}.pdf\")\n",
    "                    chunks = loader.load_and_split(self.splitter)                    \n",
    "                    docs.extend([\n",
    "                            Document(\n",
    "                                    page_content=chunk.page_content, \n",
    "                                    metadata={\n",
    "                                        'paper_id': paper.entry_id.split(\"/\")[-1],\n",
    "                                        'authors': \", \".join([author.name for author in paper.authors]),\n",
    "                                        'title': paper.title,\n",
    "                                        'chunk_id': f\"{paper.entry_id.split('/')[-1]}-{i}\"\n",
    "                                        }\n",
    "                                    ) \n",
    "                                    for i, chunk in enumerate(chunks)\n",
    "                                ]\n",
    "                        )\n",
    "            self.vectordb.add_documents(\n",
    "                docs\n",
    "            )\n",
    "\n",
    "\n",
    "    async def _arun(self, query: str):\n",
    "        async with cl.Step():\n",
    "            self._run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key to a Transformer model is the self-attention mechanism, which allows the model to analyze an entire sequence in a computationally efficient manner. Recent work has suggested the possibility that general attention mechanisms used by RNNs could be replaced by active-memory mechanisms. In this work, we evaluate whether various activememory mechanisms could replace self-attention in a Transformer. Our experiments suggest that active-memory alone achieves comparable results to the self-attention mechanism for language modelling, but optimal results are mostly achieved by using both active-memory and self-attention mechanisms together. We also note that, for some specific algorithmic tasks, active-memory mechanisms alone outperform both the self attention and a combination of the two.\n",
      "\n",
      "Introduction: The previous state-of-the-art sequence model, the recurrent neural network, has been largely supplanted by the Transformer model: [Vaswani et al., 2017]: , which is primarily built atop a self-attention mechanism. Given a task to train upon, the self-attention mechanism focuses on one token per attention head within the entire sequence at each time-step; the key to the self-attention mechanism's success is the mechanism's ability to learn which token within the entire sequence to focus on in order to achieve the best results.: The self-attention mechanism has proven successful on a variety of natural language processing tasks, but has not achieved ubiquitous success. The authors of: [Kaiser and Bengio, 2016]: pointed out that an attention mechanism would likely struggle to solve a task which required a model to focus on multiple tokens at a given time-step. Further, the authors of: [Kaiser and Sutskever, 2015]: recommended that an attention mechanism could be replaced by active-memory to alleviate these concerns.: Unlike attention, active-memory allows a model to access and change any and all elements of its memory at each timestep. The active-memory mechanism can access more than one element at each time step. In: [Kaiser and Bengio, 2016]: ,: Figure: 1: : The active memory mechanism. In this case, the activememory is implemented in a unidirectional manner, with a kernel size 3.: the authors used an active-memory system to translate English to French, and was capable of outperforming an RNN model, both with and without an attention mechanism.: Motivated by the success of attention mechanism: [Vaswani et al., 2017]: and active-memory: [Kaiser and Bengio, 2016]: , in this paper we investigate the Transformer's self-attention mechanism in comparison to a variety of active-memory mechanisms. We experiment on two types of tasks: the language modeling task and a set of algorithmic tasks.: For the language modelling task, the self-attention mechanism out-performs an active-memory mechanism used alone by a slim margin. However, a combination of both selfattention and active-memory reliably outperform both mechanisms used alone.: We also evaluated the self-attention mechanism and various active-memory mechanisms on a variety of algorithmic tasks, which can also be expressed as a sequence modeling task. Across most of the algorithmic tasks tested, the activememory mechanisms achieve equal, or superior, results to a traditional self-attention mechanism. This would appear to vindicate the hypothesis stated by: [Kaiser and Bengio, 2016]: , suggesting that the nature of the attention mechanism does indeed limit the effectiveness and accuracy of the model. Finally, we note that, for several algorithmic tasks, the mere addition of the self-attention mechanism hinders results; the active-memory mechanism alone outperforms a combination of the two separate mechanisms. This raises an unsolved problem; it would appear that, for deep learning sequence models, there is still no unambiguous model that can optimally solve all possible problems.\n",
      "\n",
      "Related Work: The Transformer model: [Vaswani et al., 2017]: is built with two separate modules, the self-attention mechanism and the feedforward mechanism, which are stacked atop each other for multiple layers. The feedforward mechanism is an intrasequence analysis, where the output for each token in the sequence is dependent only on the token at the same timestep, and independent of all other time-steps. On the other hand, the self-attention mechanism is an inter-sequence analysis, where the output for each time-step is dependent upon the entire sequence. The self-attention mechanism is defined, mathematically, as:: Q t , K t , V t = x t y t = concat(head 1,t , head 2,t , . . . , head n,t )W o head i = Attention(Q t W Q i , K t W K i , V t W V i ) Attention(Q, K, V ) = sof tmax( Q t K T t √ d k )V t W o R d k * k,d , W K,Q,V i R d,d k * k: The feed-forward module is defined as:: y t = W l,2 (max(W l,1 x t + b l,1 , 0)) + b l,2 W l,2 R d,d * 4 , W l,1 R d * 4,d: The Transformer model, and its variants: [Dai et al., 2019]: , have achieved remarkable results across a variety of natural language processing tasks since its inception: [Zhenzhong et al., 2019: ] [Delvin et al., 2018: ] [Yang et al., 2019b]: , and are currently investigated heavily by both academia and industry.: The Neural GPU: [Freivalds and Liepins, 2017: ] [Kaiser and Bengio, 2016: ] [Kaiser and Sutskever, 2015]: , which introduced an active-memory model, achieved impressive algorithmic results in: [Kaiser and Sutskever, 2015]: , and also achieved impressive machine translation results in: [Kaiser and Bengio, 2016]: . A Neural GPU contains a CGRU (Convolution Gated Recurrent Unit) module which is iterated repeatedly. This allows the entire sequence to be analyzed in a parallelizable and computationally efficient manner. The CGRU module is defined as:: u = sigmoid(U 1 * x + B 1 ) r = sigmoid(U 2 * x + B 2 ) y = u ⊗ x + (1 -u) ⊗ tanh(U 0 * (r ⊗ x) + B 0 )): where U * x refers to applying a convolutional operator over x, using U as a trainable kernel bank and B is a trainable bias vector. The CGRU has, since its introduction, been used in other models: [Resende et al., 2016]: .: Convolutional operators are traditionally used for image processing: [Alom et al., 2018]: , and have also been used in relation to sequential analysis in previous papers: [Yang et al., 2019a: ] [Wu et al., 2019: ] [Gehring et al., 2017: ] [Dauphin et al., 2016]: . To the best of our knowledge they have not been used explicitly to replace, or augment, the self-attention mechanism. The first sequence-to-sequence model, based on convolutional operators, was, to the best of our knowledge, introduced in: [Gehring et al., 2017]: , which replaced the thentraditional LSTM block with a series of convolutions and gated convolutional networks: [13]: , and outperformed RNNbased models in terms of both speed and accuracy. However, the model introduced in: [Gehring et al., 2017]: was followed shortly afterwards by the Transformer model, which outperformed the convolutional-based model.: The convolutional self-attention network: [Yang et al., 2019a]: was recently introduced, and bares a passing similarity to the traditional convolutional operator described in this paper. The layer of the convolutional self-attention is similar to a traditional self-attention mechanism, but where the key and value tensors are calculated as:: K h = (K h i-M/2 , ..., K h i , ..., K h i+M/2 ) V h = (V h i-M/2 , ..., V h i , ..., V h i+M/2: ) From this point, the convolutional self-attention mechanism acts in an identical manner to the traditional selfattention mechanism. This is in direct comparison to the convolutional operator described in this paper, which explicitly avoids the use of the self-attention mechanism and relies entirely on a purely convolutional operator.\n",
      "\n",
      "Approach: In this paper, we investigate whether various active-memory mechanisms could replace self-attention in a Transformer. We also evaluate the combination of self-attention and activememory mechanisms for language modelling tasks. All the active-memory mechanisms introduced in this paper were inspired by the Neural GPU, as introduced in: [Kaiser and Sutskever, 2015]: . The key allure of the Neural GPU is that the inputs of each time-step can be analyzed and altered, and we were inspired to apply a similar form of sequence modelling alongside a self-attention mechanism. We describe various convolution-based active-memory mechanisms in this section.\n",
      "\n",
      "The Convolutional Operators: The Traditional Convolutional Operator The first, and most simple, active-memory mechanism is the simple convolutional operator. The traditional convolutional operator was formally defined in: [Bai et al., 2019]: . If the task requires the sequence to be analyzed in a unidirectional manner, such as the case for language modelling, then a zerosvector of size k -1 is concatenated to the left of the input tensor so that, for the nth output token, the model only has access to the first n input tokens. This feature is crucial to avoid allowing the model 'seeing' forward through the sequence and having access to information that the model, in practice, would not yet have. This has an identical function to the masking operation of the self-attention mechanism.: If the task can be analyzed in a bidirectional manner, then the model uses a convolutional filter using the SAMEpadding, which allows for the vector to maintain its shape throughout the convolutional operator. However, when the convolutional operator is performed in this manner, the token at time-step t is dependent on the input tokens h: [t-k/2,t+k/2] ,: where k is the kernel size.: The primary flaw of a convolutional operator, in comparison to a self-attention mechanism, is that, given n layers where each kernel has a k kernel size, each token can only see k * n -n + 1 or k/2 * n -n + 1 time-steps across for unidirectional and bidirectional tasks respectively. For example, in our experiments on language modeling (Section 4), the kernel size was set to 20 and was iterated over 8 layers. Therefore, at each time-step t, the final output is capable of analyzing the input from 153 previous time-steps, well above the average sequence-size (90 tokens) in the dataset. The self-attention mechanism, in comparison, can see across a theoretically infinite context size, even using only a single layer. Given this information, the self-attention mechanism is capable of handling theoretically greater long-term dependencies than the active-memory mechanism. However, in practice, the ability of an active-memory mechanism to access and change its entire memory could overcome this limitation.: The convolutional operator is assisted further by the fact that the convolutional operator's complexity grows linearly with the sequence size, while the self-attention mechanism's complexity grows quadratically.: Numerous papers have noted that, while Transformers are parallelizable and capable of capturing long-range dependencies, the Transformer network suffers from the inability of model tokens in a recurrent manner: [Wang et al., 2019: ] [Hao et al., 2019]: . This is in direct comparison to traditional RNN models, which can capture long-range dependencies, but can struggle to capture long-range dependencies. The use of active-memory, in theory, would accomplish this task, given that the output at time-step t h t is dependent of the inputs x: [t-k,t]: where k is the kernel size. Therefore, this operation can, in theory, model recurrence. We did not explicitly test whether this does model recurrence in practice, but will focus on this in future work.: The convolutional operator is followed by the ReLU activation function.\n",
      "\n",
      "The Persistent-Convolutional Operator: The Persistent-Convolutional operator is similar to the traditional convolutional operator described above, except that the zeros vector is replaced by the a trainable vector of identical shape to the zeros vector. This allows the operator to, identical to the traditional convolutional operator, maintain an identical shape across the convolution. To keep parameterization to a minimum, the same persistent vector is used across all convolution operators in the entire model. The persistentconvolutional operator is defined as:: p ∈ W kernel size-1,hiddensize x = [p, x], y = W * x + b: where [.,.] denotes the concatenation function and p is the trainable persistent vector. Persistent vectors have been used previously in language modelling tasks: [Sukhbaatar et al., 2019]: , but never as an augmentation for convolutional operators, as far as we know.: If the model is to be analyzed in a bidirectional manner, rather then a unidirectional manner, then the persistentconvolutional operator can be redefined as: The use of a persistent vector allows for the model to have a permanent memory that, given the fact the vector is trainable, can be expressed in an optimal manner for the model. This is the equivalent of a permanent memory for the deep learning model.: p 1 , p 2 ∈ W (kernel size-1)//2,hiddensize x = [p 1 , x, p 2 ]\n",
      "\n",
      "The Highway-Convolutional Operator: The Highway-Convolutional operator is based on the highway network architecture: [5]: , which can be defined as:: a = U 0 * x + B 0 b = sigmoid(U 1 * x + B 1 ) y = a ⊗ b + x ⊗ (1 -b): The key allure of the highway network, as described in: [Srivastava et al., 2015]: , is the fact that a highway network can be trained for a large number of layers, even hundreds of layers, because information can pass, unimpeded, across each layer. The authors of: [Srivastava et al., 2015]: described these paths as 'information highways'. The use of these 'information highways' allows information to pass through the self-attention mechanism in an equally efficient manner.: In this paper, we use the hard-sigmoid function: [Kaiser and Bengio, 2016]: to stabilize gradients, which is defined as:: y = max(0, min(1, 1.2 * sigmoid(x) -0.1))\n",
      "\n",
      "Self-Attention + Convolutional Operators: The operator calculates the results of the self-attention mechanism and results of the convolutional operator independently, and then adds them together to produce the final output of the operator. This operator would allow the model to analyze the input using both the self-attention mechanism and active-memory mechanism and decide which features from both mechanisms would be most optimal. This approach has the obvious advantage of being able to take the 'best of both worlds', where the optimal features that can only be detected  The loss-per-token of the self-attention mechanism and the active-memory mechanisms on the WT3 dataset, and the difference of loss between the self-attention and the active-memory mechanisms. The lower the loss, the better the model performed. With the exception of the CGRU, all purely active-memory operators achieve a test loss less then 1.2% higher then the self-attention mechanism.: The optimal models combined the self-attention mechanism and an active-memory mechanism, and achieved a lower test loss than the self-attention mechanism and active-memory mechanisms alone.: by the self-attention mechanism, and the optimal features that can only be detected by the convolutional operator, are both available to the model. The architecture of a single layer of the \"self-attention + convolution\" operator is shown in Figure: 2: . This architecture, without the convolutional operator, is a simple Transformer layer. The output of the convolutional operator is added, element-wise, to the output of the self-attention mechanism. This allows, hypothetically, for the best-of-bothworlds, where the model has access to the self-attention mechanism and the active-memory mechanism.: Similarly, we also add the self-attention mechanism to the persistent-convolutional operator and the highwayconvolutional operator, respectively.\n",
      "\n",
      "Experiments: To evaluate the effectiveness of the various convolution-based active-memory mechanisms, we used two separate experiments; a language modelling task that is traditionally associated with attention-based mechanisms: [Shoeybi et al., 2019]: , and algorithmic tasks that are associated with active-memory models: [Kaiser and Sutskever, 2015]: . The active-memory mechanisms are experimented both independently and alongside a self-attention mechanism.\n",
      "\n",
      "Language Modelling Experimental Setup: The first task that the operators were tested with was a unidirectional language modelling task; the WikiText-3 (WT3) dataset: [Merity et al., 2016]: , tokenized using BPEtokenization: [Sennrich et al., 2016]: . The WikiText-3 dataset was sourced entirely from Wikipedia articles, contains over 3.6 millions lines of text, and is split into a training dataset, valid dataset and test dataset. The train dataset contains 103M tokens, while the valid and test dataset contain 250K tokens each.: The models used were all 8-layer models, with a hidden size of 256 and a filter size of 1024, a vocab size of 32,000, (x, y) 1 0 1 1 + 0 0 1 1 (x + y) 0 0 0 0 0 1 1 1 0 Table 2: The binary addition task. Given two numbers (in this case, the two numbers are 11 and 3), the final output is the binary version of the addition of the two input numbers (in this case, 14). kernel size of 20 and a dropout rate of 0.9. No further regularization was used. The optimizer was the Adam Optimizer: [Kingma and Ba, 2014]: and a warmup-learning rate was used, as specified in: [Vaswani et al., 2017]: . All models were implemented using Tensorflow, version 1.07, on a V100 GPU card.: Notable preprocessing was used for analyzing the WikiText-3 dataset; every character was explicitly denoted as lower-case, each hyphenwas replaced by @-@ and punctuation marks, such as fullstops and commas, were seperated by white-space. This was done to discourage the BPE to tokenize sets of characters that included punctuation marks, forcing the model to tokenize sets of characters that were only letters, therefore tokenizing a greater set of words.\n",
      "\n",
      "Experimental Results: With the exception of the CGRU operator, all active-memory mechanisms, when combined with the self-attention mechanism, outperformed the self-attention mechanism alone, achieving a lower loss-per-token. This would appear to vindicate the proposition of both this paper and [2], suggesting that, indeed, active-memory mechanisms and self-attention are comparable. However, no model that purely used an active-memory mechanism outperformed the self-attention mechanism for language modelling.: We note that, if the dropout rate was decreased to 0.7, all operators, with the exception of CGRU, all models achieved superior results to the self-attention mechanism at a the same dropout rate. However, these models did not achieve superior results to the self-attention model with a dropout-rate of 0.9. This would imply that self-attention mechanisms are more sensitive to dropout rates compared to active-memory mechanisms.: Further, each operator, except for the CGRU operator, benefited from combining it with self-attention, allowing both operators to operate independently and concurrently. The model with the lowest loss-per-token had a self-attention mechanism and a highway-convolutional operator. It is further worth noting that the highway-convolutional operator outperformed both other convolutional operators, both with and without the addition of the self-attention mechanism.\n",
      "\n",
      "Algorithmic Tasks Experimental Setup: The second experiment for evaluating the active-memory mechanisms was on various algorithmic tasks:: • Reverse: Given an array X of size L, the model is trained to return the array Y, where Y[0] = X[-1]. In order to effectively perform this task, the model must be capable of analyzing the start of the input vector at the very end, and vice-versa.: (x, y): 1 0 1 0 1 × 0 1 1 0 0 (x × y) 0 0 0 1 1 1 1 1 1 0 0 • Sort: Given an array of randomly order integers, the model is trained to return an array that accurately order the input integers. The entire vector must be remembered and analyzed at each time-step.: • Addition: Given two binary numbers, the model is trained to return an array that represents the addition in the form of a third binary number. An example of the addition task is shown in Table: 2: .: • Multiply: Multiplies two binary numbers, as shown in Table: 3: .: • Not: If the input is 1, then not returns 0. Else, the not function returns 1. The output relies only on the input at the current time-step.: • Remember: Given a series of random numbers of sequence size N , followed by a sequence of zeros of identical size, the model is trained to output a series of zeros of size N , followed by the random numbers. In order for this task to be performed, the model must be able to remember tokens over an increasingly long sequence.: All data for the algorithmic tasks were generated in an online manner. For three of the tasks, Sort, Addition and Multiply, the model must focus on multiple tokens at every timestep. In comparison, the Reverse task, the Not task and the Remember task only require the model to focus on a single token at every time-step.: The model that was used for algorithmic tasks contains 4 layers, with a hidden size of 128 units, a filter size of 512 and a kernel size of 20. Each model was trained for a maximum of 100 epochs, where each epoch contains 100 iterations. At the end of each epoch, the model was exposed to an online batch, containing 32 test cases. If the model achieved an accuracy of 100% on the online test batch, the sequence-size of the data is increased, therefore increasing its complexity.: For the Reverse, Sort, Not and Remember task, when the model achieved a 100% accuracy, the sequence was increased by 1. For the Addition and Multiply task, the sequence was increased by 2.: The model was initially trained only for sequences that are 5 tokens long and was not introduced to a larger sequence until the model was capable of achieving 100% accuracy on this sequence-size. We found that this form of curriculum learning was essential: if a model was initially trained on a sequence of several dozen tokens, each operator was incapable of achieving a reasonable accuracy.: The vocabulary size was different for each task. The Reverse task had a vocabulary size of 100, while the Sort task and the Remember task had a vocabulary size of 20. We noted that whenever the vocab size was increased the model would achieve less accurate results. Because all tokens in the Addition, Multiply and Not tasks are either 0, 1, or the separator, the vocabulary size is set to 3.\n",
      "\n",
      "Experimental Results: Each model was tested for each task, and the highest sequence that the model could achieve within 100 epochs was recorded. Each experiment was performed three times, and the average sequence size is presented in Table: 4: . For example, the selfattention mechanism managed to achieve a 100% accuracy for a sequence of 41 tokens for the Reverse task, but could not achieve a 100% accuracy for both the Sort task and the Addition task for a sequence size of 20 (the Sort task achieved a maximum sequence size of 14, while the Addition task achieved a maximum size of 7). Of the six algorithmic tasks tested, active-memory mechanisms were used, either solely or in combination with the self-attention mechanism, in the best-performing model of five of these tasks. For example, the self-attention mechanism achieved an average sequence size of 41.0 for the Reverse Task and 14.0 for the Sort Task, which are lower than those achieved by the \"self-attention + persistent-convolution\" mechanism: (43.7 and 23.3, respectively): . Furthermore, for the Addition and Multiply Tasks, the active-memory mechanisms across the board outperformed both the self-attention mechanism and the combination of the self-attention mechanism and the active-memory mechanism. For example, the traditional convolution operator, for the Addition Task, outperformed the self-attention mechanism and the \"self-attention + convolutional\" mechanism by 34.0 and 4.7 respectively. The results show that the active-memory mechanisms achieve equal, or superior, results to a traditional self-attention mechanism.: Self-attention, used alone, only performed optimally on the Remember task, and equally well on the Not task. Interestingly, across all models for the Addition and Multiply tasks, the self-attention mechanism reliably led to poor results; not only does the self-attention mechanism, alone, achieve the poorest results, but the combination of the self-attention mechanism and any active-memory system performed worse then the active-memory system alone. This is in direct contrast to the Sort task and the Reverse task, where the combination of self-attention mechanism and the active-memory achieve the best results.: The self-attention mechanism would, in theory, outperform active-memory mechanisms for the Remember task. This is because, in order to adequately perform the Remember task, the model must be capable of calculating an output based on long-range dependencies, which active-memory cannot match at a large enough sequence length. Other tasks do require a long-range dependency in order to operate well at large sequence sizes, but are dependent on the model performing other tasks as well. For example, the addition task requires to model long range-dependencies and perform binary addition. The self-attention mechanism, although it can learn these long-range dependencies, cannot access all necessary tokens at a given time to adequately perform binary addition. This is vindicated by the experimental results. In Table: 4: , the self-attention mechanism achieved the highest results on the Remember task. This would suggest that, if the algorithmic task only requires a long-range dependency, then the selfattention mechanism will outperform active-memory mech- The average sequence length that each operator was capable of gaining 100% accuracy within 100 epochs over 3 runs. The higher the sequence size, the better the model learned. For the Reverse and Sort tasks, the combination of self-attention mechanism and persistent-convolution achieved the best results. For the Sort, Addition, and Multiply tasks, the self-attention mechanism was beaten by the active-memory mechanisms. For the Addition and Multiply tasks, the mere use of a self-attention mechanism alongside an active-memory mechanism actively decreased results. The highest possible sequence that can be learned over these epochs is 104 in the Not task. The self-attention mechanism achieved the best result only for the Remember task.: anisms when used alone. In comparison, the self-attention mechanism is incapable of matching the results of activememory for all other tasks. These findings appear to vindicate the statement made by: Kaiser et. al [2]: ; whenever the sequential task requires the model to focus on multiple tokens at every time-step, using an attention mechanism will lead to extremely poor results, especially in comparison to active-memory models.: It is worth noting that, for each of the active-memory mechanisms operating alone, none of the three achieved a 100% accuracy for any sequence over a size of 37 for the Remember task. This is because, given the kernel size of 20 and 4 layers, the model is only capable of seeing 37 time-steps across. Therefore, the model cannot see 37 time-steps across and, therefore, cannot perform the Remember task at this sequence size or any larger sequence size. This displays the importance of utilizing both a self-attention mechanism, which can be utilized for analyzing long-range dependencies, and an active-memory mechanism, which can extract features that the self-attention mechanism cannot.\n",
      "\n",
      "Discussion of Results: The experiments above suggest that, across most tasks, a combination of a self-attention mechanism and an activememory mechanism, at worst, perform comparably to a purely attention-based model, and at best surpass an attention model, with the exception of the Remember task. However, for some algorithmic tasks, we note that the mere inclusion of a self-attention mechanism actively hinders performance.: Models that combine both the attention mechanism and active-memory mechanisms outperformed both attentiononly and active-memory-only models for language modelling. This suggests that, for language modelling tasks, both active-memory mechanisms and attention mechanisms are capable of extracting features that the other mechanism is not capable of extracting, and that both mechanisms operate optimally when used alongside each other.: The findings are further abstracted by studying the effect of various algorithmic tasks; in cases where only a single token needs to be focused on, the self-attention mechanism matches the most ardent active-memory, while active-memory mecha-nisms radically outperform self-attention for other tasks. This would imply that various time-dependencies that cannot be analyzed by a self-attention mechanism can be analyzed by active-memory.: It is worth noting that, for the Not function, all models learn optimally. This is likely due to the fact that the output of each time-step depends only on the input at this time-step, and each model can analyze this dependency equally efficiently. Also, based on the results of the Remember task, the self-attention mechanism can attain greater long-range dependency in comparison to the active-memory mechanisms.: Finally, we note that, for the Remember function, both mechanisms, when used alone, outperform the two mechanisms used together. For every other task, a combination of the self-attention and active-memory would improve upon at least one of the mechanisms when used alone. We are unsure exactly what has led to this result. This will require further investigation in the future.\n",
      "\n",
      "Conclusion: In this paper we investigate the Transformer's self-attention mechanism in comparison to a variety of active-memory mechanisms. We experiment on two types of tasks: the language modeling task and the algorithmic task. Our results show that the self-attention mechanism can be improved by an active-memory mechanism alone or by a combination of the two. Our results have implications for wider sequence modeling tasks, which are currently dominated by self-attention based models.: Our code and models used in experiments are available at: https://github.com/Anon-111/Active-Memory.: In the future, we will further explore the use of activememory for sequence-to-sequence tasks, such as machine translation. We will also analyze the empirical differences between the studied algorithmic tasks, and investigate why the self-attention mechanism may assist one task but harm another.\n",
      "===================\n",
      "We address representation learning for large-scale instance-level image retrieval. Apart from backbone, training pipelines and loss functions, popular approaches have focused on different spatial pooling and attention mechanisms, which are at the core of learning a powerful global image representation. There are different forms of attention according to the interaction of elements of the feature tensor (local and global) and the dimensions where it is applied (spatial and channel). Unfortunately, each study addresses only one or two forms of attention and applies it to different problems like classification, detection or retrieval.: We present global-local attention module (GLAM), which is attached at the end of a backbone network and incorporates all four forms of attention: local and global, spatial and channel. We obtain a new feature tensor and, by spatial pooling, we learn a powerful embedding for image retrieval. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.\n",
      "\n",
      "Introduction: Instance-level image retrieval is at the core of visual representation learning and is connected with many problems of visual recognition and machine learning, for instance metric learning: [30,: 26]: , few-shot learning: [42]: and unsupervised learning: [8]: . Many large-scale open datasets: [3,: 37,: 16,: 29,: 53]: , and competitions 1 have accelerated progress in instance-level image retrieval, which has been transformed by deep learning: [3]: .: Many studies on instance-level image retrieval focus on learning features from convolutional neural networks (CNN), while others focus on re-ranking, for instance by graph-based methods: [11]: . The former can be distinguished according to feature types: local descriptors, reminiscent of SIFT: [27]: , where an image is mapped to a few hundred vectors; and global descriptors, where an image is mapped to a 1 https://www.kaggle.com/c/landmark-retrieval-2020 single vector. In fact, deep learning has brought global descriptors with astounding performance, while allowing efficient search. Our study belongs to this type.: Studies on global descriptors have focused on spatial pooling: [2,: 37]: . The need for compact, discriminative representations that are resistant to clutter has naturally given rise to spatial attention methods: [24,: 28]: . Different kinds of attention have been studied in many areas of computer vision research. There is also channel attention: [20,: 9]: ; local attention, applied independently to elements of the representation (feature map): [54,: 25]: ; global attention, based on interaction between elements: [52,: 9]: ; and combinations thereof. Unfortunately, each study has been limited to one or two kinds of attention only; attention is not always learned; and applications vary.: It is the objective of our work to perform a comprehensive study of all forms of attention above, apply them to instance-level image retrieval and provide a detailed account of their interaction and impact on performance. As shown in Figure: 1: , we collect contextual information from images with both local and global attention, giving rise to two parallel network streams. Importantly, each operates on both spatial locations and feature channels. Local attention is about individual locations and channels; global is about interaction between locations and between channels. The extracted information is separately embedded in local and global attention feature maps, which are combined in a global-local attention feature map before pooling.: Our contributions can be summarized as follows:: 1. We propose a novel network that consists of both global and local attention for image retrieval. This is the first study that employs both mechanisms.\n",
      "\n",
      "Each of the global and local attention mechanisms: comprises both spatial and channel attention. 3. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.: A l c c × 1 × 1 × + F l c A l s 1 × h × w × + F l × c × h × w F × + c × h × w F gl A g c c × c × F g c A g s hw × hw × + F g\n",
      "\n",
      "Related work: Instance-level image retrieval Studies on instance-level image retrieval can be roughly, but not exclusively, divided into three types: (1) studies on global descriptors: [3,: 16,: 24,: 53,: 2,: 37]: ; (2) studies on local descriptors and geometry-based re-ranking: [29,: 45,: 40,: 53]: ; (3) re-ranking by graph-based methods: [11,: 21,: 55]: . The first two types of studies focus on the feature representation, while the last type focuses on re-ranking extracted features.: Studies on global descriptors focus on spatial pooling of CNN feature maps into vectors, including MAC: [38]: , SPoC: [2]: , CroW: [24]: , R-MAC: [48,: 15,: 16]: , GeM: [37]: , and NetVLAD: [1,: 25]: , as well as learning the representation: [3,: 15,: 16,: 36,: 37]: . Studies before deep learning dominated image retrieval were mostly based on local descriptors like SIFT: [27]: and bag-of-words representation: [32]: or aggregated descriptors like VLAD: [22]: or ASMK: [46]: . Local descriptors have been revived in deep learning, e.g. with DELF: [29]: , DELG: [5]: and ASMK extensions: [45,: 47]: .: We focus on learning a global descriptor in this work, because it is the most efficient in terms of storage and search. However, our generic attention mechanism produces a feature tensor and could be applicable to local descriptors as well, if global pooling were replaced by local feature detection. Re-ranking methods are complementary to the representation and we do not consider them in this work.: Attention Attention mechanisms have been first proposed in image classification studies focusing on channel at-: METHOD LOCAL GLOBAL LRN RET Spatial Channel Spatial Channel: SENet: [20]: ECA-Net: [51]: GCNet: [6]: CBAM: [54]: GE: [19]: NL-Net: [52]: AA-Net: [4]: SAN: [59]: N 3 Net [34] A 2 -Net: [9]: GSoP: [14]: OnA: [23]: AGeM: [17]: CroW: [24]: CRN: [25]: DELF: [29]: DELG: [5]: Tolias et al.: [47]: SOLAR: [28]: Ours Table: 1: : Related work on attention. LRN: learned; RET: applied to instance-level image retrieval.: tention: [20,: 51,: 6]: , spatial attention: [19]: or both, like CBAM: [54]: . In image retrieval, CroW: [24]: also employs  both spatial and channel attention and can be seen as a precursor of CBAM, but, like other studies of spatial attention on retrieval: [41,: 23,: 17]: , it is not learned. CRN: [25]: applies spatial attention for feature reweighting and is learned. Learned spatial attention mechanisms are common for local descriptors: [29,: 5,: 47]: .: c × h × w c × 1 × 1 c × 1 × 1 F A l c: We call the above methods local attention, in the sense that elements of the feature tensor (channels / spatial locations), are weighted independently, based on contextual information obtained by pooling or learned. By constrast, by global attention we refer to mechanisms that model interaction between elements of the feature tensor, for example between channels or between locations.: In image classification, non-local neural network (NL-Net): [52]: is maybe the first global attention mechanism, followed by similar studies: [4,: 59,: 34]: . It is global spatial attention, allowing interaction between any pair of spatial locations. Similarly, there are studies of global channel attention, allowing interaction between channels: [9,: 14]: . Global attention has focused mostly on image recognition and has been applied to either spatial or channel attention so far, not both. In image retrieval, SOLAR: [28]: is a direct application of the global spatial attention mechanism of: [52]: .: Table: 1: attempts to categorize related work on attention according to whether attention is local or global, spatial or channel, whether it is learned and whether it is applied to instance-level image retrieval. We observe that all methods limit to one or two forms of attention only. Of those studies that focus on image retrieval, many are not learned: [23,: 17,: 24]: , and of those that are, some are designed for local descriptors: [29,: 47]: .: By contrast, we provide a comprehensive study of all forms of attention, global and local, spatial and channel, to obtain a learned representation in the form of a tensor that can be used in any way. We spatially pool it into a global descriptor and we study the relative gain of different forms of attention in image retrieval.: feature map conv 1 × 1 conv 3 × 3 conv 5 × 5 conv 7 × 7 concat conv 1 × 1 attention map c × h × w 4c × h × w 1 × h × w c × h × w dilated conv F F A l s\n",
      "\n",
      "Global-local attention: We design a global-local attention module (GLAM), which is attached at the end of a backbone network. Figure: 1: illustrates its main components. We are given a c × h × w feature tensor F, where c is the number of channels, and h × w is the spatial resolution. Local attention collects context from the image and applies pooling to obtain a c × 1 × 1 local channel attention map A l c and a 1 × h × w local spatial attention map A l s . Global attention allows interaction between channels, resulting in a c × c global channel attention map A g c , and between spatial locations, resulting in a hw × hw global spatial attention map A g s . The feature maps produced by the two attention streams are combined with the original one by a learned fusion mechanism into the global-local attention feature map F gl before being spatially pooled into a global image descriptor.\n",
      "\n",
      "Local attention: We extract an 1D channel and a 2D spatial attention map to weigh the feature map in the corresponding dimensions.: Local channel attention Following ECA-Net: [51]: , this attention captures local channel information. As shown in Figure: 2: , we are given a c × h × w feature tensor F from our backbone. We first reduce it to a c × 1 × 1 tensor by global average pooling (GAP). Channel attention is then captured by a 1D convolution of kernel size k along the channel dimension, where k controls the extent of cross-channel interaction. This is followed by a sigmoid function, resulting in the c × 1 × 1 local channel attention map A l c . Local spatial attention Inspired by the inception module: [43]: and similar to: [25]: , this attention map captures local spatial information at different scales. As shown in Figure: 3: , given the same c × h × w feature tensor F from our backbone, we obtain a new tensor F with channels reduced to c , using a 1 × 1 convolution. We then extract local spatial contextual information using convolutional filters of kernel size 3 × 3, 5 × 5, and 7 × 7, which are efficiently implemented by 3 × 3 dilated convolutions: [7,: 57]: with dilation parameter 1, 2, and 3 respectively. The resulting features, along with one obtained by 1 × 1 convolution on F , are concatenated into a 4c × h × w tensor. Finally, we obtain the 1 × h × w local spatial attention map A l s by a 1 × 1 convolution that reduces the channel dimension to 1.: feature map GAP conv1d(k) conv1d(k) sigmoid sigmoid × × softmax attention feature map 1 × c 1 × c 1 × c Qc c × c hw × c Vc A g c c × h × w 1 × c 1 × c Kc F Gc: The middle column of Figure: 6: shows heat maps of local spatial attention, localizing target objects in images.: Local attention feature map We use the local channel attention map A l c to weigh F in the channel dimension: F l c := F A l c + F.: (1): We then use local spatial attention map A l s to weigh F l c in the spatial dimensions, resulting in the c × h × w local attention feature map: F l = F l c A l s + F l c .: (2): Here, A B denotes an element-wise multiplication of tensors A and B, with broadcasting when one tensor is smaller. We adopt the choice of applying channel followed by spatial attention from convolutional block attention module CBAM: [54]: . However, apart from computing A l s at different scales, both attention maps are obtained from the original tensor F rather than sequentially. In addition, both (1) and (2) include residual connections, while CBAM includes a single residual connection over both steps.\n",
      "\n",
      "Global attention: We extract two matrices capturing global pairwise channel and spatial interaction to weigh the feature map. Global channel attention We introduce a global channel attention mechanism that captures global channel interaction. This mechanism is based on the non-local neural network: [52]: , but with the idea of 1D convolution from ECA-Net: [51]: . As shown in Figure: 4: , we are given the c × h × w feature tensor F from our backbone. We apply GAP and squeeze spatial dimensions, followed by a 1D convolution of kernel size k and a sigmoid function, to obtain 1×c query Q c and key K c tensors. The value tensor V c is obtained by mere reshaping of F to hw×c, without GAP. Next, we form the outer product of K c and Q c , followed by softmax over channels to obtain a c × c global channel attention map: feature map conv 1 × 1 conv 1 × 1 conv 1 × 1 × × softmax conv 1 × 1 attention feature map c × hw Qs hw × hw c × h × w c × hw Vs c × h × w A g s c × h × w c × hw Kc F Gs: A g c = softmax(K c Q c ).: (3): Finally, this attention map is multiplied with V c and the matrix product V c A g c is reshaped back to c × h × w to give the global channel attention feature map G c . In GSoP: [14]: and A 2 -Net: [9]: , a c × c global channel attention map is obtained by multiplication of hw × c matrices; (3) is more efficient, using only an outer product of 1 × c vectors.: Global spatial attention Since ordinary convolution applies only a local neighborhood at a time, it cannot capture global contextual information. Thus, we apply non-local filtering: [52]: , which is a form of self-attention: [49]: in the spatial dimensions. As shown in Figure: 5: , we are given the same c × h × w feature tensor F from our backbone. By using three 1 × 1 convolutions, which reduce channels to c , and flattening spatial dimensions to hw, we obtain c × hw query Q s , key K s , and value V s tensors, where each column is a feature vector corresponding to a particular spatial location. We capture pairwise similarities of these vectors by matrix multiplication of K s and Q s , followed by softmax over locations to obtain a hw × hw global spatial attention map:: A g s = softmax(K s Q s ).: (4): This attention map is multiplied with V s and the matrix product V s A g s is reshaped back to c × h × w by expanding the spatial dimensions. Finally, using a 1 × 1 convolution, which increases channels back to c, we obtain the c × h × w global spatial attention feature map G s .: The right column of Figure: 6: shows heat maps for global spatial attention, localizing target objects in images.: Global attention feature map We use the global channel attention feature map F c to weigh F element-wise: F g c = F G c .: (5): We then use global spatial attention feature map G s to weigh F g c element-wise, resulting in the c × h × w global attention feature map: F g = F g c G s + F g c . (: 6: ): Similarly to F l in (: 1: ) and (: 2: ), we apply channel attention first, followed by spatial attention. However, unlike (1), there is no residual connection in (: 5: ). This choice is supported by early experiments.\n",
      "\n",
      "Global-local attention: Feature fusion As shown in Figure: 1: , we combine the local and global attention feature maps, F l and F g , with the original feature F. While concatenation and summation are common operations for feature combination, we use a weighted average with weights w l , w g , w respectively, obtained by softmax over three learnable scalar parameters, to obtain a c × h × w global-local attention feature map: F gl = w l F l + w g F l + wF.: (7): EfficientDet: [44]: has shown that this is the most effective, among a number of choices, for fusion of features across different scales.: Pooling We apply GeM: [37]: , a learnable spatial pooling mechanism, to feature map F gl (7), followed by a fullyconnected (FC) layer with dropout and batch normalization. The final embedding is obtained by 2 -normalization.\n",
      "\n",
      "Experiments\n",
      "\n",
      "Datasets: Training set There are a number of open landmark datasets commonly used for training in image retrieval studies, including neural code (NC): [3]: , neural code clean (NCclean): [16]: , as well as Google Landmarks v1 (GLDv1): [29]: and v2 (GLDv2): [53]: . Table: 2: shows relevant statistics. These datasets can be categorized into noisy and clean. The clean sets were obtained from the original noisy sets for more effective training: [16,: 53]: . The original noisy datasets are much larger, but they have high intra-class variability.  Evaluation set and metrics We use four common evaluation datasets for landmark image retrieval: Oxford5k (Ox5k): [32]: , Paris6k (Par6k): [33]: , as well as Revisited Oxford (ROxford or ROxf) and Paris (RParis or RPar): [35]: .: ROxford and RParis are used with and without one million distractors (R1M): [28]: and evaluated using the Medium and Hard protocols: [35]: . We evaluate using mean Average Precision (mAP) and mean precision at 10 (mP@10).\n",
      "\n",
      "Implementation details: We train on 8 TITAN RTX 2080Ti GPUs. All models are pre-trained on ImageNet: [39]: and implemented in PyTorch: [31]: . For fair comparisons, we set a training environment similar to the those of compared studies: [56,: 53,: 28,: 35]: . We employ ResNet101: [18]: as a backbone model. The kernel size k of ECANet in subsection 3.1 is set to 3. The parameter p of GeM in subsection 3.3 is set to 3 and the dimension d of final embeddings to 512. We adopt ArcFace: [10]: , a cosine-softmax based loss, with a margin of 0.3. We use stochastic gradient descent with initial learning rate 10 -3 , momentum 0.9 and weight decay 10 -5 .: We adopt the batch sampling of Yokoo et al.: [56]: where mini-batch samples with similar aspect ratios are resized to a particular size. Here, we use a batch size of 64. For image augmentation, we apply scaling, random cropping, and varied illumination. At inference, we apply a multi-resolution representation: [16]: to query and database images.: Our method is denoted as GLAM (global-local attention module). Using the backbone model alone is referred to as baseline. It is compatible with recent models based on ResNet101-GeM trained with ArcFace: [53,: 28]: . Adding our local attention (subsection 3.1) to the baseline model is denoted +local, while adding our global attention (subsection 3.2) is denoted +global. Since we focus on representation learning, we do not consider post-processing methods like geometry-based re-ranking: [29,: 40,: 53]: or graph-based re-ranking: [11,: 21,: 55]: [53,: 28]: . All models use ResNet101-GeM. Red: best results. Blue: GLAM higher than SOLAR: [28]: on GLDv1-noisy.: GLDv2-noisy has 2.6 times more images than GLDv2clean, the latter is superior by a large margin. This shows that, in training, a cleaner dataset can be more important than a larger one. By contrast, NC-clean has the worst performance despite being clean, aparently because it is: [53]: is the only model other than ours trained on GLDv2-clean, while: [28]: is trained on GLDv1-noisy and compared in Table: 3: .: too small. To achieve best possible performance, we use GLDv2-clean as a training set in the remaining experiments.\n",
      "\n",
      "Comparisons on same training set: It is common to compare methods regardless of training sets as more become available, e.g.,: [35,: 28]: . Since GLDv2-clean is relatively new, Weyand et al.: [53]: , which introduced the dataset, is the only study that has trained the same backbone with the same settings (ResNet101-GeM with ArcFace) on GLDv2-clean.: Our baseline is lower than: [53]: , because our dimensinality is 512, while other models based on ResNet101 use 2048. Yet, Table: 3: shows that our best model trained on GLDv2-clean outperforms: [53]: by a large margin. But the most important comparison is with SOLAR: [28]: , also based on selfattention, which has trained ResNet101-GeM on GLDv1noisy. On this training set, our best model clearly outperforms: [28]: despite lower dimensionality. With this model, we outperform previous best methods on most benchmarks except mP@10 on RParis (medium) and RParis+R1M (medium), where we are outperformed by: [37,: 35]: . These results demonstrate that our approach is effective for landmark image retrieval. Figure: 7: shows some\n",
      "\n",
      "Comparison with state of the art\n",
      "\n",
      "Ablation study: Our ablation study uses the Google Landmark v2 clean dataset (GLDv2-clean): [53]: for training, which is shown to be the most effective in Table: 3: Table: 9: : mAP comparison of using multiresolution representation (Multi) or not (Single) on query or database.\n",
      "\n",
      "Effect of attention modules: We ablate the effect of our local and global attention networks as well as their combination. Table: 5: shows the results, which are more finegrained than those of Table: 4: . In particular, it shows the effect of the channel and spatial variants of both local and global attention. We observe that, when used alone, the channel and spatial variants of local attention are harmful in most cases. Even the combination, baseline+local, is not always effective. By contrast, when used alone, the channel and spatial variants of global attention are mostly beneficial, especially the latter. Their combination, baseline+global, is impressive, bringing gain of up to 7.5%. Importantly, the combination baseline+global+local improves further by up to another 2.8%. This result shows the necessity of local attention in the final model.\n",
      "\n",
      "CBAM vs. our local spatial attention: We experiment with the local spatial attention of CBAM: [54]: . CBAM applies average and max-pooling to input features and concatenates the two for spatial attention. We apply this variant to our local spatial attention module for comparison.: For the CBAM style module, we keep the overall design of our module as shown in Figure: 3: , but apply average and max-pooling to each of the four convolutional layer outputs before concatenation. Table: 6: shows that the CBAM style module is considerably worse than ours on all benchmarks except Paris6k, where it is only slightly better.: Concatenation vs. sum for feature fusion We use a softmax-based weighted average of local and global attention feature maps with the original feature map: (7): . Here, we compare this weighted average with weighted concatenation, where concatenation replaces the sum operation in: (7): . As shown in Table: 7: , the weighted average outperforms the weighted concatenation.: Fixed-size vs. group-size sampling Numerous studies have proposed methods for constructing batches according to image size for efficient training. For instance, Gordo et al.: [16]: , DELF: [29]: , and Yokoo et al.: [56]: employed different image sizes per batch for training instead of a single fixed size. We adopt the method of Yokoo et al., which constructs a batch with images of similar aspect ratio, so that the images can be resized to a size with an aspect ratio that is similar to their own. We call this method group-size sampling. Table: 8: compares fixed-size (224 × 224) with groupsize sampling. We observe that maintaining aspect ratios by using dynamic input sizes is much more effective.\n",
      "\n",
      "Multi-resolution: We use the multi-resolution representation: [16]: for the final feature of an image at inference time. This method: (1) resizes an image into multiple scales; (2) extracts features from the resized images; and (3) averages the features to obtain the final feature of the image. The method is applied to both query and database images to enhance ranking results, especially for small target objects. Table: 9: compares the four cases of applying this method or not to query or database images.\n",
      "\n",
      "Conclusion: We have introduced a novel approach that extracts global and local contextual information using attention mechanisms for instance-level image retrieval. It is manifested as a network architecture consisting of global and local attention components, each operating on both spatial and channel dimensions. This constitutes a comprehensive study and empirical evaluation of all four forms of attention that have previously been studied only in isolation. Our findings indicate that the gain (or loss) brought by one form of attention alone strongly depends on the presence of the others, with the maximum gain appearing when all forms are present. The output is a modified feature tensor that can be used in any way, for instance with local feature detection instead of spatial pooling for image retrieval.: With the advent of vision transformers: [12,: 58]: and their recent application to image retrieval: [13]: , attention is expected to play a more and more significant role in vision. According to our classification, transformers perform global spatial attention alone. It is of great interest to investigate the role of the other forms of attention, where our approach may yield a basic building block of such architectures. One may even envision an extension to language models, where transformers originate from: [50]: .\n",
      "===================\n",
      "This paper aims to tackle the challenging task of oneshot object counting. Given an image containing novel, previously unseen category objects, the goal of the task is to count all instances in the desired category with only one supporting bounding box example. To this end, we propose a counting model by which you only need to Look At One instance (LaoNet). First, a feature correlation module combines the Self-Attention and Correlative-Attention modules to learn both inner-relations and inter-relations. It enables the network to be robust to the inconsistency of rotations and sizes among different instances. Second, a Scale Aggregation mechanism is designed to help extract features with different scale information. Compared with existing few-shot counting methods, LaoNet achieves state-of-the-art results while learning with a high convergence speed. The code will be available soon.\n",
      "\n",
      "INTRODUCTION: Object counting has become increasingly important due to its wide range of applications such as crowd surveillance, traffic monitoring, wildlife conservation and inventory management. Most of the existing counting methods: [1,: 2,: 3]: focus on a particular, single category. However, when applying them into new categories, their performances will drop catastrophically. Meanwhile, it is extremely difficult and costly to collect all categories and label them for training.: For humans, the generalization ability allows them to learn and deal with various vision tasks without much prior knowledge and experience. We are amazed by this remarkable ability and in this work, we focus on this learning paradigm and design a network to efficiently recognize and count new categories given only one example. We follow the few-shot setting in: [4]: and modify it to one-shot object counting. That is, the model takes an image with unseen novel categories and a supporting bounding box containing an example instance of desired category as input, and then predicts the object count in the image.: However, there are two main challenges. First, the object counting task includes many different categories, and even several categories exist within a same image. Moreover in few-shot setting, these categories will not overlap between training and inference. This means that the model needs to have a strong distinguishing ability between features of different categories, and meanwhile, an effective associating ability among instances sharing the same category. Second, in one-shot counting, the model learns from only one supporting instance. Much of the difficulty results from the fact that the supporting sample may differ from other instances in, for example, sizes and poses. Hence, the model is required to be invariant towards these variations without seeing the commonalities across different instances.: Therefore, in this paper, we propose an effective network named LaoNet for one-shot object counting. It consists of three main parts: feature extraction, feature correlation and the density regressor, as shown in Figure: 1: . The feature correlation model and the feature extraction model are elaborately designed to address the above two challenges.: We propose the feature correlation based on Self-Attention and Correlative-Attention modules to learn innerrelations and inter-relations respectively. The Self-Attention encourages the model to focus more on important features and their correlations, improving the efficiency of information refinement. Previous few-shot counting methods: [4,: 5]: usually leverage on a convolution operation to match the similarities between image features and supporting features. However, as the kernel is derived from supporting features with the default size and rotation angle, the convolution operation will greatly depend on the quality of supporting features and the consistency of physical properties among different instances. Instead, our designed feature correlation model benefits from two kinds of attention modules and addresses the above problem by considering all correlations.: We further propose a Scale Aggregation mechanism in scale extraction to deal with scale variations among different categories and different instances. By learning features from multi-subspace, the model aggregates various scale information while maintaining a spatial consistency.: To summarize, our contribution is threefold.: • We design a novel network named LaoNet (A network by which you only need to Look At One instance) for one-shot object counting. By combining Self-Attention and Correlative-Attention modules, LaoNet exploits the correlation among novel category objects with high accuracy and efficiency.: • We propose a Scale Aggregation mechanism to extract more comprehensive features and fuse multi-scale information from the supporting box.: • The experimental results show that our model achieves state-of-the-art results with significant improvements on FSC-147: [4]: and COCO: [6]: datasets under the oneshot setting without fine-tuning.\n",
      "\n",
      "RELATED WORKS: Object counting methods can be briefly divided into two types. Detection based methods: [7]: count the number of objects by exhaustively detecting every target in images. But they rely on the complex labels such as bounding boxes. Regression based methods: [1,: 2]: learn to count by predicting a density map, in which each value represents the density of target objects at the corresponding location. The count prediction equals to the total sum of density map. Nevertheless, most of the counting methods are category specifically, e.g. for human crowd: [1,: 2,: 8,: 9,: 10,: 11]: , for cars: [3,: 12]: , for plants: [13]: or for cells: [14,: 15]: . They focus on only one category and will loss the original satisfied performance when transferring to other categories. Moreover, most traditional approaches usually rely on tens of thousands of instances to train a counting model: [2,: 8,: 9,: 11,: 3,: 12]: .: To reduce considerably the number of samples needed to train a counting model for a particular category, recently, fewshot counting task has been developed. The key lies in the generalization ability of the model to deal with novel categories from few labeled examples. The study: [16]: proposes a Generic Matching Network (GMN) for class-agnostic counting. However it still needs several dozens to hundreds examples of a novel category for adaptation and good performance. CFOCNet is introduced to match and utilize the similarity between objects within the same category: [5]: . The work: [4]: presents a Few Shot Adaptation and Matching Network (Fam-Net) to learn feature correlations and few-shot adaptation and also introduces a few-shot counting dataset named FSC-147.: When the number of labeled example decreases to one, the task evolves into one-shot counting. In other visual tasks, researchers develop methods for one-shot segmentation: [17]: and one-shot object detection: [18,: 19]: . Compared to the fewshot setting which usually uses at least three instances for each object: [4]: , the one-shot setting, where only one instance is available, is clearly more challenging.: It is worth mentioning that detection based approaches: [20,: 21,: 22]: are inferior for the tasks of few-shot and one-shot counting. One main reason is that it requires extra and costly bounding-box annotations of all instances in the training stage while one-shot counting approach which we focus on depends on dot annotations and only one supporting box. To illustrate this point further, we perform experiments in Section 4.3 to compare with detection based approaches and validate the proposed network for one-shot counting.\n",
      "\n",
      "APPROACH\n",
      "\n",
      "Problem Definition: One-shot object counting consists of a training set (I t , s t , y t ) ∈ T and a query set (I q , s q ) ∈ Q, in which categories are mutually exclusive. Each input for the model contains an image I and a supporting bounding box s annotating one object of the desired category. In training set, abundant point annotations y t are available to supervise the model. In inference stage, we aim the model to learn to count the novel objects in I q with a supporting category instance sampled by s q .\n",
      "\n",
      "Feature Correlation: As the model is required to learn to count from only one supporting object, seizing the correlation between features with high efficiency is quite important. Therefore, we build the feature correlation model in our one-shot network based on Self-Attention and Correlative-Attention modules, for learning the inner-relations and inter-relations respectively.: As illustrated in Figure: 1: (violet block), our Self-Attention module consists of a Multi-head Attention (MA) and a layer normalization (LN). We first introduce the definition of attention: [23]: , given the query Q, key K and value vector V :: A(Q, K, V | W ) = S( (QW Q )(KW K ) T √ d + P E)(V W V ),: (1): where S is the softmax function and 1 √ d is a scaling factor based on the vector dimension d. W : W Q , W K , W V ∈ R d×d are weight matrices for projections and P E is the position embedding.: To leverage on more representation subspaces, we adopt the extending form with multi attention heads:: M A(Q, K, V ) = Concat(head 1 , .., head h )W O where head i = A(Q, K, V | W i ).: (: ): 2: The representation dimensions are divided by parallel attention heads, where parameter matrices: W i : W Q i , W K i , W V i ∈ R d×d/h and W O ∈ R d×d .: One challenging problem in counting task is the existence of many complex interfering things. To efficiently weaken the negative influence by those irrelevant background, we apply Multi-head Self-Attention in image features to learn innerrelations and encourage the model to focus more on repetitive objects that can be counted.: We denote the feature sequences of the query image and the supporting box region as X and S, with sizes X ∈ R HW ×C and S ∈ R hw×C . And the refined query feature is calculated by:: X = LN (M A(X Q , X K , X V ) + X).: (3): A layer normalization (LN) is adopted to balance the value scales.: Meanwhile, as there is only one supporting object in oneshot counting problem, refining the salient features within the object is necessary and helpful for counting efficiency and accuracy. Therefore we apply another Self-Attention module to supporting feature and get refined S.: Previous few-shot counting methods: [4,: 5]: usually adopt a convolution operation where the supporting features act as kernels to match the similarities for target category. However, the results will greatly depend on the quality of supporting features and the consistency of objects' properties, including rotations and scales.: To this end, we propose a Correlative-Attention module to learn inter-relations between query and supporting features and alleviate the constraints of irrelevant properties.: Specifically, we extend the MA by learning correlations between different feature sequences and add a feed-forward network (FFN) to fuse the features, i.e.,: X * = Corr( X, S) = G(M A( XQ , SK , SV ) + X). (4): G includes two LNs and a FFN in the form of residual (light blue block in Figure: 1: ). Finally, X * and S will be fed into the cycle as new feature sequences where each cycle consists of two Self-Attention modules and a Correlative-Attention module.\n",
      "\n",
      "Feature Extraction and Scale Aggregation: To extract feature sequences from images, we use VGG-19 as our backbone. For query image, the output of the final level is directly flattened and transmitted into Self-Attention module. For the supporting box, as there are uncontrollable scale variations among instances due to the perspective, we propose a Scale Aggregation mechanism to fuse different scale information.: Given l as the number of layers in CNN, we aggregate the feature maps among different scales:: S = Concat(F l (s), F l-1 (s), ..., F l+1-δ (s)),: (5): where F i represents a feature map at i th level and δ ∈ [1, l] decides the number of layers taken for aggregation. Meanwhile, we leverage on identifying position embedding to help the model distinguish the integrated scale information in attention model. By adopting the fixed sinusoidal absolute position embedding: [23]: , feature sequences from different scales can still maintain the consistency between positions, i.e., P E (posj ,2i) = sin(pos j /10000 2i/d ), P E (posj ,2i+1) = cos(pos j /10000 2i/d ).: (: ): 6: i is the dimension and pos j is the position for j th feature map.\n",
      "\n",
      "Training Loss: We use Euclidean distance to measure the difference between estimated density map and ground truth density map, which is generated based on annotated points following: [1]: . The loss is defined as follows:: L E = ||D gt -D|| 2 2 , (: 7: ): where D is the estimated density map and D gt is the ground truth density map. To improve the local pattern consistency, we also adopt a SSIM loss followed the calculation in: [8]: . By integrating the above two loss functions, we have: L = L E + λL SSIM ,: (8): where λ is the balanced weight.\n",
      "\n",
      "EXPERIMENTS\n",
      "\n",
      "Implement Details and Evaluation Metrics: We design the density regressor by an upsampling layer and three convolution layers with ReLU activation. The kernel sizes of first two layers are 3 × 3 and that of last is 1 × 1. Random scaling and flipping are adopted for each training image. Adam: [24]: with a learning rate 0.5 × 10 -5 is used to optimize the parameters. We set the number of attention heads h as 4, the correlation cycle T as 2, the number of aggregated layers δ as 2, and the loss balanced parameter λ as 10 -4 . Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are used to measure the performance of our methods. They are defined by:: M AE = 1 M M i=1 N gt i -N i , RM SE = 1 M M i=1 (N gt i -N i ) 2 ),: (9): where M and N gt are the number of images and the groundtruth count, respectively. The predicted count N is calculated by integrating the estimated density map D. MS-COCO: [6]: is a large dataset widely used in object detection and instance segmentation. In val2017 set, there are 80 common object categories with 5,000 images in complex everyday scenes. We follow: [17]: to generate four train/test splits which each contains 60 training and 20 testing categories.\n",
      "\n",
      "Comparison with Few-Shot Approaches: We hold experiments on above two few-shot counting datasets to evaluate the proposed network. As there are few existing  methods specifically designed for one-shot counting, for comprehensive evaluation, we modify FamNet: [4]: and CFOC-Net: [5]: for this setting and also compare with other few-shot counting approaches: [25,: 26,: 16,: 27,: 17]: . First, quantitative results on FSC-147 are shown in Table 1. We list seven results of previous few-shot detection and counting methods in 3-shot setting and two results of stateof-the-art counting methods in 1-shot setting for comparison. The result of FamNet: [4]: uses the adaptation strategy during testing.: It is worth noticing that our one-shot LaoNet outperforms all of previous few-shot methods, even those in 3 shot set- Second, Table: 2: shows the results on each of four folds of COCO val2017. Methods with † in the upper part of the table follow the experiment setting in: [5]: . That is, the supporting examples are chosen from all instances in the dataset during training and testing, which is laborious and costly under the need of all instances annotated by bounding boxes. While our setting allows only one fixed instance for each image, we reconduct the experiment of CFOCNet: [5]: . As the result shows, our method maintains a great performance on COCO dataset.\n",
      "\n",
      "Discussions: Contribution of Different Terms. We study the accuracy contributions of different terms in FSC-147. The result is shown in Table: 3: , each row whereof reports the results after removing one component or one term from LaoNet. The Self-Attention modules for the two feature sequences to learn inner-relations increase the accuracy in testing set by 19.9% and 15.7% for MAE, 9.5% and 13.1% for RMSE, respectively. Compared to other two terms, the Self-Attention modules contribute most to the performance of our model.: The Scale Aggregation mechanism helps more on RMSE. The result demonstrates a robustness contribution under the multi-scale aggregation. Finally, the SSIM loss further improves the counting accuracy by both lower MAE and RMSE. Convergence Speed. We hold experiments to measure the convergence speed and the performance stability. We pick FamNet: [4]: as the baseline for LaoNet with a pre-trained CNN backbone and an Adam optimizer. We train both two models on FSC-147 and report the validation MAE for 100 epochs.: As shown in Figure: 3: , our model has faster convergence speed and better stability than FamNet. With just 2 epoches, our method achieves a low counting error which FamNet has to reach after 40 epochs. Meanwhile, the convergence of our method is smooth and stable, while that of Famet is jagged, with multiple sharp peaks and the highest error of 70. Comparison with Object Detectors. Object detectors can be used for counting task with the number of predicted detections. However, even these detectors work with categories which they are trained on instead of one-shot setting, their counting performances are still limited. We select images of FSC-147-COCO subset from FSC147 Val and Test sets which share categories with MS-COCO dataset and conduct quantitative experiments.: As the results shown in Table: 4: , we compare LaoNet with several object detectors which are well pre-trained with thou-\n",
      "\n",
      "CONCLUSION: This paper targets one-shot object counting, which requires the counting model to count objects of new categories by looking at only one instance. We propose an efficient network named LaoNet to address this challenge. LaoNet includes a feature correlation module to learn both inner-relations and inter-relations and a scale aggregation module to extract multi-scale information for improving robustness. Without any fine-tuning in inference, our LaoNet outperforms previous state-of-the-art few-shot counting methods with a high convergence speed. In the future, we consider applying our model to a wider range of one-shot vision tasks.\n",
      "===================\n",
      "Attention mechanisms are a central property of cognitive systems allowing them to selectively deploy cognitive resources in a flexible manner. Attention has been long studied in the neurosciences and there are numerous phenomenological models that try to capture its core properties. Recently attentional mechanisms have become a dominating architectural choice of machine learning and are the central innovation of Transformers. The dominant intuition and formalism underlying their development has drawn on ideas of keys and queries in database management systems. In this work, we propose an alternative Bayesian foundation for attentional mechanisms and show how this unifies different attentional architectures in machine learning. This formulation allows to to identify commonality across different attention ML architectures as well as suggest a bridge to those developed in neuroscience. We hope this work will guide more sophisticated intuitions into the key properties of attention architectures as well suggest new ones.\n",
      "\n",
      "Introduction: Designing neural network architectures with favourable inductive biases lies behind many recent successes in Deep Learning: (Baxter, 2000): . In particular, the attention mechanism has allowed language models to achieve human like generation abilities previously thought impossible: (Vaswani et al., 2017): . The success of the attention mechanism as a domain agnostic architecture has prompted it to be adopted across a huge range of tasks and domains notably reaching state-of-the-art performance in visual reasoning and segmentation tasks: (Dosovitskiy et al., 2021;: Wang et al., 2022): . Despite it's success, the role of the attention mechanism remains poorly understood. Indeed, it is unclear to what extent it relates to theories of cognitive attention which inspired it: (Lindsay, 2020): . Here, we aim to provide a parsimonious description grounded in principles of probabilistic inference. This Bayesian perspective provides both a principled method for specifying prior beliefs and reasoning explicitly about the role of the attention variables. Further, understanding the fundamental computation permits us a unified description of different attention mechanisms in the literature. This proceeds in two parts.: First, we show that 'soft' attention mechanisms (e.g. selfattention, cross-attention, graph attention, which we call transformer attention herafter) can be understood probabilistically as taking an expectation over possible connectivity structures, providing an interesting link between softmax-based attention and marginal likelihood.: Second, we extend the uncertainty over connectivity to a bayesian setting which, in turn, provides a theoretical grounding for iterative attention mechanisms (slot-attention, perciever and block-slot attention): (Locatello et al., 2020;: Singh et al., 2022;: Jaegle et al., 2021): and Modern Continuous Hopfield Networks: (Ramsauer et al., 2021): .: Additionally, we apply iterative attention to Predictive Coding Networks, an influential theory in computational neuroscience, creating a new theoretical bridge between machine learning and cognitive science.: Attention(Q, K, V ) = p(E | Q, K) sof tmax( QW Q W T K K T √ d k ) V = E p(E|Q,K) [V ]: A key observation is that the attention matrix can be seen as the posterior distribution over an adjacency structure, E, and the full mechanism as computing an expectation of the value function V (X) over the posterior beliefs about the possible relationships that exist between key and query.: This formalism provides an alternate Bayesian theoretical framing within which to understand attention models, which contrasts with the original framing in terms of database management systems and data retrieval, providing a unifying framework to describe different attention architectures. Describing their difference only in terms of their edge relationships supporting more effective analysis and development of new architectures. Additionally providing a principled understanding of the difference between hard and soft attention models.\n",
      "\n",
      "Contributions: • A unifying probabilistic framework for understanding attention mechanisms.: • We show self-attention and cross-attention can be seen as computing a marginal likelihood over possible network structures.: • We show that slot-attention, block-slot-attention and modern continuous hopfield networks can all be seen as collapsed variational inference, where the possible network structures form the collapsed variables.: • Provide a bridge to Bayesian conceptions of attention from computational neuroscience, through the lens of Predictive Coding Networks.: • Provide a framework for reasoning about hard attention, and efficient approximations to the attention mechanism.\n",
      "\n",
      "Related Work: Attention as bi-level optimisation Mapping feed-forward architecture to a minimisation step on a related energy function has been called unfolded optimisation: (Frecon et al., 2022): . Taking this perspective can lead to insights about the inductive biases involved for each architecture. It has been shown that the cross-attention mechanism can be viewed as an optimisation step on the energy function of a form of Hopfield Network: (Ramsauer et al., 2021): , providing a link between attention and associative memory. Whilst: (Yang et al., 2022): extend this view to account for self-attention. Our framework distinguishes hopfield attention, which does not allow an arbritary value matrix, from the standard attention mechanisms. Whilst there remains a strong theoretical connection, it places the Hopfield Energy as an instance of variational free energy, aligning more closely with iterative attention mechanisms such as slotattention.: Relationship to gaussian mixture model Previous works that have taken a probabilistic perspective on the attention mechanism note the connection to inference in a gaussian mixture model: (Gabbur et al., 2021;: Nguyen et al., 2022;: Ding et al., 2020): . Indeed: (Annabi et al., 2022): directly show the connection between the Hopfield energy and the variational free energy of a gaussian mixture model. Although gaussian mixture models, a special case of the framework we present here, are enough to explain cross attention they do not capture slot or self-attention. Further our framework allows us to extend the structural inductive biases beyond what can be expressed in a gaussian mixture model and capture the relationship to hard attention.: Latent alignment and hard attention Several attempts have been made to combine the benefits of soft (differentiability) and hard attention. Most approaches proceed by sampling, e.g., using the REINFORCE estimator: (Deng et al., 2018): or a topK approximation: (Shankar et al., 2018): . The one most similar to ours embeds the full forward-backward algorithm within a forward pass: (Kim et al., 2017): , our approach differs by offering a parsimonious description in terms of marginalisation over an implicit graphical model.: Collapsed Inference Collapsed variational inference has most notably been employed in topic modelling: (Teh et al., 2006): . To our knowledge, linking collapsed inference to attention in deep learning is completely novel.\n",
      "\n",
      "Transformer Attention\n",
      "\n",
      "ATTENTION AS EXPECTATION: We begin by demonstrating transformer attention is best seen as an expectation over latent variables. In the case of self and cross-attention, the expectation of a neural network with respect to possible adjacency structures.: Let x = (x 1 , .., x n ) be observed variables, φ be some set of latent variables, and y a variable we need to predict. Given a latent variable model p(y, x, φ) = p(y | x, φ)p(x, φ), where p(y | x, φ) is parameterised by some function v(y, x, φ) e.g. a neural network.: Our goal is to find p(y | x), however φ are unobserved so we calculate the marginal likelihood.: p(y | x) = φ p(φ | x)v(y, x, φ): Importantly, the softmax function is a natural representation for the posterior: p(φ | x) = p(x, φ) φ p(x, φ) p(φ | x) = sof tmax(ln p(x, φ)): Hence, transformer attention can be seen as weighting v(x, φ) by the posterior distribution p(φ | x).: p(y | x) = φ sof tmax(ln p(x, φ))v(y, x, φ) = E p(φ|x) [v(y, x, φ)]: (1): We claim (: 1: ) is exactly the equation underlying self and cross-attention. To make a more direct connection, we present the specific generative models corresponding to them. The latent variables φ are identified as possible relationships, or edges, between each of the observed variables x (keys and queries).: A natural formalism for modelling these graphical relationships is Markov Random Fields.\n",
      "\n",
      "PAIRWISE MARKOV RANDOM FIELDS: Given a set of random variables X = (X v ) v∈V with probability distribution [p] and a graph G = (V, E). The variables form a pairwise Markov random field (MRF) with respect to G if the joint density function P (X = x) = p(x) factorises as follows: p(x) = 1 Z exp v∈V ψ v + e∈E ψ e: where Z is the partition function ψ v (x v ) and ψ e = ψ u,v (x u , x v ) are known as the node and edge potentials respectively 1 .: Beyond the typical set-up, we add a structural prior p(E) over the adjacency structure of the underlying graph.: p(x, E) = P (x | E)P (E) = 1 Z p(E) exp v∈V ψ v + e∈E ψ e: We briefly remark that (1) respects factorisation of [p] in the following sense; if the distribution admits a factorisation with respect to the latent variables p(x, φ) = i f i (x, φ i ) and v(x, φ) = i v i (x, φ i ) then (applying the linearity of expectation) we may write: E p(φ|x) [v(x, φ)] = i E p(φi|x) [v i ]: (2): Permitting each factor to be marginalised independently.: In the case of an MRF, such a factorisation is natural. If the distibution over edges factorises into local distributions: p(E) = i p(E i ) (using independence properties of the MRF) we can write p(x, E) = 1 Z i f i (x, E i ) where each f i = P (E i ) exp v∈V ψ v e∈Ei ψ e is itself an unnor- malised MRF.: To recover cross-attention and self-attention are such models with we need only specify a structural prior and potential functions.\n",
      "\n",
      "CROSS ATTENTION: • Key nodes K = (x 1 , .., x n ): 1 See: (Shah et al., 2021): for a precise definition. • Query nodes Q: = (x ′ 1 , ..., x ′ m ) • Structural prior p(E) = m i=1 p(E i ), where E i ∼ U nif orm{(x 1 , x ′ i ), .., (x n , x ′ i )}: , such that each query node is uniformly likely to connect to each key node.: • Edge potentials ψ(x j , x ′ i ) = x ′T i W T Q W K x j: , in effect measuring the similarity of x j and x ′ i under a certain transformation.: • Value function V i (K, Q, E i ) = W V x s(Ei): , a linear transformation applied to the node, x s(Ei) , the start of the edge E i .: Taking the posterior expectation in each of the factors defined in two (2) gives the standard cross-attention mechanism: E p(Ei|Q,K) [V i ] = j sof tmax j (x ′T i W T Q W K x j )W V x j E p(E|Q,K) [V ] = sof tmax(Q T W T Q Q K K)W V K 3.0.4. SELF ATTENTION • Nodes K = Q = (x 1 , .., x n ) • Structural prior p(E) = n i=1 p(E → i ): , where E → i ∼ U nif orm{(x 1 , x i ), .., (x n , x i )}, such that each node is uniformly likely to connect to every other node.: • Edge potentials ψ(k j , k i ) = x T i W T Q W K x j , in effect measuring the similarity of x j and x ′ i under a certain transformation.\n",
      "\n",
      "• Value function: V i (K, Q, E i ) = W V x s(Ei): , a linear transformation applied to the node, x s(Ei) , the start of the edge E i .: Again, taking the posterior expectation in each of the factors defined in two (2) gives the standard self-attention mechanism: E p(Ei|Q,K) [V i ] = j sof tmax j (x T i W T Q W K x j )W V x j E p(E|Q,K) [V ] = sof tmax(K T W T Q W K K)W V K\n",
      "\n",
      "Iterative Attention: We continue by extending attention to full Bayesian inference. In essence applying the attention trick, marginalisation of attention variables, to the variational free energy (a.k.a the ELBO).: Modern Continuous Hopfield Networks can be seen as a particular instance of this class of system, allowing us to reproduce the 'hopfield attention' updates of: (Ramsauer et al., 2021): within a probabilistic context. Under different structural priors we recover other iterative attention models; slot-attention: (Locatello et al., 2020): , block-slot attention: (Singh et al., 2022): and Perciever: (Jaegle et al., 2021): . Further, we showcase a specific advantage of bayesian attention, hard attention.\n",
      "\n",
      "COLLAPSED INFERENCE: We present a version of collapsed variational inference: (Teh et al., 2006): showing how this results in a bayesian attention mechanism. The term attention mechanism is apt due to the surprising similarity in form between the variational updates (6) and neural attention mechanism (1).: Our setting is the latent variable model p(x, z, φ), where x are observed variables, and z, φ, are latent variables. Typically we wish to infer z given x.: Collapsed inference proceeds by marginalising out the extraneous latent variables φ p(x, z): = φ p(x, z, φ): (3): We define a recognition density q(z) ∼ N (z; µ) and optimise the variational free energy with respect to the parameters, µ, of this distribution.: min µ F (x, µ) = E q [ln q µ (z) -ln p(x, z)]: Under a typical Laplace approximation, we can write the variational free energy as F ≈ -ln p(x, µ): 2: . Substituting in (3) and taking the derivative with respect to the variational parameters yields,: F (x, µ) = -ln φ p(x, µ, φ) ∂F ∂µ = - 1 φ p(x, µ, φ) φ ∂ ∂µ p(x, µ, φ): (4): Which connects bayesian attention with the standard attention (1). To clarify this, we employ the log-derivative trick, substituting p θ = e ln p θ and re-express (4) in two ways:: ∂F ∂µ = - φ sof tmax φ (ln p(x, µ, φ)) ∂ ∂µ ln p(x, µ, φ): (5): ∂F ∂µ = E p(φ|x,µ) [- ∂ ∂µ ln p(x, µ, φ)]: (6): The first form reveals the softmax which is ubiquitous in all attention models. The second, suggests the variational update should be evaluated as the expectation of the typical variational gradient (the term within the square brackets) with respect to the posterior over the parameters represented by the random variable φ.: In other words, bayesian attention is exactly the nueral attention mechanism applied iteratively, where the value function is the variational free energy gradient. We derive updates for a general MRF before again recovering (iterative) attention models in the literature by specifying particular distributions.\n",
      "\n",
      "FREE ENERGY OF A MARGINALISED MRF: Recall the factorised MRF, p(E): = i p(E i ). p(x, E) = 1 Z i f i (x, E i ) with each f i = P (E i ) exp v∈V ψ v e∈Ei ψ e .: Independence properties mean the marginalisation necessary for collapsed inference can be simplified: E p(x, E) = 1 Z i Ei f i (x, E i ): In an inference setting the nodes are partitioned into observed nodes, x, and latent nodes, z. The variational free energy (4) and the associated forms of it's derivative can be expressed: F (x, µ, θ) = - i ln Ei f i (x, µ, E i ) ∂F ∂µ j = - i Ei sof tmax(f i (x, µ, E i )) ∂f i ∂µ j: Similar to hard attention approaches, the random variable E is an explicit alignment variable. However, unlike hard attention, we avoid inferring E explicitly using the collapsed inference approach outlined above.\n",
      "\n",
      "QUADRATIC POTENTIALS AND THE CONVEX CONCAVE PROCEDURE: We follow: (Ramsauer et al., 2021): in using the CCCP to derive a fixed point equation, which necessarily reduces the free energy.: Assuming the node potentials are quadratic ψ(x i ) = -1 2 x 2 i and the edge potentials have the form ψ(x i , x j ) = x i W x j .: µ * j = i Ei sof tmax(g i (x, µ, E i )) ∂g i ∂µ j (7): Where: g i = e∈Ei ψ e .: By way of the CCCP: (Yuille & Rangarajan, 2001): , this fixed point equation has the property F (x, µ * j , θ) ≤ F (x, µ j , θ) with equality if and only if µ * j is a stationary point of F .: We follow the 3 in specifying specific structural priors and potential functions to recover different iterative attention mechanisms.\n",
      "\n",
      "HOPFIELD-STYLE CROSS ATTENTION: Let the observed x = (x 1 , .., x n ) and latent nodes z = (z 1 , .., z m ) have the following structural prior p(E) = m i=1 p(E i ), where E i ∼ U nif orm{(x 1 , z i ), .., (x n , z i )}. And define edge potentials ψ(x j , z i ) = z i Q T Kx j , Application of (: 7: ): µ * i = j sof tmax j (µ i W T Q W K x j )W T Q W K x j: When µ i is initialised to some query ξ the system: (Ramsauer et al., 2021): the fixed point update is given by: µ * i (ξ) = E p(Ei|x,ξ) [W T Q W K x t(Ei) ]. When the patterns x are well separated, µ * i (ξ) ≈ W T Q W K x j: , where W T Q W K x j is the closest vector and hence can be used as an associative memory. 4.0.5. SLOT ATTENTION Slot attention: (Locatello et al., 2020): is an object centric learning module built on top of an iterative attention mechanism. Here we show this is a simple adjustment of the prior beliefs on our edge set.: With the same set of nodes and potentials, replace the prior over edges with p(E) = n j=1 p(E j ), E j ∼ U nif orm{(x j , z 1 ), .., (x j , z m )}: µ * i = j sof tmax i (µ i Q T Kx j )Q T Kx j: Whilst the original slot attention employed an RNN to aid the basic update shown here, the important feature is that the softmax is taken over the 'slots', µ. This forces competition between slots to account for the observed variables, forcing object centric representations. For example, if the observed variables x are image patches, the slots are forced to cluster similar patches together in order increase the overall likelihood of said patches. The word cluster is accurate, in fact there is an exact equivalence between this mechanism and a step of EM on a gaussian mixture model. 4.0.6. BLOCK SLOT ATTENTION: (Singh et al., 2022): suggest combining an associative memory ability with an object-centric slot-like ability and provide an iterative scheme for doing so, alternating between slot-attention and hopfield updates.  ∼ U nif orm{(x j , z 1 ), .., (x j , z m )}, Ẽk ∼ U nif orm{(z 1 , m k ), .., (z m , m k )}, with edge potentials between X and Z given by ψ(x j , z i ) = z i Q T Kx j and between Z and M , ψ(z i , m k ) = z i • m k applying (7) gives: µ * i = j sof tmax i (µ i Q T Kx j )Q T Kx j + k sof tmax k (µ i • m k )m k: In the original block-slot attention each slot z i is broken into blocks, where each block can access block-specific memories i.e. z k } k≤l . Allowing objects to be represented by slots which in turn disentangle features of each object in different blocks. We presented a single block version above, however it is easy to see that the update extends to the multiple block version applying (7) gives: µ * i = j sof tmax i (µ i Q T Kx j )Q T Kx j + k,b sof tmax k (µ (b) i • m (b) k )m (b) k\n",
      "\n",
      "Predictive Coding Networks: Predictive Coding Networks (PCN) have emerged as an influential theory in computational neuroscience: (Rao & Ballard, 1999;: Friston & Kiebel, 2009;: Buckley et al., 2017): . Building on theories of perception as inference and the Bayesian brain, PCNs perform approximate Bayesian inference by minimising the variational free energy which is manifested in the minimisation of local prediction errors. The continuous time dynamics at an individual neuron are given by: ∂F ∂µ i = - φ - k φ ǫ φ + φ + k φ ǫ φ w φ: Where ǫ are prediction errors, w represent synaptic strength and k are node specific precisions representing uncertainty in the generative model: (Millidge et al., 2022): .: A natural extension is to apply collapsed inference over the set of incoming and out going connection, i.e. a locally factorised prior over possible connectivity. In the notation of the previous section, we have an MRF with a hierarchical structure Z = {Z (0) , ..., Z (l) , ..., Z (N ) } where the prior on edges factorises into layerwise p(E: (l) ) = {(z i , z j ) : (z i , z j ) ∈ Z (l-1) × Z (l) } and potential func- tions φ(z i , z j ) = ǫ 2 i,j = k j (z j -w i,j z i ) 2 . ∂F ∂µ i = - φ - sof tmax(-ǫ φ 2 )k φ ǫ φ + φ + sof tmax(-ǫ φ 2 )k φ ǫ φ w φ: The resulting dynamics induce a \"normalisation\" across prediction errors received by a neuron through the softmax function. This dovetails nicely with theories of attention as normalisation in psychology and neuroscience. In contrast previous predictive coding based theories of attention have focused on the precision terms, k, due to their ability to up and down regulate the impact of prediction errors: (Feldman & Friston, 2010: ). Here we see the softmax term can also perform this regulation, while also exhibiting the fast winner-takes-all dynamics that are associated with cognitive attention.\n",
      "\n",
      "Discussion: In this section we will briefly discuss what can be gained from looking at the attention mechanism as a problem of inference.\n",
      "\n",
      "HARD ATTENTION: Recall (1) neural attention may be viewed as calculating an expectation over latent variables E p(φ|x): [v(x, φ): ]. Here the mechanism is 'soft' because we weight multiple possibilities of attention variable φ. Hard attention, on the other hand, proceeds with a single sample from p(φ | x). It has been argued this is more biological, more interpretable and has lower computational complexity. Previously the inferior performance of hard-attention has been attributed to it's hard to train, stochastic nature. However, our framing of soft attention as exact marginalisation offers an alternate explanation. Stochastic approximations (hard attention) will always suffer compared with exact marginalisation (soft attention). Further our framework provides a method for seamlessly interchanging hard and soft-attention. Since the distribution p(φ | x) a the categorical distribution, at any point (during training or inference) it is possible to implement hard attention by taking a single sample φ * from p(φ | x) yielding v(x, φ * ).: There are two issues with this approach to collapsing the attention distribution. First, the single sample will collapse any uncertainty, secondly calculation of p(φ | x), in order to sample, still incurs a quadratic penalty O(n 2 ). However we can employ tools from probability theory to help us analyse the cost of sampling, and linear approximations to the attention distribution.\n",
      "\n",
      "EFFICIENT TRANSFORMERS: Consider some distribution q attempting to approximate p(φ | x) we can quantify the information loss with the relative entropy L[p, q] D KL [q(φ) || p(φ | x)] = H[q] + E q [p(φ | x)]: In the hard attention approximation a single sample from p is used as an approximation L[p, q] = -ln p(φ * | x) and perhaps intuitively E[L] = H[p] i.e. hard attention is a good approximation when the attention distribution is low-entropy which can be controlled by the temperature parameter (Appendix ??).: Many of the efficient alternatives to attention, such as lowrank and linear approximations, can be cast as approximating p(φ | x) with q(φ | x) where calculating q is less expensive than exact marginalisation. Estimating L could be used to quantify the relative information loss when using these alternatives. Another direction taken to reduce computational complexity of the attention mechanism is sparsification the attention matrix, which in our framework reduces to adjustments to the prior over edges (Appendix ??).\n",
      "\n",
      "NEW DESIGNS: The main difference between the description presented and previous probabilistic descriptions is to view soft attention as a principled, exact, probabilistic calculation, with respect to an implicit probabilistic model, as opposed to an impoverished approximation. This leads to possibility of designing new attention mechanisms by altering the distribution that the mechanism marginalises over, either by adjusting the structural prior, or the potential functions. We hope this will enable new architectures to be designed in a principled manner.\n",
      "===================\n",
      "Timeseries analytics is of great importance in many real-world applications. Recently, the Transformer model, popular in natural language processing, has been leveraged to learn high quality feature embeddings from timeseries, core to the performance of various timeseries analytics tasks. However, the quadratic time and space complexities limit Transformers' scalability, especially for long timeseries. To address these issues, we develop a timeseries analytics tool, RITA, which uses a novel attention mechanism, named group attention, to address this scalability issue. Group attention dynamically clusters the objects based on their similarity into a small number of groups and approximately computes the attention at the coarse group granularity. It thus significantly reduces the time and space complexity, yet provides a theoretical guarantee on the quality of the computed attention. The dynamic scheduler of RITA continuously adapts the number of groups and the batch size in the training process, ensuring group attention always uses the fewest groups needed to meet the approximation quality requirement. Extensive experiments on various timeseries datasets and analytics tasks demonstrate that RITA outperforms the state-of-the-art in accuracy and is significantly faster -with speedups of up to 63X.\n",
      "\n",
      "INTRODUCTION: Motivation. Many data driven applications involve processing massive timeseries data, including IoT: [11]: , medical AI: [14]: , stock market: [27]: , and so on. As such, there is a great need for timeseries analytics, such as forecasting: [8]: , classification: [20]: , clustering: [31]: , similarity search: [39]: , and anomaly detection: [50]: , with applications ranging from automatically diagnosing diseases: [5]: , recognizing human activities: [29]: , to stopping financial fraud: [59]: .: Effective feature extraction: [40]: lies at the core of almost all these timeseries analytics tasks. Recently researchers: [61]: have started leveraging the self-supervised pre-training methodology of Transformers: [4,: 16,: 52]: , which have proven remarkably successful in natural language processing (NLP), to automatically learn high quality feature embeddings from timeseries. In NLP, self-supervised pre-training exploits the sequential patterns (correlations) among the words in sentences to produce contextualized feature embeddings. Timeseries bear similarity to natural language, because in timeseries data the sequential order among the values (stock price, volume, etc.) over time matters. That is, each value is highly correlated with other values observed before or after it. Therefore, * Corresponding Author pre-training a Transformer model which takes the correlations among different observations into account is a natural idea to learn feature embeddings from timeseries. Indeed, the experiments in: [61]: confirm that Transformer-based methods outperform traditional timeseries analytics techniques.: However, existing work: [61]: that directly applies Transformers to learn features from timeseries data have been shown not to be scalable to long timeseries: [30]: . The idea of self-attention: [52]: is central to pre-training methods in NLP: It computes pairwise correlations among different semantic units in a sequence (in NLP, a sentence); as such, it has quadratic time and space complexity in the length of the input sequence. Such an approach places limits on the model's scalability, especially when handling large sequences, which are common in real-world timeseries applications such as IoT, medical AI, and finance: [6,: 34,: 62]: . Predictions about timeseries may need to look at months or years of historical data to make accurate predictions, spanning hundreds of thousands of samples. As an example, in collaboration with a research hospital we have been developing a seizure classifier that automatically detects seizures based on EEG signals (timeseries) collected during the clinical observation of patients. As seizures last only a few seconds, we chunk long EEG data into many 2 second segments and detect seizures at a segment level. However, the classification of a particular segment depends on up to 12 hours of prior signal to determine if one 2 second segment indicates seizure or not, because seizure diagnosis needs to consider long-term trends in the EEG data: [6]: . The number of segments in 12 hours is more than 21k. This is far larger than the number of semantic units the typical NLP tasks expect. For example, BERT: [16]: limits the number of units to 512 and even massive models like GPT-3: [4]: limit the number of units to 2048.: Although in NLP some lower-complexity methods have been proposed to approximately compute self-attention: [10,: 26,: 54]: , their performance degrades dramatically when used on timeseries, due to the gap between natural language and timeseries, as we will show in our experiments. Proposed Approach. To tackle the aforementioned problem, we develop RITA, a Transformer-based timeseries analytics tool, which uses a novel attention mechanism, called group attention, to scale to long timeseries.: Leveraging the periodicity of timeseries, RITA chunks the input timeseries into segments and dynamically clusters the segments into a small number (denoted as 𝑁 ) of groups. Segments in the same group possess similar feature embeddings during the current training iteration, thus enabling them to approximately share the computation of attention. As the timeseries increases in length, more sharing opportunities become available. RITA then computes the self-attention at a group level and produces a compressed group attention matrix. In this way, group attention eliminates both computation and memory bottlenecks in Transformer-style models and thus more scalable to long timeseries.: However, making this idea effective and efficient in Transformer architectures is challenging for several reasons:: • Efficiently Producing High Quality Feature Embeddings. Although RITA computes the attention matrix at a group level, to preserve the quality of the feature embeddings, it still has to produce different embeddings for different segments. This is because even if some segments share the attention score temporally, it does not mean they should have the same feature embedding. However, using the group attention matrix, the existing self-attention mechanism will only produce a single feature vector for each group. A naive solution would be to restore the original attention matrix from the group attention matrix. However, in this case we again get an attention matrix with quadratic space complexity. Because GPUs have limited memory, GPU memory will remain a bottleneck in group attention.: • The Number of Groups N. In RITA, the number of groups 𝑁 is a crucial factor that balances the speed up and the quality of attention approximation. A small 𝑁 will lead to a large speedup, but the approximation errors can also be significant. On the other hand, although a large 𝑁 tends to produce high-quality approximations, it inevitably slows down the training process. Therefore, an appropriate 𝑁 is essential to the performance of group attention. However, 𝑁 depends on the distributional properties of the dataset. Furthermore, like the classical transformer models, RITA stacks multiple attention layers to produce better embeddings. Ideally, different layers should also use different values of 𝑁 . In addition, during the model training phrase, group attention should use different values of 𝑁 at different iterations to adapt to the varying feature embeddings. This makes manually setting appropriate 𝑁 almost impossible.: • Batch Size. Moreover, as we want to dynamically adjust 𝑁 during training, a fixed batch size is sub-optimal: as 𝑁 decreases, the memory usage of a single sample decreases. This allows a larger batch size which is beneficial, because: (1) it makes full use of GPU memory; (2) high-parallelism across the samples in a big batch brings better performance. Our experimental study shows that doubling the batch size reduces the training time by 30%, while still preserving the quality of the model. Thus, RITA should dynamically adjust batch size as 𝑁 changes.: To address the above problems, we first propose an embedding aggregation strategy and a customized group softmax function to replace the classical softmax function: [52]: . Together they ensure RITA is able to directly use the compressed attention matrix to produce different feature embeddings for different segments. We theoretically show the embeddings RITA produces in this way are identical to those produced by first re-storing the original large attention matrix. Thus RITA is able to produce high quality embeddings without introducing extra overhead. Further, we design a GPU friendly algorithm to group the segments in parallel, effectively minimizing the grouping cost.   Second, we design an adaptive scheduler which dynamically decides an appropriate 𝑁 for each group attention layer during the training process. It starts with a large 𝑁 and iteratively merges groups that are similar to each other. Guided by an error bound on the approximated self-attention that users can tolerate, it automatically determines if two groups are mergeable, performing merging efficiently in a GPU-friendly way.\n",
      "\n",
      "RITA Encoder\n",
      "\n",
      "Scale & Input: Moreover, we propose a learning-based method to model the correlation between the number of groups 𝑁 and the batch size 𝐵. This model is used to predict 𝐵 for a given 𝑁 when training RITA. Specifically, we first sample some 𝑁 values in a reasonable range. For each sampled 𝑁 , we find a batch size that consumes up to a certain percentage of GPU memory in a cost-efficient way. Using a small set of mathematical functions as a prior, RITA learns a model with only a few <N, B> pairs as ground truth labels.: Our experiments on public timeseries benchmarks and the MGH EEG data: [6]: confirm that RITA outperforms state-of-the-art methods in accuracy on various timeseries analytics tasks, while our group attention mechanism achieves a 63X speedup with much less memory required, compared to existing self-attention mechanisms: [10,: 52,: 54]: . Contributions. The key contributions of this work include:: • Our group attention mechanism leverages the periodicity of timeseries, reducing the time and space complexity of the selfattention mechanism with accuracy guarantees, allowing RITA to scale to long timeseries data.: • Guided by an approximation error bound, our adaptive scheduler dynamically adapts the number of groups and the batch size to the distribution properties of the evolving feature embeddings, making group attention efficient and easily tunable.: • We conduct experiments on various datasets and different analytics tasks, demonstrating that RITA is 4 to 63 times faster than the state-of-the-art while achieving better accuracy when handling long timeseries (length ≥ 2000).\n",
      "\n",
      "BACKGROUND: We provide some background on the canonical self-attention module in the Transformer: [52]: . A self-attention module takes 𝑛 hidden embedding vectors 𝐻 ∈ R 𝑛 * 𝑑 ℎ as input, then projects them to queries (𝑄), keys (𝐾) and values (𝑉 ) and performs Scaled-dot Product Attention, which given input hidden state 𝐻 , is computed by:: 𝑄 = 𝐻𝑊 𝑄 , 𝐾 = 𝐻𝑊 𝐾 , 𝑉 = 𝐻𝑊 𝑉 𝑂 = 𝐴𝑉 = 𝑆𝑜 𝑓 𝑡𝑀𝑎𝑥 ( 𝑄𝐾 𝑇 √︁ 𝑑 𝑘 )𝑉: (1): Where Given a matrix 𝑀 ∈ R 𝐿 * 𝑛 , the softmax function normalizes 𝑀 to ensure the sum of each row equals to 1, as shown below.: 𝑊 𝑄 ∈ R 𝑑 ℎ * 𝑑 𝑘 ,𝑊 𝐾 ∈ R 𝑑 ℎ * 𝑑 𝑘 ,𝑊 𝑉 ∈ R 𝑑 ℎ *: 𝑆𝑜 𝑓 𝑡𝑀𝑎𝑥 (𝑀 𝑖,𝑗 ) = 𝑒𝑥𝑝 (𝑀 𝑖,𝑗 ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑀 𝑖,𝑘 ): (2): Note the attention matrix A is an 𝑛×𝑛 matrix, where 𝑛 represents the number of elements in the input sequence (e.g. words in NLP).\n",
      "\n",
      "RITA OVERVIEW: Given a collection of unlabeled timeseries, RITA first pre-trains a Transformer-style model to produce high quality feature embeddings for timeseries data. This pre-trained model is then used to support various downstream tasks, similar to BERT: [16]: . Next, we overview the model architecture of RITA. We show how RITA supports various downstream tasks in Appendix A.7.: As shown in Fig.: 1: , RITA is consist of two components: (1) Timeaware Convolution Layer (2) RITA Encoder. Time-aware Convolution Layer fills the gap between timeseries and natural language. Despite their high-level similarity, there is a big gap between timeseries and natural language. First, in natural language each word, as a discrete semantic unit, has an independent meaning, while each element in a timeseries is a continuous, numerical value and does not necessarily constitute an independent event. Furthermore, the input sequences are single-channeled in NLP, but often multi-channeled in timeseries (i.e., sensor data often consists of several related channels).: RITA leverages the classical convolution: [28]: strategy to solve this problem. Convolution is widely used to capture the local structures of an image. We use convolution to chunk one input timeseries into a sequence of windows and learn the local structure of each window, similar to the discrete semantic units in natural language. It also discovers the correlations across different channels, thus naturally solving the multi-channel problem.: More specifically, treating a multi-variate timeseries of length 𝑛 and with 𝑚 variables as an n × m matrix 𝑇 , RITA uses 𝑑 convolution kernels to chunk 𝑇 into n windows and produce one d-dimensional embedding per window using the convolution operation: [28]: . Each convolution kernel corresponds to a w × m matrix, where 𝑤 defines the number of timestamps that each convolution kernel covers, identical to the window size in sliding window. RITA Encoder functions as Transformer Encoder as described in the original Transformer work: [52]: . It takes the embeddings of 𝑛 semantic units 𝑋 1 , 𝑋 2 , ..., 𝑋 𝑛 (𝑋 𝑖 ∈ 𝑅 𝑑 ) as input (e.g. embeddings of 𝑛 windows for a timeseries), then models the correlations between the semantic units and outputs 𝑌 1 , ..., 𝑌 𝑛 (𝑌 𝑖 ∈ 𝑅 𝑑 ) as the contextaware embedding of each unit.: What makes RITA Encoder different from Transformer Encoder is that: at the core of Transformer Encoder lies self-attention mechanism which incurs a 𝑂 (𝑛 2 ) time complexity and memory usage. This quadratic cost becomes prohibitive for long timeseries and limits the scalablity of Transformer-based models. To make the attention computation efficient yet high-quality, we replace the canonical self-attention with our proposed group attention. Self-supervised Pretraining. Inspired by the \"cloze text\" pretraining task in NLP, we designed a mask-and-predict task as the pretraining task for our model. The timeseries is randomly masked and the model should recover the masked values based on corresponding contextual information.: To be specific, we generate masks on time-stamps, with a mask rate 𝑝. The timeseries is scaled to be non-negative and the values across all the channels on the masked timestamps are set to be -1, an impossible value on normal timestamps. Then the masked timeseries is fed into RITA and the output representation is translated to the recovered timeseries by a Transpose Convolution layer.\n",
      "\n",
      "GROUP ATTENTION MECHANISM: Group attention, a novel and efficient approximate attention mechanism, addresses the performance bottleneck of self-attention in the vanilla Transformer. In this section, we first introduce the framework of group attention and then theoretically establish the bound of its approximation error.\n",
      "\n",
      "The Idea of Group Attention: As periodicity is a natural property of timeseries: [56]: , similar windows frequently occur. Similar windows result in similar queries/keys for attention computation, bringing opportunities for saving computation.: As discussed in Sec. 2, 𝐴 𝑖 𝑗 , the attention score of window 𝑖 onto window 𝑗, is determined by the inner product between the query vector of window 𝑖 and the key vector of window 𝑗, that is, 𝑞 𝑖 • 𝑘 𝑗 . Given another window 𝑥, if window 𝑥 has the similar key vector to window 𝑗, that is,: 𝑘 𝑗 ≈ 𝑘 𝑥 , then 𝑞 𝑖 • 𝑘 𝑗 ≈ 𝑞 𝑖 • 𝑘 𝑥 . In other words, 𝐴 𝑖 𝑗 ≈ 𝐴 𝑖𝑥 when 𝑘 𝑗 ≈ 𝑘 𝑥 .: This observation inspires our group attention mechanism. That is, we group the windows by their similarity in keys. Assuming all windows in the same group have the same attention score onto another window 𝑘, we then only compute the attention once by using one single key to represent this group, for example the centroid of the group of keys. This thus saves significant computation cost.: Better yet, after grouping 𝑛 windows into 𝑁 groups, group attention compresses the attention matrix from an 𝑛×𝑛 matrix to an 𝑛×𝑁 matrix. Because 𝑁 (number of groups) tends to be much smaller than 𝑛 (number of windows) due to the periodicity of timeseries, group attention consumes much less memory than the original self-attention mechanism, successfully eliminating the memory bottleneck. Note that it also doesn't hurt quality all that much, as confirmed in our experiments (Sec. 6.2). We now discuss how to efficiently compute the output feature embeddings using the small compressed group attention matrix.\n",
      "\n",
      "Problem: Producing Embeddings w/ Group Attention Matrix: As described in the Background, once we have acquired the attention matrix 𝐴, canonical self-attention computes the output embedding 𝑂 as O = AV . Because 𝐴 is an 𝑛 × 𝑛 matrix and 𝑉 is an 𝑛 × 𝑑 𝑣 matrix, the matrix product operation still produces an 𝑛 × 𝑑 𝑣 matrix 𝑂. That is, it produces a 𝑑 𝑣 dimensional feature vector for each window. However, our group attention will produce an 𝑛 × 𝑁 attention matrix 𝐴 , where 𝑁 corresponds to the number of groups. In this case the matrix product will produce a 𝑁 ×𝑑 𝑣 matrix 𝑂. That is, it produces a feature vector for each group. However, our goal is to produce different embeddings for different windows, because even if some windows share the attention score temporally, it does not mean they should have the same feature embedding. A Naive Solution. A naive solution would be to restore the full attention matrix 𝐴 from the group attention matrix 𝐴. For example, given one group composed of 𝑤𝑖𝑛 𝑖 and 𝑤𝑖𝑛 𝑗 , we map its group attention vector in 𝐴 into two rows that correspond to 𝑤𝑖𝑛 𝑖 and 𝑤𝑖𝑛 𝑗 in 𝐴. However, in this case we again get a 𝑛 × 𝑛 attention matrix; and GPU memory remains a bottleneck in group attention.\n",
      "\n",
      "Solution: Embedding Aggregation and Group SoftMax: Using an embedding aggregation operation and a group softmax function, RITA produces 𝑛 embeddings without restoring the full attention matrix. Fig.: 2: shows the workflow of group attention. Embedding Aggregation. The idea is inspired by the observation on the matrix product operation O = AV conducted on the fully restored attention matrix 𝐴.: Given an element 𝑂 𝑖,𝑗 of 𝑂 corresponding to the 𝑗 𝑡ℎ dimension of 𝑤𝑖𝑛 𝑖 's feature vector, 𝑂 𝑖,𝑗 = 𝑎 𝑖 •𝑣 𝑗 , where vector a i ∈ R n denotes the 𝑖 𝑡ℎ row of the attention matrix 𝐴 and vector v j ∈ R n denotes the 𝑗 𝑡ℎ dimension of all the 𝑛 feature vectors. Given: a i =< a 1 i , a 2 i , • • • , a n i > and v j =< v 1 j , v 2 j , • • • , v n j >, 𝑂 𝑖,𝑗 = n k=1 a k i v k j .: As an example, assume 𝑤𝑖𝑛 1 and 𝑤𝑖𝑛 2 belong to the same group: 𝐺 1 . Then 𝑎 1 𝑖 = 𝑎 2 𝑖 = 𝑎 1 𝑖 , where 𝑎 1 𝑖 ∈ 𝐴 corresponds to the attention of group 𝐺 1 onto 𝑤𝑖𝑛 𝑖 . Therefore, 𝑎 1 𝑖 𝑣 1 𝑗 + 𝑎 2 𝑖 𝑣 2 𝑗 = 𝑎 1 𝑖 (𝑣 1 𝑗 + 𝑣 2 𝑗 ).: As an immediate generalization of the above analysis, if we aggregate up the windows that belong to the same group and convert the n-dimensional feature vector 𝑣 𝑗 into a 𝑁 -dimensional group feature vector 𝑣 𝑗 beforehand, we could directly use the group attention vector 𝑎 𝑖 and the group feature vector 𝑣 𝑗 to compute 𝑂 𝑖,𝑗 .: Using embedding aggregation, RITA is able to produce the feature embedding 𝑂 that is identical to the embedding 𝑂 produced by using the full attention matrix 𝐴 and the embedding matrix 𝑉 . Group Softmax Function. In canonical self-attention the atten-: tion matrix 𝐴 is computed as 𝐴 = SoftMax ( QK T √ d k: ). To compute 𝐴, we have to first compute 𝑄𝐾 𝑇 (denoted as 𝑃) which is an 𝑛 × 𝑛 matrix. Then normalizing the 𝑃 matrix with softmax produces the attention matrix 𝐴.: Group attention follows the same procedure. But after grouping keys into 𝐾, 𝑄 𝐾 𝑇 produces an 𝑛 × 𝑁 matrix 𝑃. Due to the nonlinearity of the softmax function, applying softmax directly on 𝑃 will result in a group attention matrix 𝐴 from which we are not able to recover a full attention matrix that is identical to first restoring 𝑃 to 𝑃 and then applying softmax on 𝑃. The 𝐴 matrix produced by the latter is desirable, as we want to approximate the original attention matrix as accurately as possible. However, restoring the small 𝑛 × 𝑁 𝑃 matrix is not memory efficient, as it will end up with a full 𝑛 × 𝑛 matrix 𝑃.: To solve the above problems, we introduce a new group softmax function to replace the original softmax function (Eq. 2).\n",
      "\n",
      "𝐺𝑟𝑜𝑢𝑝𝑆𝑜 𝑓 𝑡𝑀𝑎𝑥: ( 𝑃 𝑖,𝑗 ) = 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) 𝑁 -1 𝑘=0 𝑐𝑜𝑢𝑛𝑡 𝑘 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ): (3): In Eq. 3, 𝑐𝑜𝑢𝑛𝑡 𝑘 represents the number of windows that Group 𝐺 𝑘 contains. Compared to the original softmax, our group softmax considers each group 𝐺 𝑘 as 𝑐𝑜𝑢𝑛𝑡 𝑘 elements and counts it 𝑐𝑜𝑢𝑛𝑡 𝑘 times when summing up the exponential of each group's 𝑃 𝑖,𝑘 . In this way, the group softmax function operating on the small 𝑃 matrix will produce exactly the same result to the softmax function operating on the full 𝑃 matrix. Theoretical Guarantee. In Appendix A.4, we prove that the group softmax function and the embedding aggregation operation produce the same output feature embedding with the naive method that has to first restore the big full attention matrix.: We show an efficient implementation of the embedding aggregation operation and group softmax function in Appendix A.2, Alg. 1. Time Complexity. The time complexity of Alg. 1 is 𝑂 (𝑛𝑁𝑑) and the space complexity is 𝑂 (𝑛𝑁 ), while the time and space complexity of the original self-attention mechanism are 𝑂 (𝑛 2 𝑑) and 𝑂 (𝑛 2 ).\n",
      "\n",
      "Error Bound: Group attention produces a group attention matrix 𝐴 which approximates the attention matrix 𝐴 produced by the classical self-attention with a bounded error, as shown in Lemma 1.: Lemma 1. Let 𝑅 be the radius of the ball where all key vectors live; 𝑘 𝑖 be the representative of the group that contains key 𝑘 𝑖 . Let 𝐴 denote the full attention matrix restored from 𝐴. Suppose the distance between 𝑘 𝑖 and 𝑘: 𝑖 (|| k 𝑖 -k 𝑖 ||) satisfies: || k 𝑖 -k 𝑖 || ≤ d. Then ∀ 𝜖 > 1, if d ≤ ln(𝜖 ) 2R , 1 𝜖 ≤ A i,j A i,j ≤ 𝜖: Lemma 1 shows that the error bound 𝜖 of the group attention is determined by the distance 𝑑. As discussed in Sec. 5.1, it inspires us to design a strategy to dynamically determine the number of groups 𝑁 -the most critical parameter of group attention. Please refer to Appendix A.5 for the proof.\n",
      "\n",
      "GPU Friendly Grouping Method: In this section, we discuss the implementation of a grouping method. To make group attention efficient and effective, the grouping method has to satisfy the following requirements:: (1) Tight distance bound: to ensure the approximation quality, the distance between each key and its group representative should be minimized according to Lemma 1.: (2) Lightweight: to ensure the performance gain, the grouping method must be lightweight, at worst not exceeding the complexity of group attention itself (𝑂 (𝑁𝑛)).: (3) GPU friendly: to take advantage of GPUs, we prefer a grouping method that mainly consists of matrix operations, which can be efficiently executed on a GPU.: To satisfy the above requirements, after thorough investigation on various clustering algorithms, we design a GPU friendly Kmeans: [35]: as the grouping method.: First, K-means minimizes the overall distance between any object and its cluster center, hence naturally satisfying Requirement 1.: Second, given 𝑁 centers, in each iteration the time and space complexity of K-means is 𝑂 (𝑛𝑁 ). Usually, the iteration goes until convergence. However, we observe that rather than seeking a perfect K-means clustering, training a few iterations is sufficient to get a good grouping for group attention, because typically the later iterations only slightly update the clustering and group attention is robust to such imperfection.: Third, we design a GPU-friendly implementation of K-means. The performance bottleneck of K-means comes from the distance computation between each vector and its center, that is,: |v i -c j | = √︃ (v i -c j ) 2 , i ∈ [1, n], j ∈ [1, N ]. The performance bot- tleneck is 𝑣 𝑖 -𝑐 𝑗 .: We instead use a different formulation: |𝑣 𝑖 -: 𝑐 𝑗 | = |v i -c j | = √︃ |v i | 2 + |c j | 2 -2v i • c j , i ∈ [1, n], j ∈ [1, N ]: . This is because in this formulation, the performance bottleneck is 𝑣 𝑖 • 𝑐 𝑗 , which could be implemented as a matrix product operation. Although the complexity of the two formulations is the same, in GPUs matrix product is much more efficient than pairwise difference.\n",
      "\n",
      "ADAPTIVE SCHEDULER: Next, we present the adaptive scheduler of RITA which addresses the challenges of determining an appropriate number of groups 𝑁 and accordingly the batch size 𝐵, as described in Introduction.: Using a dynamic scheduling method we propose, the scheduler automatically determines and adjusts 𝑁 and 𝐵 based on the distributional properties of the feature embeddings produced over the iterative training process, while guaranteed to produce high quality attention approximation that meets the requirement of users. In Sec. 5.1 we show how RITA automatically determines 𝑁 . Then we introduce in Sec. 5.2 the learning-based method which given an 𝑁 , immediately predicts a good batch size.\n",
      "\n",
      "Dynamically Determining the Number of Groups N: Without loss of generality, we use one group attention module as an example to show how RITA automatically gets an appropriate 𝑁 . The adaptive scheduler of RITA starts with a large 𝑁 and decreases it dynamically. This is because in the training process of RITA, the feature embeddings produced epoch by epoch tend to get stabler and stabler and gradually converge, thus no need to increase 𝑁 . RITA reduces the number of groups by merging similar groups. Intuitively, given two groups, we could measure their similarity based on the distance of their centers. If the distance between their centers is smaller than a distance threshold, then the two groups could be merged. However, setting an appropriate distance threshold seems hard -as difficult as setting an appropriate 𝑁 .: To solve this problem, RITA leverages the error bound of group attention introduced in Sec.: 4: 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑘 𝑖 |𝑐 𝑘 𝑖 -𝑐 𝑘 𝑗 | + |𝑥 -𝑐 𝑘 𝑖 | ≤ 𝑑, 𝑖, 𝑗 ∈ [1, 𝑚]: (4): merging them into one cluster still meets the error bound 𝜖.: Please refer to Appendix A.6 for the proof. Finding the Mergable Clusters. We formulate the problem of finding mergeable clusters using graph theory:: (1) each cluster is a node in the graph;: (2) if 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 and 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 satisfy:: 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 |𝑐 𝑖 -𝑐 𝑗 |+|𝑥 -𝑐 𝑖 | ≤ 𝑑, and 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 |𝑐 𝑗 -𝑐 𝑖 |+|𝑥 -𝑐 𝑗 | ≤ 𝑑: there is an undirected edge between 𝑛𝑜𝑑𝑒 𝑖 and 𝑛𝑜𝑑𝑒 𝑗 ; In this scenario, finding the maximum number of mergeable clusters is equivalent to finding the minimal clique cover in the corresponding graph, which is an NP-hard problem: [24]: . Such heavy computation overhead is not acceptable for RITA. We thus offer a simplified solution:: (1) Halve the clusters into two sets 𝑆 1 , 𝑆 2 ;: (2) If 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 ∈ 𝑆 1 and 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 ∈ 𝑆 2 satisfy:: 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 |𝑐 𝑖 -𝑐 𝑗 | + |𝑥 -𝑐 𝑖 | ≤ 𝑑, 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 |𝑐 𝑗 -𝑐 𝑖 | + |𝑥 -𝑐 𝑗 | ≤ 𝑑 2 (: 5: ): 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 is marked.: (3) Decrease the number of clusters by counting the masks in 𝑆 2 . In this solution, clusters in 𝑆 1 can be regarded as transfer nodes. If (5) holds for: (𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 ∈ 𝑆 1 , 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 1 ∈ 𝑆 2 ) and (𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 ∈ 𝑆 1 , 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 2 ∈ 𝑆 2 ): , respectively, we have,: 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 1 |𝑐 𝑗 1 -𝑐 𝑗 2 | + |𝑥 -𝑐 𝑗 1 | ≤ 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 1 |𝑐 𝑗 1 -𝑐 𝑖 | + |𝑐 𝑖 -𝑐 𝑗 2 | + |𝑥 -𝑐 𝑗 1 | ≤ 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 1 |𝑐 𝑗 1 -𝑐 𝑖 | + |𝑐 𝑖 -𝑐 𝑗 2 | + |𝑥 -𝑐 𝑗 1 | + |𝑥 -𝑐 𝑗 2 | ≤ 𝑑 (6): Thus (4) holds when merging several clusters in 𝑆 2 with one cluster in 𝑆 1 . As a result, we can greedily merge clusters in 𝑆 2 , as illustrated in step: (3): .: Assume the number of clusters decreases by 𝐷 after merging, we apply a momentum update: [42]: on the number of clusters 𝑁 , as is commonly used in machine learning to smooth the changing of 𝑁 and avoid sample selection bias. To be specific: 𝑁 𝑛𝑒𝑤 = 𝛼 (𝑁 -𝐷) + (1 -𝛼)𝑁 , where 𝛼 is a hyper-parameter for momentum.\n",
      "\n",
      "Dynamically Determining the Batch Size: Because of the dynamic grouping operation, the computational graph in deep learning training: [1]: varies from sample to sample. As a result, it is impossible to precisely compute a batch's GPU memory usage without indeed feeding it into the model. To overcome this problem, RITA learns a batch size prediction function offline; then at the RITA training time, given a number of groups 𝑁 , RITA uses this function to predict a proper batch size.: When the model architecture and hardware are fixed, the batch size depends on the length of the timeseries 𝐿 and the average group number among all attention module 𝑁 . So RITA samples several (𝐿 𝑖 , 𝑁 𝑖 ) pairs and estimate a proper batch size for each pair.: More specifically, given a user-defined timeseries maximal length 𝐿 𝑚𝑎𝑥 , we randomly sample integral points (𝐿 𝑖 , 𝑁 𝑖 ) from plane {1 ≤ 𝐿 ≤ 𝐿 𝑚𝑎𝑥 , 1 ≤ 𝑁 ≤ 𝐿}. Then we use a binary search based algorithm to find the maximal batch size 𝐵 𝑖 that consumes less than 90% available GPU memory, aiming to avoid wasting GPU memory and the risks of out of memory (OOM).: Treating these pairs as ground truth labels, we use function fitting: [18]: to learn the batch size predicting function B = f (L, N ), where B is a function of two variables 𝐿 and 𝑁 . Learning the Prediction Function. We apply curve fit from SciPy: [53]: as the function fitting tool to fit the two-variable function: 𝐵 𝑖 = 𝑓 (𝐿 𝑖 , 𝑁 𝑖 ) on plane {1 ≤ 𝐿 ≤ 𝐿 𝑚𝑎𝑥 , 1 ≤ 𝑁 ≤ 𝐿}.: We observe that applying one function to the whole plane incurs a huge estimation error. So we develop a dynamic-programming (DP) method to divide the plane into several sub-planes and apply a distinct function to each sub-plane respectively. It is optimal in minimizing the total estimation error on all sub-planes With the learned prediction function 𝑓 , we can estimate a proper batch size for any (𝐿, 𝑁 ) during training, even if it is not seen in the sampled (𝐿 𝑖 , 𝑁 𝑖 ) pairs. The Algorithms and Optimality Proof. Please refer to Appendix A.3 for the pseudo code of the binary search-based algorithm and the description of the DP method for plane-division and the proof for its optimality.\n",
      "\n",
      "EVALUATION: Our experimental study focuses on the following questions:: 1. Effectiveness and efficiency of RITA: How does RITA compare with other Transformer-based methods and traditional timeseries representation learning methods in accuracy and efficiency?: 2. Ablation Study: How do the key techniques of RITA work?\n",
      "\n",
      "Experimental Setup: Datasets. We evaluate RITA on classification and imputation tasks using 5 multi-variate and 3 uni-variate timeseries datasets.: • WISDM: [55]: is a popular multivariate timeseries dataset generated from the accelerometer in the mobile phone. The subjects performed 18 daily activities (e.g. walking, jogging). The dataset was collected from 51 subjects and the sampling rate is 20 Hz.: • HHAR dataset: [46]: contains sensing data of accelerometer collected from 9 users performing 5 activities with 12 different smartphones (varying in sampling rate). This increases the complexity of the task and thus can test the model's robustness.: • RWHAR RealWorld HAR dataset: [48]: covers 15 subjects performing 8 locomotion-style activities. Each subject wears the sensors for approximately ten minutes. The sampling rate is 50 Hz.: • ECG dataset: [34]: consists of 10,000 EEG recordings for arrhythmia classification. Each recording has an uncertain length ranging from 6 to 60 seconds sampled at 500 Hz. The ECG recordings correspond to 9 types of heart problems such as atrial fibrillation (AF) and premature atrial contraction (PAC), etc.: • MGH: [6]: is a EEG dataset collected by Mass. General Hospital. Each timeseries corresponds to the EEG data observed from one patient during their stay in ICU for a couple of days. The EEG monitoring produced data with 20 channels. The sampling rate is 200 HZ. So it produces very long timeseries.: • WISDM*/HHAR*/RWHAR* are three uni-variate datasets derived by picking one channel from WISDM/HHAR/RWHAR. Training/Validation Data Generation. We apply a sliding window on the raw timeseries to get training/validation samples. The size of the sliding window is set as 200 on small datasets (WISDM, HHAR, RWHAR), 2000 on medium size dataset (ECG), and 10,000 on the large dataset (MGH). Table: 1: shows the statics of the generated datasets. They are randomly split into training/validation set in a proportion of 0.9/0.1. In \"pretraining + few-label finetuning\" scenario, we use 100 labeled data per class for finetuning. We guarantee that training set does not overlap with the validation set. To evaluate our group attention (referred to as Group Attn.), we develop three baselines by replacing the group attention component in RITA with the classic vanilla Self-Attention: [52]: (referred to as Vanilla) and two SOTA methods that reduce the complexity of self-attention by approximation in NLP, namely, Performer: [10]: (referred to as Performer) and Linformer: [54]: (referred to as Linformer). Similar to our proposed Group Attn., Vanilla, Performer, Linformer all use RITA's time-aware convolution operation (Sec. 3) to turn timeseries segments into input feature vectors. We also compare Group Attn. against GRAIL: [40]: , which is the SOTA of the non-deep learning methods for timeseries representation learning. GRAIL supports classification tasks by feeding the learned representations into a Support-Vector Machine: [12]: or K-Nearest Neighbor: [17]: classifier. Note GRAIL only targets uni-variate timeseries and cannot support imputation tasks. Methodology. We mainly focus on two downstream tasks:: (1) Classification. First, we train Group Attn. and the baselines with full labels from scratch to test the effectiveness of RITA framework and the approximation quality of our group attention.: Second, to measure the effectiveness of self-supervised pretraining, we evaluate the accuracy of training on few labeled timeseries with/without pretraining on large scales of unlabeled timeseries. To be specific, we split the training set into a pretraining set and a finetuning set, with very few data in the latter (100 labeled samples per class in our experiment). We train the model on the cloze pretraining task with a mask rate 𝑝 = 0.2. Then we train two classification models using the finetuning set, either based on the pretrained version or from scratch. We repeat the experiment 5 times with random data splits and report the median accuracy.: (2) Imputation. We run the imputation task on the datasets used in classification as well as the large unlabeled MGH dataset, and measure the mean square error and absolute imputation error. To get timeseries with missing values, we randomly mask the values with an expected mask rate of 𝑝 = 0.2. The masked values are replaced with a special value.: Finally, to evaluate Group Attn. 's benefit on efficiency, the total time of forward computation, backward propagation, and grouping are measured for all methods in all the experiments.: To save space, we only report the average training time per epoch here and refer readers to Appendix A.8 for the inference time.: We first compare against the Transformer-based methods on multi-variate datasets (sec. 6.2, 6.3), then compare against the nondeep learning method GRAIL on uni-variate datasets (sec. 6.4). Configuration. Please refer to Appendix A.1 for the experiment configuration and hyper-parameter settings.\n",
      "\n",
      "Effectiveness: Transformer-Based Methods: We first evaluate the quality of the models trained with full labels from scratch. We then show how the pretraining of RITA increases the accuracy of the downstream tasks.\n",
      "\n",
      "full-label training (Multi-variate classification): Results shown in Figure: 3: (a) get us the following observations:: (1) RITA's advantage over TST. On all four datasets for the classification tasks, Group Attn. and the other three baselines that use RITA architecture (Vanilla, Performer, and Linformer) outperform TST. In particular, Group Attn. outperforms TST by 49 percentage points on the ECG dataset (88.48% vs 39.93%) with long timeseries. Two deficiencies in TST may cause its poor performance on the long timeseries. Firstly, TST concatenates the output embedding vector of each time stamp, then uses a linear classifier to do classification on the concatenated vector. When the timeseries is long, the linear classifier has so many parameters that it tends to overfit easily. Secondly, TST replaces Layer Normalization in vanilla Transformer with Batch Normalization. When the timeseries is long, it can only accommodate a small number of timeseries in each batch, leading to bias in Batch Normalization.: (2) Group-attention's advantage over other attention mechanisms. Group Attn. is better than Performer and Linformer on 3 out of 4 datasets for classification. Although Linformer works slightly better than Group Attn. on the ECG dataset (90.37% vs 88.84%), its performance is the worst in all other cases compared to any other RITA-based methods. Vanilla computes the attention scores precisely. Thus it is expected to work well. However, Group Attn. outperforms Vanilla on WISDM (87.50% vs 86.95%) and is very close to it on other 3 datasets. This suggests that group attention's approximation quality is good.\n",
      "\n",
      "pretraining + few label finetune (Multi-variate classification): The results shown in Table: 3: get us the following observation:: (1) Pretraining is effective. Pretraining always leads to better accuracy than training with a few labels from scratch. In particular, on WISDM data all the methods using RITA architecture increase the accuracy by at least 10%. This is impressive considering we do not have a very large unlabeled pre-training set to use.: (2) RITA's advantage over TST. our Group Attn. and other three baselines using RITA architecture (Vanilla, Performer, and Linformer) significantly outperform TST on all four classification datasets by 25 percentage points.: (3) Group Attention's advantage over other attention mechanisms. Group Attn. is better than Performer and Linformer on 3 out of 4 datasets. When compared to Vanilla, Group Attn. is better on HHAR and ECG, and comparable on the other two, further confirming its high quality on approximation. Further, we notice that Linformer struggles in this setting: in average its accuracy is worse than Vanilla by 8.22% and Group Attn. by 8.01%. This is because the low-rank projection operation introduces extra model parameters, making Linformer more easily overfit, while overfitting is especially harmful when there are only a few labeled training samples.\n",
      "\n",
      "full-dataset training (Multi-variate imputation): Similar to classification tasks, the results of imputation tasks (Table .2) show that Group Attn. consistently outperforms the baselines in training time while achieving comparable/better MSE. Again, on the large dataset MGH (length = 10,000), TST and Vanilla fail due to out of memory (OOM) errors. Methods using RITA framework (Group Attn., Performer, Linformer) all achieve very low MSE (are highly accurate). Among them Linformer is the worst.\n",
      "\n",
      "Efficiency: Transformer-based Methods: We measure the efficiency by the average training time per epoch including the cost of the forward computation + backward propagation and the grouping overhead. We first show the results on all the 5 datasets in Sec. 6.3.1. We then vary the length of the timeseries on the MGH dataset to show group attention's scalability on long timeseries in Sec. 6.3.2.\n",
      "\n",
      "Training Time: All Multi-variate Datasets: The results in Fig.: 3\n",
      "\n",
      "(b) and Table 2 lead to the below observations:: (1) Vanilla Self-Attention is not scalable. In average, it takes 2-3 minutes to train one epoch when the length of the timeseries is only 200 (WISDM, HHAR, RWHAR), takes over 15 minutes when the length increases to 2,000 (ECG), and fails on the long MGH data when the length reaches 10,000 due to out of GPU memory.: (2) Group Attn.'s advantage over all other attention mechanisms. As we have shown in Sec.: 6: than Performer and Linformer in classification and imputation tasks, while Group Attn. is always faster than Performer, Linformer, and all other baselines on all 5 multi-variate datasets, thus a win-win.: (3) The longer the timeseries, the larger the speedup. On the medium sized ECG dataset with a length of 2,000, Group Attn. has a speedup of 3.86/1.36/2.27 compared to Vanilla/Performer/Linformer. When the length increases to 10,000, the speedup on the MGH dataset increases to 6.59/7.48 compared to Performer/Linformer (Vanilla and TST failed in this case) on imputation task (Table . 2). However, even on the short WISDM, HHAR, RWHAR datasets, Group Attn. still consistently outperforms other methods, confirming that it does not introduce much overhead. This is because when the length of the timeseries gets longer, Group Attn. gets more opportunities to find windows with similar properties.\n",
      "\n",
      "Training time: Varying the Length: In this experiment, we truncate the original MGH timseries into sequences with the lengths at 2000/4000/6000/8000/10000, and compare Group Attn. against Vanilla and other attention mechanisms. Vanilla cannot handle sequences longer than 8000.: The results in Fig.: 4: again show that the longer the timeseries, the larger the speed up. With comparable MSE, Group Attn. outperforms Vanilla by 63X. Moreover, as the length increases from 2000 to 10000, the training time of Group Attn. only increases from 31.2 seconds to 54.4 seconds per epoch. The reason is that as the timeseires becomes longer, there are more grouping opportunities because of the similarity of the timeseries segments.\n",
      "\n",
      "Comparison to Non-deep Learning Methods: We compare against GRAIL, the SOTA of non-deep learning timeseries representation learning. We use the three uni-variate datasets, because GRAIL only targets uni-variate timeseries. Results in Fig.: 5: show that on all 3 datasets RITA significantly outperforms GRAIL in accuracy by 45, 16, and 21 percentage points because of the expressive power of Transformer. Moreover, thanks to the GPU-friendly design of RITA, it is at least 2× faster than GRAIL in training time.\n",
      "\n",
      "Ablation Study\n",
      "\n",
      "Adaptive Scheduler: To evaluate the effectiveness of RITA's adaptive scheduler (Sec. 5), we compare it against a baseline using a fixed group number 𝑁 . We vary 𝑁 and the error bound threshold 𝜖 used by RITA.: From the results in Table: 4: we get the following observations:: (1) Adaptive Scheduler is better than fixed 𝑁 . Training with Adaptive Scheduler already achieves better or comparable performance compared to the best performing 𝑁 . More specifically, on the MGH dataset, dynamic scheduler always achieves better accuracy and is much faster compared to fixed 𝑁 . On the ECG dataset, although fixed 𝑁 is slightly better than adaptive scheduler in accuracy when setting the N as 512, it runs much slower than adaptive scheduler. Of course, finding the best 𝑁 that balances the accuracy and running time requires careful tuning.: (2) Adaptive Scheduler is tuning free. It is robust on both accuracy and running time when 𝜖 varies, while the results of fixed 𝑁 vary significantly when the value of 𝑁 changes. Therefore, Adaptive Scheduler frees the users from tuning the 𝜖 threshold, while it is hard to find an appropriate 𝑁 for a given dataset.  Table: 5: : RITA Pretraining: increasing sizes of pretrain set.\n",
      "\n",
      "The Sizes of the Pretraining Data: Next, we evaluate how the number of unlabeled data influences the effectiveness of pretraining. To get empirical results, we pretrain RITA on WISDM dataset with 20%/40%/60%/80% of the pretraining data and finetune each pretrained model with 100 labels per class. The results in Table: 5: show that: (1) The more pretraining data, the larger the improvement. The accuracy increases with the sizes of the pretraining data; (2) Marginal utility diminishing.: The first 20% pretraining data gives a 10.38% improvement accuracy (72.94% vs 62.56%), while the remaining 80% pretraining data only gives an additional improvement of 2.12% (75.06% vs 72.94%).\n",
      "\n",
      "RELATED WORK 7.1 Timeseries Analytics: There is a great deal of prior work on timeseries analytics methods. This work can be divided into three categories: (1) non-deep learning methods; (2) CNN/RNN-based deep learning methods; and (3) Transformer-based deep learning methods. Traditional Methods. These methods, such as TS-CHIEF: [45]: , HIVE-COTE: [33]: , ROCKET: [15]: have achieved notable performance on public datasets. Despite that, traditional methods suffer from one or more issues: they (1) rely on expert knowledge for feature extraction;: (2) incur heavy computation cost and are inappropriate for GPU devices; (3) support only uni-variate timeseries; (4) perform classification solely. Some work: [61]: shows that the transformedbased methods outperform these traditional methods especially on multi-variate timeseries.: In particular, as the SOTA of timeseries representation learning, GRAIL: [40]: extracts landmarks from data and computes the representations with the combination of the landmarks. However, GRAIL only supports uni-variate timeseries. Our experiments (Sec. 6.4) show that RITA significantly outperforms GRAIL in both effectiveness and efficiency on uni-variate timeseries.: CNN/RNN-based Deep Learning Methods. CNN-based methods, such as InceptionTime: [21]: and Resnet: [19]: , are good at classification tasks, but can not handle generative tasks such as forecasting because of the inductive bias of convolution networks. RNN-based methods, such as Brit: [7]: and deepAR: [44]: , are capable for classification, regression and generation. However, the recurrent structure brings a lot of problems: (1) limiting the model's ability in capturing long-range correlation; (2) notoriously difficult to train: [41]: because of gradient vanishing and exploding problem. As a result, such methods can hardly scale to very long timeseries. Transformer-based Deep Learning Methods. Given that Transformer is the best choice for backbone in almost all sequence modeling tasks, some effort has been made to apply Transformer to timeseries analytics. Targeting forecasting of uni-variate timeseries, LogTrans: [30]: introduced a log sparsity assumption to attention computation. Informer: [62]: pushes LogTrans a step further and scales forecasting to multi-variate timeseries. Autoformer: [57]: performs forecasting by decomposing timeseries into two parts, i.e. the trend part and the seasonal part.: For imputation tasks, CDSA: [37]: outperforms statistical methods and the SOTA of RNN-based method Brit: [7]: on 3 public and 2 competition datasets. For timeseries classification, AutoTransformer: [43]: performs architecture search to adapt to the tasks in different domains. For timeseries anomaly detection, Anomaly Transformer: [58]: outperforms many widely-used methods such as OmniAnomaly: [47]: , assuming the attention score maps show Gaussian distribution.: All of these works are designed for specific tasks, rather than functioning as a representation learning framework to serve different downstream tasks. To fill this gap, some researchers proposed a Transformer-based architecture, called TST: [61]: . Like RITA, TST supports regression, classification, and unsupervised learning through the \"cloze test\" pretraining task on timeseries. However, TST directly uses the classical Vanilla self-attention, thus not scalable to long timeseries as shown in our experiments (Sec. 6.3.2).\n",
      "\n",
      "Efficient Transformers: The need of improving the scalability of Transformers has led to more efficient variations of Transformers, especially for accommodating long text data in NLP: [49]: .: Introducing fixed/random patterns to self-attention mechanism is an intuitive idea. Sparse Transformer: [9]: and Longformer: [3]: only compute attention at fixed intervals. ETC: [2]: and BigBird: [60]: use global-local attention: the attention computation is limited within a fixed radius, while some auxiliary tokens are added to attend/get attended globally. The deficiencies of fixed attention patterns are obvious: it heavily depends on users to give an optimal setting.: To decrease the reliance on human labor, some works seek to introduce learnable/adaptive attention patterns instead of fixed patterns. Reformer: [26]: proposed only computing the dominant attention terms based on their observation of sparsity in attention matrix from language/image data. Such sparsity is intuitive in language data, in which a word's attention mainly focuses on the nearby sentences. However, attention in timeseries data shows strong seasonal patterns rather than sparse patterns, mainly as result of the periodicity of timeseries data. Therefore, such works do not work well for timeseries.: Apart from introducing attention patterns, some works seek to solve this problem with applied mathematics techniques. Linformer: [54]: performs a projection to decrease the size of query, key and value matrices before attention computation, because the attention matrix tends to be low-ranked. Performer: [10]: uses linear functions to approximate the kernel function softmax, making attention computation commutative. When the sequence length is far greater than the dimension of embedding vectors, Performer benefits from changing the order of matrix multiplication. Linformer and Performer do not depend on the unique properties of language data, thus potentially fitting timeseries better than other techniques, which is why we compared against them in our experiments. However as shown in Sec. 6, our group attention significantly outperforms them in both accuracy and efficiency (training time), because group attention fully leverages the periodicity of timeseries.\n",
      "\n",
      "CONCLUSION: In this work, we presented RITA, an automatic, self-supervised, and scalable timeseries analytics tool. RITA effectively adapts Transformer, popular in NLP, into timeseries analytics. As the key component of RITA, group attention eliminates the performance bottleneck of the classical self-attention mechanisms, thus successfully scaling RITA to highly complex, long timeseries data. Our experiments confirm that RITA significantly speeds up the state-of-the-art by 63X with a better accuracy.\n",
      "\n",
      "A APPENDIX: SUPPLEMENTARY MATERIAL A.1 Experiment Configuration and: Hyper-parameter Settings: Configuration. All models were trained on an NVIDIA Tesla V100 16GB GPU. All the methods are optimized with AdamW: [36]: of which the starting learning rate and weight decay parameter are both 1𝑒 -4 . In full-label training scenario, we train the models for 100 epochs. In \"pretraining + few-label finetuning scenario\", as the pretrained models require fewer epochs to converge: [61]: , we train the model for 50 epochs. For a fair comparison, the baselines use a maximal batch size within GPU's capacity during training.: As for model hyper-parameter setting, RITA and the baselines use a Transformer structure balancing Vanilla 's accuracy and efficiency: 8-layer stack of 2-head attention with hidden vectors in dimension of 64. Convolution kernel size is set to 5 by default. We set the error bound threshold (𝜖, Sec. 5.1) of Group Attention to 2, as it balances the accuracy and the efficiency in general on all datasets. Because Linformer requires the users to set the sizes of projection matrix, in different settings we choose an accuracyefficiency balancing one among {64,128,256,512}.\n",
      "\n",
      "A.2 Efficient Computation of Group Attention Algorithm 1 Efficient Computation of Group Attention: Require: 𝑄, 𝑉 , 𝑅, 𝐶𝑂𝑈 𝑁𝑇 , 𝐵𝐸𝐿𝑂𝑁 𝐺 Ensure: 𝑄, 𝑉 ∈ R 𝑛 * 𝑑 ,𝑅 ∈ R 𝑁 * 𝑑 ,𝐶𝑂𝑈 𝑁𝑇 ∈ N 𝑁 ,𝐵𝐸𝐿𝑂𝑁 𝐺 ∈ N 𝑛 1: function group_attention(𝑄, 𝑉 , 𝑅): 2:: for 𝑖 = 0 → 𝑁 -1 do 3:: 𝑣 𝑖 ← 𝑛-1 𝑗 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑗 == 𝑖 )𝑣 𝑗 4:: 𝑃 ← 𝑄𝑅 𝑇 5:: for 𝑖 = 0 → 𝑛 -1 do 6:: for 𝑗 = 0 → 𝑁 -1 do 7:: 𝑤 𝑖,𝑗 ← 𝑒𝑥𝑝 ( 𝑃 𝑖,𝑗 )𝐶𝑂𝑈 𝑁𝑇 𝑗 8:: for 𝑖 = 0 → 𝑛 -1 do 9:: 𝑠 𝑖 ← 𝑁 -1 𝑗 =0 𝑤 𝑖,𝑗: 10: :: for 𝑖 = 0 → 𝑛 -1 do 11:: 𝑜 𝑖 ← 𝑁 -1 𝑗 =0 𝑒𝑥𝑝 ( 𝑃 𝑖,𝑗 ) 𝑠 𝑖 𝑣 𝑗 12:: return 𝑂: In Alg. 1, we denote 𝐶𝑂𝑈 𝑁𝑇 𝑖 to be the size of the 𝑖 𝑡ℎ group, 𝑁 to be the number of groups, r 𝑖 to be the representative key of the 𝑖 𝑡ℎ group and R to be the matrix consisting of all r 𝑖 , 𝐵𝐸𝐿𝑂𝑁𝐺 𝑖 to be the group that k 𝑖 belongs to. 𝑄, 𝑉 are the packing matrices of query vectors and value vectors as described in Sec.2. Alg. 1 outputs the packing matrix 𝑂 for new feature emebddings {𝑜 1 , ..., 𝑜 𝑛 }, where 𝑜 𝑖 corresponds to the feature embedding of 𝑤𝑖𝑛 𝑖 . Lines 2-3 implement the embedding aggregation operation, while   We describe Alg. 3 and intuitively show its optimality. We assume that Scipy: [53]: learns an optimal function in Line 4 so that function COST gives the optimal estimation error when fitting the points in set 𝑆. When fitting very few points, we assign an infinite cost to prevent a biased fitting function (Line 2). 𝑔(𝑛) denotes the minimal estimation error for points in sub-plane {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑁 ≤ 𝑛}. In Lines 11-13, we enumerate all possible ways of cutting {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑁 ≤ 𝑛} horizontally into two sub-plane {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑁 ≤ 𝑖} and {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑖 ≤ 𝑁 ≤ 𝑛} by iterating 𝑖 from 1 to n. Choosing the cutting strategy that minimizes estimation error gets us a 𝑔(𝑙 1 ) with minimal estimation error for sub-plane {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑁 ≤ 𝑙 1 }, which is recorded as 𝑓 𝑙 1 ,𝑙 2 in Line 14. 𝑑𝑝 (𝑙) denotes the minimal estimation error for sub-plane {𝐿 ≤ 𝑙 }. We enumerate all the possible ways of cutting {𝐿 ≤ 𝑙 } vertically into two sub-plane {𝐿 ≤ 𝑖} and {𝑖 ≤ 𝐿 ≤ 𝑙 } by iterating 𝑖 from 1 to 𝑙 (Line 17: -19): . Finally, we have the minimal estimation error for the whole plane as 𝑑𝑝 (𝐿 𝑚𝑎𝑥 ). Based on the above discussion, this algorithm guarantees to not miss any better solution, hence optimal.: A.: 4: The Correctness of Group Attention Lemma 3. Assuming the windows belonging to the same group 𝐺 𝑖 have the same key vector, i.e. 𝑘 𝑗 = 𝑟 𝑖 (𝑤𝑖𝑛 𝑗 ∈ 𝐺 𝑖 ), then the feature embedding 𝑂 produced by the original self-attention mechanism is identical to the output of our group attention mechanism implemented in Algorithm 1.: Proof. Denote 𝑘 𝑗 to be the representative vectors of 𝑘 𝑗 , i.e. 𝑘 𝑗 = 𝑟 𝑖 = 𝑘 𝑗 (𝑤𝑖𝑛 𝑗 ∈ 𝐺 𝑖 ). Algorithm 1 gives that: 𝑣 𝑖 = 𝑛-1 ∑︁ 𝑗 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑗 == 𝑖 )v 𝑗 , 𝑃 𝑖,𝑗 = q 𝑖 • r 𝑗 𝑠 𝑖 = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 ( 𝑃 𝑖,𝑗 )𝐶𝑂𝑈 𝑁𝑇 𝑗 , 𝑜 𝑖 = 𝑁 -1 ∑︁ 𝑗 =0 𝑃 𝑖,𝑗 𝑠 𝑖 𝑣 𝑗: (7): By the canonical self-attention mechanism introduced in Sec. 2, we get:: 𝑃 𝑖,𝑗 = q 𝑖 • k j , 𝐴 𝑖,𝑗 = 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) , o 𝑖 = 𝑛-1 ∑︁ 𝑗 =0 𝐴 𝑖,𝑗 v 𝑗: (8): With 7 and 8, we have: 𝑛-1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) = 𝑛-1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (q 𝑖 • k 𝑗 ) = 𝑁 -1 ∑︁ 𝑗 =0 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 )𝑒𝑥𝑝 (q 𝑖 • k 𝑥 ) = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (q 𝑖 • r 𝑗 ) 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 ) = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (q 𝑖 • r 𝑗 )𝐶𝑂𝑈 𝑁𝑇 𝑗 = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 ( 𝑃 𝑖,𝑗 )𝐶𝑂𝑈 𝑁𝑇 𝑗 = 𝑠 𝑖: (9): Further,: o 𝑖 = 𝑛-1 ∑︁ 𝑗 =0 𝐴 𝑖,𝑗 v j = 𝑁 -1 ∑︁ 𝑗 =0 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 )𝐴 𝑖,𝑥 v 𝑥 = 𝑁 -1 ∑︁ 𝑗 =0 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 ) 𝑒𝑥𝑝 (𝑃 𝑖,𝑥 ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) v 𝑥 = 𝑁 -1 ∑︁ 𝑗 =0 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 ) 𝑒𝑥𝑝 (q 𝑖 • k 𝑥 ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) v 𝑥 = 𝑁 -1 ∑︁ 𝑗 =0 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 ) 𝑒𝑥𝑝 (q 𝑖 • r j ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) v 𝑥 = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (q 𝑖 • r j ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 )v 𝑥 = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (q 𝑖 • r j ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) 𝑣 𝑗: (10): Combining (: 7: ), (9) (10), we have o i = N -1 j=0 P i,j s i v j = o i . This concludes that the output of our group attention is identical to vanilla self-attention's. □: A.5 The Proof of Error Bound (Lemma 1): Proof. We have: 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) = 𝑒𝑥𝑝 (q 𝑖 • k 𝑗 ) 𝑒𝑥𝑝 (q 𝑖 • k 𝑗 ) = 𝑒𝑥𝑝 (q 𝑖 • ( k 𝑗 -k 𝑗 )) = 𝑒𝑥𝑝 (||q 𝑖 || • || k 𝑗 -k 𝑗 || • 𝑐𝑜𝑠 (q 𝑖 , k 𝑗 -k 𝑗 )): (11): So: 𝑒𝑥𝑝 (-𝑑𝑅) ≤ 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) ≤ 𝑒𝑥𝑝 (𝑑𝑅): (12): Then we have: A i,j ≤ 𝜖. This proves Lemma 1.: A.: 6: The Proof of Merge Operation (Lemma 2): Proof. Denote the cluster size of 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑘 to be 𝑛 𝑘 .After mergeing, the new center will be:: 𝑐 ′ = 𝑚 𝑖=1\n",
      "\n",
      "A.7 Downstream Tasks: RITA supports a variety of downstream tasks. In this section, we show that with minimal modification RITA can effectively support classification, imputation and forecasting tasks. Other unsupervised tasks such as similarity search or clustering are naturally supported by extracting feature embeddings from RITA.\n",
      "\n",
      "A.7.1 Classification: To classify timeseries, we input timeseries to the model as described in Sec.\n",
      "\n",
      "A.7.3 Forecasting: Forecasting can be regarded as a special case of imputation, in which all missing values are at the end of timeseries.: So like in imputation task, we scale the timeseries to nonnegative and use a special value (-1) to indicate the values to be predicted:: 𝑇 𝑜𝑏𝑠𝑒𝑟 𝑣𝑒𝑑 (𝑖, 𝑗) = 𝑇 𝑟𝑒𝑎𝑙 (𝑖, 𝑗) 𝑖 ≤ 𝑡 𝑜𝑏𝑠𝑒𝑟 𝑣𝑒𝑑 -1 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒: (18): Where 𝑡 𝑜𝑏𝑠𝑒𝑟 𝑣𝑒𝑑 is the observed timestamp. Then the output representations are fed into a Transpose Convolution layer using Mean Squared Error as loss function, as described above.: A.7.4 Other Unsupervised Tasks RITA naturally supports other unsupervised tasks, such as similarity search and clustering: [25,: 31,: 32]: , by producing the embedding of one timeseries (output representation of the special token [CLS]).: Clustering can be performed on the embeddings with flexible choice of distance metrics. Similarly, a high dimensional similarity search system: [22,: 23,: 38]: can be built on the embeddings.\n",
      "\n",
      "A.8 Inference Time: Dataset Length TST: [61]: In this section, we present the average inference time on validation sets. The results in Table. 6 and 7 correspond to the average inference time on validation sets of classification and imputation tasks, respectively. Consistent with the results in Section. 6.3, our method Group Attn. outperforms the baselines on both classification and imputation tasks, particularly on the datasets comprising long timeseries (ECG and MGH).\n",
      "===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muraf\\Courses\\arxiv_llm\\venv\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from arxiv_bot.search import TEIFile\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index import Document, VectorStoreIndex\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "splitter = SpacyTextSplitter(chunk_size=1024, chunk_overlap=100, separator=\"\\n\\n\")\n",
    "for paper in os.listdir(\"./output/\"):\n",
    "    tei_object = TEIFile(f\"./output/{paper}\")\n",
    "    print(tei_object.text)\n",
    "    print(\"===================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='5890981b-8362-4a80-9cd6-7cf661effa8b', embedding=None, metadata={'page_label': '1', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='a970d4d7e2cfc588d7ecbb820b81062b422baf9760e2fc006ef01d191ed6009d', text='Is Attention All What You Need ? - An Empirical Investigation on\\nConvolution-Based Active Memory and Self-Attention\\nThomas Dowdell ,Hongyu Zhang\\nThe University of Newcastle, NSW, Australia\\ntomjamesdowdell@gmail.com, hongyu.zhang@newcastle.edu.au\\nAbstract\\nThe key to a Transformer model is the self-attention\\nmechanism, which allows the model to analyze an\\nentire sequence in a computationally efﬁcient man-\\nner. Recent work has suggested the possibility\\nthat general attention mechanisms used by RNNs\\ncould be replaced by active-memory mechanisms.\\nIn this work, we evaluate whether various active-\\nmemory mechanisms could replace self-attention\\nin a Transformer. Our experiments suggest that\\nactive-memory alone achieves comparable results\\nto the self-attention mechanism for language mod-\\nelling, but optimal results are mostly achieved by\\nusing both active-memory and self-attention mech-\\nanisms together. We also note that, for some spe-\\nciﬁc algorithmic tasks, active-memory mechanisms\\nalone outperform both the self attention and a com-\\nbination of the two.\\n1 Introduction\\nThe previous state-of-the-art sequence model, the recurrent\\nneural network, has been largely supplanted by the Trans-\\nformer model [Vaswani et al. , 2017 ], which is primarily built\\natop a self-attention mechanism. Given a task to train upon,\\nthe self-attention mechanism focuses on one token per atten-\\ntion head within the entire sequence at each time-step; the\\nkey to the self-attention mechanism’s success is the mecha-\\nnism’s ability to learn which token within the entire sequence\\nto focus on in order to achieve the best results.\\nThe self-attention mechanism has proven successful on\\na variety of natural language processing tasks, but has not\\nachieved ubiquitous success. The authors of [Kaiser and Ben-\\ngio, 2016 ]pointed out that an attention mechanism would\\nlikely struggle to solve a task which required a model to fo-\\ncus on multiple tokens at a given time-step. Further, the au-\\nthors of [Kaiser and Sutskever, 2015 ]recommended that an\\nattention mechanism could be replaced by active-memory to\\nalleviate these concerns.\\nUnlike attention, active-memory allows a model to access\\nand change any and all elements of its memory at each time-\\nstep. The active-memory mechanism can access more than\\none element at each time step. In [Kaiser and Bengio, 2016 ],\\nFigure 1: The active memory mechanism. In this case, the active-\\nmemory is implemented in a unidirectional manner, with a kernel\\nsize 3.\\nthe authors used an active-memory system to translate En-\\nglish to French, and was capable of outperforming an RNN\\nmodel, both with and without an attention mechanism.\\nMotivated by the success of attention mechanism [Vaswani\\net al. , 2017 ]and active-memory [Kaiser and Bengio, 2016 ],\\nin this paper we investigate the Transformer’s self-attention\\nmechanism in comparison to a variety of active-memory\\nmechanisms. We experiment on two types of tasks: the lan-\\nguage modeling task and a set of algorithmic tasks.\\nFor the language modelling task, the self-attention mecha-\\nnism out-performs an active-memory mechanism used alone\\nby a slim margin. However, a combination of both self-\\nattention and active-memory reliably outperform both mech-\\nanisms used alone.\\nWe also evaluated the self-attention mechanism and vari-\\nous active-memory mechanisms on a variety of algorithmic\\ntasks, which can also be expressed as a sequence modeling\\ntask. Across most of the algorithmic tasks tested, the active-\\nmemory mechanisms achieve equal, or superior, results to a\\ntraditional self-attention mechanism. This would appear to\\nvindicate the hypothesis stated by [Kaiser and Bengio, 2016 ],\\nsuggesting that the nature of the attention mechanism does\\nindeed limit the effectiveness and accuracy of the model. Fi-\\nnally, we note that, for several algorithmic tasks, the mere\\naddition of the self-attention mechanism hinders results; the\\nactive-memory mechanism alone outperforms a combination\\nof the two separate mechanisms. This raises an unsolved\\nproblem; it would appear that, for deep learning sequence\\nmodels, there is still no unambiguous model that can opti-\\nmally solve all possible problems.\\n2 Related Work\\nThe Transformer model [Vaswani et al. , 2017 ]is built with\\ntwo separate modules, the self-attention mechanism and thearXiv:1912.11959v2  [cs.LG]  30 Dec 2019', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5430d76f-6244-48c6-87be-690624578208', embedding=None, metadata={'page_label': '2', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='1b0a79a8af54ac7820591d7216cccc782348d2bace6711cf8650ad6100826795', text='feedforward mechanism, which are stacked atop each other\\nfor multiple layers. The feedforward mechanism is an intra-\\nsequence analysis, where the output for each token in the\\nsequence is dependent only on the token at the same time-\\nstep, and independent of all other time-steps. On the other\\nhand, the self-attention mechanism is an inter-sequence anal-\\nysis, where the output for each time-step is dependent upon\\nthe entire sequence. The self-attention mechanism is deﬁned,\\nmathematically, as:\\nQt,Kt,Vt=xt\\nyt=concat (head 1,t,head 2,t,...,head n,t)Wo\\nhead i=Attention (QtWQ\\ni,KtWK\\ni,VtWV\\ni)\\nAttention (Q,K,V ) =softmax (QtKT\\nt√\\ndk)Vt\\nWoϵRdk∗k,d,WK,Q,V\\niϵRd,dk∗k\\nThe feed-forward module is deﬁned as:\\nyt=Wl,2(max(Wl,1xt+bl,1,0)) +bl,2\\nWl,2ϵRd,d∗4,Wl,1ϵRd∗4,d\\nThe Transformer model, and its variants [Daiet al. , 2019 ],\\nhave achieved remarkable results across a variety of natural\\nlanguage processing tasks since its inception [Zhenzhong et\\nal., 2019 ] [Delvin et al. , 2018 ] [Yang et al. , 2019b ], and are\\ncurrently investigated heavily by both academia and industry.\\nThe Neural GPU [Freivalds and Liepins, 2017 ] [Kaiser\\nand Bengio, 2016 ] [Kaiser and Sutskever, 2015 ], which in-\\ntroduced an active-memory model, achieved impressive al-\\ngorithmic results in [Kaiser and Sutskever, 2015 ], and also\\nachieved impressive machine translation results in [Kaiser\\nand Bengio, 2016 ]. A Neural GPU contains a CGRU (Con-\\nvolution Gated Recurrent Unit) module which is iterated re-\\npeatedly. This allows the entire sequence to be analyzed in\\na parallelizable and computationally efﬁcient manner. The\\nCGRU module is deﬁned as:\\nu=sigmoid (U1∗x+B1)\\nr=sigmoid (U2∗x+B2)\\ny=u⊗x+ (1−u)⊗tanh(U0∗(r⊗x) +B0))\\nwhere U * x refers to applying a convolutional operator over\\nx, using Uas a trainable kernel bank and Bis a trainable bias\\nvector. The CGRU has, since its introduction, been used in\\nother models [Resende et al. , 2016 ].\\nConvolutional operators are traditionally used for image\\nprocessing [Alom et al. , 2018 ], and have also been used in\\nrelation to sequential analysis in previous papers [Yang et al. ,\\n2019a ] [Wuet al. , 2019 ] [Gehring et al. , 2017 ] [Dauphin\\net al. , 2016 ]. To the best of our knowledge they have not\\nbeen used explicitly to replace, or augment, the self-attention\\nmechanism. The ﬁrst sequence-to-sequence model, based on\\nconvolutional operators, was, to the best of our knowledge,\\nintroduced in [Gehring et al. , 2017 ], which replaced the then-\\ntraditional LSTM block with a series of convolutions and\\ngated convolutional networks [13], and outperformed RNN-\\nbased models in terms of both speed and accuracy. However,the model introduced in [Gehring et al. , 2017 ]was followed\\nshortly afterwards by the Transformer model, which outper-\\nformed the convolutional-based model.\\nThe convolutional self-attention network [Yang et al. ,\\n2019a ]was recently introduced, and bares a passing similar-\\nity to the traditional convolutional operator described in this\\npaper. The layer of the convolutional self-attention is similar\\nto a traditional self-attention mechanism, but where the key\\nand value tensors are calculated as:\\nKh= (Kh\\ni−M/2,...,Kh\\ni,...,Kh\\ni+M/2)\\nVh= (Vh\\ni−M/2,...,Vh\\ni,...,Vh\\ni+M/2)\\nFrom this point, the convolutional self-attention mech-\\nanism acts in an identical manner to the traditional self-\\nattention mechanism. This is in direct comparison to the con-\\nvolutional operator described in this paper, which explicitly\\navoids the use of the self-attention mechanism and relies en-\\ntirely on a purely convolutional operator.\\n3 Approach\\nIn this paper, we investigate whether various active-memory\\nmechanisms could replace self-attention in a Transformer.\\nWe also evaluate the combination of self-attention and active-\\nmemory mechanisms for language modelling tasks. All the\\nactive-memory mechanisms introduced in this paper were\\ninspired by the Neural GPU, as introduced in [Kaiser and\\nSutskever, 2015 ]. The key allure of the Neural GPU is that\\nthe inputs of each time-step can be analyzed and altered, and\\nwe were inspired to apply a similar form of sequence mod-\\nelling alongside a self-attention mechanism. We describe\\nvarious convolution-based active-memory mechanisms in this\\nsection.\\n3.1 The Convolutional Operators\\nThe Traditional Convolutional Operator\\nThe ﬁrst, and most simple, active-memory mechanism is the\\nsimple convolutional operator. The traditional convolutional\\noperator was formally deﬁned in [Baiet al. , 2019 ]. If the task\\nrequires the sequence to be analyzed in a unidirectional man-\\nner, such as the case for language modelling, then a zeros-\\nvector of size k – 1 is concatenated to the left of the input\\ntensor so that, for the nthoutput token, the model only has\\naccess to the ﬁrst ninput tokens. This feature is crucial to\\navoid allowing the model ‘seeing’ forward through the se-\\nquence and having access to information that the model, in\\npractice, would not yet have. This has an identical function\\nto the masking operation of the self-attention mechanism.\\nIf the task can be analyzed in a bidirectional manner,\\nthen the model uses a convolutional ﬁlter using the SAME-\\npadding, which allows for the vector to maintain its shape\\nthroughout the convolutional operator. However, when the\\nconvolutional operator is performed in this manner, the to-\\nken at time-step tis dependent on the input tokens h[t-k/2,t+k/2] ,\\nwhere kis the kernel size.\\nThe primary ﬂaw of a convolutional operator, in compar-\\nison to a self-attention mechanism, is that, given nlayers\\nwhere each kernel has a kkernel size, each token can only', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='606d1ad4-bedf-482e-8698-24aaaadd7d7f', embedding=None, metadata={'page_label': '3', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='26f80ec3533febd5c4ef46562cd11e30111018e3518c92d90f9fef72b9c28c55', text='seek * n – n + 1 ork/2 * n - n + 1 time-steps across for unidi-\\nrectional and bidirectional tasks respectively. For example, in\\nour experiments on language modeling (Section 4), the kernel\\nsize was set to 20 and was iterated over 8 layers. Therefore, at\\neach time-step t, the ﬁnal output is capable of analyzing the\\ninput from 153 previous time-steps, well above the average\\nsequence-size (90 tokens) in the dataset. The self-attention\\nmechanism, in comparison, can see across a theoretically in-\\nﬁnite context size, even using only a single layer. Given this\\ninformation, the self-attention mechanism is capable of han-\\ndling theoretically greater long-term dependencies than the\\nactive-memory mechanism. However, in practice, the abil-\\nity of an active-memory mechanism to access and change its\\nentire memory could overcome this limitation.\\nThe convolutional operator is assisted further by the fact\\nthat the convolutional operator’s complexity grows linearly\\nwith the sequence size, while the self-attention mechanism’s\\ncomplexity grows quadratically.\\nNumerous papers have noted that, while Transformers are\\nparallelizable and capable of capturing long-range dependen-\\ncies, the Transformer network suffers from the inability of\\nmodel tokens in a recurrent manner [Wang et al. , 2019 ] [Hao\\net al. , 2019 ]. This is in direct comparison to traditional RNN\\nmodels, which can capture long-range dependencies, but can\\nstruggle to capture long-range dependencies. The use of\\nactive-memory, in theory, would accomplish this task, given\\nthat the output at time-step t htis dependent of the inputs\\nx[t-k,t] where k is the kernel size. Therefore, this operation\\ncan, in theory, model recurrence. We did not explicitly test\\nwhether this does model recurrence in practice, but will focus\\non this in future work.\\nThe convolutional operator is followed by the ReLU acti-\\nvation function.\\nThe Persistent-Convolutional Operator\\nThe Persistent-Convolutional operator is similar to the tradi-\\ntional convolutional operator described above, except that the\\nzeros vector is replaced by the a trainable vector of identi-\\ncal shape to the zeros vector. This allows the operator to,\\nidentical to the traditional convolutional operator, maintain an\\nidentical shape across the convolution. To keep parameteriza-\\ntion to a minimum, the same persistent vector is used across\\nall convolution operators in the entire model. The persistent-\\nconvolutional operator is deﬁned as:\\np∈Wkernel size−1,hidden size\\nx= [p,x],y=W∗x+b\\nwhere [.,.] denotes the concatenation function and pis the\\ntrainable persistent vector. Persistent vectors have been used\\npreviously in language modelling tasks [Sukhbaatar et al. ,\\n2019 ], but never as an augmentation for convolutional opera-\\ntors, as far as we know.\\nIf the model is to be analyzed in a bidirectional man-\\nner, rather then a unidirectional manner, then the persistent-\\nconvolutional operator can be redeﬁned as:\\np1,p2∈W(kernel size−1)//2,hidden size\\nx= [p1,x,p 2]\\nFigure 2: The Self-Attention + Convolutional Operator Transformer.\\nThe use of a persistent vector allows for the model to have a\\npermanent memory that, given the fact the vector is trainable,\\ncan be expressed in an optimal manner for the model. This is\\nthe equivalent of a permanent memory for the deep learning\\nmodel.\\nThe Highway-Convolutional Operator\\nThe Highway-Convolutional operator is based on the high-\\nway network architecture [5], which can be deﬁned as:\\na=U0∗x+B0\\nb=sigmoid (U1∗x+B1)\\ny=a⊗b+x⊗(1−b)\\nThe key allure of the highway network, as described in\\n[Srivastava et al. , 2015 ], is the fact that a highway network\\ncan be trained for a large number of layers, even hundreds\\nof layers, because information can pass, unimpeded, across\\neach layer. The authors of [Srivastava et al. , 2015 ]described\\nthese paths as ’information highways’. The use of these ’in-\\nformation highways’ allows information to pass through the\\nself-attention mechanism in an equally efﬁcient manner.\\nIn this paper, we use the hard-sigmoid function [Kaiser and\\nBengio, 2016 ]to stabilize gradients, which is deﬁned as:\\ny=max(0,min (1,1.2∗sigmoid (x)−0.1))\\n3.2 Self-Attention + Convolutional Operators\\nThe operator calculates the results of the self-attention mech-\\nanism and results of the convolutional operator indepen-\\ndently, and then adds them together to produce the ﬁnal out-\\nput of the operator. This operator would allow the model to\\nanalyze the input using both the self-attention mechanism and\\nactive-memory mechanism and decide which features from\\nboth mechanisms would be most optimal. This approach has\\nthe obvious advantage of being able to take the ‘best of both\\nworlds’, where the optimal features that can only be detected', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f90639a2-3329-4abc-94b4-a1b295006c4c', embedding=None, metadata={'page_label': '4', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='20643145356be3933ca7df23337aee52a907e2ebac9ecb11c89466d3455634cb', text='Model Loss per Token\\nCGRU 1.6834 (+0.1645)\\nConvolution 1.5358 (+0.0169)\\nPersistent-Convolution 1.5341 (+0.0152)\\nHighway-Convolution 1.5327 (+0.0138)\\nSelf-Attention 1.5189 (+0.0)\\nSelf-Attention + Convolution 1.4912 (-0.0277)\\nSelf-Attention + Persistent-Convolution 1.4905 (-0.0284)\\nSelf-Attention + Highway-Convolution 1.4869 (-0.032)\\nTable 1: The loss-per-token of the self-attention mechanism and the\\nactive-memory mechanisms on the WT3 dataset, and the difference\\nof loss between the self-attention and the active-memory mecha-\\nnisms. The lower the loss, the better the model performed. With the\\nexception of the CGRU, all purely active-memory operators achieve\\na test loss less then 1.2% higher then the self-attention mechanism.\\nThe optimal models combined the self-attention mechanism and an\\nactive-memory mechanism, and achieved a lower test loss than the\\nself-attention mechanism and active-memory mechanisms alone.\\nby the self-attention mechanism, and the optimal features that\\ncan only be detected by the convolutional operator, are both\\navailable to the model.\\nThe architecture of a single layer of the “self-attention +\\nconvolution” operator is shown in Figure 2. This architec-\\nture, without the convolutional operator, is a simple Trans-\\nformer layer. The output of the convolutional operator is\\nadded, element-wise, to the output of the self-attention mech-\\nanism. This allows, hypothetically, for the best-of-both-\\nworlds, where the model has access to the self-attention\\nmechanism and the active-memory mechanism.\\nSimilarly, we also add the self-attention mechanism\\nto the persistent-convolutional operator and the highway-\\nconvolutional operator, respectively.\\n4 Experiments\\nTo evaluate the effectiveness of the various convolution-based\\nactive-memory mechanisms, we used two separate experi-\\nments; a language modelling task that is traditionally associ-\\nated with attention-based mechanisms [Shoeybi et al. , 2019 ],\\nand algorithmic tasks that are associated with active-memory\\nmodels [Kaiser and Sutskever, 2015 ]. The active-memory\\nmechanisms are experimented both independently and along-\\nside a self-attention mechanism.\\n4.1 Language Modelling\\nExperimental Setup\\nThe ﬁrst task that the operators were tested with was\\na unidirectional language modelling task; the WikiText-3\\n(WT3) dataset [Merity et al. , 2016 ], tokenized using BPE-\\ntokenization [Sennrich et al. , 2016 ]. The WikiText-3 dataset\\nwas sourced entirely from Wikipedia articles, contains over\\n3.6 millions lines of text, and is split into a training dataset,\\nvalid dataset and test dataset. The train dataset contains 103M\\ntokens, while the valid and test dataset contain 250K tokens\\neach.\\nThe models used were all 8-layer models, with a hidden\\nsize of 256 and a ﬁlter size of 1024, a vocab size of 32,000,(x, y) 1 0 1 1 + 0 0 1 1\\n(x + y) 0 0 0 0 0 1 1 1 0\\nTable 2: The binary addition task. Given two numbers (in this case,\\nthe two numbers are 11 and 3), the ﬁnal output is the binary version\\nof the addition of the two input numbers (in this case, 14).\\nkernel size of 20 and a dropout rate of 0.9. No further regu-\\nlarization was used. The optimizer was the Adam Optimizer\\n[Kingma and Ba, 2014 ]and a warmup-learning rate was used,\\nas speciﬁed in [Vaswani et al. , 2017 ]. All models were im-\\nplemented using Tensorﬂow, version 1.07, on a V100 GPU\\ncard.\\nNotable preprocessing was used for analyzing the\\nWikiText-3 dataset; every character was explicitly denoted as\\nlower-case, each hyphen -was replaced by @-@ and punc-\\ntuation marks, such as fullstops and commas, were seperated\\nby white-space. This was done to discourage the BPE to tok-\\nenize sets of characters that included punctuation marks, forc-\\ning the model to tokenize sets of characters that were only\\nletters, therefore tokenizing a greater set of words.\\nExperimental Results\\nWith the exception of the CGRU operator, all active-memory\\nmechanisms, when combined with the self-attention mech-\\nanism, outperformed the self-attention mechanism alone,\\nachieving a lower loss-per-token. This would appear to vin-\\ndicate the proposition of both this paper and [2], suggesting\\nthat, indeed, active-memory mechanisms and self-attention\\nare comparable. However, no model that purely used an\\nactive-memory mechanism outperformed the self-attention\\nmechanism for language modelling.\\nWe note that, if the dropout rate was decreased to 0.7, all\\noperators, with the exception of CGRU, all models achieved\\nsuperior results to the self-attention mechanism at a the same\\ndropout rate. However, these models did not achieve superior\\nresults to the self-attention model with a dropout-rate of 0.9.\\nThis would imply that self-attention mechanisms are more\\nsensitive to dropout rates compared to active-memory mech-\\nanisms.\\nFurther, each operator, except for the CGRU operator, ben-\\neﬁted from combining it with self-attention, allowing both\\noperators to operate independently and concurrently. The\\nmodel with the lowest loss-per-token had a self-attention\\nmechanism and a highway-convolutional operator. It is fur-\\nther worth noting that the highway-convolutional operator\\noutperformed both other convolutional operators, both with\\nand without the addition of the self-attention mechanism.\\n4.2 Algorithmic Tasks\\nExperimental Setup\\nThe second experiment for evaluating the active-memory\\nmechanisms was on various algorithmic tasks:\\n•Reverse: Given an array Xof size L, the model is trained\\nto return the array Y, where Y[0] = X[-1] . In order to\\neffectively perform this task, the model must be capable\\nof analyzing the start of the input vector at the very end,\\nand vice-versa.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1afd121e-8966-465c-9bfc-65cc6aa81201', embedding=None, metadata={'page_label': '5', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='3486dac79454a1634092a67a8d2328c0af4d6bdd4704870aff69ea15d232f7d7', text='(x, y) 1 0 1 0 1 × 0 1 1 0 0\\n(x×y) 0 0 0 1 1 1 1 1 1 0 0\\nTable 3: The binary multiplication task. The above example contains\\ntwo input numbers, 12 and 21, and the output number 252.\\n•Sort: Given an array of randomly order integers, the\\nmodel is trained to return an array that accurately order\\nthe input integers. The entire vector must be remem-\\nbered and analyzed at each time-step.\\n•Addition: Given two binary numbers, the model is\\ntrained to return an array that represents the addition in\\nthe form of a third binary number. An example of the\\naddition task is shown in Table 2.\\n•Multiply: Multiplies two binary numbers, as shown in\\nTable 3.\\n•Not: If the input is 1, then not returns 0. Else, the not\\nfunction returns 1. The output relies only on the input at\\nthe current time-step.\\n•Remember: Given a series of random numbers of se-\\nquence sizeN, followed by a sequence of zeros of iden-\\ntical size, the model is trained to output a series of zeros\\nof sizeN, followed by the random numbers. In order\\nfor this task to be performed, the model must be able to\\nremember tokens over an increasingly long sequence.\\nAll data for the algorithmic tasks were generated in an on-\\nline manner. For three of the tasks, Sort, Addition and Mul-\\ntiply, the model must focus on multiple tokens at every time-\\nstep. In comparison, the Reverse task, the Not task and the\\nRemember task only require the model to focus on a single\\ntoken at every time-step.\\nThe model that was used for algorithmic tasks contains 4\\nlayers, with a hidden size of 128 units, a ﬁlter size of 512 and\\na kernel size of 20. Each model was trained for a maximum of\\n100 epochs, where each epoch contains 100 iterations. At the\\nend of each epoch, the model was exposed to an online batch,\\ncontaining 32 test cases. If the model achieved an accuracy of\\n100% on the online test batch, the sequence-size of the data\\nis increased, therefore increasing its complexity.\\nFor the Reverse, Sort, Not and Remember task, when the\\nmodel achieved a 100% accuracy, the sequence was increased\\nby 1. For the Addition and Multiply task, the sequence was\\nincreased by 2.\\nThe model was initially trained only for sequences that are\\n5 tokens long and was not introduced to a larger sequence un-\\ntil the model was capable of achieving 100% accuracy on this\\nsequence-size. We found that this form of curriculum learn-\\ning was essential: if a model was initially trained on a se-\\nquence of several dozen tokens, each operator was incapable\\nof achieving a reasonable accuracy.\\nThe vocabulary size was different for each task. The Re-\\nverse task had a vocabulary size of 100, while the Sort task\\nand the Remember task had a vocabulary size of 20. We noted\\nthat whenever the vocab size was increased the model would\\nachieve less accurate results. Because all tokens in the Addi-\\ntion, Multiply and Not tasks are either 0, 1, or the separator,the vocabulary size is set to 3.\\nExperimental Results\\nEach model was tested for each task, and the highest sequence\\nthat the model could achieve within 100 epochs was recorded.\\nEach experiment was performed three times, and the average\\nsequence size is presented in Table 4. For example, the self-\\nattention mechanism managed to achieve a 100% accuracy\\nfor a sequence of 41 tokens for the Reverse task, but could not\\nachieve a 100% accuracy for both the Sort task and the Addi-\\ntion task for a sequence size of 20 (the Sort task achieved\\na maximum sequence size of 14, while the Addition task\\nachieved a maximum size of 7). Of the six algorithmic tasks\\ntested, active-memory mechanisms were used, either solely\\nor in combination with the self-attention mechanism, in the\\nbest-performing model of ﬁve of these tasks. For example,\\nthe self-attention mechanism achieved an average sequence\\nsize of 41.0 for the Reverse Task and 14.0 for the Sort Task,\\nwhich are lower than those achieved by the “self-attention\\n+ persistent-convolution” mechanism (43.7 and 23.3, respec-\\ntively). Furthermore, for the Addition and Multiply Tasks, the\\nactive-memory mechanisms across the board outperformed\\nboth the self-attention mechanism and the combination of the\\nself-attention mechanism and the active-memory mechanism.\\nFor example, the traditional convolution operator, for the Ad-\\ndition Task, outperformed the self-attention mechanism and\\nthe “self-attention + convolutional” mechanism by 34.0 and\\n4.7 respectively. The results show that the active-memory\\nmechanisms achieve equal, or superior, results to a traditional\\nself-attention mechanism.\\nSelf-attention, used alone, only performed optimally on the\\nRemember task, and equally well on the Not task. Interest-\\ningly, across all models for the Addition and Multiply tasks,\\nthe self-attention mechanism reliably led to poor results;\\nnot only does the self-attention mechanism, alone, achieve\\nthe poorest results, but the combination of the self-attention\\nmechanism and any active-memory system performed worse\\nthen the active-memory system alone. This is in direct con-\\ntrast to the Sort task and the Reverse task, where the com-\\nbination of self-attention mechanism and the active-memory\\nachieve the best results.\\nThe self-attention mechanism would, in theory, outperform\\nactive-memory mechanisms for the Remember task. This is\\nbecause, in order to adequately perform the Remember task,\\nthe model must be capable of calculating an output based\\non long-range dependencies, which active-memory cannot\\nmatch at a large enough sequence length. Other tasks do\\nrequire a long-range dependency in order to operate well at\\nlarge sequence sizes, but are dependent on the model per-\\nforming other tasks as well. For example, the addition task re-\\nquires to model long range-dependencies and perform binary\\naddition. The self-attention mechanism, although it can learn\\nthese long-range dependencies, cannot access all necessary\\ntokens at a given time to adequately perform binary addition.\\nThis is vindicated by the experimental results. In Table 4, the\\nself-attention mechanism achieved the highest results on the\\nRemember task. This would suggest that, if the algorithmic\\ntask only requires a long-range dependency, then the self-\\nattention mechanism will outperform active-memory mech-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d63dc621-f44c-4d74-b0d8-ef87ac5bb0dc', embedding=None, metadata={'page_label': '6', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='a315150ba7609afab285b5b5e7133ed6eeaee8fa3d9827c9d12485ea3e2d23f7', text='Model Reverse Sort Addition Multiply Not Remember\\nCGRU 7.7 5.7 16.3 9.0 104.0 9.0\\nSelf-Attention 41.0 14.0 7.0 7.0 104.0 57.0\\nConvolution 17.7 20.7 41.0 17.0 104.0 36.0\\nPersistent-Convolution 25.0 20.3 38.3 16.3 104.0 35.0\\nHighway-Convolution 19.7 16.7 35.7 13.7 104.0 33.0\\nSelf-Attention + Convolution 41.0 23.3 36.3 12.3 104.0 26.0\\nSelf-Attention + Persistent-Convolution 43.7 23.3 35.0 12.3 104.0 27.0\\nSelf-Attention + Highway-Convolution 41.0 20.0 34.3 11.7 104.0 24.7\\nTable 4: The average sequence length that each operator was capable of gaining 100% accuracy within 100 epochs over 3 runs. The\\nhigher the sequence size, the better the model learned. For the Reverse and Sort tasks, the combination of self-attention mechanism and\\npersistent-convolution achieved the best results. For the Sort, Addition, and Multiply tasks, the self-attention mechanism was beaten by the\\nactive-memory mechanisms. For the Addition and Multiply tasks, the mere use of a self-attention mechanism alongside an active-memory\\nmechanism actively decreased results. The highest possible sequence that can be learned over these epochs is 104 in the Not task. The\\nself-attention mechanism achieved the best result only for the Remember task.\\nanisms when used alone. In comparison, the self-attention\\nmechanism is incapable of matching the results of active-\\nmemory for all other tasks. These ﬁndings appear to vindi-\\ncate the statement made by Kaiser et. al [2]; whenever the\\nsequential task requires the model to focus on multiple to-\\nkens at every time-step, using an attention mechanism will\\nlead to extremely poor results, especially in comparison to\\nactive-memory models.\\nIt is worth noting that, for each of the active-memory mech-\\nanisms operating alone, none of the three achieved a 100%\\naccuracy for any sequence over a size of 37 for the Remem-\\nber task. This is because, given the kernel size of 20 and\\n4 layers, the model is only capable of seeing 37 time-steps\\nacross. Therefore, the model cannot see 37 time-steps across\\nand, therefore, cannot perform the Remember task at this se-\\nquence size or any larger sequence size. This displays the im-\\nportance of utilizing both a self-attention mechanism, which\\ncan be utilized for analyzing long-range dependencies, and\\nan active-memory mechanism, which can extract features that\\nthe self-attention mechanism cannot.\\n4.3 Discussion of Results\\nThe experiments above suggest that, across most tasks, a\\ncombination of a self-attention mechanism and an active-\\nmemory mechanism, at worst, perform comparably to a\\npurely attention-based model, and at best surpass an attention\\nmodel, with the exception of the Remember task. However,\\nfor some algorithmic tasks, we note that the mere inclusion\\nof a self-attention mechanism actively hinders performance.\\nModels that combine both the attention mechanism and\\nactive-memory mechanisms outperformed both attention-\\nonly and active-memory-only models for language mod-\\nelling. This suggests that, for language modelling tasks,\\nboth active-memory mechanisms and attention mechanisms\\nare capable of extracting features that the other mechanism is\\nnot capable of extracting, and that both mechanisms operate\\noptimally when used alongside each other.\\nThe ﬁndings are further abstracted by studying the effect of\\nvarious algorithmic tasks; in cases where only a single token\\nneeds to be focused on, the self-attention mechanism matches\\nthe most ardent active-memory, while active-memory mecha-nisms radically outperform self-attention for other tasks. This\\nwould imply that various time-dependencies that cannot be\\nanalyzed by a self-attention mechanism can be analyzed by\\nactive-memory.\\nIt is worth noting that, for the Not function, all models learn\\noptimally. This is likely due to the fact that the output of each\\ntime-step depends only on the input at this time-step, and each\\nmodel can analyze this dependency equally efﬁciently. Also,\\nbased on the results of the Remember task, the self-attention\\nmechanism can attain greater long-range dependency in com-\\nparison to the active-memory mechanisms.\\nFinally, we note that, for the Remember function, both\\nmechanisms, when used alone, outperform the two mecha-\\nnisms used together. For every other task, a combination of\\nthe self-attention and active-memory would improve upon at\\nleast one of the mechanisms when used alone. We are unsure\\nexactly what has led to this result. This will require further\\ninvestigation in the future.\\n5 Conclusion\\nIn this paper we investigate the Transformer’s self-attention\\nmechanism in comparison to a variety of active-memory\\nmechanisms. We experiment on two types of tasks: the\\nlanguage modeling task and the algorithmic task. Our re-\\nsults show that the self-attention mechanism can be improved\\nby an active-memory mechanism alone or by a combina-\\ntion of the two. Our results have implications for wider se-\\nquence modeling tasks, which are currently dominated by\\nself-attention based models.\\nOur code and models used in experiments are available at:\\nhttps://github.com/Anon-111/Active-Memory .\\nIn the future, we will further explore the use of active-\\nmemory for sequence-to-sequence tasks, such as machine\\ntranslation. We will also analyze the empirical differences\\nbetween the studied algorithmic tasks, and investigate why\\nthe self-attention mechanism may assist one task but harm\\nanother.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0f729b8e-76e1-4caf-a3df-e1e0c748ac73', embedding=None, metadata={'page_label': '7', 'file_name': '1912.11959v2.pdf', 'file_path': 'pdfs\\\\1912.11959v2.pdf', 'file_type': 'application/pdf', 'file_size': 399865, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='d41a1e17554070c02784e7f184c5c00bac6bca6dc197488e6249af6267337eaa', text='References\\n[Alom et al. , 2018 ]Md Zahangir Alom, Tarek M. Taha,\\nChristopher Yakopcic, Stefan Westbery, Paheding Sidike,\\nMst Sharmina Nasrin, Brian C Van Esesn, Abdul A S.\\nAwwal, and Vijayan K. Asari. The history began from\\nalexnet: A comprehensive survey on deep learning ap-\\nproaches. arXiv preprint arXiv:1803.01164 , 2018.\\n[Baiet al. , 2019 ]Shaojie Bai, J. Zico Kolter, and Vladlen\\nKoltun. An empirical evaluation of generic convolutional\\nand recurrent networks for sequence modeling. arXiv\\npreprint arXiv:1803.01271 , 2019.\\n[Daiet al. , 2019 ]Zihang Dai, Zhilin Yang, Yiming Yang,\\nJaime Carbonell, Quoc V . Le, and Ruslan Salakhutdi-\\nnov. Transformer-XL: Attentive language models beyond\\na ﬁxed-length context. arXiv preprint arXiv:1901.02860 ,\\n2019.\\n[Dauphin et al. , 2016 ]Yann N. Dauphin, Angela Fan,\\nMichael Auli, and David Grangier. Language model-\\ning with gated convolutional networks. arXiv preprint\\narXiv:1612.08083 , 2016.\\n[Delvin et al. , 2018 ]Jacob Delvin, Ming-Wei Chang, Ken-\\nton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understand-\\ning.arXiv preprint arXiv:1810.04805 , 2018.\\n[Freivalds and Liepins, 2017 ]Karlis Freivalds and Renars\\nLiepins. Improving the neural gpu architecture for algo-\\nrithm learning. arXiv preprint arXiv:1702.08727 , 2017.\\n[Gehring et al. , 2017 ]Jonas Gehring, Michael Auli, David\\nGrangier, Denis Yarats, and Yann Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint\\narXiv:1705.03122 , 2017.\\n[Haoet al. , 2019 ]Jie Hao, Xing Wang, Baosong Yang,\\nLongyue Wang, Jinfeng Zhang, and Zhaopeng Tu.\\nModeling recurrence for transformer. arXiv preprint\\narXiv:1904.03092 , 2019.\\n[Kaiser and Bengio, 2016 ]Lukasz Kaiser and Samy Bengio.\\nCan active memory replace attention. arXiv preprint\\narXiv:1610.08613 , 2016.\\n[Kaiser and Sutskever, 2015 ]Lukasz Kaiser and Ilya\\nSutskever. Neural gpus learn algorithms. arXiv preprint\\narXiv:1511.08228 , 2015.\\n[Kingma and Ba, 2014 ]Diederik P. Kingma and Jimmy Ba.\\nAdam: A method for stochastic optimization. arXiv\\npreprint arXiv:1412.6980 , 2014.\\n[Merity et al. , 2016 ]Stephen Merity, Caiming Xiong, James\\nBradbury, and Richard Socher. Pointer sentinel mixture\\nmodels. arXiv preprint arXiv:1609.07843 , 2016.[Resende et al. , 2016 ]Danilo J. Resende, Shakir Mohamed,\\nIvo Danihelka, Karol Gregor, and Daan Wiestra. One shot\\ngeneralization in deep generative models. arXiv preprint\\narXiv:1603.05106 , 2016.\\n[Sennrich et al. , 2016 ]Rico Sennrich, Barry Haddow, and\\nAlexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 ,\\n2016.\\n[Shoeybi et al. , 2019 ]Mohammad Shoeybi, Mostofa Pat-\\nwary, Raul Puri, Patrick LeGresley, Jared Casper, and\\nBryan Catanzaro. Megatron-lm: Training multi-billion pa-\\nrameter language models using model parallelism. arXiv\\npreprint arXiv:1909.08053 , 2019.\\n[Srivastava et al. , 2015 ]Rupseh Kumar Srivastava, Klaus\\nGreff, and J ¨urgen Schmidhuber. Highway networks. arXiv\\npreprint arXiv:1505.00387 , 2015.\\n[Sukhbaatar et al. , 2019 ]Sainbayar Sukhbaatar, Edouard\\nGrave, Guillaume Lample, Herve Jegou, and Armand\\nJoulin. Augmenting self-attention with persistent memory.\\narXiv preprint arXiv:1907.01470 , 2019.\\n[Vaswani et al. , 2017 ]Ashish Vaswani, Noam Shazeer, Niki\\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\\nneed. arXiv preprint arXiv:1706.03762 , 2017.\\n[Wang et al. , 2019 ]Zhiwei Wang, Yao Ma, Zitao Liu, and\\nJiliang Tang. R-transformer: Recurrent neural network\\nenhanced transformer. arXiv preprint arXiv:1907.05572 ,\\n2019.\\n[Wuet al. , 2019 ]Felix Wu, Angela Fan, Alexei Baevski,\\nYann N. Dauphin, and Michael Auli. Pay less attention\\nwith lightweight and dynamic attention. arXiv preprint\\narXiv:1901.10430 , 2019.\\n[Yang et al. , 2019a ]Baosong Yang, Longyue Wang,\\nDerek F. Wong, Lidia S. Chao, and Zhaopeng Tu.\\nConvolutional self-attention networks. arXix preprint\\narXiv:1904.03107 , 2019.\\n[Yang et al. , 2019b ]Zhilin Yang, Zihang Dai, Yiming Yang,\\nJaime Carbonell, Ruslan Salakhutdinov, and Quoc V . Le.\\nXlnet: Generalized autoregressive pretraining for lan-\\nguage understanding. arXiv preprint arXiv:1906.08237 ,\\n2019.\\n[Zhenzhong et al. , 2019 ]Lan Zhenzhong, Mingda Chen, Se-\\nbastian Goodman, Kevin Gimpel, Piyush Sharma, and\\nRadu Soricut. Albert: A lite bert for self-supervised\\nlearning of language representations. arXiv preprint\\narXiv:1909.11942 , 2019.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='530922f8-65e5-4e48-8980-8cefb72c792b', embedding=None, metadata={'page_label': '1', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='b624fc55dbf835eb3678e85c5dbad12ae47f08e2a6011bdd592e3b82626554e9', text='All the attention you need:\\nGlobal-local, spatial-channel attention for image retrieval\\nChull Hwan Song\\nOdd ConceptsHye Joo Han\\nOdd ConceptsYannis Avrithis\\nInria, Univ Rennes, CNRS, IRISA\\nAbstract\\nWe address representation learning for large-scale\\ninstance-level image retrieval. Apart from backbone, train-\\ning pipelines and loss functions, popular approaches have\\nfocused on different spatial pooling and attention mecha-\\nnisms, which are at the core of learning a powerful global\\nimage representation. There are different forms of attention\\naccording to the interaction of elements of the feature tensor\\n(local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses\\nonly one or two forms of attention and applies it to different\\nproblems like classiﬁcation, detection or retrieval.\\nWe present global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network and\\nincorporates all four forms of attention: local and global,\\nspatial and channel. We obtain a new feature tensor and, by\\nspatial pooling, we learn a powerful embedding for image\\nretrieval. Focusing on global descriptors, we provide em-\\npirical evidence of the interaction of all forms of attention\\nand improve the state of the art on standard benchmarks.\\n1. Introduction\\nInstance-level image retrieval is at the core of visual rep-\\nresentation learning and is connected with many problems\\nof visual recognition and machine learning, for instance\\nmetric learning [30, 26], few-shot learning [42] and unsu-\\npervised learning [8]. Many large-scale open datasets [3,\\n37, 16, 29, 53], and competitions1have accelerated progress\\nin instance-level image retrieval, which has been trans-\\nformed by deep learning [3].\\nMany studies on instance-level image retrieval focus\\non learning features from convolutional neural networks\\n(CNN), while others focus on re-ranking , for instance by\\ngraph-based methods [11]. The former can be distinguished\\naccording to feature types: local descriptors , reminiscent of\\nSIFT [27], where an image is mapped to a few hundred vec-\\ntors; and global descriptors , where an image is mapped to a\\n1https://www.kaggle.com/c/landmark-retrieval-2020single vector. In fact, deep learning has brought global de-\\nscriptors with astounding performance, while allowing efﬁ-\\ncient search. Our study belongs to this type.\\nStudies on global descriptors have focused on spatial\\npooling [2, 37]. The need for compact, discriminative rep-\\nresentations that are resistant to clutter has naturally given\\nrise to spatial attention methods [24, 28]. Different kinds\\nof attention have been studied in many areas of computer\\nvision research. There is also channel attention [20, 9]; lo-\\ncal attention , applied independently to elements of the rep-\\nresentation (feature map) [54, 25]; global attention , based\\non interaction between elements [52, 9]; and combinations\\nthereof. Unfortunately, each study has been limited to one or\\ntwo kinds of attention only; attention is not always learned;\\nand applications vary.\\nIt is the objective of our work to perform a compre-\\nhensive study of all forms of attention above, apply them\\nto instance-level image retrieval and provide a detailed ac-\\ncount of their interaction and impact on performance. As\\nshown in Figure 1, we collect contextual information from\\nimages with both local andglobal attention, giving rise to\\ntwo parallel network streams. Importantly, each operates\\non both spatial locations andfeature channels . Local at-\\ntention is about individual locations and channels; global is\\nabout interaction between locations and between channels.\\nThe extracted information is separately embedded in local\\nand global attention feature maps, which are combined in a\\nglobal-local attention feature map before pooling.\\nOur contributions can be summarized as follows:\\n1. We propose a novel network that consists of both\\nglobal and local attention for image retrieval. This is\\nthe ﬁrst study that employs both mechanisms.\\n2. Each of the global and local attention mechanisms\\ncomprises both spatial and channel attention.\\n3. Focusing on global descriptors, we provide empirical\\nevidence of the interaction of all forms of attention and\\nimprove the state of the art on standard benchmarks.\\n1arXiv:2107.08000v1  [cs.CV]  16 Jul 2021', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3330ec81-ab0d-484d-81df-7b4de04312ea', embedding=None, metadata={'page_label': '2', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='947525f560903fc4e631aa81372e24c733ddd6c401d5a258ba532d250b93b908', text='Al\\nc\\nc×1×1× +Fl\\nc Al\\ns\\n1×h×w× +Fl\\n×\\nc×h×wF\\n× +\\nc×h×wFgl\\nAg\\nc\\nc×c×Fg\\ncAg\\ns\\nhw×hw× +Fg\\n×wl\\nw\\nwg\\nchannel attention spatial attentionfusionlocal attention\\nglobal attention\\nFigure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\\ntion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\\n(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\\nc),\\nlocal spatial (Al\\ns),global channel (Ag\\nc) and global spatial (Ag\\ns). The input feature map Fis weighted into local ( Fl) and\\nglobal ( Fg) attention feature maps, which are fused with Fto yield the global-local attention feature map Fgl. The diagram\\nis abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\\n2. Related work\\nInstance-level image retrieval Studies on instance-level\\nimage retrieval can be roughly, but not exclusively, di-\\nvided into three types: (1) studies on global descriptors\\n[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\\ngeometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\\nby graph-based methods [11, 21, 55]. The ﬁrst two types\\nof studies focus on the feature representation, while the last\\ntype focuses on re-ranking extracted features.\\nStudies on global descriptors focus on spatial pooling\\nof CNN feature maps into vectors, including MAC [38],\\nSPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\\nand NetVLAD [1, 25], as well as learning the representa-\\ntion[3, 15, 16, 36, 37]. Studies before deep learning dom-\\ninated image retrieval were mostly based on local descrip-\\ntorslike SIFT [27] and bag-of-words representation [32] or\\naggregated descriptors like VLAD [22] or ASMK [46]. Lo-\\ncal descriptors have been revived in deep learning, e.g. with\\nDELF [29], DELG [5] and ASMK extensions [45, 47].\\nWe focus on learning a global descriptor in this work, be-\\ncause it is the most efﬁcient in terms of storage and search.\\nHowever, our generic attention mechanism produces a fea-\\nture tensor and could be applicable to local descriptors as\\nwell, if global pooling were replaced by local feature detec-\\ntion. Re-ranking methods are complementary to the repre-\\nsentation and we do not consider them in this work.\\nAttention Attention mechanisms have been ﬁrst proposed\\ninimage classiﬁcation studies focusing on channel at-METHODLOCAL GLOBALLRNRET\\nSpatial Channel Spatial Channel\\nSENet [20] ✓ ✓\\nECA-Net [51] ✓ ✓\\nGCNet [6] ✓ ✓\\nCBAM [54] ✓ ✓ ✓\\nGE [19] ✓ ✓\\nNL-Net [52] ✓ ✓\\nAA-Net [4] ✓ ✓\\nSAN [59] ✓ ✓\\nN3Net [34] ✓ ✓\\nA2-Net [9] ✓ ✓\\nGSoP [14] ✓ ✓\\nOnA [23] ✓ ✓\\nAGeM [17] ✓ ✓\\nCroW [24] ✓ ✓ ✓\\nCRN [25] ✓ ✓ ✓\\nDELF [29] ✓ ✓ ✓\\nDELG [5] ✓ ✓ ✓\\nTolias et al. [47] ✓ ✓ ✓\\nSOLAR [28] ✓ ✓ ✓\\nOurs ✓ ✓ ✓ ✓ ✓ ✓\\nTable 1: Related work on attention. LRN: learned; RET: ap-\\nplied to instance-level image retrieval.\\ntention [20, 51, 6], spatial attention [19] or both, like\\nCBAM [54]. In image retrieval , CroW [24] also employs\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e73e0a23-1343-4d73-9ff3-29d676927b37', embedding=None, metadata={'page_label': '3', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='ad3da50b8030ddbab40d085b8831e59c8801e6eebe7dec86b0e0afc9edc8f3ac', text='feature map\\nGAP\\nconv1d( k)\\nsigmoid\\nattention mapc×h×w\\nc×1×1\\nc×1×1F\\nAl\\nc\\nFigure 2: Local channel attention.\\nboth spatial and channel attention and can be seen as a pre-\\ncursor of CBAM, but, like other studies of spatial attention\\non retrieval [41, 23, 17], it is not learned. CRN [25] ap-\\nplies spatial attention for feature reweighting and is learned.\\nLearned spatial attention mechanisms are common for local\\ndescriptors [29, 5, 47].\\nWe call the above methods local attention , in the sense\\nthat elements of the feature tensor (channels / spatial loca-\\ntions), are weighted independently, based on contextual in-\\nformation obtained by pooling or learned. By constrast, by\\nglobal attention we refer to mechanisms that model inter-\\naction between elements of the feature tensor, for example\\nbetween channels or between locations.\\nInimage classiﬁcation ,non-local neural network (NL-\\nNet) [52] is maybe the ﬁrst global attention mechanism, fol-\\nlowed by similar studies [4, 59, 34]. It is global spatial at-\\ntention , allowing interaction between any pair of spatial lo-\\ncations. Similarly, there are studies of global channel atten-\\ntion, allowing interaction between channels [9, 14]. Global\\nattention has focused mostly on image recognition and has\\nbeen applied to either spatial or channel attention so far, not\\nboth. In image retrieval , SOLAR [28] is a direct application\\nof the global spatial attention mechanism of [52].\\nTable 1 attempts to categorize related work on atten-\\ntion according to whether attention is local or global, spa-\\ntial or channel, whether it is learned and whether it is ap-\\nplied to instance-level image retrieval. We observe that all\\nmethods limit to one or two forms of attention only. Of\\nthose studies that focus on image retrieval, many are not\\nlearned [23, 17, 24], and of those that are, some are de-\\nsigned for local descriptors [29, 47].\\nBy contrast, we provide a comprehensive study of all\\nforms of attention, global and local, spatial and channel, to\\nobtain a learned representation in the form of a tensor that\\ncan be used in any way. We spatially pool it into a global\\ndescriptor and we study the relative gain of different forms\\nof attention in image retrieval.feature map\\nconv1×1\\nconv3×3 conv5×5 conv7×7\\nconcat\\nconv1×1\\nattention mapc×h×w\\n4c′×h×w\\n1×h×wc′×h×w\\ndilated\\nconvF\\nF′\\nAl\\ns\\nFigure 3: Local spatial attention. Convolutional layers in\\nblue implemented by dilated convolutions with kernel size\\n3×3and dilation factors 1,3,5.\\n3. Global-local attention\\nWe design a global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network. Figure 1\\nillustrates its main components. We are given a c×h×w\\nfeature tensor F, where cis the number of channels, and\\nh×wis the spatial resolution. Local attention collects con-\\ntext from the image and applies pooling to obtain a c×1×1\\nlocal channel attention map Al\\ncand a 1×h×wlocal spa-\\ntial attention map Al\\ns. Global attention allows interaction\\nbetween channels, resulting in a c×cglobal channel at-\\ntention map Ag\\nc, and between spatial locations, resulting in\\nahw×hwglobal spatial attention map Ag\\ns. The feature\\nmaps produced by the two attention streams are combined\\nwith the original one by a learned fusion mechanism into\\ntheglobal-local attention feature map Fglbefore being spa-\\ntially pooled into a global image descriptor.\\n3.1. Local attention\\nWe extract an 1D channel and a 2D spatial attention map\\nto weigh the feature map in the corresponding dimensions.\\nLocal channel attention Following ECA-Net [51], this\\nattention captures local channel information. As shown in\\nFigure 2, we are given a c×h×wfeature tensor Ffrom our\\nbackbone. We ﬁrst reduce it to a c×1×1tensor by global\\naverage pooling (GAP). Channel attention is then captured\\nby a 1D convolution of kernel size kalong the channel di-\\nmension, where kcontrols the extent of cross-channel inter-\\naction. This is followed by a sigmoid function, resulting in\\nthec×1×1local channel attention map Al\\nc.\\nLocal spatial attention Inspired by the inception mod-\\nule [43] and similar to [25], this attention map captures local\\nspatial information at different scales. As shown in Figure 3,\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aeb904bc-80ca-467c-8758-e075cfb5c3c2', embedding=None, metadata={'page_label': '4', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='b4c4c24bb228e289f05db71639bdd907366bfe6372ef45e0e1ade4715c69c5c7', text='feature map GAP\\nconv1d( k) conv1d( k)\\nsigmoid sigmoid\\n×\\n× softmax\\nattention feature map1×c 1×c\\n1×c Qc\\nc×chw×c Vc\\nAg\\nc\\nc×h×w1×c\\n1×c KcF\\nGc\\nFigure 4: Global channel attention.\\ngiven the same c×h×wfeature tensor Ffrom our back-\\nbone, we obtain a new tensor F′with channels reduced to\\nc′, using a 1×1convolution. We then extract local spatial\\ncontextual information using convolutional ﬁlters of kernel\\nsize3×3,5×5, and 7×7, which are efﬁciently imple-\\nmented by 3×3dilated convolutions [7, 57] with dilation\\nparameter 1, 2, and 3 respectively. The resulting features,\\nalong with one obtained by 1×1convolution on F′, are\\nconcatenated into a 4c′×h×wtensor. Finally, we obtain\\nthe1×h×wlocal spatial attention map Al\\nsby a 1×1\\nconvolution that reduces the channel dimension to 1.\\nThe middle column of Figure 6 shows heat maps of local\\nspatial attention, localizing target objects in images.\\nLocal attention feature map We use the local channel\\nattention map Al\\ncto weigh Fin the channel dimension\\nFl\\nc:=F⊙Al\\nc+F. (1)\\nWe then use local spatial attention map Al\\nsto weigh Fl\\nc\\nin the spatial dimensions, resulting in the c×h×wlocal\\nattention feature map\\nFl=Fl\\nc⊙Al\\ns+Fl\\nc. (2)\\nHere,A⊙Bdenotes an element-wise multiplication of ten-\\nsorsAandB, with broadcasting when one tensor is smaller.\\nWe adopt the choice of applying channel followed by spa-\\ntial attention from convolutional block attention module\\nCBAM [54]. However, apart from computing Al\\nsat differ-\\nent scales, both attention maps are obtained from the orig-\\ninal tensor Frather than sequentially. In addition, both (1)\\nand (2) include residual connections, while CBAM includes\\na single residual connection over both steps.\\n3.2. Global attention\\nWe extract two matrices capturing global pairwise chan-\\nnel and spatial interaction to weigh the feature map.feature map\\nconv1×1 conv1×1 conv1×1\\n×\\n× softmax\\nconv1×1\\nattention feature mapc′×hw Qs\\nhw×hw\\nc×h×wc′×hw Vs\\nc′×h×wAg\\nsc×h×w\\nc′×hw KcF\\nGs\\nFigure 5: Global spatial attention.\\nGlobal channel attention We introduce a global channel\\nattention mechanism that captures global channel interac-\\ntion. This mechanism is based on the non-local neural net-\\nwork [52], but with the idea of 1D convolution from ECA-\\nNet [51]. As shown in Figure 4, we are given the c×h×w\\nfeature tensor Ffrom our backbone. We apply GAP and\\nsqueeze spatial dimensions, followed by a 1D convolution\\nof kernel size kand a sigmoid function, to obtain 1×cquery\\nQcandkeyKctensors. The value tensorVcis obtained by\\nmere reshaping of Ftohw×c, without GAP. Next, we form\\nthe outer product of KcandQc, followed by softmax over\\nchannels to obtain a c×cglobal channel attention map\\nAg\\nc= softmax( Kc⊤Qc). (3)\\nFinally, this attention map is multiplied with Vcand the ma-\\ntrix product VcAg\\ncis reshaped back to c×h×wto give the\\nglobal channel attention feature map Gc. In GSoP [14] and\\nA2-Net [9], a c×cglobal channel attention map is obtained\\nby multiplication of hw×cmatrices; (3) is more efﬁcient,\\nusing only an outer product of 1×cvectors.\\nGlobal spatial attention Since ordinary convolution ap-\\nplies only a local neighborhood at a time, it cannot capture\\nglobal contextual information. Thus, we apply non-local ﬁl-\\ntering [52], which is a form of self-attention [49] in the spa-\\ntial dimensions. As shown in Figure 5, we are given the\\nsame c×h×wfeature tensor Ffrom our backbone. By\\nusing three 1×1convolutions, which reduce channels to c′,\\nand ﬂattening spatial dimensions to hw, we obtain c′×hw\\nqueryQs,keyKs, and valueVstensors, where each col-\\numn is a feature vector corresponding to a particular spatial\\nlocation. We capture pairwise similarities of these vectors\\nby matrix multiplication of KsandQs, followed by soft-\\nmax over locations to obtain a hw×hwglobal spatial at-\\ntention map :\\nAg\\ns= softmax( K⊤\\nsQs). (4)\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3f1e825b-6f63-4fad-a422-da353909ef9c', embedding=None, metadata={'page_label': '5', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='f3b5c3aed2a4779ebaa4f097fa401a46883112875b71c0e541c37b3252f5b173', text='This attention map is multiplied with Vsand the matrix\\nproduct VsAg\\nsis reshaped back to c′×h×wby expanding\\nthe spatial dimensions. Finally, using a 1×1convolution,\\nwhich increases channels back to c, we obtain the c×h×w\\nglobal spatial attention feature map Gs.\\nThe right column of Figure 6 shows heat maps for global\\nspatial attention, localizing target objects in images.\\nGlobal attention feature map We use the global channel\\nattention feature map Fcto weigh Felement-wise\\nFg\\nc=F⊙Gc. (5)\\nWe then use global spatial attention feature map Gsto\\nweighFg\\ncelement-wise, resulting in the c×h×wglobal\\nattention feature map\\nFg=Fg\\nc⊙Gs+Fg\\nc. (6)\\nSimilarly to Flin (1) and (2), we apply channel attention\\nﬁrst, followed by spatial attention. However, unlike (1),\\nthere is no residual connection in (5). This choice is sup-\\nported by early experiments.\\n3.3. Global-local attention\\nFeature fusion As shown in Figure 1, we combine the\\nlocal and global attention feature maps, FlandFg, with\\nthe original feature F. While concatenation and summation\\nare common operations for feature combination, we use a\\nweighted average with weights wl,wg,wrespectively, ob-\\ntained by softmax over three learnable scalar parameters, to\\nobtain a c×h×wglobal-local attention feature map\\nFgl=wlFl+wgFl+wF. (7)\\nEfﬁcientDet [44] has shown that this is the most effective,\\namong a number of choices, for fusion of features across\\ndifferent scales.\\nPooling We apply GeM [37], a learnable spatial pooling\\nmechanism, to feature map Fgl(7), followed by a fully-\\nconnected (FC) layer with dropout and batch normalization.\\nThe ﬁnal embedding is obtained by ℓ2-normalization.\\n4. Experiments\\n4.1. Datasets\\nTraining set There are a number of open landmark\\ndatasets commonly used for training in image retrieval stud-\\nies, including neural code (NC) [3], neural code clean (NC-\\nclean) [16], as well as Google Landmarks v1 (GLDv1) [29]\\nand v2 (GLDv2) [53]. Table 2 shows relevant statistics.\\nThese datasets can be categorized into noisy and clean. The\\nclean sets were obtained from the original noisy sets for\\nmore effective training [16, 53]. The original noisy datasets\\nare much larger, but they have high intra-class variability.\\n(a) input (b) local (c) global\\nFigure 6: Local and global spatial attention . Left: input\\nimages. Middle: local spatial attention heat maps. Right:\\nglobal spatial attention heat maps. Red (blue) means higher\\n(lower) attention weight.\\nEach class can include visually dissimilar images such as\\nexterior and interior views of a building or landmark, in-\\ncluding ﬂoor plans and paintings inside. The clean datasets\\nfocus on views directly relevant to landmark recognition but\\nhave a much smaller number of images.\\nEvaluation set and metrics We use four common eval-\\nuation datasets for landmark image retrieval: Oxford5k\\n(Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox-\\nford (ROxford orROxf) and Paris (RParis orRPar) [35].\\nROxford andRParis are used with and without one million\\ndistractors (R1M) [28] and evaluated using the Medium and\\nHard protocols [35]. We evaluate using mean Average Pre-\\ncision (mAP) and mean precision at 10 (mP@10).\\n4.2. Implementation details\\nWe train on 8 TITAN RTX 2080Ti GPUs. All models are\\npre-trained on ImageNet [39] and implemented in PyTorch\\n[31]. For fair comparisons, we set a training environment\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='adc89e72-4033-44fa-82b7-aa2b1f0885e4', embedding=None, metadata={'page_label': '6', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='515990f84628399fa7a265e73dfcd82cdd898ac29c5ad47dd58c833e141ba991', text='Figure 7: Examples of our ranking results. In each row, the ﬁrst image on the left (pink dotted outline) is a query image with a\\ntarget object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images\\nfor the query; red solid outline: negative.\\nsimilar to the those of compared studies [56, 53, 28, 35]. We\\nemploy ResNet101 [18] as a backbone model. The kernel\\nsizekof ECANet in subsection 3.1 is set to 3. The param-\\neterpof GeM in subsection 3.3 is set to 3 and the dimen-\\nsiondof ﬁnal embeddings to 512. We adopt ArcFace [10],\\na cosine-softmax based loss, with a margin of 0.3. We use\\nstochastic gradient descent with initial learning rate 10−3,\\nmomentum 0.9 and weight decay 10−5.\\nWe adopt the batch sampling of Yokoo et al. [56] where\\nmini-batch samples with similar aspect ratios are resized to\\na particular size. Here, we use a batch size of 64. For image\\naugmentation, we apply scaling, random cropping, and var-\\nied illumination. At inference, we apply a multi-resolution\\nrepresentation [16] to query and database images.\\nOur method is denoted as GLAM ( global-local atten-\\ntion module ). Using the backbone model alone is referred\\nto as baseline . It is compatible with recent models based\\non ResNet101-GeM trained with ArcFace [53, 28]. Adding\\nour local attention (subsection 3.1) to the baseline model is\\ndenoted +local , while adding our global attention (subsec-\\ntion 3.2) is denoted +global . Since we focus on representa-\\ntion learning, we do not consider post-processing methods\\nlike geometry-based re-ranking [29, 40, 53] or graph-based\\nre-ranking [11, 21, 55].\\n4.3. Benchmarking\\nNoisy vs. clean training sets We begin by training our\\nbest model (baseline+local+global) on all training sets of\\nTable 2, except NC-noisy because some images are cur-\\nrently unavailable. As shown in Table 3, even thoughTRAIN SET #IMAGES #CLASSES\\nNC-noisy 213,678 672\\nNC-clean 27,965 581\\nSfM-120k 117,369 713\\nGLDv1-noisy 1,225,029 14, 951\\nGLDv2-noisy 4,132,914 203,094\\nGLDv2-clean 1,580,470 81,313\\nTable 2: Statistics of different training sets.\\nMETHOD TRAIN SET DIM OXF5KPAR6KRMEDIUMRHARD\\nROxfRParROxfRPar\\nGeM-Siamese [37, 35] SfM-120k 2048 87.8 92.7 64.7 77.2 38.5 56.3\\nSOLAR [28] GLDv1-noisy 2048 – – 69.9 81.6 47.9 64.5\\nGLDv2 [53] GLDv2-clean 2048 – – 74.2 84.9 51.6 70.3\\nGLAM (Ours) NC-clean 512 77.8 85.8 51.6 68.1 20.9 44.7\\nGLDv1-noisy 512 92.8 95.0 73.7 83.5 49.8 69.4\\nGLDv2-noisy 512 93.3 95.3 75.7 86.0 53.1 73.8\\nGLDv2-clean 512 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 3: mAP comparison of our best model (base-\\nline+local+global) trained on different training sets against\\n[53, 28]. All models use ResNet101-GeM. Red: best results.\\nBlue: GLAM higher than SOLAR [28] on GLDv1-noisy.\\nGLDv2-noisy has 2.6 times more images than GLDv2-\\nclean, the latter is superior by a large margin. This shows\\nthat, in training, a cleaner dataset can be more important\\nthan a larger one. By contrast, NC-clean has the worst\\nperformance despite being clean, aparently because it is\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ff21aef7-f3ab-4e3a-a66e-e287be801912', embedding=None, metadata={'page_label': '7', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='84fff119eb186832bfb6e7159c22c2e0684e3988253fd5fb4ba5c278baa1f6fb', text='METHOD TRAIN SET DIMBASE MEDIUM HARD\\nOx5k Par6kROxf +R1MRPar +R1MROxf +R1MRPar +R1M\\nmAP mAP mAP mP mAP mP mAP mP mAP mP mAP mP mAP mP mAP mP mAP mP\\nSPoC-V16 [2, 35] [O] 512 53.1∗– 38.0 54.6 17.1 33.3 59.8 93.0 30.3 83.0 11.4 20.9 0.9 2.9 32.4 69.7 7.6 30.6\\nSPoC-R101 [35] [O] 2048 – – 39.8 61.0 21.5 40.4 69.2 96.7 41.6 92.0 12.4 23.8 2.8 5.6 44.7 78.0 15.3 54.4\\nCroW-V16 [24, 35] [O] 512 70.8 79.7 41.4 58.8 22.5 40.5 62.9 94.4 34.1 87.1 13.9 25.7 3.0 6.6 36.9 77.9 10.3 45.1\\nCroW-R101 [35] [O] 2048 – – 42.4 61.9 21.2 39.4 70.4 97.1 42.7 92.9 13.3 27.7 3.3 9.3 47.2 83.6 16.3 61.6\\nMAC-V16-Siamese [36, 35] [O] 512 80.0 82.9 37.8 57.8 21.8 39.7 59.2 93.3 33.6 87.1 14.6 27.0 7.4 11.9 35.9 78.4 13.2 54.7\\nMAC-R101-Siamese [35] [O] 2048 – – 41.7 65.0 24.2 43.7 66.2 96.4 40.8 93.0 18.0 32.9 5.7 14.4 44.1 86.3 18.2 67.7\\nRMAC-V16-Siamese [36, 35] [O] 512 80.1 85.0 42.5 62.8 21.7 40.3 66.2 95.4 39.9 88.9 12.0 26.1 1.7 5.8 40.9 77.1 14.8 54.0\\nRMAC-R101-Siamese [35] [O] 2048 – – 49.8 68.9 29.2 48.9 74.0 97.7 49.3 93.7 18.5 32.2 4.5 13.0 52.1 87.1 21.3 67.4\\nRMAC-R101-Triplet [16, 35] NC-clean 2048 86.1 94.5 60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0\\nGeM-R101-Siamese [37, 35] SfM-120k 2048 87.8 92.7 64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3\\nAGeM-R101-Siamese [17] SfM-120k 2048 – – 67.0 – – – 78.1 – – – 40.7 – – – 57.3 – – –\\nSOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048 – – 69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6\\nDELG-GeM-R101-ArcFace [5] GLDv1-noisy 2048 – – 73.2 – 54.8 – 82.4 – 61.8 – 51.2 – 30.3 – 64.7 – 35.5 –\\nGeM-R101-ArcFace [53] GLDv2-clean 2048 – – 74.2 – – – 84.9 – – – 51.6 – – – 70.3 – – –\\nGLAM-GeM-R101-ArcFace baseline GLDv2-clean 512 91.9 94.5 72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7\\n+local GLDv2-clean 512 91.2 95.4 73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1\\n+global GLDv2-clean 512 92.3 95.3 77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0\\n+global+local GLDv2-clean 512 94.2 95.6 78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0\\nTable 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16:\\nVGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet).∗: dimension d= 256 [2]. mP: mP@10. Red:\\nbest results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only\\nmodel other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\\ntoo small. To achieve best possible performance, we use\\nGLDv2-clean as a training set in the remaining experiments.\\nComparisons on same training set It is common to com-\\npare methods regardless of training sets as more become\\navailable, e.g., [35, 28]. Since GLDv2-clean is relatively\\nnew, Weyand et al. [53], which introduced the dataset, is the\\nonly study that has trained the same backbone with the same\\nsettings (ResNet101-GeM with ArcFace) on GLDv2-clean.\\nOur baseline is lower than [53], because our dimensinality is\\n512, while other models based on ResNet101 use 2048. Yet,\\nTable 3 shows that our best model trained on GLDv2-clean\\noutperforms [53] by a large margin. But the most impor-\\ntant comparison is with SOLAR [28], also based on self-\\nattention, which has trained ResNet101-GeM on GLDv1-\\nnoisy. On this training set, our best model clearly outper-\\nforms [28] despite lower dimensionality.\\nComparison with state of the art Table 4 shows the\\nperformance of four variants of our model, i.e. baseline\\nwith or without local/global attention, and compares them\\nagainst state-of-the-art (SOTA) methods based on global de-\\nscriptors without re-ranking on the complete set of bench-\\nmarks, including distractors. Both local and global atten-\\ntion bring signiﬁcant gain over the baseline. The effect\\nof global is stronger, while the gain of the two is addi-\\ntive in the combination. The best results are achieved by\\nthe global-local attention network (baseline+global+local).\\nWith this model, we outperform previous best methods\\non most benchmarks except mP@10 on RParis (medium)\\nandRParis+R1M (medium), where we are outperformed\\nby [37, 35]. These results demonstrate that our approach is\\neffective for landmark image retrieval. Figure 7 shows someMETHOD OXF5KPAR6KRMEDIUM RHARD\\nROxfRParROxfRPar\\nGLAM baseline 91.9 94.5 72.8 84.2 49.9 69.7\\n+local-channel 91.3 95.3 72.2 85.8 48.3 73.1\\n+local-spatial 91.0 95.1 72.1 85.3 48.3 71.9\\n+local 91.2 95.4 73.7 86.5 52.6 75.0\\n+global-channel 92.5 94.4 73.3 84.4 49.8 70.1\\n+global-spatial 92.4 95.1 73.2 86.3 50.0 72.7\\n+global 92.3 95.3 77.2 86.7 57.4 75.0\\n+global+local 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 5: mAP comparison of spatial and channel variants\\nof our local (+local, subsection 3.1) and global (+global,\\nsubsection 3.1) attention modules to the baseline.\\nMETHOD OXF5KPAR6KRMEDIUM RHARD\\nROxfRParROxfRPar\\nCBAM style 93.8 95.7 75.6 88.4 53.3 76.8\\nGLAM (Ours) 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 6: mAP comparison between CBAM style and our\\nlocal spatial attention.\\nexamples of our ranking results.\\n4.4. Ablation study\\nOur ablation study uses the Google Landmark v2 clean\\ndataset (GLDv2-clean) [53] for training, which is shown to\\nbe the most effective in Table 3.\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d6414640-a245-4e09-98fe-1ae714bd982b', embedding=None, metadata={'page_label': '8', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='45d0acb29e345bbd285918e3d9358b91be9b78e2272755f7a73f248d8b749e2c', text='METHOD OXF5KPAR6KRMEDIUM RHARD\\nROxfRParROxfRPar\\nConcatenate 89.5 95.1 73.6 86.5 54.0 73.7\\nSum (Ours) 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 7: mAP comparison between weighted concatenation\\nand weighted average for feature fusion.\\nMETHOD OXF5KPAR6KRMEDIUM RHARD\\nROxfRParROxfRPar\\nFixed-size 76.1 82.6 55.7 68.4 29.2 47.5\\nGroup-size (Ours) 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 8: mAP comparison between ﬁxed-size ( 224×224)\\nand group-size sampling methods.\\nQUERY DATABASE OXF5KPAR6KRMEDIUM RHARD\\nROxfRParROxfRPar\\nSingle Single 93.3 95.2 76.9 87.1 58.6 74.7\\nMulti Single 93.9 95.4 78.0 87.7 59.0 75.5\\nSingle Multi 93.6 95.6 77.0 87.8 57.1 76.0\\nMulti Multi 94.2 95.6 78.6 88.5 60.2 76.8\\nTable 9: mAP comparison of using multiresolution repre-\\nsentation (Multi) or not (Single) on query or database.\\nEffect of attention modules We ablate the effect of our\\nlocal and global attention networks as well as their com-\\nbination. Table 5 shows the results, which are more ﬁne-\\ngrained than those of Table 4. In particular, it shows the ef-\\nfect of the channel and spatial variants of both local and\\nglobal attention. We observe that, when used alone, the\\nchannel and spatial variants of local attention are harmful\\nin most cases. Even the combination, baseline+local, is not\\nalways effective. By contrast, when used alone, the channel\\nand spatial variants of global attention are mostly beneﬁcial,\\nespecially the latter. Their combination, baseline+global, is\\nimpressive, bringing gain of up to 7.5%. Importantly, the\\ncombination baseline+global+local improves further by up\\nto another 2.8%. This result shows the necessity of local\\nattention in the ﬁnal model.\\nCBAM vs. our local spatial attention We experiment\\nwith the local spatial attention of CBAM [54]. CBAM ap-\\nplies average and max-pooling to input features and con-\\ncatenates the two for spatial attention. We apply this vari-\\nant to our local spatial attention module for comparison.\\nFor the CBAM style module, we keep the overall design\\nof our module as shown in Figure 3, but apply average and\\nmax-pooling to each of the four convolutional layer outputs\\nbefore concatenation. Table 6 shows that the CBAM stylemodule is considerably worse than ours on all benchmarks\\nexcept Paris6k, where it is only slightly better.\\nConcatenation vs. sum for feature fusion We use a\\nsoftmax-based weighted average of local and global atten-\\ntion feature maps with the original feature map (7). Here,\\nwe compare this weighted average with weighted concate-\\nnation, where concatenation replaces the sum operation\\nin (7). As shown in Table 7, the weighted average outper-\\nforms the weighted concatenation.\\nFixed-size vs. group-size sampling Numerous studies\\nhave proposed methods for constructing batches according\\nto image size for efﬁcient training. For instance, Gordo et\\nal. [16], DELF [29], and Yokoo et al. [56] employed dif-\\nferent image sizes per batch for training instead of a single\\nﬁxed size. We adopt the method of Yokoo et al., which con-\\nstructs a batch with images of similar aspect ratio, so that\\nthe images can be resized to a size with an aspect ratio that\\nis similar to their own. We call this method group-size sam-\\npling . Table 8 compares ﬁxed-size ( 224×224) with group-\\nsize sampling. We observe that maintaining aspect ratios by\\nusing dynamic input sizes is much more effective.\\nMulti-resolution We use the multi-resolution representa-\\ntion [16] for the ﬁnal feature of an image at inference time.\\nThis method: (1) resizes an image into multiple scales; (2)\\nextracts features from the resized images; and (3) averages\\nthe features to obtain the ﬁnal feature of the image. The\\nmethod is applied to both query and database images to en-\\nhance ranking results, especially for small target objects.\\nTable 9 compares the four cases of applying this method or\\nnot to query or database images.\\n5. Conclusion\\nWe have introduced a novel approach that extracts global\\nand local contextual information using attention mecha-\\nnisms for instance-level image retrieval. It is manifested as\\na network architecture consisting of global and local atten-\\ntion components, each operating on both spatial and chan-\\nnel dimensions. This constitutes a comprehensive study and\\nempirical evaluation of all four forms of attention that have\\npreviously been studied only in isolation. Our ﬁndings indi-\\ncate that the gain (or loss) brought by one form of attention\\nalone strongly depends on the presence of the others, with\\nthe maximum gain appearing when all forms are present.\\nThe output is a modiﬁed feature tensor that can be used in\\nany way, for instance with local feature detection instead of\\nspatial pooling for image retrieval.\\nWith the advent of vision transformers [12, 58] and their\\nrecent application to image retrieval [13], attention is ex-\\npected to play a more and more signiﬁcant role in vi-\\nsion. According to our classiﬁcation, transformers perform\\nglobal spatial attention alone. It is of great interest to in-\\nvestigate the role of the other forms of attention, where our\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='213dea60-e4c4-4ac8-bd94-cd3360c4ba9a', embedding=None, metadata={'page_label': '9', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='325ca1cfae7c16f17deb140ce20f9e5cbdde948b602697534115c2aa64ac0872', text='approach may yield a basic building block of such archi-\\ntectures. One may even envision an extension to language\\nmodels, where transformers originate from [50].\\nReferences\\n[1] Relja Arandjelovi ´c, Petr Gronat, Akihiko Torii, Tomas Pa-\\njdla, and Josef Sivic. NetVLAD: CNN architecture for\\nweakly supervised place recognition. In CVPR , 2016. 2\\n[2] Artem Babenko and Victor Lempitsky. Aggregating Local\\nDeep Features for Image Retrieval. In ICCV , 2015. 1, 2, 7\\n[3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and\\nVictor Lempitsky. Neural Codes for Image Retrieval. In\\nECCV , 2014. 1, 2, 5\\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\\nand Quoc V . Le. Attention augmented convolutional net-\\nworks. In ICCV , 2019. 2, 3\\n[5] Bingyi Cao, Andr ´e Araujo, and Jack Sim. Unifying deep\\nlocal and global features for image search. In ECCV , 2020.\\n2, 3, 7\\n[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.\\nGCNet: Non-Local Networks Meet Squeeze-Excitation Net-\\nworks and Beyond. In ICCV , 2019. 2\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for seman-\\ntic image segmentation. arXiv preprint arXiv:1706.05587 ,\\n2017. 4\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In ICML , 2020. 1\\n[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. Aˆ2-nets: Double attention networks.\\nInNeurIPS , 2018. 1, 2, 3, 4\\n[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. ArcFace: Additive Angular Margin Loss for Deep\\nFace Recognition. In CVPR , 2019. 6\\n[11] Michael Donoser and Horst Bischof. Diffusion Processes for\\nRetrieval Revisited. In CVPR , 2013. 1, 2, 6\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 , 2020. 8\\n[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\\nHerv ´e J´egou. Training vision transformers for image re-\\ntrieval. Technical report, 2021. 8\\n[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\\nsecond-order pooling convolutional networks. In CVPR ,\\n2019. 2, 3, 4\\n[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. Deep image retrieval: Learning global representations\\nfor image search. In ECCV , 2016. 2\\n[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. End-to-end learning of deep visual representations for\\nimage retrieval. IJCV , 2017. 1, 2, 5, 6, 7, 8[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie. Attention-\\naware generalized mean pooling for image retrieval. arXiv\\npreprint arXiv:1811.00202 , 2018. 2, 3, 7\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In CVPR ,\\n2016. 6\\n[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in con-\\nvolutional neural networks. In NeurIPS , 2018. 2\\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\\nSqueeze-and-Excitation Networks. In CVPR , 2018. 1, 2\\n[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,\\nand Ondrej Chum. Efﬁcient diffusion on region manifolds:\\nRecovering small objects with compact cnn representations.\\nInCVPR , 2017. 2, 6\\n[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into com-\\npact codes. PAMI , (99):1–1, 2011. 2\\n[23] Albert Jimenez, Jose M. Alvarez, and Xavier Gir ´o-i-Nieto.\\nClass weighted convolutional features for visual instance\\nsearch. In BMVC , 2017. 2, 3\\n[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.\\nCrossdimensional weighting for aggregated deep convolu-\\ntional features. In ECCV , 2016. 1, 2, 3, 7\\n[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.\\nLearned Contextual Feature Reweighting for Image Geo-\\nLocalization. In CVPR , 2017. 1, 2, 3\\n[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\\nProxy anchor loss for deep metric learning. In CVPR , 2020.\\n1\\n[27] David G. Lowe. Distinctive image features from scale-\\ninvariant keypoints. In IJCV , 2004. 1, 2\\n[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian\\nMikolajczyk. SOLAR: Second-Order Loss and Attention for\\nImage Retrieval. In ECCV , 2020. 1, 2, 3, 5, 6, 7\\n[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,\\nand Bohyung Han. Large Scale Image Retrieval with Atten-\\ntive Deep Local Features. In ICCV , 2017. 1, 2, 3, 5, 6, 8\\n[30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\\nSavarese. Deep metric learning via lifted structured feature\\nembedding. In CVPR , 2016. 1\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas K ¨opf, Edward Yang, Zach DeVito, Martin Raison,\\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\\nperative style, high-performance deep learning. In NeurIPS ,\\n2019. 5\\n[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Object retrieval with large vocabularies\\nand fast spatial matching. In CVPR , 2007. 2, 5\\n[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Lost in quantization:Improving particu-\\nlar object retrieval in large scale image databases. In CVPR ,\\n2008. 5\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f220815a-8b89-44c9-b18d-8143c4fcb46c', embedding=None, metadata={'page_label': '10', 'file_name': '2107.08000v1.pdf', 'file_path': 'pdfs\\\\2107.08000v1.pdf', 'file_type': 'application/pdf', 'file_size': 14696725, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='ec19fa8f62a85d22e9c18627d56cc67ac015b1d45a9806d68b0d54579c3f7d89', text='[34] Tobias Pl ¨otz and Stefan Roth. Neural nearest neighbors net-\\nworks. In NeurIPS , 2018. 2, 3\\n[35] Filip Radenovi ´c, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ond ˇrej Chum. Revisiting Oxford and Paris:\\nLarge-Scale Image Retrieval Benchmarking. In CVPR , 2018.\\n5, 6, 7\\n[36] Filip Radenovi ´c, Giorgos Tolias, and Ond ˇrej Chum. CNN\\nimage retrieval learns from BoW: Unsupervised ﬁne-tuning\\nwith hard examples. In ECCV , 2016. 2, 7\\n[37] Filip Radenovi ´c, Giorgos Tolias, and Ond ˇrej Chum. Fine-\\nTuning CNN Image Retrieval with No Human Annotation.\\nInTPAMI , 2019. 1, 2, 5, 6, 7\\n[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson,\\nand Atsuto Maki. Visual Instance Retrieval with Deep Con-\\nvolutional Networks. In CoRR , 2015. 2\\n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge. In International booktitle of Computer Vision , 2015.\\n5\\n[40] Oriane Sim ´eoni, Yannis Avrithis, and Ondrej Chum. Local\\nfeatures and visual words emerge in activations. In CVPR ,\\n2019. 2, 6\\n[41] O. Sim ´eoni, A. Iscen, G. Tolias, Y . Avrithis, and O. Chum.\\nGraph-based particular object discovery. Machine Vision and\\nApplications , 30(2):243–254, 3 2019. 3\\n[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\\nical networks for few-shot learning. In NeurIPS , 2017. 1\\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\nconvolutions. In CVPR , 2015. 3\\n[44] Mingxing Tan, Ruoming Pang, and Quoc V . Le. EfﬁcientDet:\\nScalable and Efﬁcient Object Detection. In CVPR , 2020. 5\\n[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack\\nSim. Detect-to-retrieve: Efﬁcient regional aggregation for\\nimage search. In CVPR , 2019. 2\\n[46] Giorgios Tolias, Yannis Avrithis, and Herv ´e J´egou. To aggre-\\ngate or not to aggregate: Selective match kernels for image\\nsearch. In ICCV , 2013. 2\\n[47] Giorgos Tolias, Tomas Jenicek, and Ond ˇrej Chum. Learn-\\ning and aggregating deep local descriptors for instance-level\\nrecognition. In ECCV , 2020. 2, 3\\n[48] Giorgos Tolias, Ronan Sicre, and Herv ´e J´egou. Particular ob-\\nject retrieval with integral max-pooling of CNN activations.\\nInICLR , 2016. 2\\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 4\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 9\\n[51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\\nmeng Zuo, and Qinghua Hu. ECA-Net: Efﬁcient Chan-\\nnel Attention for Deep Convolutional Neural Networks. In\\nCVPR , 2020. 2, 3, 4[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local Neural Networks. In CVPR , 2018. 1, 2,\\n3, 4\\n[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle Landmarks Dataset v2 - A Large-Scale Benchmark\\nfor Instance-Level Recognition and Retrieval. In CVPR ,\\n2020. 1, 2, 5, 6, 7\\n[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\\nKweon. CBAM: Convolutional Block Attention Module. In\\nECCV , 2018. 1, 2, 4, 8\\n[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and\\nShin’ichi Satoh. Efﬁcient image retrieval via decoupling dif-\\nfusion into online and ofﬂine processing. In AAAI , 2019. 2,\\n6\\n[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi\\nIizuka. Two-stage Discriminative Re-ranking for Large-scale\\nLandmark Retrieval. In arXiv:2003.11211 , 2020. 6, 8\\n[57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\\nresidual networks. In CVPR , 2017. 4\\n[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\\nto-token vit: Training vision transformers from scratch on\\nimagenet. arXiv preprint arXiv:2101.11986 , 2021. 8\\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\\nself-attention for image recognition. In CVPR , 2020. 2, 3\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c39b0faa-c5d9-4b07-aaa5-d5a284f48fff', embedding=None, metadata={'page_label': '1', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='800b98e3b638fb0634e559bd57b0c3d345db10a45e0cdc0c4113ebdcbd2a3f0e', text='OBJECT COUNTING: YOU ONLY NEED TO LOOK AT ONE\\nHui LIN, Xiaopeng HONG, Yabin WANG\\nSchool of Cyber Science and Engineering, Xi’an Jiaotong University, China\\nEmails: linhuixjtu@gmail.com; hongxiaopeng@ieee.org; iamwangyabin@stu.xjtu.edu.cn\\nABSTRACT\\nThis paper aims to tackle the challenging task of one-\\nshot object counting. Given an image containing novel, pre-\\nviously unseen category objects, the goal of the task is to\\ncount all instances in the desired category with only one sup-\\nporting bounding box example. To this end, we propose a\\ncounting model by which you only need to Look At One in-\\nstance (LaoNet). First, a feature correlation module combines\\nthe Self-Attention and Correlative-Attention modules to learn\\nboth inner-relations and inter-relations. It enables the network\\nto be robust to the inconsistency of rotations and sizes among\\ndifferent instances. Second, a Scale Aggregation mechanism\\nis designed to help extract features with different scale infor-\\nmation. Compared with existing few-shot counting methods,\\nLaoNet achieves state-of-the-art results while learning with a\\nhigh convergence speed. The code will be available soon.\\nIndex Terms —Object Counting, One-Shot Learning, At-\\ntention Mechanism\\n1. INTRODUCTION\\nObject counting has become increasingly important due to its\\nwide range of applications such as crowd surveillance, trafﬁc\\nmonitoring, wildlife conservation and inventory management.\\nMost of the existing counting methods [1, 2, 3] focus on a par-\\nticular, single category. However, when applying them into\\nnew categories, their performances will drop catastrophically.\\nMeanwhile, it is extremely difﬁcult and costly to collect all\\ncategories and label them for training.\\nFor humans, the generalization ability allows them to\\nlearn and deal with various vision tasks without much prior\\nknowledge and experience. We are amazed by this remark-\\nable ability and in this work, we focus on this learning\\nparadigm and design a network to efﬁciently recognize and\\ncount new categories given only one example. We follow the\\nfew-shot setting in [4] and modify it to one-shot object count-\\ning. That is, the model takes an image with unseen novel cate-\\ngories and a supporting bounding box containing an example\\ninstance of desired category as input, and then predicts the\\nobject count in the image.\\nHowever, there are two main challenges. First , the object\\ncounting task includes many different categories, and evenseveral categories exist within a same image. Moreover in\\nfew-shot setting, these categories will not overlap between\\ntraining and inference. This means that the model needs to\\nhave a strong distinguishing ability between features of differ-\\nent categories, and meanwhile, an effective associating abil-\\nity among instances sharing the same category. Second , in\\none-shot counting, the model learns from only one support-\\ning instance. Much of the difﬁculty results from the fact that\\nthe supporting sample may differ from other instances in, for\\nexample, sizes and poses. Hence, the model is required to\\nbe invariant towards these variations without seeing the com-\\nmonalities across different instances.\\nTherefore, in this paper, we propose an effective network\\nnamed LaoNet for one-shot object counting. It consists of\\nthree main parts: feature extraction, feature correlation and\\nthe density regressor, as shown in Figure 1. The feature corre-\\nlation model and the feature extraction model are elaborately\\ndesigned to address the above two challenges.\\nWe propose the feature correlation based on Self-\\nAttention and Correlative-Attention modules to learn inner-\\nrelations and inter-relations respectively. The Self-Attention\\nencourages the model to focus more on important features and\\ntheir correlations, improving the efﬁciency of information re-\\nﬁnement. Previous few-shot counting methods [4, 5] usually\\nleverage on a convolution operation to match the similarities\\nbetween image features and supporting features. However,\\nas the kernel is derived from supporting features with the de-\\nfault size and rotation angle, the convolution operation will\\ngreatly depend on the quality of supporting features and the\\nconsistency of physical properties among different instances.\\nInstead, our designed feature correlation model beneﬁts from\\ntwo kinds of attention modules and addresses the above prob-\\nlem by considering all correlations.\\nWe further propose a Scale Aggregation mechanism in\\nscale extraction to deal with scale variations among different\\ncategories and different instances. By learning features from\\nmulti-subspace, the model aggregates various scale informa-\\ntion while maintaining a spatial consistency.\\nTo summarize, our contribution is threefold.\\n• We design a novel network named LaoNet (A network\\nby which you only need to Look At One instance) for\\none-shot object counting. By combining Self-AttentionarXiv:2112.05993v1  [cs.CV]  11 Dec 2021', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='27ca9c89-1fd0-4d5d-a74f-63b83a2c985f', embedding=None, metadata={'page_label': '2', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='cc265564b7d4159a6ccec79248598cc0f4897fcc89d9056d57fa6e43b039350c', text='Fig. 1 . The overall architecture of the proposed LaoNet for one-shot object counting. Both the query image and the supporting\\nbox are fed into CNN to extract features. Supporting features are aggregated among scales. Then the ﬂatten features with unique\\nposition embedding are transmitted into feature correlation model with Self-Attentions and Correlative Attentions. Finally, a\\ndensity regressor is adopted to predict the ﬁnal density map.\\nand Correlative-Attention modules, LaoNet exploits\\nthe correlation among novel category objects with high\\naccuracy and efﬁciency.\\n• We propose a Scale Aggregation mechanism to extract\\nmore comprehensive features and fuse multi-scale in-\\nformation from the supporting box.\\n• The experimental results show that our model achieves\\nstate-of-the-art results with signiﬁcant improvements\\non FSC-147 [4] and COCO [6] datasets under the one-\\nshot setting without ﬁne-tuning.\\n2. RELATED WORKS\\nObject counting methods can be brieﬂy divided into two\\ntypes. Detection based methods [7] count the number of ob-\\njects by exhaustively detecting every target in images. But\\nthey rely on the complex labels such as bounding boxes. Re-\\ngression based methods [1, 2] learn to count by predicting a\\ndensity map, in which each value represents the density of\\ntarget objects at the corresponding location. The count pre-\\ndiction equals to the total sum of density map.\\nNevertheless, most of the counting methods are category\\nspeciﬁcally, e.g. for human crowd [1, 2, 8, 9, 10, 11], for\\ncars [3, 12], for plants [13] or for cells [14, 15]. They focus\\non only one category and will loss the original satisﬁed per-\\nformance when transferring to other categories. Moreover,\\nmost traditional approaches usually rely on tens of thousands\\nof instances to train a counting model [2, 8, 9, 11, 3, 12].\\nTo reduce considerably the number of samples needed to\\ntrain a counting model for a particular category, recently, few-\\nshot counting task has been developed. The key lies in the\\ngeneralization ability of the model to deal with novel cate-\\ngories from few labeled examples. The study [16] proposes aGeneric Matching Network (GMN) for class-agnostic count-\\ning. However it still needs several dozens to hundreds exam-\\nples of a novel category for adaptation and good performance.\\nCFOCNet is introduced to match and utilize the similarity be-\\ntween objects within the same category [5]. The work [4]\\npresents a Few Shot Adaptation and Matching Network (Fam-\\nNet) to learn feature correlations and few-shot adaptation and\\nalso introduces a few-shot counting dataset named FSC-147.\\nWhen the number of labeled example decreases to one,\\nthe task evolves into one-shot counting. In other visual tasks,\\nresearchers develop methods for one-shot segmentation [17]\\nand one-shot object detection [18, 19]. Compared to the few-\\nshot setting which usually uses at least three instances for\\neach object [4], the one-shot setting, where only one instance\\nis available, is clearly more challenging.\\nIt is worth mentioning that detection based approaches\\n[20, 21, 22] are inferior for the tasks of few-shot and one-shot\\ncounting. One main reason is that it requires extra and costly\\nbounding-box annotations of allinstances in the training stage\\nwhile one-shot counting approach which we focus on depends\\non dot annotations and only onesupporting box. To illus-\\ntrate this point further, we perform experiments in Section 4.3\\nto compare with detection based approaches and validate the\\nproposed network for one-shot counting.\\n3. APPROACH\\n3.1. Problem Deﬁnition\\nOne-shot object counting consists of a training set\\n(It,st,yt)∈T and a query set (Iq,sq)∈Q, in which cate-\\ngories are mutually exclusive. Each input for the model con-\\ntains an image Iand a supporting bounding box sannotating\\none object of the desired category. In training set, abundant', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a224d8e-6d3b-41ec-8423-3f197920648e', embedding=None, metadata={'page_label': '3', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='a43ec105cbd109c9956806cf124fa18dff18d8dfbb22c38a3bc9ca82ba16b7a1', text='point annotations ytare available to supervise the model. In\\ninference stage, we aim the model to learn to count the novel\\nobjects inIqwith a supporting category instance sampled by\\nsq.\\n3.2. Feature Correlation\\nAs the model is required to learn to count from only one sup-\\nporting object, seizing the correlation between features with\\nhigh efﬁciency is quite important. Therefore, we build the\\nfeature correlation model in our one-shot network based on\\nSelf-Attention and Correlative-Attention modules, for learn-\\ning the inner-relations and inter-relations respectively.\\nAs illustrated in Figure 1 (violet block), our Self-\\nAttention module consists of a Multi-head Attention (MA)\\nand a layer normalization (LN). We ﬁrst introduce the deﬁ-\\nnition of attention [23], given the query Q, keyKand value\\nvectorV:\\nA(Q,K,V|W) =S((QWQ)(KWK)T\\n√\\nd+PE)(VWV),\\n(1)\\nwhereSis the softmax function and1√\\ndis a scaling factor\\nbased on the vector dimension d.W:WQ,WK,WV∈\\nRd×dare weight matrices for projections and PEis the posi-\\ntion embedding.\\nTo leverage on more representation subspaces, we adopt\\nthe extending form with multi attention heads:\\nMA(Q,K,V ) =Concat (head 1,..,headh)WO\\nwherehead i=A(Q,K,V|Wi).(2)\\nThe representation dimensions are divided by parallel atten-\\ntion heads, where parameter matrices Wi:WQ\\ni,WK\\ni,WV\\ni∈\\nRd×d/handWO∈Rd×d.\\nOne challenging problem in counting task is the existence\\nof many complex interfering things. To efﬁciently weaken the\\nnegative inﬂuence by those irrelevant background, we apply\\nMulti-head Self-Attention in image features to learn inner-\\nrelations and encourage the model to focus more on repetitive\\nobjects that can be counted.\\nWe denote the feature sequences of the query image and\\nthe supporting box region as XandS, with sizes X∈\\nRHW×CandS∈Rhw×C. And the reﬁned query feature\\nis calculated by:\\n˜X=LN(MA(XQ,XK,XV) +X). (3)\\nA layer normalization (LN) is adopted to balance the value\\nscales.\\nMeanwhile, as there is only one supporting object in one-\\nshot counting problem, reﬁning the salient features within the\\nobject is necessary and helpful for counting efﬁciency and\\naccuracy. Therefore we apply another Self-Attention module\\nto supporting feature and get reﬁned ˜S.Previous few-shot counting methods [4, 5] usually adopt\\na convolution operation where the supporting features act as\\nkernels to match the similarities for target category. However,\\nthe results will greatly depend on the quality of supporting\\nfeatures and the consistency of objects’ properties, including\\nrotations and scales.\\nTo this end, we propose a Correlative-Attention module\\nto learn inter-relations between query and supporting features\\nand alleviate the constraints of irrelevant properties.\\nSpeciﬁcally, we extend the MA by learning correlations\\nbetween different feature sequences and add a feed-forward\\nnetwork (FFN) to fuse the features, i.e.,\\nX∗=Corr (˜X,˜S) =G(MA(˜XQ,˜SK,˜SV) +˜X).(4)\\nGincludes two LNs and a FFN in the form of residual (light\\nblue block in Figure 1). Finally, X∗and˜Swill be fed into the\\ncycle as new feature sequences where each cycle consists of\\ntwo Self-Attention modules and a Correlative-Attention mod-\\nule.\\n3.3. Feature Extraction and Scale Aggregation\\nTo extract feature sequences from images, we use VGG-19 as\\nour backbone. For query image, the output of the ﬁnal level\\nis directly ﬂattened and transmitted into Self-Attention mod-\\nule. For the supporting box, as there are uncontrollable scale\\nvariations among instances due to the perspective, we pro-\\npose a Scale Aggregation mechanism to fuse different scale\\ninformation.\\nGivenlas the number of layers in CNN, we aggregate the\\nfeature maps among different scales:\\nS=Concat (Fl(s),Fl−1(s),...,Fl+1−δ(s)), (5)\\nwhereFirepresents a feature map at ithlevel andδ∈[1,l]\\ndecides the number of layers taken for aggregation.\\nMeanwhile, we leverage on identifying position embed-\\nding to help the model distinguish the integrated scale in-\\nformation in attention model. By adopting the ﬁxed sinu-\\nsoidal absolute position embedding [23], feature sequences\\nfrom different scales can still maintain the consistency be-\\ntween positions, i.e.,\\nPE (pos j,2i)=sin(posj/100002i/d),\\nPE (pos j,2i+1)=cos(posj/100002i/d).(6)\\niis the dimension and posjis the position for jthfeature map.\\n3.4. Training Loss\\nWe use Euclidean distance to measure the difference between\\nestimated density map and ground truth density map, which is\\ngenerated based on annotated points following [1]. The loss\\nis deﬁned as follows:\\nLE=||Dgt−D||2\\n2, (7)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a5dac28-c952-428b-afa9-1f93d8e7a5cf', embedding=None, metadata={'page_label': '4', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='39347736f6f8c90b74a8c090e3e7a546812ac3957ed93bd326703ca01cd52c82', text='whereDis the estimated density map and Dgtis the ground\\ntruth density map. To improve the local pattern consistency,\\nwe also adopt a SSIM loss followed the calculation in [8]. By\\nintegrating the above two loss functions, we have\\nL=LE+λLSSIM, (8)\\nwhereλis the balanced weight.\\n4. EXPERIMENTS\\n4.1. Implement Details and Evaluation Metrics\\nWe design the density regressor by an upsampling layer and\\nthree convolution layers with ReLU activation. The kernel\\nsizes of ﬁrst two layers are 3 × 3 and that of last is 1 × 1. Ran-\\ndom scaling and ﬂipping are adopted for each training image.\\nAdam [24] with a learning rate 0.5×10−5is used to optimize\\nthe parameters. We set the number of attention heads has 4,\\nthe correlation cycle Tas 2, the number of aggregated layers\\nδas 2, and the loss balanced parameter λas10−4.\\nMean Absolute Error (MAE) and Root Mean Squared Er-\\nror (RMSE) are used to measure the performance of our meth-\\nods. They are deﬁned by:\\nMAE =1\\nMM∑\\ni=1⏐⏐Ngt\\ni−Ni⏐⏐,\\nRMSE =\\ued6a\\ued6b\\ued6b√1\\nMM∑\\ni=1(Ngt\\ni−Ni)2),(9)\\nwhereMandNgtare the number of images and the ground-\\ntruth count, respectively. The predicted count Nis calculated\\nby integrating the estimated density map D.\\n4.2. Datesets\\nFSC-147 [4] contains a total of 6135 images collected for\\nfew-shot counting problem. In each image, three randomly\\nselected object instances are annotated by bounding boxes\\nwhile other instances are annotated by points. 89 object cat-\\negories with 3,659 images are divided for training set. Each\\n29 categories with 1,286 and 1,190 images respectively are\\ndivided for validation and testing sets.\\nMS-COCO [6] is a large dataset widely used in object detec-\\ntion and instance segmentation. In val2017 set, there are 80\\ncommon object categories with 5,000 images in complex ev-\\neryday scenes. We follow [17] to generate four train/test splits\\nwhich each contains 60 training and 20 testing categories.\\n4.3. Comparison with Few-Shot Approaches\\nWe hold experiments on above two few-shot counting datasets\\nto evaluate the proposed network. As there are few existingMethodsVal Test\\nMAE RMSE MAE RMSE\\n3-shot\\nMean 53.38 124.53 47.55 147.67\\nMedian 48.68 129.70 47.73 152.46\\nFR detector [25] 45.45 112.53 41.64 141.04\\nFSOD detector [26] 36.36 115.00 32.53 140.65\\nGMN [16] 29.66 89.81 26.52 124.57\\nMAML [27] 25.54 79.44 24.90 112.68\\nFamNet [4] 23.75 69.07 22.08 99.54\\n1-shot\\nCFOCNet [5] 27.82 71.99 28.60 123.96\\nFamNet [4] 26.55 77.01 26.76 110.95\\nLaoNet (Ours) 17.11 56.81 15.78 97.15\\nTable 1 . Comparisons with previous state-of-the-art few-shot\\nmethods on FSC-147. The upper part of the table presents the\\nresults in 3-shot setting while the lower part presents 1-shot\\nresults. FamNet [4] uses the adaptation strategy during test-\\ning. It is worth noticing that our one-shot LaoNet outperforms\\nall of previous methods, even those in 3-shot setting, without\\nany ﬁne-tuning strategy.\\nGT: 33 GT: 14 GT: 35\\nPre: 35 Pre: 14 Pre: 37\\nFig. 2 . Visualizations of one-shot counting inputs and cor-\\nresponding predicted density maps. The model can perform\\ngreat counting accuracy even it has never seen strawberry, hot\\nair balloon or cashew before.\\nmethods speciﬁcally designed for one-shot counting, for com-\\nprehensive evaluation, we modify FamNet [4] and CFOC-\\nNet [5] for this setting and also compare with other few-shot\\ncounting approaches [25, 26, 16, 27, 17].\\nFirst, quantitative results on FSC-147 are shown in Ta-\\nble 1. We list seven results of previous few-shot detection and\\ncounting methods in 3-shot setting and two results of state-\\nof-the-art counting methods in 1-shot setting for comparison.\\nThe result of FamNet [4] uses the adaptation strategy during\\ntesting.\\nIt is worth noticing that our one-shot LaoNet outperforms\\nall of previous few-shot methods, even those in 3 shot set-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e703a8ed-feed-4b6f-b85d-76b926e7d7dc', embedding=None, metadata={'page_label': '5', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='e3243bba93030f51dbffa36c129b57f4ecff096719b5b982f4e42a27b7172e75', text='MethodsFold 0 Fold 1 Fold 2 Fold 3 Average\\nMAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE\\nSegment [17]†2.91 4.20 2.47 3.67 2.64 3.79 2.82 4.09 2.71 3.94\\nGMN [16]†2.97 4.02 3.39 4.56 3.00 3.94 3.30 4.40 3.17 4.23\\nCFOCNet [5]†2.24 3.50 1.78 2.90 2.66 3.82 2.16 3.27 2.21 3.37\\nFamNet [4] 2.34 3.78 1.41 2.85 2.40 2.75 2.27 3.66 2.11 3.26\\nCFOCNet [5] 2.23 4.04 1.62 2.72 1.83 3.02 2.13 3.03 1.95 3.20\\nLaoNet (Ours) 2.20 3.78 1.32 2.66 1.58 2.19 1.84 2.90 1.73 2.93\\nTable 2 . Results on each of four folds of COCO val2017. Methods with †follow the experiment setting in [5]. Our method\\nachieves great accuracy without any ﬁne-tuning on testing categories.\\nMethodsVal Test\\nMAE RMSE MAE RMSE\\nLaoNet 17.11 56.81 15.78 97.15\\n−Self-Attention ( X) 19.83 64.84 19.71 107.32\\n−Self-Attention ( S) 19.67 63.79 18.71 111.83\\n−Scale Aggregation 18.82 63.74 17.16 106.40\\n−SSIM 17.82 57.66 16.11 100.59\\nTable 3 . Ablation study for different terms. Xstands for\\nfeature sequences of query image and Sstands for that of\\nsupporting box region. Experiments are performed in FSC-\\n147 val and test.\\nting, without any ﬁne-tuning strategy. We have generated new\\nrecords by reducing the error of FamNet from 26.55 to 17.11\\nfor MAE and from 77.01 to 56.81 for RMSE in validation set,\\nfrom 26.76 to 15.78 for MAE and from 110.95 to 97.15 for\\nRMSE in testing set.\\nSecond, Table 2 shows the results on each of four folds of\\nCOCO val2017. Methods with †in the upper part of the table\\nfollow the experiment setting in [5]. That is, the supporting\\nexamples are chosen from all instances in the dataset during\\ntraining and testing, which is laborious and costly under the\\nneed of all instances annotated by bounding boxes. While our\\nsetting allows only one ﬁxed instance for each image, we re-\\nconduct the experiment of CFOCNet [5]. As the result shows,\\nour method maintains a great performance on COCO dataset.\\n4.4. Discussions\\nContribution of Different Terms. We study the accuracy\\ncontributions of different terms in FSC-147. The result is\\nshown in Table 3, each row whereof reports the results af-\\nter removing one component or one term from LaoNet. The\\nSelf-Attention modules for the two feature sequences to learn\\ninner-relations increase the accuracy in testing set by 19.9%\\nand15.7%for MAE, 9.5%and13.1%for RMSE, respec-\\ntively. Compared to other two terms, the Self-Attention mod-\\nules contribute most to the performance of our model.\\nThe Scale Aggregation mechanism helps more on RMSE.MethodsFSC147-COCO Val FSC147-COCO Test\\nMAE RMSE MAE RMSE\\nRetinaNet [20] 63.57 174.36 52.67 85.86\\nFaster R-CNN [21] 52.79 172.46 36.20 79.59\\nMask R-CNN [22] 52.51 172.21 35.56 80.00\\nFamNet [4] 39.82 108.13 22.76 45.92\\nLaoNet (Ours) 31.12 97.15 12.89 26.64\\nTable 4 . Comparisons with pre-trained object detectors on\\nFSC147-COCO splits of FSC147 which contain images with\\nCOCO categories. Even pre-trained with thousands of anno-\\ntated examples on MS-COCO dataset, these object detectors\\nstill perform unsatisﬁed accuracy on counting task.\\nThe result demonstrates a robustness contribution under the\\nmulti-scale aggregation. Finally, the SSIM loss further im-\\nproves the counting accuracy by both lower MAE and RMSE.\\nConvergence Speed. We hold experiments to measure the\\nconvergence speed and the performance stability. We pick\\nFamNet [4] as the baseline for LaoNet with a pre-trained\\nCNN backbone and an Adam optimizer. We train both two\\nmodels on FSC-147 and report the validation MAE for 100\\nepochs.\\nAs shown in Figure 3, our model has faster convergence\\nspeed and better stability than FamNet. With just 2 epoches,\\nour method achieves a low counting error which FamNet has\\nto reach after 40 epochs. Meanwhile, the convergence of our\\nmethod is smooth and stable, while that of Famet is jagged,\\nwith multiple sharp peaks and the highest error of 70.\\nComparison with Object Detectors. Object detectors can\\nbe used for counting task with the number of predicted de-\\ntections. However, even these detectors work with categories\\nwhich they are trained on instead of one-shot setting, their\\ncounting performances are still limited. We select images of\\nFSC-147-COCO subset from FSC147 Val and Test sets which\\nshare categories with MS-COCO dataset and conduct quanti-\\ntative experiments.\\nAs the results shown in Table 4, we compare LaoNet with\\nseveral object detectors which are well pre-trained with thou-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eee45b64-364a-4304-9de7-921db594d457', embedding=None, metadata={'page_label': '6', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='2298814f552b4cdda2a846e34c33463c17d631eadac6ef9ea8c8a6a4ef27eb7c', text='Fig. 3 . Comparisons of validation MAE during training. The\\nblue line represents our proposed LaoNet. With just one\\nepoch, it can perform a great accuracy which FamNet needs\\nto train for about 20 epochs.\\nsands of annotated examples on MS-COCO. Nevertheless,\\nour method, which counts unseen categories, still outperforms\\nthe detection based methods which have met those categories\\nin training, by a large margin.\\n5. CONCLUSION\\nThis paper targets one-shot object counting, which requires\\nthe counting model to count objects of new categories by\\nlooking at only one instance. We propose an efﬁcient network\\nnamed LaoNet to address this challenge. LaoNet includes\\na feature correlation module to learn both inner-relations\\nand inter-relations and a scale aggregation module to extract\\nmulti-scale information for improving robustness. Without\\nany ﬁne-tuning in inference, our LaoNet outperforms previ-\\nous state-of-the-art few-shot counting methods with a high\\nconvergence speed. In the future, we consider applying our\\nmodel to a wider range of one-shot vision tasks.\\n6. REFERENCES\\n[1] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua\\nGao, and Yi Ma, “Single-image crowd counting via\\nmulti-column convolutional neural network,” in CVPR ,\\n2016.\\n[2] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong\\nGong, “Bayesian loss for crowd count estimation with\\npoint supervision,” in ICCV , 2019.\\n[3] Debojit Biswas, Hongbo Su, Chengyi Wang, Jason\\nBlankenship, and Aleksandar Stevanovic, “An auto-\\nmatic car counting system using overfeat framework,”\\nSensors (Basel) , 2017.\\n[4] Viresh Ranjan, Udbhav Sharma, Thu Nguyen, and Minh\\nHoai, “Learning to count everything,” in CVPR , 2021.\\n[5] Shuo-Diao Yang, Hung-Ting Su, Winston H Hsu, and\\nWen-Chin Chen, “Class-agnostic few-shot object count-\\ning,” in WACV , 2021.\\n[6] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\\nC Lawrence Zitnick, “Microsoft coco: Common objects\\nin context,” in ECCV . Springer, 2014.[7] Prithvijit Chattopadhyay, Ramakrishna Vedantam,\\nRamprasaath R Selvaraju, Dhruv Batra, and Devi\\nParikh, “Counting everyday objects in everyday\\nscenes,” in CVPR , 2017, pp. 1135–1144.\\n[8] Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su,\\n“Scale aggregation network for accurate and efﬁcient\\ncrowd counting,” in ECCV , 2018.\\n[9] Weizhe Liu, Mathieu Salzmann, and Pascal Fua,\\n“Context-aware crowd counting,” in CVPR , 2019.\\n[10] Boyu Wang, Huidong Liu, Dimitris Samaras, and\\nMinh Hoai Nguyen, “Distribution matching for crowd\\ncounting,” Advances in Neural Information Processing\\nSystems , vol. 33, 2020.\\n[11] Hui Lin, Xiaopeng Hong, Zhiheng Ma, Xing Wei, Yun-\\nfeng Qiu, Yaowei Wang, and Yihong Gong, “Direct\\nmeasure matching for crowd counting,” in IJCAI , 2021.\\n[12] Thomas Moranduzzo and Farid Melgani, “Automatic\\ncar counting method for unmanned aerial vehicle im-\\nages,” TGRS , 2013.\\n[13] M ´elissande Machefer, Franc ¸ois Lemarchand, Vir-\\nginie Bonnefond, Alasdair Hitchins, and Panagiotis\\nSidiropoulos, “Mask r-cnn reﬁtting strategy for plant\\ncounting and sizing in uav imagery,” Remote Sensing ,\\n2020.\\n[14] Thorsten Falk, Dominic Mai, Robert Bensch, ¨Ozg¨un\\nC ¸ ic ¸ek, Ahmed Abdulkadir, Yassine Marrakchi, Anton\\nB¨ohm, Jan Deubner, Zoe J ¨ackel, Katharina Seiwald,\\net al., “U-net: deep learning for cell counting, detec-\\ntion, and morphometry,” Nature methods , 2019.\\n[15] Weidi Xie, J Alison Noble, and Andrew Zisserman,\\n“Microscopy cell counting and detection with fully con-\\nvolutional regression networks,” Computer methods in\\nbiomechanics and biomedical engineering: Imaging &\\nVisualization , 2018.\\n[16] Erika Lu, Weidi Xie, and Andrew Zisserman, “Class-\\nagnostic counting,” in ACCV , 2018.\\n[17] Claudio Michaelis, Ivan Ustyuzhaninov, Matthias\\nBethge, and Alexander S Ecker, “One-shot instance seg-\\nmentation,” arXiv preprint , 2018.\\n[18] Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, and\\nTyng-Luh Liu, “One-shot object detection with co-\\nattention and co-excitation,” in NIPS , 2019.\\n[19] Xiang Li, Lin Zhang, Yau Pun Chen, Yu-Wing Tai, and\\nChi-Keung Tang, “One-shot object detection without\\nﬁne-tuning,” arXiv preprint , 2020.\\n[20] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,\\nand Piotr Doll ´ar, “Focal loss for dense object detection,”\\ninICCV , 2017.\\n[21] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian\\nSun, “Faster r-cnn: Towards real-time object detection\\nwith region proposal networks,” NIPS , 2015.\\n[22] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross\\nGirshick, “Mask r-cnn,” in ICCV , 2017.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ceaead4-159e-4cd1-8f41-8aab9baeca3e', embedding=None, metadata={'page_label': '7', 'file_name': '2112.05993v1.pdf', 'file_path': 'pdfs\\\\2112.05993v1.pdf', 'file_type': 'application/pdf', 'file_size': 742571, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='e644a54df419ad506321de15524d4ff8f6ab2464502fe679a159f5f435ce2148', text='[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\\nand Illia Polosukhin, “Attention is all you need,” in\\nNIPS , 2017.\\n[24] Diederik P Kingma and Jimmy Lei Ba, “Adam:\\nAmethod for stochastic optimization,” .\\n[25] Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi\\nFeng, and Trevor Darrell, “Few-shot object detection\\nvia feature reweighting,” in ICCV , 2019.\\n[26] Qi Fan, Wei Zhuo, Chi-Keung Tang, and Yu-Wing Tai,\\n“Few-shot object detection with attention-rpn and multi-\\nrelation detector,” in CVPR , 2020.\\n[27] Chelsea Finn, Pieter Abbeel, and Sergey Levine,\\n“Model-agnostic meta-learning for fast adaptation of\\ndeep networks,” in ICML , 2017.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='380e26da-ba2b-4051-b6f0-9ab584b9ca37', embedding=None, metadata={'page_label': '1', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='99b6e20b26a1d13f7bfe53f2f1cee25b4a9a16bd46213319384e3e5ffa962ff1', text='arXiv:2304.04556v1  [cs.LG]  7 Apr 2023Attention: Marginal Probability is All You Need?\\nRyan Singh1Christopher L. Buckley1\\nAbstract\\nAttention mechanisms are a central property of\\ncognitive systems allowing them to selectively\\ndeploy cognitive resources in a ﬂexible manner.\\nAttention has been long studied in the neuro-\\nsciences and there are numerous phenomenolog-\\nical models that try to capture its core proper-\\nties. Recently attentional mechanisms have be-\\ncome a dominating architectural choice of ma-\\nchine learning and are the central innovation of\\nTransformers. The dominant intuition and for-\\nmalism underlying their development has drawn\\non ideas of keys and queries in database man-\\nagement systems. In this work, we propose an\\nalternative Bayesian foundation for attentional\\nmechanisms and show how this uniﬁes differ-\\nent attentional architectures in machine learning.\\nThis formulation allows to to identify common-\\nality across different attention ML architectures\\nas well as suggest a bridge to those developed in\\nneuroscience. We hope this work will guide more\\nsophisticated intuitions into the key properties of\\nattention architectures as well suggest new ones.\\n1. Introduction\\nDesigning neural network architectures with favourable\\ninductive biases lies behind many recent successes in\\nDeep Learning ( Baxter ,2000 ). In particular, the attention\\nmechanism has allowed language models to achieve hu-\\nman like generation abilities previously thought impossib le\\n(Vaswani et al. ,2017 ). The success of the attention mech-\\nanism as a domain agnostic architecture has prompted it\\nto be adopted across a huge range of tasks and domains\\nnotably reaching state-of-the-art performance in visual r ea-\\nsoning and segmentation tasks ( Dosovitskiy et al. ,2021 ;\\nWang et al. ,2022 ).\\nDespite it’s success, the role of the attention mechanism\\nremains poorly understood. Indeed, it is unclear to what\\n1School of Engineering and Informatics, University of Susse x.\\nCorrespondence to: Ryan Singh <rs773@sussex.ac.uk >.extent it relates to theories of cognitive attention which i n-\\nspired it ( Lindsay ,2020 ). Here, we aim to provide a parsi-\\nmonious description grounded in principles of probabilist ic\\ninference. This Bayesian perspective provides both a prin-\\ncipled method for specifying prior beliefs and reasoning\\nexplicitly about the role of the attention variables. Furth er,\\nunderstanding the fundamental computation permits us a\\nuniﬁed description of different attention mechanisms in th e\\nliterature. This proceeds in two parts.\\nFirst, we show that ‘soft’ attention mechanisms (e.g. self-\\nattention, cross-attention, graph attention, which we cal l\\ntransformer attention herafter) can be understood prob-\\nabilistically as taking an expectation over possible con-\\nnectivity structures, providing an interesting link betwe en\\nsoftmax-based attention and marginal likelihood.\\nSecond, we extend the uncertainty over connectiv-\\nity to a bayesian setting which, in turn, provides a\\ntheoretical grounding for iterative attention mecha-\\nnisms (slot-attention, perciever and block-slot attentio n)\\n(Locatello et al. ,2020 ;Singh et al. ,2022 ;Jaegle et al. ,\\n2021 ) and Modern Continuous Hopﬁeld Networks\\n(Ramsauer et al. ,2021 ).\\nAdditionally, we apply iterative attention to Predictive C od-\\ning Networks, an inﬂuential theory in computational neuro-\\nscience, creating a new theoretical bridge between machine\\nlearning and cognitive science.\\nAttention (Q,K,V) =p(E|Q,K)\\ued17\\ued1a\\ued19\\ued18\\nsoftmax (QWQWT\\nKKT\\n√dk)V\\n=Ep(E|Q,K)[V]\\nA key observation is that the attention matrix can be seen\\nas the posterior distribution over an adjacency structure, E,\\nand the full mechanism as computing an expectation of the\\nvalue function V(X)over the posterior beliefs about the\\npossible relationships that exist between key and query.\\nThis formalism provides an alternate Bayesian theoreti-\\ncal framing within which to understand attention mod-\\nels, which contrasts with the original framing in terms of\\ndatabase management systems and data retrieval, providing', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='473a389b-a9bc-402d-86f7-16c249dc049e', embedding=None, metadata={'page_label': '2', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='804b094d82f3be1082ec18f85a44ad2856c4e10c5449f7efb8f40b3551647e2d', text='Attention: Marginal Probabiliy is All You Need?\\na unifying framework to describe different attention archi -\\ntectures. Describing their difference only in terms of thei r\\nedge relationships supporting more effective analysis and\\ndevelopment of new architectures. Additionally providing\\na principled understanding of the difference between hard\\nand soft attention models.\\nContributions\\n• A unifying probabilistic framework for understanding\\nattention mechanisms.\\n• We show self-attention and cross-attention can be seen\\nas computing a marginal likelihood over possible net-\\nwork structures.\\n• We show that slot-attention, block-slot-attention and\\nmodern continuous hopﬁeld networks can all be seen\\nas collapsed variational inference, where the possible\\nnetwork structures form the collapsed variables.\\n• Provide a bridge to Bayesian conceptions of attention\\nfrom computational neuroscience, through the lens of\\nPredictive Coding Networks.\\n• Provide a framework for reasoning about hard at-\\ntention, and efﬁcient approximations to the attention\\nmechanism.\\n2. Related Work\\nAttention as bi-level optimisation Mapping feed-forward\\narchitecture to a minimisation step on a related energy func -\\ntion has been called unfolded optimisation ( Frecon et al. ,\\n2022 ). Taking this perspective can lead to insights about\\nthe inductive biases involved for each architecture. It has\\nbeen shown that the cross-attention mechanism can be\\nviewed as an optimisation step on the energy function of\\na form of Hopﬁeld Network ( Ramsauer et al. ,2021 ), pro-\\nviding a link between attention and associative memory.\\nWhilst ( Yang et al. ,2022 ) extend this view to account for\\nself-attention. Our framework distinguishes hopﬁeld atte n-\\ntion, which does not allow an arbritary value matrix, from\\nthe standard attention mechanisms. Whilst there remains\\na strong theoretical connection, it places the Hopﬁeld En-\\nergy as an instance of variational free energy, aligning mor e\\nclosely with iterative attention mechanisms such as slot-\\nattention.\\nRelationship to gaussian mixture model Previous works\\nthat have taken a probabilistic perspective on the attentio n\\nmechanism note the connection to inference in a gaussian\\nmixture model ( Gabbur et al. ,2021 ;Nguyen et al. ,2022 ;\\nDing et al. ,2020 ). Indeed ( Annabi et al. ,2022 ) directly\\nshow the connection between the Hopﬁeld energy and the\\nvariational free energy of a gaussian mixture model. Al-\\nthough gaussian mixture models, a special case of theframework we present here, are enough to explain cross\\nattention they do not capture slot or self-attention. Furth er\\nour framework allows us to extend the structural inductive\\nbiases beyond what can be expressed in a gaussian mixture\\nmodel and capture the relationship to hard attention.\\nLatent alignment and hard attention Several attempts\\nhave been made to combine the beneﬁts of soft (dif-\\nferentiability) and hard attention. Most approaches pro-\\nceed by sampling, e.g., using the REINFORCE es-\\ntimator ( Deng et al. ,2018 ) or atopK approximation\\n(Shankar et al. ,2018 ). The one most similar to ours em-\\nbeds the full forward-backward algorithm within a forward\\npass ( Kim et al. ,2017 ), our approach differs by offering a\\nparsimonious description in terms of marginalisation over\\nan implicit graphical model.\\nCollapsed Inference Collapsed variational inference has\\nmost notably been employed in topic modelling ( Teh et al. ,\\n2006 ). To our knowledge, linking collapsed inference to\\nattention in deep learning is completely novel.\\n3. Transformer Attention\\n3.0.1. A TTENTION AS EXPECTATION\\nWe begin by demonstrating transformer attention is best\\nseen as an expectation over latent variables. In the case\\nof self and cross-attention, the expectation of a neural net -\\nwork with respect to possible adjacency structures.\\nLetx= (x1,..,xn)be observed variables, φbe some\\nset of latent variables, and ya variable we need to pre-\\ndict. Given a latent variable model p(y,x,φ) =p(y|\\nx,φ)p(x,φ), wherep(y|x,φ)is parameterised by some\\nfunctionv(y,x,φ)e.g. a neural network.\\nOur goal is to ﬁnd p(y|x), howeverφare unobserved so\\nwe calculate the marginal likelihood.\\np(y|x) =∑\\nφp(φ|x)v(y,x,φ)\\nImportantly, the softmax function is a natural representa-\\ntion for the posterior\\np(φ|x) =p(x,φ)∑\\nφp(x,φ)\\np(φ|x) =softmax (lnp(x,φ))\\nHence, transformer attention can be seen as weighting\\nv(x,φ)by the posterior distribution p(φ|x).\\np(y|x) =∑\\nφsoftmax (lnp(x,φ))v(y,x,φ)\\n=Ep(φ|x)[v(y,x,φ)](1)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='82cfa96d-ab11-4116-9c05-d792af83eec5', embedding=None, metadata={'page_label': '3', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='6995c82104fd36ee0a58a6e46d335f5ea281a5b887d1dad0878499818d42ffe2', text='Attention: Marginal Probabiliy is All You Need?\\nWe claim ( 1) is exactly the equation underlying self and\\ncross-attention. To make a more direct connection, we\\npresent the speciﬁc generative models corresponding to\\nthem. The latent variables φare identiﬁed as possible rela-\\ntionships , or edges, between each of the observed variables\\nx(keys and queries).\\nA natural formalism for modelling these graphical relation -\\nships is Markov Random Fields.\\n3.0.2. P AIRWISE MARKOV RANDOM FIELDS\\nGiven a set of random variables X= (Xv)v∈Vwith prob-\\nability distribution [p]and a graph G= (V,E). The vari-\\nables form a pairwise Markov random ﬁeld (MRF) with\\nrespect toGif the joint density function P(X=x) =p(x)\\nfactorises as follows\\np(x) =1\\nZexp(∑\\nv∈Vψv+∑\\ne∈Eψe)\\nwhereZis the partition function ψv(xv)andψe=\\nψu,v(xu,xv)are known as the node and edge potentials\\nrespectively1.\\nBeyond the typical set-up, we add a structural prior p(E)\\nover the adjacency structure of the underlying graph.\\np(x,E) =P(x|E)P(E)\\n=1\\nZp(E)exp(∑\\nv∈Vψv+∑\\ne∈Eψe)\\nWe brieﬂy remark that ( 1) respects factorisation of [p]in the\\nfollowing sense; if the distribution admits a factorisatio n\\nwith respect to the latent variables p(x,φ) =∏\\nifi(x,φi)\\nandv(x,φ) =∑\\nivi(x,φi)then (applying the linearity of\\nexpectation) we may write\\nEp(φ|x)[v(x,φ)] =∑\\niEp(φi|x)[vi] (2)\\nPermitting each factor to be marginalised independently.\\nIn the case of an MRF, such a factorisation is natural. If\\nthe distibution over edges factorises into local distribut ions\\np(E) =∏\\nip(Ei)(using independence properties of the\\nMRF) we can write p(x,E) =1\\nZ∏\\nifi(x,Ei)where each\\nfi=P(Ei)exp∑\\nv∈Vψv∑\\ne∈Eiψeis itself an unnor-\\nmalised MRF.\\nTo recover cross-attention and self-attention are such mod -\\nels with we need only specify a structural prior and poten-\\ntial functions.\\n3.0.3. C ROSS ATTENTION\\n• Key nodes K= (x1,..,xn)\\n1See ( Shah et al. ,2021 ) for a precise deﬁnition.x1\\nx2\\nx3\\nx4\\nxnx′\\nmx′\\n3x′\\n2x′\\n1\\n......\\na Cross Attentionx1x2\\nx3\\nx4\\nx5x6xnx1x1x1x1x1x1x1\\nb Self Attention\\nx1\\nx2\\nx3\\nx4\\nxnzmz3z2z1\\n......\\nc Modern Continous Hop-\\nﬁeld Networkx1\\nx2\\nx3\\nx4\\nxnzmzmzmzmzmz3z3z3z3z3z2z2z2z2z2z1z1z1z1z1\\n......\\nd Slot Attention\\nFigure 1. Comparison of different attention modules in the liter-\\nature, the highlighted edges is representative of the margi nalisa-\\ntion being performed for the random variable E1, in1aand1b\\nall nodes are observed, as opposed to 1cand1d, where there are\\nlatent nodes (indicated in grey).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='77615da1-6df3-4af4-87d6-21e24aa2b05a', embedding=None, metadata={'page_label': '4', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='b45d545af85277ed9edf6bc079dc1ab42646ea484f050d716410438f060b6735', text='Attention: Marginal Probabiliy is All You Need?\\n• Query nodes Q= (x′\\n1,...,x′\\nm)\\n• Structural prior p(E) =∏m\\ni=1p(Ei), whereEi∼\\nUniform {(x1,x′\\ni),..,(xn,x′\\ni)}, such that each query\\nnode is uniformly likely to connect to each key node.\\n• Edge potentials ψ(xj,x′\\ni) =x′T\\niWT\\nQWKxj, in effect\\nmeasuring the similarity of xjandx′\\niunder a certain\\ntransformation.\\n• Value function Vi(K,Q,E i) =WVxs(Ei), a linear\\ntransformation applied to the node, xs(Ei), the start of\\nthe edgeEi.\\nTaking the posterior expectation in each of the factors de-\\nﬁned in two ( 2) gives the standard cross- attention mecha-\\nnism\\nEp(Ei|Q,K)[Vi] =∑\\njsoftmax j(x′T\\niWT\\nQWKxj)WVxj\\nEp(E|Q,K)[V] =softmax (QTWT\\nQQKK)WVK\\n3.0.4. S ELFATTENTION\\n• NodesK=Q= (x1,..,xn)\\n• Structural prior p(E) =∏n\\ni=1p(E→\\ni), whereE→\\ni∼\\nUniform {(x1,xi),..,(xn,xi)}, such that each node\\nis uniformly likely to connect to every other node.\\n• Edge potentials ψ(kj,ki) =xT\\niWT\\nQWKxj, in effect\\nmeasuring the similarity of xjandx′\\niunder a certain\\ntransformation.\\n• Value function Vi(K,Q,E i) =WVxs(Ei), a linear\\ntransformation applied to the node, xs(Ei), the start of\\nthe edgeEi.\\nAgain, taking the posterior expectation in each of the fac-\\ntors deﬁned in two ( 2) gives the standard self- attention\\nmechanism\\nEp(Ei|Q,K)[Vi] =∑\\njsoftmax j(xT\\niWT\\nQWKxj)WVxj\\nEp(E|Q,K)[V] =softmax (KTWT\\nQWKK)WVK\\n4. Iterative Attention\\nWe continue by extending attention to full Bayesian infer-\\nence. In essence applying the attention trick, marginali-\\nsation of attention variables, to the variational free ener gy\\n(a.k.a the ELBO).\\nModern Continuous Hopﬁeld Networks can be seen as\\na particular instance of this class of system, allow-\\ning us to reproduce the ‘hopﬁeld attention’ updates of(Ramsauer et al. ,2021 ) within a probabilistic context. Un-\\nder different structural priors we recover other iterative\\nattention models; slot-attention ( Locatello et al. ,2020 ),\\nblock-slot attention ( Singh et al. ,2022 ) and Perciever\\n(Jaegle et al. ,2021 ). Further, we showcase a speciﬁc ad-\\nvantage of bayesian attention, hard attention.\\n4.0.1. C OLLAPSED INFERENCE\\nWe present a version of collapsed variational inference\\n(Teh et al. ,2006 ) showing how this results in a bayesian\\nattention mechanism. The term attention mechanism is apt\\ndue to the surprising similarity in form between the varia-\\ntional updates ( 6) and neural attention mechanism ( 1).\\nOur setting is the latent variable model p(x,z,φ), wherex\\nare observed variables, and z,φ, are latent variables. Typi-\\ncally we wish to infer zgivenx.\\nCollapsed inference proceeds by marginalising out the ex-\\ntraneous latent variables φ\\np(x,z) =∑\\nφp(x,z,φ) (3)\\nWe deﬁne a recognition density q(z)∼N(z;µ)and opti-\\nmise the variational free energy with respect to the parame-\\nters,µ, of this distribution.\\nmin\\nµF(x,µ) =Eq[lnqµ(z)−lnp(x,z)]\\nUnder a typical Laplace approximation, we can write the\\nvariational free energy as F≈ −lnp(x,µ)2. Substituting\\nin (3) and taking the derivative with respect to the varia-\\ntional parameters yields,\\nF(x,µ) =−ln∑\\nφp(x,µ,φ)\\n∂F\\n∂µ=−1∑\\nφp(x,µ,φ)∑\\nφ∂\\n∂µp(x,µ,φ) (4)\\nWhich connects bayesian attention with the standard atten-\\ntion ( 1). To clarify this, we employ the log-derivative trick,\\nsubstituting pθ=elnpθand re-express ( 4) in two ways:\\n∂F\\n∂µ=−∑\\nφsoftmax φ(lnp(x,µ,φ))∂\\n∂µlnp(x,µ,φ)\\n(5)\\n∂F\\n∂µ=Ep(φ|x,µ)[−∂\\n∂µlnp(x,µ,φ)] (6)\\nThe ﬁrst form reveals the softmax which is ubiquitous in\\nall attention models. The second, suggests the variational\\n2See appendix for a more principled derivation taking accoun t\\nof higher order terms', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='13815825-6039-482c-871f-33606c6d549a', embedding=None, metadata={'page_label': '5', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='b89cfde33a23bed027cb1e61c07dab506b110ce0a5bcd5c559725b7e047c4913', text='Attention: Marginal Probabiliy is All You Need?\\nupdate should be evaluated as the expectation of the typi-\\ncal variational gradient (the term within the square brack-\\nets) with respect to the posterior over the parameters repre -\\nsented by the random variable φ.\\nIn other words, bayesian attention is exactly the nueral\\nattention mechanism applied iteratively, where the value\\nfunction is the variational free energy gradient. We derive\\nupdates for a general MRF before again recovering (itera-\\ntive) attention models in the literature by specifying part ic-\\nular distributions.\\n4.0.2. F REE ENERGY OF A MARGINALISED MRF\\nRecall the factorised MRF, p(E) =∏\\nip(Ei).\\np(x,E) =1\\nZ∏\\nifi(x,Ei)with each fi=\\nP(Ei)exp∑\\nv∈Vψv∑\\ne∈Eiψe. Independence prop-\\nerties mean the marginalisation necessary for collapsed\\ninference can be simpliﬁed\\n∑\\nEp(x,E) =1\\nZ∏\\ni∑\\nEifi(x,Ei)\\nIn an inference setting the nodes are partitioned into ob-\\nserved nodes, x, and latent nodes, z. The variational free\\nenergy ( 4) and the associated forms of it’s derivative can be\\nexpressed\\nF(x,µ,θ) =−∑\\niln∑\\nEifi(x,µ,E i)\\n∂F\\n∂µj=−∑\\ni∑\\nEisoftmax (fi(x,µ,E i))∂fi\\n∂µj\\nSimilar to hard attention approaches, the random variable\\nEis an explicit alignment variable. However, unlike hard\\nattention, we avoid inferring Eexplicitly using the col-\\nlapsed inference approach outlined above.\\n4.0.3. Q UADRATIC POTENTIALS AND THE CONVEX\\nCONCAVE PROCEDURE\\nWe follow ( Ramsauer et al. ,2021 ) in using the CCCP to\\nderive a ﬁxed point equation, which necessarily reduces the\\nfree energy.\\nAssuming the node potentials are quadratic ψ(xi) =−1\\n2x2\\ni\\nand the edge potentials have the form ψ(xi,xj) =xiWxj.\\nµ∗\\nj=∑\\ni∑\\nEisoftmax (gi(x,µ,E i))∂gi\\n∂µj(7)\\nWheregi=∑\\ne∈Eiψe.\\nBy way of the CCCP ( Yuille & Rangarajan ,2001 ), this\\nﬁxed point equation has the property F(x,µ∗\\nj,θ)≤F(x,µj,θ)with equality if and only if µ∗\\njis a stationary\\npoint ofF.\\nWe follow the 3in specifying speciﬁc structural priors and\\npotential functions to recover different iterative attent ion\\nmechanisms.\\n4.0.4. H OPFIELD -STYLE CROSS ATTENTION\\nLet the observed x= (x1,..,xn)and latent nodes z=\\n(z1,..,zm)have the following structural prior p(E) =∏m\\ni=1p(Ei), whereEi∼Uniform {(x1,zi),..,(xn,zi)}.\\nAnd deﬁne edge potentials ψ(xj,zi) =ziQTKxj, Appli-\\ncation of ( 7)\\nµ∗\\ni=∑\\njsoftmax j(µiWT\\nQWKxj)WT\\nQWKxj\\nWhenµiis initialised to some query ξthe system\\n(Ramsauer et al. ,2021 ) the ﬁxed point update is given by\\nµ∗\\ni(ξ) =Ep(Ei|x,ξ)[WT\\nQWKxt(Ei)]. When the patterns x\\nare well separated, µ∗\\ni(ξ)≈WT\\nQWKxj, whereWT\\nQWKxj\\nis the closest vector and hence can be used as an associative\\nmemory.\\n4.0.5. S LOT ATTENTION\\nSlot attention ( Locatello et al. ,2020 ) is an object centric\\nlearning module built on top of an iterative attention mech-\\nanism. Here we show this is a simple adjustment of the\\nprior beliefs on our edge set.\\nWith the same set of nodes and potentials, replace the\\nprior over edges with p(E) =∏n\\nj=1p(Ej),Ej∼\\nUniform {(xj,z1),..,(xj,zm)}\\nµ∗\\ni=∑\\njsoftmax i(µiQTKxj)QTKxj\\nWhilst the original slot attention employed an RNN to aid\\nthe basic update shown here, the important feature is that\\nthe softmax is taken over the ‘slots’, µ. This forces compe-\\ntition between slots to account for the observed variables,\\nforcing object centric representations. For example, if th e\\nobserved variables xare image patches, the slots are forced\\nto cluster similar patches together in order increase the ov er-\\nall likelihood of said patches. The word cluster is accurate ,\\nin fact there is an exact equivalence between this mecha-\\nnism and a step of EM on a gaussian mixture model.\\n4.0.6. B LOCK SLOT ATTENTION\\n(Singh et al. ,2022 ) suggest combining an associative mem-\\nory ability with an object-centric slot-like ability and pr o-\\nvide an iterative scheme for doing so, alternating between\\nslot-attention and hopﬁeld updates.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0f61caae-8a6a-448f-927a-434aabd5bbc4', embedding=None, metadata={'page_label': '6', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='5013d300177af9e4330046692d60b8b86c6aee055d1b7496bf1b49ab006c7897', text='Attention: Marginal Probabiliy is All You Need?\\nx1\\nx2\\nx3\\nx4\\nxnzmzmzmzmzmz3z3z3z3z3z2z2z2z2z2z1z1z1z1z1m1\\nm2\\nm3\\nm4\\nml......\\n...\\nFigure 2. Block Slot Attention\\nOur framework permits us to ﬂexibly combine different\\nattention mechanisms through different latent graph struc -\\ntures, allowing us to derive a model informed version of\\nblock-slot attention. In this setting we have three sets of\\nvariablesX, the observations, Zthe latent variables to be\\ninferred and Mwhich are parameters.\\nDeﬁne the pairwise MRF X={x1,...,xn},\\nZ={z1,...,zm}andM={m1,...,ml}with a\\nprior over edges p(E) =∏m\\nj=1p(Ej)∏l\\nk=1p(˜Ek),\\nEj∼Uniform {(xj,z1),..,(xj,zm)},˜Ek∼\\nUniform {(z1,mk),..,(zm,mk)}, with edge poten-\\ntials between XandZgiven byψ(xj,zi) =ziQTKxj\\nand between ZandM,ψ(zi,mk) =zi·mk\\napplying ( 7) gives\\nµ∗\\ni=∑\\njsoftmax i(µiQTKxj)QTKxj\\n+∑\\nksoftmax k(µi·mk)mk\\nIn the original block-slot attention each slot ziis broken\\ninto blocks, where each block can access block-speciﬁc\\nmemories i.e. z(b)\\nican has possible connections to mem-\\nory nodes {m(b)\\nk}k≤l. Allowing objects to be represented\\nby slots which in turn disentangle features of each object\\nin different blocks. We presented a single block version\\nabove, however it is easy to see that the update extends to\\nthe multiple block version applying ( 7) gives\\nµ∗\\ni=∑\\njsoftmax i(µiQTKxj)QTKxj\\n+∑\\nk,bsoftmax k(µ(b)\\ni·m(b)\\nk)m(b)\\nk5. Predictive Coding Networks\\nPredictive Coding Networks (PCN) have emerged\\nas an inﬂuential theory in computational neuro-\\nscience ( Rao & Ballard ,1999 ;Friston & Kiebel ,2009 ;\\nBuckley et al. ,2017 ). Building on theories of perception\\nas inference and the Bayesian brain, PCNs perform approx-\\nimate Bayesian inference by minimising the variational\\nfree energy which is manifested in the minimisation of\\nlocal prediction errors. The continuous time dynamics at\\nan individual neuron are given by\\n∂F\\n∂µi=−∑\\nφ−kφǫφ+∑\\nφ+kφǫφwφ\\nWhereǫare prediction errors, wrepresent synaptic strength\\nandkare node speciﬁc precisions representing uncertainty\\nin the generative model ( Millidge et al. ,2022 ).\\nA natural extension is to apply collapsed inference over\\nthe set of incoming and out going connection, i.e. a lo-\\ncally factorised prior over possible connectivity. In the n o-\\ntation of the previous section, we have an MRF with a hi-\\nerarchical structure Z={Z(0),...,Z(l),...,Z(N)}where\\nthe prior on edges factorises into layerwise p(E(l)) =\\n{(zi,zj) : (zi,zj)∈Z(l−1)×Z(l)}and potential func-\\ntionsφ(zi,zj) =ǫ2\\ni,j=kj(zj−wi,jzi)2.\\n∂F\\n∂µi=−∑\\nφ−softmax (−ǫφ2)kφǫφ\\n+∑\\nφ+softmax (−ǫφ2)kφǫφwφ\\nThe resulting dynamics induce a “normalisation” across\\nprediction errors received by a neuron through the softmax\\nfunction. This dovetails nicely with theories of attention\\nas normalisation in psychology and neuroscience. In con-\\ntrast previous predictive coding based theories of attenti on\\nhave focused on the precision terms, k, due to their abil-\\nity to up and down regulate the impact of prediction errors\\n(Feldman & Friston ,2010 ). Here we see the softmax term\\ncan also perform this regulation, while also exhibiting the\\nfast winner-takes-all dynamics that are associated with co g-\\nnitive attention.\\n5.1. Discussion\\nIn this section we will brieﬂy discuss what can be gained\\nfrom looking at the attention mechanism as a problem of\\ninference.\\n5.1.1. H ARD ATTENTION\\nRecall ( 1) neural attention may be viewed as calculating an\\nexpectation over latent variables Ep(φ|x)[v(x,φ)]. Here the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='89e90620-d682-47f8-8027-cf30eaa89aa1', embedding=None, metadata={'page_label': '7', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='ad787b96b89cdf6d7b0858ea3882cf5ed41e0f5fd146e0996213bd9939131bb2', text='Attention: Marginal Probabiliy is All You Need?\\nmechanism is ‘soft’ because we weight multiple possibil-\\nities of attention variable φ. Hard attention, on the other\\nhand, proceeds with a single sample from p(φ|x). It has\\nbeen argued this is more biological, more interpretable and\\nhas lower computational complexity. Previously the infe-\\nrior performance of hard-attention has been attributed to\\nit’s hard to train, stochastic nature. However, our framing\\nof soft attention as exact marginalisation offers an altern ate\\nexplanation. Stochastic approximations (hard attention)\\nwill always suffer compared with exact marginalisation\\n(soft attention). Further our framework provides a method\\nfor seamlessly interchanging hard and soft-attention. Sin ce\\nthe distribution p(φ|x)a the categorical distribution, at\\nany point (during training or inference) it is possible to im -\\nplement hard attention by taking a single sample φ∗from\\np(φ|x)yieldingv(x,φ∗).\\nThere are two issues with this approach to collapsing the\\nattention distribution. First, the single sample will coll apse\\nany uncertainty, secondly calculation of p(φ|x), in order\\nto sample, still incurs a quadratic penalty O(n2). However\\nwe can employ tools from probability theory to help us anal-\\nyse the cost of sampling, and linear approximations to the\\nattention distribution.\\n5.1.2. E FFICIENT TRANSFORMERS\\nConsider some distribution qattempting to approximate\\np(φ|x)we can quantify the information loss with the rela-\\ntive entropy\\nL[p,q]≜DKL[q(φ)||p(φ|x)] =H[q]+Eq[p(φ|x)]\\nIn the hard attention approximation a single sample from\\npis used as an approximation L[p,q] =−lnp(φ∗|x)\\nand perhaps intuitively E[L] =H[p]i.e. hard attention\\nis a good approximation when the attention distribution is\\nlow-entropy which can be controlled by the temperature pa-\\nrameter (Appendix ??).\\nMany of the efﬁcient alternatives to attention, such as low-\\nrank and linear approximations, can be cast as approximat-\\ningp(φ|x)withq(φ|x)where calculating qis less expen-\\nsive than exact marginalisation. Estimating Lcould be used\\nto quantify the relative information loss when using these\\nalternatives. Another direction taken to reduce computa-\\ntional complexity of the attention mechanism is sparsiﬁca-\\ntion the attention matrix, which in our framework reduces\\nto adjustments to the prior over edges (Appendix ??).\\n5.1.3. N EWDESIGNS\\nThe main difference between the description presented and\\nprevious probabilistic descriptions is to view soft attent ion\\nas a principled, exact, probabilistic calculation, with re -\\nspect to an implicit probabilistic model, as opposed to an\\nimpoverished approximation. This leads to possibility ofdesigning new attention mechanisms by altering the distri-\\nbution that the mechanism marginalises over, either by ad-\\njusting the structural prior, or the potential functions. W e\\nhope this will enable new architectures to be designed in a\\nprincipled manner.\\nReferences\\nAnnabi, L., Pitti, A., and Quoy, M. On the Re-\\nlationship Between Variational Inference and\\nAuto-Associative Memory, October 2022. URL\\nhttp://arxiv.org/abs/2210.08013 .\\narXiv:2210.08013 [cs].\\nBaxter, J. A Model of Inductive Bias Learning. Journal\\nof Artiﬁcial Intelligence Research , 12:149–198, March\\n2000. ISSN 1076-9757. doi: 10.1613/jair.731. URL\\nhttps://www.jair.org/index.php/jair/article/view/10 253.\\nBuckley, C. L., Kim, C. S., McGregor, S., and Seth,\\nA. K. The free energy principle for action and\\nperception: A mathematical review. Journal of\\nMathematical Psychology , 81:55–79, December 2017.\\nISSN 0022-2496. doi: 10.1016/j.jmp.2017.09.004. URL\\nhttps://www.sciencedirect.com/science/article/pii/S 0022249617300962 .\\nDeng, Y ., Kim, Y ., Chiu, J., Guo, D., and Rush, A.\\nLatent Alignment and Variational Attention. In\\nAdvances in Neural Information Processing Sys-\\ntems, volume 31. Curran Associates, Inc., 2018. URL\\nhttps://proceedings.neurips.cc/paper/2018/hash/b691 334ccf10d4ab144d672f7783c8a3-Abstract.html .\\nDing, N., Fan, X., Lan, Z., Schuurmans, D., and Soricut, R.\\nAttention that does not Explain Away, September 2020.\\nURLhttp://arxiv.org/abs/2009.14308 .\\narXiv:2009.14308 [cs, stat].\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Min-\\nderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and\\nHoulsby, N. An Image is Worth 16x16 Words: Trans-\\nformers for Image Recognition at Scale, June 2021.\\nURLhttp://arxiv.org/abs/2010.11929 .\\narXiv:2010.11929 [cs] version: 2.\\nFeldman, H. and Friston, K. Attention, Uncer-\\ntainty, and Free-Energy. Frontiers in Human\\nNeuroscience , 4, 2010. ISSN 1662-5161. URL\\nhttps://www.frontiersin.org/articles/10.3389/fnhum. 2010.00215 .\\nFrecon, J., Gasso, G., Pontil, M., and Salzo, S. Breg-\\nman Neural Networks. In Proceedings of the\\n39th International Conference on Machine Learn-\\ning, pp. 6779–6792. PMLR, June 2022. URL\\nhttps://proceedings.mlr.press/v162/frecon22a.html .\\nISSN: 2640-3498.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='49785aa8-9ab4-4aaa-9ef5-ec2347060597', embedding=None, metadata={'page_label': '8', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='1b9aa55e6544acee57da090ef2524b131e79cf6229008f9baca1f388ad36d751', text='Attention: Marginal Probabiliy is All You Need?\\nFriston, K. and Kiebel, S. Predictive coding under\\nthe free-energy principle. Philosophical Trans-\\nactions of the Royal Society B: Biological Sci-\\nences , 364(1521):1211–1221, May 2009. ISSN\\n0962-8436. doi: 10.1098/rstb.2008.0300. URL\\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC266670 3/.\\nGabbur, P., Bilkhu, M., and Movellan, J. Probabilistic\\nAttention for Interactive Segmentation, July 2021.\\nURLhttp://arxiv.org/abs/2106.15338 .\\narXiv:2106.15338 [cs].\\nJaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zis-\\nserman, A., and Carreira, J. Perceiver: General\\nPerception with Iterative Attention. In Proceedings\\nof the 38th International Conference on Machine\\nLearning , pp. 4651–4664. PMLR, July 2021. URL\\nhttps://proceedings.mlr.press/v139/jaegle21a.html .\\nISSN: 2640-3498.\\nKim, Y ., Denton, C., Hoang, L., and Rush, A. M.\\nStructured Attention Networks, February 2017.\\nURLhttp://arxiv.org/abs/1702.00887 .\\narXiv:1702.00887 [cs].\\nLindsay, G. W. Attention in Psychology, Neuroscience,\\nand Machine Learning. Frontiers in Computational\\nNeuroscience , 14, 2020. ISSN 1662-5188. URL\\nhttps://www.frontiersin.org/articles/10.3389/fncom. 2020.00029 .\\nLocatello, F., Weissenborn, D., Unterthiner, T., Ma-\\nhendran, A., Heigold, G., Uszkoreit, J., Doso-\\nvitskiy, A., and Kipf, T. Object-Centric Learn-\\ning with Slot Attention, October 2020. URL\\nhttp://arxiv.org/abs/2006.15055 .\\narXiv:2006.15055 [cs, stat].\\nMillidge, B., Song, Y ., Salvatori, T., Lukasiewicz, T., and\\nBogacz, R. A Theoretical Framework for Inference and\\nLearning in Predictive Coding Networks, August 2022.\\nURLhttp://arxiv.org/abs/2207.12316 .\\narXiv:2207.12316 [cs].\\nNguyen, T. M., Nguyen, T. M., Le, D. D. D., Nguyen,\\nD. K., Tran, V .-A., Baraniuk, R., Ho, N., and\\nOsher, S. Improving Transformers with Proba-\\nbilistic Attention Keys. In Proceedings of the\\n39th International Conference on Machine Learn-\\ning, pp. 16595–16621. PMLR, June 2022. URL\\nhttps://proceedings.mlr.press/v162/nguyen22c.html .\\nISSN: 2640-3498.\\nRamsauer, H., Sch¨ aﬂ, B., Lehner, J., Seidl, P., Widrich,\\nM., Adler, T., Gruber, L., Holzleitner, M., Pavlovi´ c,\\nM., Sandve, G. K., Greiff, V ., Kreil, D., Kopp, M.,\\nKlambauer, G., Brandstetter, J., and Hochreiter, S.\\nHopﬁeld Networks is All You Need, April 2021.URLhttp://arxiv.org/abs/2008.02217 .\\narXiv:2008.02217 [cs, stat].\\nRao, R. P. N. and Ballard, D. H. Predictive cod-\\ning in the visual cortex: a functional interpre-\\ntation of some extra-classical receptive-ﬁeld ef-\\nfects. Nature Neuroscience , 2(1):79–87, January\\n1999. ISSN 1546-1726. doi: 10.1038/4580. URL\\nhttps://www.nature.com/articles/nn0199_79 .\\nNumber: 1 Publisher: Nature Publishing Group.\\nShah, A., Shah, D., and Wornell, G. On Learn-\\ning Continuous Pairwise Markov Random Fields.\\nInProceedings of The 24th International Con-\\nference on Artiﬁcial Intelligence and Statistics ,\\npp. 1153–1161. PMLR, March 2021. URL\\nhttps://proceedings.mlr.press/v130/shah21a.html .\\nISSN: 2640-3498.\\nShankar, S., Garg, S., and Sarawagi, S. Surprisingly Easy\\nHard-Attention for Sequence to Sequence Learning. In\\nProceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing , pp. 640–645, Brus-\\nsels, Belgium, October 2018. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/D18-1065. URL\\nhttps://aclanthology.org/D18-1065 .\\nSingh, G., Kim, Y ., and Ahn, S. Neural Block-\\nSlot Representations, November 2022. URL\\nhttp://arxiv.org/abs/2211.01177 .\\narXiv:2211.01177 [cs].\\nTeh, Y ., Newman, D., and Welling, M. A Collapsed\\nVariational Bayesian Inference Algorithm for Latent\\nDirichlet Allocation. In Advances in Neural Information\\nProcessing Systems , volume 19. MIT Press, 2006. URL\\nhttps://proceedings.neurips.cc/paper_files/paper/20 06/hash/532b7cbe070a3579f424988a040752f2-Abstract.h tml.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\\nJones, L., Gomez, A. N., Kaiser, L., and Polosukhin,\\nI. Attention Is All You Need, December 2017.\\nURLhttp://arxiv.org/abs/1706.03762 .\\narXiv:1706.03762 [cs].\\nWang, W., Bao, H., Dong, L., Bjorck, J., Peng,\\nZ., Liu, Q., Aggarwal, K., Mohammed, O. K.,\\nSinghal, S., Som, S., and Wei, F. Image as a\\nForeign Language: BEiT Pretraining for All Vi-\\nsion and Vision-Language Tasks, August 2022.\\nURLhttp://arxiv.org/abs/2208.10442 .\\narXiv:2208.10442 [cs].\\nYang, Y ., Huang, Z., and Wipf, D. Transform-\\ners from an Optimization Perspective, May 2022.\\nURLhttp://arxiv.org/abs/2205.13891 .\\narXiv:2205.13891 [cs].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5409fd2-7759-4d4d-b8ed-40503510aa01', embedding=None, metadata={'page_label': '9', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='63ce423cb82f11f4a189b02b5e7fad7c7f980199f9a8f6ae1a879dd89bec4088', text='Attention: Marginal Probabiliy is All You Need?\\nYuille, A. L. and Rangarajan, A. The Concave-Convex\\nProcedure (CCCP). In Advances in Neural Information\\nProcessing Systems , volume 14. MIT Press, 2001. URL\\nhttps://proceedings.neurips.cc/paper/2001/hash/a012 869311d64a44b5a0d567cd20de04-Abstract.html .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0bad7133-fa6b-4a1b-acd0-79d5f5542c8a', embedding=None, metadata={'page_label': '10', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='dc9a60866c57c4542bb31e9995cda0f21c1b58a45b8ee16f228a93f6af29affc', text='This figure \"example_figures.png\" is available in \"png\"\\n format from:\\nhttp://arxiv.org/ps/2304.04556v1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9fa894e9-00fc-4f9f-9dce-fad0b13b3989', embedding=None, metadata={'page_label': '11', 'file_name': '2304.04556v1.pdf', 'file_path': 'pdfs\\\\2304.04556v1.pdf', 'file_type': 'application/pdf', 'file_size': 181064, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='77a8b9e7d933d129ed710296248691c490592434b799dd625006abe8acee06c6', text='This figure \"example_graphics.PNG\" is available in \"PNG\"\\n format from:\\nhttp://arxiv.org/ps/2304.04556v1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4b3c8227-c217-4f27-adf0-03fa6d8108cb', embedding=None, metadata={'page_label': '1', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='393dbf743b9977d3b79e9d9b3f604f6c15658149c87c1400e7e07daf67ea7797', text='RITA: Group Attention is All You Need for Timeseries Analytics\\nJiaming Liang\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nliangjm@seas.upenn.eduLei Cao∗\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nlcao@csail.mit.eduSamuel Madden\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nmadden@csail.mit.edu\\nZachary Ives\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nzives@cis.upenn.eduGuoliang Li\\nTsinghua University\\nBeijing, China\\nliguoliang@tsinghua.edu.cn\\nABSTRACT\\nTimeseries analytics is of great importance in many real-world\\napplications. Recently, the Transformer model, popular in natu-\\nral language processing, has been leveraged to learn high quality\\nfeature embeddings from timeseries, core to the performance of\\nvarious timeseries analytics tasks. However, the quadratic time and\\nspace complexities limit Transformers’ scalability, especially for\\nlong timeseries. To address these issues, we develop a timeseries an-\\nalytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention , to address this scalability issue. Group attention dy-\\nnamically clusters the objects based on their similarity into a small\\nnumber of groups and approximately computes the attention at\\nthe coarse group granularity. It thus significantly reduces the time\\nand space complexity, yet provides a theoretical guarantee on the\\nquality of the computed attention. The dynamic scheduler of RITA\\ncontinuously adapts the number of groups and the batch size in the\\ntraining process, ensuring group attention always uses the fewest\\ngroups needed to meet the approximation quality requirement. Ex-\\ntensive experiments on various timeseries datasets and analytics\\ntasks demonstrate that RITA outperforms the state-of-the-art in\\naccuracy and is significantly faster — with speedups of up to 63X.\\n1 INTRODUCTION\\nMotivation. Many data driven applications involve processing\\nmassive timeseries data, including IoT [11], medical AI [14], stock\\nmarket [ 27], and so on. As such, there is a great need for timeseries\\nanalytics, such as forecasting [ 8], classification [ 20], clustering [ 31],\\nsimilarity search [ 39], and anomaly detection [ 50], with applications\\nranging from automatically diagnosing diseases [ 5], recognizing\\nhuman activities [29], to stopping financial fraud [59].\\nEffective feature extraction [ 40] lies at the core of almost all\\nthese timeseries analytics tasks. Recently researchers [ 61] have\\nstarted leveraging the self-supervised pre-training methodology of\\nTransformers [ 4,16,52], which have proven remarkably successful\\nin natural language processing (NLP), to automatically learn high\\nquality feature embeddings from timeseries. In NLP, self-supervised\\npre-training exploits the sequential patterns (correlations) among\\nthe words in sentences to produce contextualized feature embed-\\ndings. Timeseries bear similarity to natural language, because in\\ntimeseries data the sequential order among the values (stock price,\\nvolume, etc.) over time matters. That is, each value is highly cor-\\nrelated with other values observed before or after it. Therefore,\\n∗Corresponding Authorpre-training a Transformer model which takes the correlations\\namong different observations into account is a natural idea to learn\\nfeature embeddings from timeseries. Indeed, the experiments in [ 61]\\nconfirm that Transformer-based methods outperform traditional\\ntimeseries analytics techniques.\\nHowever, existing work [ 61] that directly applies Transformers\\nto learn features from timeseries data have been shown not to be\\nscalable to long timeseries [ 30]. The idea of self-attention [ 52] is\\ncentral to pre-training methods in NLP: It computes pairwise cor-\\nrelations among different semantic units in a sequence (in NLP, a\\nsentence); as such, it has quadratic time and space complexity in\\nthe length of the input sequence. Such an approach places limits on\\nthe model’s scalability, especially when handling large sequences,\\nwhich are common in real-world timeseries applications such as\\nIoT, medical AI, and finance [ 6,34,62]. Predictions about timeseries\\nmay need to look at months or years of historical data to make ac-\\ncurate predictions, spanning hundreds of thousands of samples. As\\nan example, in collaboration with a research hospital we have been\\ndeveloping a seizure classifier that automatically detects seizures\\nbased on EEG signals (timeseries) collected during the clinical ob-\\nservation of patients. As seizures last only a few seconds, we chunk\\nlong EEG data into many 2 second segments and detect seizures at\\na segment level. However, the classification of a particular segment\\ndepends on up to 12 hours of prior signal to determine if one 2\\nsecond segment indicates seizure or not, because seizure diagnosis\\nneeds to consider long-term trends in the EEG data [ 6]. The number\\nof segments in 12 hours is more than 21k. This is far larger than\\nthe number of semantic units the typical NLP tasks expect. For\\nexample, BERT [ 16] limits the number of units to 512 and even\\nmassive models like GPT-3 [4] limit the number of units to 2048.\\nAlthough in NLP some lower-complexity methods have been\\nproposed to approximately compute self-attention [ 10,26,54], their\\nperformance degrades dramatically when used on timeseries, due\\nto the gap between natural language and timeseries, as we will\\nshow in our experiments.\\nProposed Approach. To tackle the aforementioned problem, we\\ndevelop RITA , a Transformer-based timeseries analytics tool, which\\nuses a novel attention mechanism, called group attention , to scale\\nto long timeseries.\\nLeveraging the periodicity of timeseries, RITA chunks the input\\ntimeseries into segments and dynamically clusters the segments\\ninto a small number (denoted as 𝑁) of groups. Segments in the\\nsame group possess similar feature embeddings during the current\\ntraining iteration, thus enabling them to approximately share the\\n1arXiv:2306.01926v1  [cs.LG]  2 Jun 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='212f47a2-adf9-4142-a14e-8b893efedfa1', embedding=None, metadata={'page_label': '2', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='92cc0b55e42fee4ecbaf5eccfff11820800de8f1600e6c8055c0422cd7688161', text='computation of attention. As the timeseries increases in length,\\nmore sharing opportunities become available. RITA then computes\\nthe self-attention at a group level and produces a compressed group\\nattention matrix . In this way, group attention eliminates both com-\\nputation and memory bottlenecks in Transformer-style models and\\nthus more scalable to long timeseries.\\nHowever, making this idea effective and efficient in Transformer\\narchitectures is challenging for several reasons:\\n•Efficiently Producing High Quality Feature Embeddings.\\nAlthough RITA computes the attention matrix at a group level, to\\npreserve the quality of the feature embeddings, it still has to pro-\\nduce different embeddings for different segments. This is because\\neven if some segments share the attention score temporally, it does\\nnot mean they should have the same feature embedding. However,\\nusing the group attention matrix, the existing self-attention mech-\\nanism will only produce a single feature vector for each group. A\\nnaive solution would be to restore the original attention matrix\\nfrom the group attention matrix. However, in this case we again\\nget an attention matrix with quadratic space complexity. Because\\nGPUs have limited memory, GPU memory will remain a bottleneck\\nin group attention.\\n•The Number of Groups N. In RITA, the number of groups\\n𝑁is a crucial factor that balances the speed up and the quality of\\nattention approximation. A small 𝑁will lead to a large speedup,\\nbut the approximation errors can also be significant. On the other\\nhand, although a large 𝑁tends to produce high-quality approxima-\\ntions, it inevitably slows down the training process. Therefore, an\\nappropriate 𝑁is essential to the performance of group attention.\\nHowever,𝑁depends on the distributional properties of the dataset.\\nFurthermore, like the classical transformer models, RITA stacks\\nmultiple attention layers to produce better embeddings. Ideally,\\ndifferent layers should also use different values of 𝑁. In addition,\\nduring the model training phrase, group attention should use dif-\\nferent values of 𝑁at different iterations to adapt to the varying\\nfeature embeddings. This makes manually setting appropriate 𝑁\\nalmost impossible.\\n•Batch Size. Moreover, as we want to dynamically adjust 𝑁\\nduring training, a fixed batch size is sub-optimal: as 𝑁decreases,\\nthe memory usage of a single sample decreases. This allows a larger\\nbatch size which is beneficial, because: (1) it makes full use of GPU\\nmemory; (2) high-parallelism across the samples in a big batch\\nbrings better performance. Our experimental study shows that\\ndoubling the batch size reduces the training time by 30%, while still\\npreserving the quality of the model. Thus, RITA should dynamically\\nadjust batch size as 𝑁changes.\\nTo address the above problems, we first propose an embedding\\naggregation strategy and a customized group softmax function to\\nreplace the classical softmax function [ 52]. Together they ensure\\nRITA is able to directly use the compressed attention matrix to\\nproduce different feature embeddings for different segments. We\\ntheoretically show the embeddings RITA produces in this way are\\nidentical to those produced by first re-storing the original large\\nattention matrix. Thus RITA is able to produce high quality embed-\\ndings without introducing extra overhead. Further, we design a GPU\\nfriendly algorithm to group the segments in parallel , effectively\\nminimizing the grouping cost.\\nP0PositionEmbeddingW1+++Window Embedding+E0\\nRawTimeseriesTime-aware ConvolutionW[CLS]W2⊗\\n.....WnP1P2.....Pn.....E1E2En.....O0O1O2On.....RITA Encoder\\nScale & InputFigure 1: RITA Architecture\\nSecond, we design an adaptive scheduler which dynamically de-\\ncides an appropriate 𝑁for each group attention layer during the\\ntraining process. It starts with a large 𝑁and iteratively merges\\ngroups that are similar to each other. Guided by an error bound on\\nthe approximated self-attention that users can tolerate, it automati-\\ncally determines if two groups are mergeable, performing merging\\nefficiently in a GPU-friendly way.\\nMoreover, we propose a learning-based method to model the\\ncorrelation between the number of groups 𝑁and the batch size 𝐵.\\nThis model is used to predict 𝐵for a given𝑁when training RITA.\\nSpecifically, we first sample some 𝑁values in a reasonable range.\\nFor each sampled 𝑁, we find a batch size that consumes up to a\\ncertain percentage of GPU memory in a cost-efficient way. Using a\\nsmall set of mathematical functions as a prior, RITA learns a model\\nwith only a few <N, B> pairs as ground truth labels.\\nOur experiments on public timeseries benchmarks and the MGH\\nEEG data [ 6] confirm that RITA outperforms state-of-the-art meth-\\nods in accuracy on various timeseries analytics tasks, while our\\ngroup attention mechanism achieves a 63X speedup with much\\nless memory required, compared to existing self-attention mecha-\\nnisms [10, 52, 54].\\nContributions. The key contributions of this work include:\\n•Our group attention mechanism leverages the periodicity of\\ntimeseries, reducing the time and space complexity of the self-\\nattention mechanism with accuracy guarantees, allowing RITA to\\nscale to long timeseries data.\\n•Guided by an approximation error bound, our adaptive sched-\\nuler dynamically adapts the number of groups and the batch size\\nto the distribution properties of the evolving feature embeddings,\\nmaking group attention efficient and easily tunable.\\n•We conduct experiments on various datasets and different ana-\\nlytics tasks, demonstrating that RITA is 4 to 63 times faster than\\nthe state-of-the-art while achieving better accuracy when handling\\nlong timeseries (length ≥2000).\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60ad65f3-ac75-4d6a-aab5-95bc66be7048', embedding=None, metadata={'page_label': '3', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='fcf8cd4e8280b6bc583520b9a875717733ca0d4cb2224b72cb0ba8687649095a', text='2 BACKGROUND\\nWe provide some background on the canonical self-attention mod-\\nule in the Transformer[ 52]. Aself-attention module takes 𝑛hidden\\nembedding vectors 𝐻∈R𝑛∗𝑑ℎas input, then projects them to\\nqueries (𝑄), keys (𝐾) and values ( 𝑉) and performs Scaled-dot Prod-\\nuct Attention, which given input hidden state 𝐻, is computed by:\\n𝑄=𝐻𝑊𝑄,𝐾=𝐻𝑊𝐾,𝑉=𝐻𝑊𝑉\\n𝑂=𝐴𝑉=𝑆𝑜𝑓𝑡𝑀𝑎𝑥(𝑄𝐾𝑇\\n√︁\\n𝑑𝑘)𝑉(1)\\nWhere𝑊𝑄∈R𝑑ℎ∗𝑑𝑘,𝑊𝐾∈R𝑑ℎ∗𝑑𝑘,𝑊𝑉∈R𝑑ℎ∗𝑑𝑣are projection\\nmatrices for generating 𝑄,𝐾,𝑉 .𝑄∈R𝑛∗𝑑𝑘is also regarded as the\\npacking of𝑛query vectors{𝑞1,...,𝑞𝑛}with dimension 𝑑𝑘into a\\nmatrix.𝐾∈R𝑛∗𝑑𝑘,𝑉∈R𝑛∗𝑑𝑣are regarded as the packing of key\\nvectors{𝑘1,...,𝑘𝑛}and value vectors{𝑣1,...,𝑣𝑛}in the same way.\\nGiven a matrix 𝑀∈R𝐿∗𝑛, the softmax function normalizes 𝑀\\nto ensure the sum of each row equals to 1, as shown below.\\n𝑆𝑜𝑓𝑡𝑀𝑎𝑥(𝑀𝑖,𝑗)=𝑒𝑥𝑝(𝑀𝑖,𝑗)\\nÍ𝑛−1\\n𝑘=0𝑒𝑥𝑝(𝑀𝑖,𝑘)(2)\\nNote the attention matrix A is an 𝑛×𝑛matrix, where 𝑛represents\\nthe number of elements in the input sequence (e.g. words in NLP).\\n3 RITA OVERVIEW\\nGiven a collection of unlabeled timeseries, RITA first pre-trains\\na Transformer-style model to produce high quality feature em-\\nbeddings for timeseries data. This pre-trained model is then used\\nto support various downstream tasks, similar to BERT [ 16]. Next,\\nwe overview the model architecture of RITA. We show how RITA\\nsupports various downstream tasks in Appendix A.7.\\nAs shown in Fig. 1, RITA is consist of two components: (1) Time-\\naware Convolution Layer (2) RITA Encoder.\\nTime-aware Convolution Layer fills the gap between timeseries\\nand natural language. Despite their high-level similarity, there is a\\nbig gap between timeseries and natural language. First, in natural\\nlanguage each word, as a discrete semantic unit, has an indepen-\\ndent meaning, while each element in a timeseries is a continuous,\\nnumerical value and does not necessarily constitute an independent\\nevent. Furthermore, the input sequences are single-channeled in\\nNLP, but often multi-channeled in timeseries (i.e., sensor data often\\nconsists of several related channels).\\nRITA leverages the classical convolution [ 28] strategy to solve\\nthis problem. Convolution is widely used to capture the local struc-\\ntures of an image. We use convolution to chunk one input timeseries\\ninto a sequence of windows and learn the local structure of each\\nwindow, similar to the discrete semantic units in natural language.\\nIt also discovers the correlations across different channels, thus\\nnaturally solving the multi-channel problem.\\nMore specifically, treating a multi-variate timeseries of length 𝑛\\nand with𝑚variables as an n×mmatrix𝑇, RITA uses𝑑convolution\\nkernels to chunk 𝑇intonwindows and produce one d-dimensional\\nembedding per window using the convolution operation [ 28]. Each\\nconvolution kernel corresponds to a w×mmatrix, where 𝑤defines\\nthe number of timestamps that each convolution kernel covers,\\nidentical to the window size in sliding window.\\nRITA Encoder functions as Transformer Encoder as described in\\nthe original Transformer work[ 52]. It takes the embeddings of 𝑛semantic units 𝑋1,𝑋2,...,𝑋𝑛(𝑋𝑖∈𝑅𝑑)as input (e.g. embeddings of\\n𝑛windows for a timeseries), then models the correlations between\\nthe semantic units and outputs 𝑌1,...,𝑌𝑛(𝑌𝑖∈𝑅𝑑)as the context-\\naware embedding of each unit.\\nWhat makes RITA Encoder different from Transformer Encoder\\nis that: at the core of Transformer Encoder lies self-attention mech-\\nanism which incurs a 𝑂(𝑛2)time complexity and memory usage.\\nThis quadratic cost becomes prohibitive for long timeseries and\\nlimits the scalablity of Transformer-based models. To make the\\nattention computation efficient yet high-quality, we replace the\\ncanonical self-attention with our proposed group attention .\\nSelf-supervised Pretraining. Inspired by the “cloze text” pre-\\ntraining task in NLP, we designed a mask-and-predict task as the\\npretraining task for our model. The timeseries is randomly masked\\nand the model should recover the masked values based on corre-\\nsponding contextual information.\\nTo be specific, we generate masks on time-stamps, with a mask\\nrate𝑝. The timeseries is scaled to be non-negative and the values\\nacross all the channels on the masked timestamps are set to be -1,\\nan impossible value on normal timestamps. Then the masked time-\\nseries is fed into RITA and the output representation is translated\\nto the recovered timeseries by a Transpose Convolution layer.\\n4 GROUP ATTENTION MECHANISM\\nGroup attention, a novel and efficient approximate attention mecha-\\nnism, addresses the performance bottleneck of self-attention in the\\nvanilla Transformer. In this section, we first introduce the frame-\\nwork of group attention and then theoretically establish the bound\\nof its approximation error.\\n4.1 The Idea of Group Attention\\nAs periodicity is a natural property of timeseries [ 56], similar\\nwindows frequently occur. Similar windows result in similar\\nqueries/keys for attention computation, bringing opportunities for\\nsaving computation.\\nAs discussed in Sec. 2, 𝐴𝑖𝑗, the attention score of window 𝑖onto\\nwindow𝑗, is determined by the inner product between the query\\nvector of window 𝑖and the key vector of window 𝑗, that is,𝑞𝑖·𝑘𝑗.\\nGiven another window 𝑥, if window 𝑥has the similar key vector\\nto window𝑗, that is,𝑘𝑗≈𝑘𝑥, then𝑞𝑖·𝑘𝑗≈𝑞𝑖·𝑘𝑥. In other words,\\n𝐴𝑖𝑗≈𝐴𝑖𝑥when𝑘𝑗≈𝑘𝑥.\\nThis observation inspires our group attention mechanism. That\\nis, we group the windows by their similarity in keys. Assuming\\nall windows in the same group have the same attention score onto\\nanother window 𝑘, we then only compute the attention once by\\nusing one single key to represent this group, for example the centroid\\nof the group of keys. This thus saves significant computation cost.\\nBetter yet, after grouping 𝑛windows into 𝑁groups, group atten-\\ntion compresses the attention matrix from an 𝑛×𝑛matrix to an 𝑛×𝑁\\nmatrix. Because 𝑁(number of groups) tends to be much smaller\\nthan𝑛(number of windows) due to the periodicity of timeseries,\\ngroup attention consumes much less memory than the original\\nself-attention mechanism, successfully eliminating the memory\\nbottleneck. Note that it also doesn’t hurt quality all that much, as\\nconfirmed in our experiments (Sec. 6.2).\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='84a8b73f-1af7-4cb4-806a-fed90c9c5c02', embedding=None, metadata={'page_label': '4', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='456100fe842ce4b180956399fbbe40389a161d2c095de011dd371071ff50e39d', text='GroupingAverageKQMatMulAttentionMatrixWeightedSoftMaxVSumAggregateTransposeMatMulOutputQ K VFigure 2: Group Attention\\n4.2 Computing the Output Feature Embedding\\nWe now discuss how to efficiently compute the output feature\\nembeddings using the small compressed group attention matrix.\\n4.2.1 Problem: Producing Embeddings w/ Group Attention Matrix\\nAs described in the Background, once we have acquired the at-\\ntention matrix 𝐴, canonical self-attention computes the output\\nembedding𝑂asO=AV. Because𝐴is an𝑛×𝑛matrix and𝑉is an\\n𝑛×𝑑𝑣matrix, the matrix product operation still produces an 𝑛×𝑑𝑣\\nmatrix𝑂. That is, it produces a 𝑑𝑣dimensional feature vector for\\neach window . However, our group attention will produce an 𝑛×𝑁\\nattention matrix e𝐴, where𝑁corresponds to the number of groups.\\nIn this case the matrix product will produce a 𝑁×𝑑𝑣matrixe𝑂. That\\nis, it produces a feature vector for each group . However, our goal\\nis to produce different embeddings for different windows, because\\neven if some windows share the attention score temporally, it does\\nnot mean they should have the same feature embedding.\\nA Naive Solution. A naive solution would be to restore the full\\nattention matrix 𝐴from the group attention matrix e𝐴. For example,\\ngiven one group composed of 𝑤𝑖𝑛𝑖and𝑤𝑖𝑛𝑗, we map its group\\nattention vector in e𝐴into two rows that correspond to 𝑤𝑖𝑛𝑖and\\n𝑤𝑖𝑛𝑗in𝐴. However, in this case we again get a 𝑛×𝑛attention\\nmatrix; and GPU memory remains a bottleneck in group attention.\\n4.2.2 Solution: Embedding Aggregation and Group SoftMax\\nUsing an embedding aggregation operation and a group softmax\\nfunction, RITA produces 𝑛embeddings without restoring the full\\nattention matrix. Fig. 2 shows the workflow of group attention.\\nEmbedding Aggregation. The idea is inspired by the observation\\non the matrix product operation O=AVconducted on the fully\\nrestored attention matrix 𝐴.\\nGiven an element 𝑂𝑖,𝑗of𝑂corresponding to the 𝑗𝑡ℎdimension of\\n𝑤𝑖𝑛𝑖’s feature vector, 𝑂𝑖,𝑗=𝑎𝑖·𝑣𝑗, where vector ai∈Rndenotes the\\n𝑖𝑡ℎrow of the attention matrix 𝐴and vector vj∈Rndenotes the 𝑗𝑡ℎ\\ndimension of all the 𝑛feature vectors. Given ai=<a1\\ni,a2\\ni,···,an\\ni>\\nandvj=<v1\\nj,v2\\nj,···,vn\\nj>,𝑂𝑖,𝑗=Ín\\nk=1ak\\nivk\\nj.\\nAs an example, assume 𝑤𝑖𝑛 1and𝑤𝑖𝑛 2belong to the same group\\n𝐺1. Then𝑎1\\n𝑖=𝑎2\\n𝑖=e𝑎1\\n𝑖, where e𝑎1\\n𝑖∈e𝐴corresponds to the attention\\nof group𝐺1onto𝑤𝑖𝑛𝑖. Therefore, 𝑎1\\n𝑖𝑣1\\n𝑗+𝑎2\\n𝑖𝑣2\\n𝑗=e𝑎1\\n𝑖(𝑣1\\n𝑗+𝑣2\\n𝑗).\\nAs an immediate generalization of the above analysis, if we ag-\\ngregate up the windows that belong to the same group and convert\\nthe n-dimensional feature vector 𝑣𝑗into a𝑁-dimensional group fea-\\nture vector e𝑣𝑗beforehand, we could directly use the group attention\\nvectore𝑎𝑖and the group feature vector e𝑣𝑗to compute 𝑂𝑖,𝑗.Using embedding aggregation, RITA is able to produce the fea-\\nture embedding e𝑂that is identical to the embedding 𝑂produced\\nby using the full attention matrix 𝐴and the embedding matrix 𝑉.\\nGroup Softmax Function. In canonical self-attention the atten-\\ntion matrix 𝐴is computed as 𝐴=SoftMax(QKT√\\ndk). To compute 𝐴,\\nwe have to first compute 𝑄𝐾𝑇(denoted as 𝑃) which is an 𝑛×𝑛\\nmatrix. Then normalizing the 𝑃matrix with softmax produces the\\nattention matrix 𝐴.\\nGroup attention follows the same procedure. But after grouping\\nkeys into e𝐾,𝑄e𝐾𝑇produces an 𝑛×𝑁matrix e𝑃. Due to the non-\\nlinearity of the softmax function, applying softmax directly on e𝑃\\nwill result in a group attention matrix e𝐴from which we are not able\\nto recover a full attention matrix that is identical to first restoring\\ne𝑃to𝑃and then applying softmax on 𝑃. The𝐴matrix produced\\nby the latter is desirable, as we want to approximate the original\\nattention matrix as accurately as possible. However, restoring the\\nsmall𝑛×𝑁e𝑃matrix is not memory efficient, as it will end up with\\na full𝑛×𝑛matrix𝑃.\\nTo solve the above problems, we introduce a new group softmax\\nfunction to replace the original softmax function (Eq. 2).\\n𝐺𝑟𝑜𝑢𝑝𝑆𝑜𝑓𝑡𝑀𝑎𝑥(g𝑃𝑖,𝑗)=𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑁−1\\n𝑘=0𝑐𝑜𝑢𝑛𝑡𝑘𝑒𝑥𝑝(𝑃𝑖,𝑘)(3)\\nIn Eq. 3,𝑐𝑜𝑢𝑛𝑡𝑘represents the number of windows that Group\\n𝐺𝑘contains. Compared to the original softmax, our group softmax\\nconsiders each group 𝐺𝑘as𝑐𝑜𝑢𝑛𝑡𝑘elements and counts it 𝑐𝑜𝑢𝑛𝑡𝑘\\ntimes when summing up the exponential of each group’s 𝑃𝑖,𝑘. In\\nthis way, the group softmax function operating on the small e𝑃\\nmatrix will produce exactly the same result to the softmax function\\noperating on the full 𝑃matrix.\\nTheoretical Guarantee. In Appendix A.4, we prove that the group\\nsoftmax function and the embedding aggregation operation produce\\nthe same output feature embedding with the naive method that has\\nto first restore the big full attention matrix.\\nWe show an efficient implementation of the embedding aggrega-\\ntion operation and group softmax function in Appendix A.2, Alg. 1.\\nTime Complexity. The time complexity of Alg. 1 is 𝑂(𝑛𝑁𝑑)and\\nthe space complexity is 𝑂(𝑛𝑁), while the time and space complexity\\nof the original self-attention mechanism are 𝑂(𝑛2𝑑)and𝑂(𝑛2).\\n4.3 Error Bound\\nGroup attention produces a group attention matrix e𝐴which approxi-\\nmates the attention matrix 𝐴produced by the classical self-attention\\nwith a bounded error , as shown in Lemma 1.\\nLemma 1. Let𝑅be the radius of the ball where all key vectors\\nlive;e𝑘𝑖be the representative of the group that contains key 𝑘𝑖. Let𝐴\\ndenote the full attention matrix restored from e𝐴. Suppose the distance\\nbetween e𝑘𝑖and𝑘𝑖(||ek𝑖−k𝑖||)satisfies:||ek𝑖−k𝑖||≤d.\\nThen∀𝜖>1, ifd≤ln(𝜖)\\n2R,1\\n𝜖≤Ai,j\\nAi,j≤𝜖\\nLemma 1 shows that the error bound 𝜖of the group attention is\\ndetermined by the distance 𝑑. As discussed in Sec. 5.1, it inspires\\nus to design a strategy to dynamically determine the number of\\ngroups𝑁– the most critical parameter of group attention. Please\\nrefer to Appendix A.5 for the proof.\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='21f8b7ce-d9c7-44ae-8865-9a063ca1e4bf', embedding=None, metadata={'page_label': '5', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='829194a8202fa185f5bfc40d8bcb6891ebbd0c8a4c382caa9d1358a9f47a87b5', text='4.4 GPU Friendly Grouping Method\\nIn this section, we discuss the implementation of a grouping method.\\nTo make group attention efficient and effective, the grouping\\nmethod has to satisfy the following requirements:\\n(1) Tight distance bound: to ensure the approximation quality,\\nthe distance between each key and its group representative should\\nbe minimized according to Lemma 1.\\n(2) Lightweight: to ensure the performance gain, the grouping\\nmethod must be lightweight, at worst not exceeding the complexity\\nof group attention itself ( 𝑂(𝑁𝑛)).\\n(3) GPU friendly: to take advantage of GPUs, we prefer a group-\\ning method that mainly consists of matrix operations, which can\\nbe efficiently executed on a GPU.\\nTo satisfy the above requirements, after thorough investigation\\non various clustering algorithms, we design a GPU friendly K-\\nmeans [35] as the grouping method.\\nFirst, K-means minimizes the overall distance between any object\\nand its cluster center, hence naturally satisfying Requirement 1.\\nSecond, given 𝑁centers, in each iteration the time and space\\ncomplexity of K-means is 𝑂(𝑛𝑁). Usually, the iteration goes until\\nconvergence. However, we observe that rather than seeking a per-\\nfect K-means clustering, training a few iterations is sufficient to\\nget a good grouping for group attention, because typically the later\\niterations only slightly update the clustering and group attention\\nis robust to such imperfection.\\nThird, we design a GPU-friendly implementation of K-means.\\nThe performance bottleneck of K-means comes from the dis-\\ntance computation between each vector and its center, that is,\\n|vi−cj|=√︃\\n(vi−cj)2,i∈[1,n],j∈[1,N]. The performance bot-\\ntleneck is𝑣𝑖−𝑐𝑗. We instead use a different formulation: |𝑣𝑖−\\n𝑐𝑗|=|vi−cj|=√︃\\n|vi|2+|cj|2−2vi·cj,i∈[1,n],j∈[1,N]. This is\\nbecause in this formulation, the performance bottleneck is 𝑣𝑖·𝑐𝑗,\\nwhich could be implemented as a matrix product operation. Al-\\nthough the complexity of the two formulations is the same, in GPUs\\nmatrix product is much more efficient than pairwise difference.\\n5 ADAPTIVE SCHEDULER\\nNext, we present the adaptive scheduler of RITA which addresses\\nthe challenges of determining an appropriate number of groups\\n𝑁and accordingly the batch size 𝐵, as described in Introduction.\\nUsing a dynamic scheduling method we propose, the scheduler\\nautomatically determines and adjusts 𝑁and𝐵based on the distri-\\nbutional properties of the feature embeddings produced over the\\niterative training process, while guaranteed to produce high quality\\nattention approximation that meets the requirement of users.\\nIn Sec. 5.1 we show how RITA automatically determines 𝑁. Then\\nwe introduce in Sec. 5.2 the learning-based method which given an\\n𝑁, immediately predicts a good batch size.\\n5.1 Dynamically Determining the Number of\\nGroups N\\nWithout loss of generality, we use one group attention module as\\nan example to show how RITA automatically gets an appropriate 𝑁.\\nThe adaptive scheduler of RITA starts with a large 𝑁and decreases\\nit dynamically. This is because in the training process of RITA, thefeature embeddings produced epoch by epoch tend to get stabler\\nand stabler and gradually converge, thus no need to increase 𝑁.\\nRITA reduces the number of groups by merging similar groups.\\nIntuitively, given two groups, we could measure their similarity\\nbased on the distance of their centers. If the distance between\\ntheir centers is smaller than a distance threshold, then the two\\ngroups could be merged. However, setting an appropriate distance\\nthreshold seems hard – as difficult as setting an appropriate 𝑁.\\nTo solve this problem, RITA leverages the error bound of group\\nattention introduced in Sec. 4.3. It only requires users to set an\\nerror bound 𝜖, and then uses Lemma 1 to translate 𝜖to a distance\\nthreshold𝑑. RITA then uses Lemma 2 to determine if merging some\\ngiven clusters still meets the error bound threshold 𝜖.\\nLemma 2. Denote𝑐𝑘to be the cluster center of 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘. Assume\\nthe existing grouping satisfies ∀k,max\\nx∈cluster k|ck−x|≤d, thus satis-\\nfying an error bound 𝜖by Lemma 1. If there exist 𝑚clusters, namely,\\n𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘1,𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘2,...,𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑘𝑚, satisfying that:\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘𝑖|𝑐𝑘𝑖−𝑐𝑘𝑗|+|𝑥−𝑐𝑘𝑖|≤𝑑,𝑖,𝑗∈[1,𝑚] (4)\\nmerging them into one cluster still meets the error bound 𝜖.\\nPlease refer to Appendix A.6 for the proof.\\nFinding the Mergable Clusters. We formulate the problem of\\nfinding mergeable clusters using graph theory:\\n(1) each cluster is a node in the graph;\\n(2) if𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖and𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗satisfy:\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖|𝑐𝑖−𝑐𝑗|+|𝑥−𝑐𝑖|≤𝑑, and𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗|𝑐𝑗−𝑐𝑖|+|𝑥−𝑐𝑗|≤𝑑\\nthere is an undirected edge between 𝑛𝑜𝑑𝑒𝑖and𝑛𝑜𝑑𝑒𝑗;\\nIn this scenario, finding the maximum number of mergeable\\nclusters is equivalent to finding the minimal clique cover in the\\ncorresponding graph, which is an NP-hard problem [ 24]. Such\\nheavy computation overhead is not acceptable for RITA. We thus\\noffer a simplified solution:\\n(1) Halve the clusters into two sets 𝑆1,𝑆2;\\n(2) If𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖∈𝑆1and𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗∈𝑆2satisfy:\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖|𝑐𝑖−𝑐𝑗|+|𝑥−𝑐𝑖|≤𝑑, 𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗|𝑐𝑗−𝑐𝑖|+|𝑥−𝑐𝑗|≤𝑑\\n2\\n(5)\\n𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗is marked.\\n(3) Decrease the number of clusters by counting the masks in 𝑆2.\\nIn this solution, clusters in 𝑆1can be regarded as transfer nodes.\\nIf(5)holds for(𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖∈𝑆1,𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1∈𝑆2)and(𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖∈\\n𝑆1,𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗2∈𝑆2), respectively, we have,\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1|𝑐𝑗1−𝑐𝑗2|+|𝑥−𝑐𝑗1|\\n≤𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1|𝑐𝑗1−𝑐𝑖|+|𝑐𝑖−𝑐𝑗2|+|𝑥−𝑐𝑗1|\\n≤𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1|𝑐𝑗1−𝑐𝑖|+|𝑐𝑖−𝑐𝑗2|+|𝑥−𝑐𝑗1|+|𝑥−𝑐𝑗2|≤𝑑(6)\\nThus (4)holds when merging several clusters in 𝑆2with one\\ncluster in𝑆1. As a result, we can greedily merge clusters in 𝑆2, as\\nillustrated in step(3).\\nAssume the number of clusters decreases by 𝐷after merging,\\nwe apply a momentum update [ 42] on the number of clusters 𝑁, as\\nis commonly used in machine learning to smooth the changing of\\n𝑁and avoid sample selection bias. To be specific: 𝑁𝑛𝑒𝑤=𝛼(𝑁−\\n𝐷)+(1−𝛼)𝑁, where𝛼is a hyper-parameter for momentum.\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e0a6b86-7f11-4bfe-9292-71f12d4f98ea', embedding=None, metadata={'page_label': '6', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='e4eed7f598028156094c544e7580dc8c373be8350a2718db0940197e6f87aa95', text='5.2 Dynamically Determining the Batch Size\\nBecause of the dynamic grouping operation, the computational\\ngraph in deep learning training [ 1] varies from sample to sample. As\\na result, it is impossible to precisely compute a batch’s GPU memory\\nusage without indeed feeding it into the model. To overcome this\\nproblem, RITA learns a batch size prediction function offline; then\\nat the RITA training time, given a number of groups 𝑁, RITA uses\\nthis function to predict a proper batch size.\\nWhen the model architecture and hardware are fixed, the batch\\nsize depends on the length of the timeseries 𝐿and the average\\ngroup number among all attention module 𝑁. So RITA samples\\nseveral(𝐿𝑖,𝑁𝑖)pairs and estimate a proper batch size for each pair.\\nMore specifically, given a user-defined timeseries maximal length\\n𝐿𝑚𝑎𝑥, we randomly sample integral points (𝐿𝑖,𝑁𝑖)from plane\\n{1≤𝐿≤𝐿𝑚𝑎𝑥,1≤𝑁≤𝐿}. Then we use a binary search based\\nalgorithm to find the maximal batch size 𝐵𝑖that consumes less than\\n90%available GPU memory, aiming to avoid wasting GPU memory\\nand the risks of out of memory (OOM).\\nTreating these pairs as ground truth labels, we use function\\nfitting [18] to learn the batch size predicting function B=f(L,N),\\nwhere B is a function of two variables 𝐿and𝑁.\\nLearning the Prediction Function. We apply curve fit from\\nSciPy [ 53] as the function fitting tool to fit the two-variable function\\n𝐵𝑖=𝑓(𝐿𝑖,𝑁𝑖)on plane{1≤𝐿≤𝐿𝑚𝑎𝑥,1≤𝑁≤𝐿}.\\nWe observe that applying one function to the whole plane incurs\\na huge estimation error. So we develop a dynamic-programming\\n(DP) method to divide the plane into several sub-planes and apply\\na distinct function to each sub-plane respectively. It is optimal in\\nminimizing the total estimation error on all sub-planes\\nWith the learned prediction function 𝑓, we can estimate a proper\\nbatch size for any (𝐿,𝑁)during training, even if it is not seen in\\nthe sampled(𝐿𝑖,𝑁𝑖)pairs.\\nThe Algorithms and Optimality Proof. Please refer to Appen-\\ndix A.3 for the pseudo code of the binary search-based algorithm\\nand the description of the DP method for plane-division and the\\nproof for its optimality.\\n6 EVALUATION\\nOur experimental study focuses on the following questions:\\n1.Effectiveness and efficiency of RITA : How does RITA com-\\npare with other Transformer-based methods and traditional time-\\nseries representation learning methods in accuracy and efficiency?\\n2.Ablation Study : How do the key techniques of RITA work?\\n6.1 Experimental Setup\\nDatasets. We evaluate RITA on classification and imputation tasks\\nusing 5 multi-variate and 3 uni-variate timeseries datasets.\\n•WISDM [55] is a popular multivariate timeseries dataset gen-\\nerated from the accelerometer in the mobile phone. The subjects\\nperformed 18 daily activities (e.g. walking, jogging). The dataset\\nwas collected from 51 subjects and the sampling rate is 20 Hz.\\n•HHAR dataset [ 46] contains sensing data of accelerometer col-\\nlected from 9 users performing 5 activities with 12 different smart-\\nphones (varying in sampling rate). This increases the complexity\\nof the task and thus can test the model’s robustness.•RWHAR RealWorld HAR dataset [ 48] covers 15 subjects per-\\nforming 8 locomotion-style activities. Each subject wears the sen-\\nsors for approximately ten minutes. The sampling rate is 50 Hz.\\n•ECG dataset [ 34] consists of 10,000 EEG recordings for arrhyth-\\nmia classification. Each recording has an uncertain length ranging\\nfrom 6 to 60 seconds sampled at 500 Hz. The ECG recordings corre-\\nspond to 9 types of heart problems such as atrial fibrillation (AF)\\nand premature atrial contraction (PAC), etc.\\n•MGH [6] is a EEG dataset collected by Mass. General Hospital.\\nEach timeseries corresponds to the EEG data observed from one\\npatient during their stay in ICU for a couple of days. The EEG\\nmonitoring produced data with 20 channels. The sampling rate is\\n200 HZ. So it produces very long timeseries.\\n•WISDM*/HHAR*/RWHAR* are three uni-variate datasets de-\\nrived by picking one channel from WISDM/HHAR/RWHAR .\\nTraining/Validation Data Generation. We apply a sliding win-\\ndow on the raw timeseries to get training/validation samples. The\\nsize of the sliding window is set as 200 on small datasets (WISDM,\\nHHAR, RWHAR), 2000 on medium size dataset (ECG), and 10,000\\non the large dataset (MGH). Table 1 shows the statics of the gen-\\nerated datasets. They are randomly split into training/validation\\nset in a proportion of 0.9/0.1. In “pretraining + few-label finetun-\\ning” scenario, we use 100 labeled data per class for finetuning. We\\nguarantee that training set does not overlap with the validation set.\\nDataset Train. Size Valid. Size Length Channel Classes\\nWISDM 28,280 3,112 200 3 18\\nHHAR 20,484 2,296 200 3 5\\nRWHAR 27,253 3,059 200 3 8\\nECG 31,091 3,551 2000 12 9\\nMGH 8,550 950 10000 21 N/A\\nTable 1: The statistics of the datasets\\nAlternative Methods. We compare RITA against the SOTA Trans-\\nformer based timeseries representation learning method TST [61].\\nTo evaluate our group attention (referred to as Group Attn. ), we\\ndevelop three baselines by replacing the group attention compo-\\nnent in RITA with the classic vanilla Self-Attention [ 52](referred\\nto as Vanilla ) and two SOTA methods that reduce the complexity\\nof self-attention by approximation in NLP, namely, Performer [ 10]\\n(referred to as Performer ) and Linformer [ 54] (referred to as Lin-\\nformer ). Similar to our proposed Group Attn., Vanilla, Performer,\\nLinformer all use RITA’s time-aware convolution operation (Sec. 3)\\nto turn timeseries segments into input feature vectors.\\nWe also compare Group Attn. against GRAIL [40], which is\\nthe SOTA of the non-deep learning methods for timeseries repre-\\nsentation learning. GRAIL supports classification tasks by feeding\\nthe learned representations into a Support-Vector Machine [ 12]\\nor K-Nearest Neighbor [ 17] classifier. Note GRAIL only targets\\nuni-variate timeseries and cannot support imputation tasks.\\nMethodology. We mainly focus on two downstream tasks:\\n(1)Classification . First, we train Group Attn. and the base-\\nlines with full labels from scratch to test the effectiveness of RITA\\nframework and the approximation quality of our group attention.\\nSecond, to measure the effectiveness of self-supervised pretrain-\\ning, we evaluate the accuracy of training on few labeled timeseries\\nwith/without pretraining on large scales of unlabeled timeseries. To\\nbe specific, we split the training set into a pretraining set and a fine-\\ntuning set, with very few data in the latter (100 labeled samples per\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f306cdf-a66e-4e99-89b9-4fd467bf65f6', embedding=None, metadata={'page_label': '7', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='5dc692b48ea4f6b9d969cbfadeb713e6c73e4c6a77f5b67fd357992f5dba5b1e', text='(a) Effectiveness (b) Efficiency\\nTrainingTime/secFigure 3: Full-label classification results (multi-variate data).\\nclass in our experiment). We train the model on the cloze pretrain-\\ning task with a mask rate 𝑝=0.2. Then we train two classification\\nmodels using the finetuning set, either based on the pretrained\\nversion or from scratch. We repeat the experiment 5 times with\\nrandom data splits and report the median accuracy.\\n(2)Imputation . We run the imputation task on the datasets used\\nin classification as well as the large unlabeled MGH dataset, and\\nmeasure the mean square error and absolute imputation error. To\\nget timeseries with missing values, we randomly mask the values\\nwith an expected mask rate of 𝑝=0.2. The masked values are\\nreplaced with a special value.\\nFinally, to evaluate Group Attn.’s benefit on efficiency , the total\\ntime of forward computation, backward propagation, and grouping\\nare measured for all methods in all the experiments.\\nTo save space, we only report the average training time per epoch\\nhere and refer readers to Appendix A.8 for the inference time.\\nWe first compare against the Transformer-based methods on\\nmulti-variate datasets (sec. 6.2, 6.3), then compare against the non-\\ndeep learning method GRAIL on uni-variate datasets (sec. 6.4).\\nConfiguration. Please refer to Appendix A.1 for the experiment\\nconfiguration and hyper-parameter settings.\\n6.2 Effectiveness: Transformer-Based Methods\\nWe first evaluate the quality of the models trained with full labels\\nfrom scratch. We then show how the pretraining of RITA increases\\nthe accuracy of the downstream tasks.\\n6.2.1 full-label training (Multi-variate classification)\\nResults shown in Figure 3(a) get us the following observations:\\n(1) RITA’s advantage over TST. On all four datasets for the clas-\\nsification tasks, Group Attn. and the other three baselines that use\\nRITA architecture (Vanilla, Performer, and Linformer) outperform\\nTST. In particular, Group Attn. outperforms TST by 49 percentage\\npoints on the ECG dataset (88.48% vs 39.93%) with long timeseries.\\nTwo deficiencies in TST may cause its poor performance on the long\\ntimeseries. Firstly, TST concatenates the output embedding vector\\nof each time stamp, then uses a linear classifier to do classification\\non the concatenated vector. When the timeseries is long, the linear\\nclassifier has so many parameters that it tends to overfit easily.\\nSecondly, TST replaces Layer Normalization in vanilla Transformer\\nwith Batch Normalization. When the timeseries is long, it can only\\naccommodate a small number of timeseries in each batch, leading\\nto bias in Batch Normalization.\\n(2) Group-attention’s advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on3 out of 4 datasets for classification. Although Linformer works\\nslightly better than Group Attn. on the ECG dataset (90.37% vs\\n88.84%), its performance is the worst in all other cases compared\\nto any other RITA-based methods. Vanilla computes the attention\\nscores precisely. Thus it is expected to work well. However, Group\\nAttn. outperforms Vanilla on WISDM (87.50% vs 86.95%) and is very\\nclose to it on other 3 datasets. This suggests that group attention’s\\napproximation quality is good.\\n6.2.2 pretraining + few label finetune (Multi-variate classification)\\nThe results shown in Table 3 get us the following observation:\\n(1) Pretraining is effective. Pretraining always leads to better\\naccuracy than training with a few labels from scratch. In particular,\\non WISDM data all the methods using RITA architecture increase\\nthe accuracy by at least 10%. This is impressive considering we do\\nnot have a very large unlabeled pre-training set to use.\\n(2) RITA’s advantage over TST. our Group Attn. and other\\nthree baselines using RITA architecture (Vanilla, Performer, and\\nLinformer) significantly outperform TST on all four classification\\ndatasets by 25 percentage points.\\n(3) Group Attention’s advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on 3\\nout of 4 datasets. When compared to Vanilla, Group Attn. is better\\non HHAR and ECG, and comparable on the other two, further con-\\nfirming its high quality on approximation. Further, we notice that\\nLinformer struggles in this setting: in average its accuracy is worse\\nthan Vanilla by 8.22% and Group Attn. by 8.01%. This is because the\\nlow-rank projection operation introduces extra model parameters,\\nmaking Linformer more easily overfit, while overfitting is especially\\nharmful when there are only a few labeled training samples.\\n6.2.3 full-dataset training (Multi-variate imputation)\\nSimilar to classification tasks, the results of imputation tasks\\n(Table.2) show that Group Attn. consistently outperforms the base-\\nlines in training time while achieving comparable/better MSE. Again,\\non the large dataset MGH (length = 10,000), TST and Vanilla fail due\\nto out of memory (OOM) errors. Methods using RITA framework\\n(Group Attn., Performer, Linformer) all achieve very low MSE (are\\nhighly accurate). Among them Linformer is the worst.\\n6.3 Efficiency: Transformer-based Methods\\nWe measure the efficiency by the average training time per epoch\\nincluding the cost of the forward computation + backward propaga-\\ntion and the grouping overhead. We first show the results on all the\\n5 datasets in Sec. 6.3.1. We then vary the length of the timeseries\\non the MGH dataset to show group attention’s scalability on long\\ntimeseries in Sec. 6.3.2.\\n6.3.1 Training Time: All Multi-variate Datasets\\nThe results in Fig. 3(b) and Table 2 lead to the below observations:\\n(1) Vanilla Self-Attention is not scalable. In average, it takes\\n2-3 minutes to train one epoch when the length of the timeseries is\\nonly 200 (WISDM, HHAR, RWHAR), takes over 15 minutes when\\nthe length increases to 2,000 (ECG), and fails on the long MGH data\\nwhen the length reaches 10,000 due to out of GPU memory.\\n(2) Group Attn.’s advantage over all other attention mecha-\\nnisms. As we have shown in Sec. 6.2, Group Attn. is more accurate\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='93dfaadf-09bc-479e-86e9-e0d4a6a233bc', embedding=None, metadata={'page_label': '8', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='1b83ea45707fe77c642ddbcf57959770d21200a4dfe4e254e796ccb72f4357db', text='Dataset LengthTST [61] Vanilla Performer Linformer Group Attn.\\nMSE Time/s MSE Time/s MSE Time/s MSE Time/s MSE Time/s\\nWISDM 200 13.30 150.3 3.240 178.1 3.449 162.6 3.852 141.9 3.277 136.7\\nHHAR 200 1.085 78.2 0.2968 97.4 0.2980 82.6 0.3198 81.1 0.2974 73.3\\nRWHAR 200 0.0882 83.9 0.0478 108.1 0.0489 89.1 0.0572 98.4 0.0478 81.3\\nECG 2000 0.0905 696.3 0.0037 857.9 0.0033 270.2 0.0035 291.38 0.0038 164.36\\nMGH 10000 N/A N/A N/A N/A 0.00014 356.2 0.00088 404.9 0.00042 54.4\\nTable 2: Imputation results (multi-variate data). The best results are marked with bold .\\nDataset Pretrain SizeTST [61] Vanilla Performer Linformer Group Attn.\\nScratch Pre. Scratch Pre. Scratch Pre. Scratch Pre. Scratch Pre.\\nWISDM 62,231 49.13% 50.03% 66.16% 75.89% 66.09% 73.97% 50.12% 67.44% 62.56% 75.06%\\nHHAR 68,294 72.56% 75.30% 75.60% 81.35% 76.52% 80.70% 65.94% 76.52% 76.17% 82.62%\\nRWHAR 63,599 69.46% 80.41% 85.68% 91.14% 87.54% 91.33% 81.03% 86.33% 86.13% 89.63%\\nECG 561,358 20.98% 27.99% 42.05% 46.16% 43.34% 45.58% 27.19% 31.34% 42.58% 46.39%\\nTable 3: Pretrain + few-label finetuning results. The best results are marked with bold .\\nTraining Time/secMSE(a) Effectiveness(b) Efficiency\\nFigure 4: Varying the lengths of timeseries.\\nthan Performer and Linformer in classification and imputation tasks,\\nwhile Group Attn. is always faster than Performer, Linformer, and\\nall other baselines on all 5 multi-variate datasets, thus a win-win .\\n(3) The longer the timeseries, the larger the speedup. On\\nthe medium sized ECG dataset with a length of 2,000, Group Attn.\\nhas a speedup of 3.86/1.36/2.27 compared to Vanilla/Performer/Lin-\\nformer. When the length increases to 10,000, the speedup on the\\nMGH dataset increases to 6.59/7.48 compared to Performer/Lin-\\nformer (Vanilla and TST failed in this case) on imputation task\\n(Table. 2). However, even on the short WISDM, HHAR, RWHAR\\ndatasets, Group Attn. still consistently outperforms other methods,\\nconfirming that it does not introduce much overhead. This is be-\\ncause when the length of the timeseries gets longer, Group Attn.\\ngets more opportunities to find windows with similar properties.\\n6.3.2 Training time: Varying the Length\\nIn this experiment, we truncate the original MGH timseries into\\nsequences with the lengths at 2000/4000/6000/8000/10000, and com-\\npare Group Attn. against Vanilla and other attention mechanisms.\\nVanilla cannot handle sequences longer than 8000.\\nThe results in Fig. 4 again show that the longer the timeseries, the\\nlarger the speed up . With comparable MSE, Group Attn. outperforms\\nVanilla by 63X. Moreover, as the length increases from 2000 to 10000,\\nthe training time of Group Attn. only increases from 31.2 seconds\\nto 54.4 seconds per epoch. The reason is that as the timeseires\\nbecomes longer, there are more grouping opportunities because of\\nthe similarity of the timeseries segments.\\nAccuracy\\nTraining Time/sec\\n(a)(b)Figure 5: Comparison to non-deep learning method (uni-\\nvariate data).\\n6.4 Comparison to Non-deep Learning Methods\\nWe compare against GRAIL, the SOTA of non-deep learning time-\\nseries representation learning. We use the three uni-variate datasets,\\nbecause GRAIL only targets uni-variate timeseries.\\nResults in Fig. 5 show that on all 3 datasets RITA significantly\\noutperforms GRAIL in accuracy by 45, 16, and 21 percentage points\\nbecause of the expressive power of Transformer. Moreover, thanks\\nto the GPU-friendly design of RITA, it is at least 2 ×faster than\\nGRAIL in training time.\\n6.5 Ablation Study\\n6.5.1 Adaptive Scheduler\\nTo evaluate the effectiveness of RITA’s adaptive scheduler (Sec. 5),\\nwe compare it against a baseline using a fixed group number 𝑁. We\\nvary𝑁and the error bound threshold 𝜖used by RITA.\\nFrom the results in Table 4 we get the following observations:\\n(1) Adaptive Scheduler is better than fixed 𝑁.Training with\\nAdaptive Scheduler already achieves better or comparable perfor-\\nmance compared to the best performing 𝑁. More specifically, on\\nthe MGH dataset, dynamic scheduler always achieves better accu-\\nracy and is much faster compared to fixed 𝑁. On the ECG dataset,\\nalthough fixed 𝑁is slightly better than adaptive scheduler in accu-\\nracy when setting the N as 512, it runs much slower than adaptive\\nscheduler. Of course, finding the best 𝑁that balances the accuracy\\nand running time requires careful tuning.\\n(2) Adaptive Scheduler is tuning free. It is robust on both\\naccuracy and running time when 𝜖varies, while the results of\\nfixed𝑁vary significantly when the value of 𝑁changes. Therefore,\\nAdaptive Scheduler frees the users from tuning the 𝜖threshold,\\nwhile it is hard to find an appropriate 𝑁for a given dataset.\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5bdd0ac6-d5d4-42b8-a60b-8434964bd5d3', embedding=None, metadata={'page_label': '9', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='8a45615ac2e105853fc7c171d7f17d530eb810c3e8f8356281762031dc7ef5ed', text='Dataset Task Scheduler Parameter Metric Time\\nECG Class.Dynamic1.5 88.34% 292.5\\n2 88.48% 236.8\\n3 87.83% 216.8\\nFixed64 87.50% 255.2\\n128 88.96% 297.2\\n256 88.82% 414.1\\n512 90.03% 662.6\\n1024 88.65% 873.7\\nMGH Imput.Dynamic1.5 0.00041 60.7\\n2 0.00040 57.9\\n3 0.00042 54.4\\nFixed128 0.00054 128.6\\n256 0.00053 190.2\\n512 0.00049 240.8\\n1024 0.00046 323.3\\nTable 4: Adaptive Scheduling VS Fixed N.\\nPretrain Data size Few-label Accuracy\\nN/A 62.56%\\n12,446 72.94%\\n24,892 72.78%\\n37,338 74.10%\\n49,784 74.22%\\n62,231 75.06%\\nTable 5: RITA Pretraining: increasing sizes of pretrain set.\\n6.5.2 The Sizes of the Pretraining Data\\nNext, we evaluate how the number of unlabeled data influences the\\neffectiveness of pretraining. To get empirical results, we pretrain\\nRITA on WISDM dataset with 20%/40%/60%/80% of the pretraining\\ndata and finetune each pretrained model with 100 labels per class.\\nThe results in Table 5 show that: (1) The more pretraining data,\\nthe larger the improvement. The accuracy increases with the\\nsizes of the pretraining data; (2) Marginal utility diminishing.\\nThe first 20% pretraining data gives a 10.38% improvement in accu-\\nracy (72.94% vs 62.56%), while the remaining 80% pretraining data\\nonly gives an additional improvement of 2.12% (75.06% vs 72.94%).\\n7 RELATED WORK\\n7.1 Timeseries Analytics\\nThere is a great deal of prior work on timeseries analytics methods.\\nThis work can be divided into three categories: (1) non-deep learn-\\ning methods; (2) CNN/RNN-based deep learning methods; and (3)\\nTransformer-based deep learning methods.\\nTraditional Methods. These methods, such as TS-CHIEF [ 45],\\nHIVE-COTE [ 33], ROCKET [ 15] have achieved notable performance\\non public datasets. Despite that, traditional methods suffer from\\none or more issues: they (1) rely on expert knowledge for feature\\nextraction; (2) incur heavy computation cost and are inappropriate\\nfor GPU devices; (3) support only uni-variate timeseries; (4) perform\\nclassification solely. Some work [ 61] shows that the transformed-\\nbased methods outperform these traditional methods especially on\\nmulti-variate timeseries.\\nIn particular, as the SOTA of timeseries representation learn-\\ning, GRAIL [ 40] extracts landmarks from data and computes the\\nrepresentations with the combination of the landmarks. However,\\nGRAIL only supports uni-variate timeseries. Our experiments (Sec. 6.4)\\nshow that RITA significantly outperforms GRAIL in both effective-\\nness and efficiency on uni-variate timeseries.CNN/RNN-based Deep Learning Methods. CNN-based methods,\\nsuch as InceptionTime [ 21] and Resnet [ 19], are good at classifica-\\ntion tasks, but can not handle generative tasks such as forecasting\\nbecause of the inductive bias of convolution networks. RNN-based\\nmethods, such as Brit [ 7] and deepAR [ 44], are capable for classifi-\\ncation, regression and generation. However, the recurrent structure\\nbrings a lot of problems: (1) limiting the model’s ability in captur-\\ning long-range correlation; (2) notoriously difficult to train [ 41]\\nbecause of gradient vanishing and exploding problem. As a result,\\nsuch methods can hardly scale to very long timeseries.\\nTransformer-based Deep Learning Methods. Given that Trans-\\nformer is the best choice for backbone in almost all sequence mod-\\neling tasks, some effort has been made to apply Transformer to\\ntimeseries analytics. Targeting forecasting of uni-variate timeseries,\\nLogTrans [ 30] introduced a log sparsity assumption to attention\\ncomputation. Informer [ 62] pushes LogTrans a step further and\\nscales forecasting to multi-variate timeseries. Autoformer [ 57] per-\\nforms forecasting by decomposing timeseries into two parts, i.e.\\nthe trend part and the seasonal part.\\nFor imputation tasks, CDSA [ 37] outperforms statistical meth-\\nods and the SOTA of RNN-based method Brit [ 7] on 3 public and\\n2 competition datasets. For timeseries classification, AutoTrans-\\nformer [ 43] performs architecture search to adapt to the tasks\\nin different domains. For timeseries anomaly detection, Anomaly\\nTransformer [ 58] outperforms many widely-used methods such\\nas OmniAnomaly [ 47], assuming the attention score maps show\\nGaussian distribution.\\nAll of these works are designed for specific tasks, rather than\\nfunctioning as a representation learning framework to serve\\ndifferent downstream tasks. To fill this gap, some researchers pro-\\nposed a Transformer-based architecture, called TST [ 61]. Like RITA,\\nTST supports regression, classification, and unsupervised learning\\nthrough the “cloze test” pretraining task on timeseries. However,\\nTST directly uses the classical Vanilla self-attention, thus not scal-\\nable to long timeseries as shown in our experiments (Sec. 6.3.2).\\n7.2 Efficient Transformers\\nThe need of improving the scalability of Transformers has led to\\nmore efficient variations of Transformers, especially for accommo-\\ndating long text data in NLP [49].\\nIntroducing fixed/random patterns to self-attention mechanism\\nis an intuitive idea. Sparse Transformer [ 9] and Longformer [ 3] only\\ncompute attention at fixed intervals. ETC [ 2] and BigBird [ 60] use\\nglobal-local attention: the attention computation is limited within\\na fixed radius, while some auxiliary tokens are added to attend/get\\nattended globally. The deficiencies of fixed attention patterns are\\nobvious: it heavily depends on users to give an optimal setting.\\nTo decrease the reliance on human labor, some works seek to\\nintroduce learnable/adaptive attention patterns instead of fixed\\npatterns. Reformer [ 26] proposed only computing the dominant\\nattention terms based on their observation of sparsity in atten-\\ntion matrix from language/image data. Such sparsity is intuitive\\nin language data, in which a word’s attention mainly focuses on\\nthe nearby sentences. However, attention in timeseries data shows\\nstrong seasonal patterns rather than sparse patterns, mainly as\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5cd203de-86d9-469f-aeff-c5d46af2226e', embedding=None, metadata={'page_label': '10', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='0b3715c694784e862030ed8d9bc7e68bd58f4b0c9412c61bb398d653216bac3b', text='result of the periodicity of timeseries data. Therefore, such works\\ndo not work well for timeseries.\\nApart from introducing attention patterns, some works seek\\nto solve this problem with applied mathematics techniques. Lin-\\nformer [ 54] performs a projection to decrease the size of query,\\nkey and value matrices before attention computation, because the\\nattention matrix tends to be low-ranked. Performer [ 10] uses linear\\nfunctions to approximate the kernel function softmax , making at-\\ntention computation commutative. When the sequence length is far\\ngreater than the dimension of embedding vectors, Performer ben-\\nefits from changing the order of matrix multiplication. Linformer\\nand Performer do not depend on the unique properties of language\\ndata, thus potentially fitting timeseries better than other techniques,\\nwhich is why we compared against them in our experiments. How-\\never as shown in Sec. 6, our group attention significantly outper-\\nforms them in both accuracy and efficiency (training time), because\\ngroup attention fully leverages the periodicity of timeseries.\\n8 CONCLUSION\\nIn this work, we presented RITA, an automatic, self-supervised, and\\nscalable timeseries analytics tool. RITA effectively adapts Trans-\\nformer, popular in NLP, into timeseries analytics. As the key com-\\nponent of RITA, group attention eliminates the performance bottle-\\nneck of the classical self-attention mechanisms, thus successfully\\nscaling RITA to highly complex, long timeseries data. Our experi-\\nments confirm that RITA significantly speeds up the state-of-the-art\\nby 63X with a better accuracy.\\nREFERENCES\\n[1]Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\\nCraig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al .\\n2016. Tensorflow: Large-scale machine learning on heterogeneous distributed\\nsystems. arXiv preprint arXiv:1603.04467 (2016).\\n[2]Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,\\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020.\\nETC: Encoding long and structured inputs in transformers. arXiv preprint\\narXiv:2004.08483 (2020).\\n[3]Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\\ndocument transformer. arXiv preprint arXiv:2004.05150 (2020).\\n[4]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al .2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 1877–1901.\\n[5]C Bui, N Pham, A Vo, A Tran, A Nguyen, and T Le. 2017. Time series forecasting\\nfor healthcare diagnosis and prognostics with the focus on cardiovascular dis-\\neases. In International conference on the development of biomedical engineering in\\nVietnam . Springer, 809–818.\\n[6]Lei Cao, Wenbo Tao, Sungtae An, Jing Jin, Yizhou Yan, Xiaoyu Liu, Wendong\\nGe, Adam Sah, Leilani Battle, Jimeng Sun, Remco Chang, M. Brandon Westover,\\nSamuel Madden, and Michael Stonebraker. 2019. Smile: A System to Support\\nMachine Learning on EEG Data at Scale. Proc. VLDB Endow. 12, 12 (2019), 2230–\\n2241.\\n[7]Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018. Brits:\\nBidirectional recurrent imputation for time series. Advances in neural information\\nprocessing systems 31 (2018).\\n[8] Chris Chatfield. 2000. Time-series forecasting . Chapman and Hall/CRC.\\n[9]Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating\\nlong sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).\\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\\nLukasz Kaiser, et al .2020. Rethinking attention with performers. arXiv preprint\\narXiv:2009.14794 (2020).\\n[11] Andrew A Cook, Göksel Mısırlı, and Zhong Fan. 2019. Anomaly detection for IoT\\ntime-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 6481–6494.\\n[12] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine\\nlearning 20, 3 (1995), 273–297.[13] David R Cox. 1958. The regression analysis of binary sequences. Journal of the\\nRoyal Statistical Society: Series B (Methodological) 20, 2 (1958), 215–232.\\n[14] Benjamin F Crabtree, Subhash C Ray, Priscilla M Schmidt, Patrick T O’Connor,\\nand David D Schmidt. 1990. The individual over time: time series applications in\\nhealth care research. Journal of clinical epidemiology 43, 3 (1990), 241–260.\\n[15] Angus Dempster, François Petitjean, and Geoffrey I. Webb. 2020. ROCKET: excep-\\ntionally fast and accurate time series classification using random convolutional\\nkernels. Data Min. Knowl. Discov. 34, 5 (2020), 1454–1495.\\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\\nProceedings of the 2019 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) . 4171–\\n4186.\\n[17] Evelyn Fix and Joseph Lawson Hodges. 1989. Discriminatory analysis. Nonpara-\\nmetric discrimination: Consistency properties. International Statistical Review/Re-\\nvue Internationale de Statistique 57, 3 (1989), 238–247.\\n[18] Philip George Guest and Philip George Guest. 2012. Numerical methods of curve\\nfitting . Cambridge University Press.\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\\nlearning for image recognition. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition . 770–778.\\n[20] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,\\nand Pierre-Alain Muller. 2019. Deep learning for time series classification: a\\nreview. Data mining and knowledge discovery 33, 4 (2019), 917–963.\\n[21] Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier,\\nDaniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar, Pierre-\\nAlain Muller, and François Petitjean. 2020. Inceptiontime: Finding alexnet for\\ntime series classification. Data Mining and Knowledge Discovery 34, 6 (2020),\\n1936–1962.\\n[22] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization\\nfor nearest neighbor search. IEEE transactions on pattern analysis and machine\\nintelligence 33, 1 (2010), 117–128.\\n[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity\\nsearch with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535–547.\\n[24] Richard M Karp. 1972. Reducibility among combinatorial problems. In Complexity\\nof computer computations . Springer, 85–103.\\n[25] Eamonn Keogh, Kaushik Chakrabarti, Michael Pazzani, and Sharad Mehrotra.\\n2001. Dimensionality reduction for fast similarity search in large time series\\ndatabases. Knowledge and information Systems 3, 3 (2001), 263–286.\\n[26] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient\\ntransformer. arXiv preprint arXiv:2001.04451 (2020).\\n[27] John Kraft and Arthur Kraft. 1977. Determinants of common stock prices: A time\\nseries analysis. The journal of finance 32, 2 (1977), 417–425.\\n[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-\\nsification with Deep Convolutional Neural Networks. In Advances in Neural\\nInformation Processing Systems , F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-\\nberger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/\\npaper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\\n[29] Oscar D Lara and Miguel A Labrador. 2012. A survey on human activity recog-\\nnition using wearable sensors. IEEE communications surveys & tutorials 15, 3\\n(2012), 1192–1209.\\n[30] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,\\nand Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-\\nneck of transformer on time series forecasting. Advances in Neural Information\\nProcessing Systems 32 (2019).\\n[31] T Warren Liao. 2005. Clustering of time series data—a survey. Pattern recognition\\n38, 11 (2005), 1857–1874.\\n[32] Rake& Agrawal King-lp Lin and Harpreet S Sawhney Kyuseok Shim. 1995. Fast\\nsimilarity search in the presence of noise, scaling, and translation in time-series\\ndatabases. In Proceeding of the 21th International Conference on Very Large Data\\nBases . 490–501.\\n[33] Jason Lines, Sarah Taylor, and Anthony Bagnall. 2018. Time Series Classification\\nwith HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based\\nEnsembles. ACM Trans. Knowl. Discov. Data 12, 5, Article 52 (jul 2018), 35 pages.\\n[34] Feifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan\\nXu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, et al .2018. An open\\naccess database for evaluating the algorithms of electrocardiogram rhythm and\\nmorphology abnormality detection. Journal of Medical Imaging and Health\\nInformatics 8, 7 (2018), 1368–1373.\\n[35] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on\\ninformation theory 28, 2 (1982), 129–137.\\n[36] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\\narXiv preprint arXiv:1711.05101 (2017).\\n[37] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, and\\nShih-Fu Chang. 2019. CDSA: cross-dimensional self-attention for multivariate,\\ngeo-tagged time series imputation. arXiv preprint arXiv:1905.09904 (2019).\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1cdc68a7-d7db-4131-9934-646534df124a', embedding=None, metadata={'page_label': '11', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='d7d66531b32f0b7867df3a1780e747d09251c04bd15d48576ce40b8b9b9a4202', text='[38] Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate\\nnearest neighbor search using hierarchical navigable small world graphs. IEEE\\ntransactions on pattern analysis and machine intelligence 42, 4 (2018), 824–836.\\n[39] Tripti Negi and Veena Bansal. 2005. Time series: Similarity search and its appli-\\ncations. In Proceedings of the International Conference on Systemics, Cybernetics\\nand Informatics: ICSCI-04, Hyderabad, India . 528–533.\\n[40] John Paparrizos and Michael J Franklin. 2019. Grail: efficient time-series repre-\\nsentation learning. Proceedings of the VLDB Endowment 12, 11 (2019), 1762–1777.\\n[41] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty\\nof training recurrent neural networks. In International conference on machine\\nlearning . PMLR, 1310–1318.\\n[42] Ning Qian. 1999. On the momentum term in gradient descent learning algorithms.\\nNeural networks 12, 1 (1999), 145–151.\\n[43] Yankun Ren, Longfei Li, Xinxing Yang, and Jun Zhou. 2022. AutoTransformer:\\nAutomatic Transformer Architecture Design for Time Series Classification. In\\nPacific-Asia Conference on Knowledge Discovery and Data Mining . Springer, 143–\\n155.\\n[44] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.\\nDeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-\\nnational Journal of Forecasting 36, 3 (2020), 1181–1191.\\n[45] Ahmed Shifaz, Charlotte Pelletier, François Petitjean, and Geoffrey I. Webb. 2020.\\nTS-CHIEF: a scalable and accurate forest algorithm for time series classification.\\nData Mining and Knowledge Discovery 34 (2020), 742–775.\\n[46] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,\\nMikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen.\\n2015. Smart devices are different: Assessing and mitigatingmobile sensing het-\\nerogeneities for activity recognition. In Proceedings of the 13th ACM conference\\non embedded networked sensor systems . 127–140.\\n[47] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust\\nanomaly detection for multivariate time series through stochastic recurrent\\nneural network. In Proceedings of the 25th ACM SIGKDD international conference\\non knowledge discovery & data mining . 2828–2837.\\n[48] Timo Sztyler and Heiner Stuckenschmidt. 2016. On-body localization of wearable\\ndevices: An investigation of position-aware activity recognition. In 2016 IEEE\\nInternational Conference on Pervasive Computing and Communications (PerCom) .\\nIEEE, 1–9.\\n[49] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient\\ntransformers: A survey. ACM Computing Surveys (CSUR) (2020).\\n[50] Mingyan Teng. 2010. Anomaly detection on time series. In 2010 IEEE International\\nConference on Progress in Informatics and Computing , Vol. 1. IEEE, 603–608.\\n[51] Patrick A Thompson. 1990. An MSE statistic for comparing forecast accuracy\\nacross series. International Journal of Forecasting 6, 2 (1990), 219–227.\\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In Advances in Neural Information Processing Systems 30: Annual Con-\\nference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA . 5998–6008.\\n[53] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\\nReddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser,\\nJonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar-\\nrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern,\\nEric Larson, C J Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,\\nDenis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\\nCharles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa,\\nPaul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Al-\\ngorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261–272.\\nhttps://doi.org/10.1038/s41592-019-0686-2\\n[54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin-\\nformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768\\n(2020).\\n[55] Gary M Weiss, Kenichi Yoneda, and Thaier Hayajneh. 2019. Smartphone and\\nsmartwatch-based biometrics using activities of daily living. IEEE Access 7 (2019),\\n133190–133202.\\n[56] Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke, and Huan Xu. 2021.\\nRobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection.\\nInProceedings of the 2021 International Conference on Management of Data (Virtual\\nEvent, China) (SIGMOD ’21) . Association for Computing Machinery, New York,\\nNY, USA, 2328–2337. https://doi.org/10.1145/3448016.3452779\\n[57] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-\\ncomposition transformers with auto-correlation for long-term series forecasting.\\nAdvances in Neural Information Processing Systems 34 (2021), 22419–22430.\\n[58] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomaly\\nTransformer: Time Series Anomaly Detection with Association Discrepancy.\\narXiv preprint arXiv:2110.02642 (2021).\\n[59] Dianmin Yue, Xiaodan Wu, Yunfeng Wang, Yue Li, and Chao-Hsien Chu. 2007. A\\nreview of data mining-based financial fraud detection research. In 2007 Interna-\\ntional Conference on Wireless Communications, Networking and Mobile Computing .Ieee, 5519–5522.\\n[60] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\\net al.2020. Big bird: Transformers for longer sequences. Advances in Neural\\nInformation Processing Systems 33 (2020), 17283–17297.\\n[61] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and\\nCarsten Eickhoff. 2021. A Transformer-based Framework for Multivariate Time\\nSeries Representation Learning. In KDD ’21: The 27th ACM SIGKDD Conference\\non Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,\\n2021. 2114–2124.\\n[62] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-\\nquence time-series forecasting. In Proceedings of AAAI .\\nA APPENDIX: SUPPLEMENTARY MATERIAL\\nA.1 Experiment Configuration and\\nHyper-parameter Settings\\nConfiguration. All models were trained on an NVIDIA Tesla V100\\n16GB GPU. All the methods are optimized with AdamW [ 36] of\\nwhich the starting learning rate and weight decay parameter are\\nboth 1𝑒−4. In full-label training scenario, we train the models for\\n100 epochs. In “pretraining + few-label finetuning scenario”, as the\\npretrained models require fewer epochs to converge [ 61], we train\\nthe model for 50 epochs. For a fair comparison, the baselines use a\\nmaximal batch size within GPU’s capacity during training.\\nAs for model hyper-parameter setting, RITA and the baselines\\nuse a Transformer structure balancing Vanilla ’s accuracy and\\nefficiency: 8-layer stack of 2-head attention with hidden vectors\\nin dimension of 64. Convolution kernel size is set to 5 by default.\\nWe set the error bound threshold ( 𝜖, Sec. 5.1) of Group Attention\\nto 2, as it balances the accuracy and the efficiency in general on\\nall datasets. Because Linformer requires the users to set the sizes\\nof projection matrix, in different settings we choose an accuracy-\\nefficiency balancing one among {64,128,256,512}.\\nA.2 Efficient Computation of Group Attention\\nAlgorithm 1 Efficient Computation of Group Attention\\nRequire:𝑄,𝑉,𝑅,𝐶𝑂𝑈𝑁𝑇,𝐵𝐸𝐿𝑂𝑁𝐺\\nEnsure:𝑄,𝑉∈R𝑛∗𝑑,𝑅∈R𝑁∗𝑑,𝐶𝑂𝑈𝑁𝑇∈N𝑁,𝐵𝐸𝐿𝑂𝑁𝐺∈N𝑛\\n1:function group_attention (𝑄,𝑉,𝑅 )\\n2: for𝑖=0→𝑁−1do\\n3: e𝑣𝑖←Í𝑛−1\\n𝑗=0(𝐵𝐸𝐿𝑂𝑁𝐺𝑗==𝑖)𝑣𝑗\\n4:e𝑃←𝑄𝑅𝑇\\n5: for𝑖=0→𝑛−1do\\n6: for𝑗=0→𝑁−1do\\n7: 𝑤𝑖,𝑗←𝑒𝑥𝑝(e𝑃𝑖,𝑗)𝐶𝑂𝑈𝑁𝑇𝑗\\n8: for𝑖=0→𝑛−1do\\n9:𝑠𝑖←Í𝑁−1\\n𝑗=0𝑤𝑖,𝑗\\n10: for𝑖=0→𝑛−1do\\n11:𝑜𝑖←Í𝑁−1\\n𝑗=0𝑒𝑥𝑝(e𝑃𝑖,𝑗)\\n𝑠𝑖e𝑣𝑗\\n12: return𝑂\\nIn Alg. 1, we denote 𝐶𝑂𝑈𝑁𝑇𝑖to be the size of the 𝑖𝑡ℎgroup,𝑁to\\nbe the number of groups, r𝑖to be the representative key of the 𝑖𝑡ℎ\\ngroup and Rto be the matrix consisting of all r𝑖,𝐵𝐸𝐿𝑂𝑁𝐺𝑖to be\\nthe group that k𝑖belongs to.𝑄,𝑉 are the packing matrices of query\\nvectors and value vectors as described in Sec.2. Alg. 1 outputs the\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4adf015e-55ca-4b9f-b042-63b86be6164d', embedding=None, metadata={'page_label': '12', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='7a5faef424c3f9afbed048e93a2e5931489813863ed98a3c476db81891d007c4', text='packing matrix 𝑂for new feature emebddings {𝑜1,...,𝑜𝑛}, where𝑜𝑖\\ncorresponds to the feature embedding of 𝑤𝑖𝑛𝑖. Lines 2-3 implement\\nthe embedding aggregation operation, while Lines 8-11 implement\\nthe group softmax function.\\nA.3 The Algorithms and Optimality Proof for\\nDynamically Determing Batch Size\\nAlgorithm 2 Binary Search for Batch Size\\nRequire:𝐿,𝑁\\nEnsure: 1≤𝐿≤𝐿𝑚𝑎𝑥,1≤𝑁≤𝐿\\n1:function binary_search (𝐿,𝑁)\\n2:𝐿←1\\n3:𝑅←𝑀𝑎𝑥𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒\\n4:𝑑𝑎𝑡𝑎←𝑅𝑎𝑛𝑑𝑜𝑚𝑇𝑖𝑚𝑒𝑆𝑒𝑟𝑖𝑒𝑠𝑖𝑛𝑙𝑒𝑛𝑔𝑡ℎ𝐿\\n5:𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙\\n6: while𝐿≤𝑅do\\n7:𝐼𝑛𝑝𝑢𝑡←𝑑𝑎𝑡𝑎×𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙\\n8:𝑀𝑜𝑑𝑒𝑙𝐹𝑜𝑟𝑤𝑎𝑟𝑑(𝐼𝑛𝑝𝑢𝑡)\\n9:𝑀𝑜𝑑𝑒𝑙𝐵𝑎𝑐𝑘𝑤𝑎𝑟𝑑\\n10:𝑢←𝑃𝑒𝑎𝑘𝑀𝑒𝑚𝑜𝑟𝑦𝑈𝑠𝑎𝑔𝑒\\n𝑇𝑜𝑡𝑎𝑙𝑀𝑒𝑚𝑜𝑟𝑦\\n11: if0.9>𝑢then\\n12: 𝐿←𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙+1\\n13: 𝐵←𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙\\n14: else\\n15: 𝑅←𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙−1\\n16:𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙←⌊𝐿+𝑅⌋\\n2\\n17: return𝐵\\nAlgorithm 3 Dynamic Programming for Plane Division\\nRequire:𝐿𝑖,𝑁𝑖,𝐵𝑖,𝐿𝑚𝑎𝑥\\nEnsure: 1≤𝐿𝑖≤𝐿𝑚𝑎𝑥,1≤𝑁𝑖≤𝐿𝑖\\n1:function cost (S)\\n2: if|𝑆|<𝑀then return+∞\\n3:𝐿,𝑁,𝐵←𝑝𝑜𝑖𝑛𝑡𝑠𝑖𝑛𝑆\\n4:𝑓←𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑓𝑖𝑡𝑡𝑖𝑛𝑔(𝐵|𝐿,𝑁)\\nreturn𝐸(𝐵,𝐿,𝑁|𝑓)\\n5:function dynamic_programming (𝐿𝑖,𝑁𝑖,𝐿𝑚𝑎𝑥 )\\n6: for𝑙1=1→𝐿𝑚𝑎𝑥 do\\n7: for𝑙2=1→𝑙1do\\n8: for𝑛=1→𝑙1do\\n9: 𝑆←𝑝𝑜𝑖𝑛𝑡𝑠𝑠𝑒𝑡𝑖𝑛{𝑙2≤𝐿≤𝑙1,𝑁≤𝑛}\\n10: 𝑔(𝑛)←𝐶𝑂𝑆𝑇(𝑆)\\n11: for𝑖=1→𝑛do\\n12: 𝑆←𝑝𝑜𝑖𝑛𝑡𝑠𝑠𝑒𝑡𝑖𝑛{𝑙2≤𝐿≤𝑙1,𝑖≤𝑁≤𝑛}\\n13: 𝑔(𝑛)←𝑚𝑖𝑛(𝑔(𝑛),𝑔(𝑖)+𝐶𝑂𝑆𝑇(𝑆))\\n14: 𝑓𝑙2,𝑙1←𝑔(𝑙1)\\n15:\\n16: for𝑙=1→𝐿𝑚𝑎𝑥 do\\n17:𝑑𝑝(𝑙)←𝑓(1,𝑙)\\n18: for𝑖=1→𝑙do\\n19: 𝑑𝑝(𝑙)←𝑚𝑖𝑛(𝑑𝑝(𝑙),𝑑𝑝(𝑖)+𝑓(𝑖,𝑙))\\nreturn𝑑𝑝(𝐿𝑚𝑎𝑥)\\nWe describe Alg. 3 and intuitively show its optimality. We assume\\nthat Scipy [ 53] learns an optimal function in Line 4 so that function\\nCOST gives the optimal estimation error when fitting the points in\\nset𝑆. When fitting very few points, we assign an infinite cost to\\nprevent a biased fitting function (Line 2). 𝑔(𝑛)denotes the minimalestimation error for points in sub-plane {𝑙2≤𝐿≤𝑙1,𝑁≤𝑛}. In\\nLines 11-13, we enumerate all possible ways of cutting {𝑙2≤𝐿≤\\n𝑙1,𝑁≤𝑛}horizontally into two sub-plane {𝑙2≤𝐿≤𝑙1,𝑁≤𝑖}and\\n{𝑙2≤𝐿≤𝑙1,𝑖≤𝑁≤𝑛}by iterating 𝑖from 1 to n. Choosing the\\ncutting strategy that minimizes estimation error gets us a 𝑔(𝑙1)with\\nminimal estimation error for sub-plane {𝑙2≤𝐿≤𝑙1,𝑁≤𝑙1}, which\\nis recorded as 𝑓𝑙1,𝑙2in Line 14.𝑑𝑝(𝑙)denotes the minimal estimation\\nerror for sub-plane {𝐿≤𝑙}. We enumerate all the possible ways\\nof cutting{𝐿≤𝑙}vertically into two sub-plane {𝐿≤𝑖}and{𝑖≤\\n𝐿≤𝑙}by iterating 𝑖from 1 to𝑙(Line 17-19). Finally, we have the\\nminimal estimation error for the whole plane as 𝑑𝑝(𝐿𝑚𝑎𝑥). Based\\non the above discussion, this algorithm guarantees to not miss any\\nbetter solution, hence optimal.\\nA.4 The Correctness of Group Attention\\nLemma 3. Assuming the windows belonging to the same group 𝐺𝑖\\nhave the same key vector, i.e. 𝑘𝑗=𝑟𝑖(𝑤𝑖𝑛𝑗∈𝐺𝑖), then the feature\\nembedding𝑂produced by the original self-attention mechanism is\\nidentical to the output of our group attention mechanism implemented\\nin Algorithm 1.\\nProof. Denote e𝑘𝑗to be the representative vectors of 𝑘𝑗, i.e.e𝑘𝑗=\\n𝑟𝑖=𝑘𝑗(𝑤𝑖𝑛𝑗∈𝐺𝑖). Algorithm 1 gives that\\ne𝑣𝑖=𝑛−1∑︁\\n𝑗=0(𝐵𝐸𝐿𝑂𝑁𝐺𝑗==𝑖)v𝑗,e𝑃𝑖,𝑗=q𝑖·r𝑗\\n𝑠𝑖=𝑁−1∑︁\\n𝑗=0𝑒𝑥𝑝(e𝑃𝑖,𝑗)𝐶𝑂𝑈𝑁𝑇𝑗,e𝑜𝑖=𝑁−1∑︁\\n𝑗=0e𝑃𝑖,𝑗\\n𝑠𝑖e𝑣𝑗(7)\\nBy the canonical self-attention mechanism introduced in Sec. 2,\\nwe get:\\n𝑃𝑖,𝑗=q𝑖·kj, 𝐴𝑖,𝑗=𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑛−1\\n𝑘=0𝑒𝑥𝑝(𝑃𝑖,𝑘),o𝑖=𝑛−1∑︁\\n𝑗=0𝐴𝑖,𝑗v𝑗 (8)\\nWith 7 and 8, we have\\n𝑛−1∑︁\\n𝑗=0𝑒𝑥𝑝(𝑃𝑖,𝑗)=𝑛−1∑︁\\n𝑗=0𝑒𝑥𝑝(q𝑖·k𝑗)\\n=𝑁−1∑︁\\n𝑗=0𝑛−1∑︁\\n𝑥=0(𝐵𝐸𝐿𝑂𝑁𝐺𝑥==𝑗)𝑒𝑥𝑝(q𝑖·k𝑥)\\n=𝑁−1∑︁\\n𝑗=0𝑒𝑥𝑝(q𝑖·r𝑗)𝑛−1∑︁\\n𝑥=0(𝐵𝐸𝐿𝑂𝑁𝐺𝑥==𝑗)\\n=𝑁−1∑︁\\n𝑗=0𝑒𝑥𝑝(q𝑖·r𝑗)𝐶𝑂𝑈𝑁𝑇𝑗\\n=𝑁−1∑︁\\n𝑗=0𝑒𝑥𝑝(e𝑃𝑖,𝑗)𝐶𝑂𝑈𝑁𝑇𝑗\\n=𝑠𝑖(9)\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97fee91f-7df4-4e4b-9c41-b7c54f7d7e7b', embedding=None, metadata={'page_label': '13', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='5c28c2842948f4b228359ad8efd59aa686e55d804f17e29c35b6e5f7f7a819aa', text='Further,\\no𝑖=𝑛−1∑︁\\n𝑗=0𝐴𝑖,𝑗vj\\n=𝑁−1∑︁\\n𝑗=0𝑛−1∑︁\\n𝑥=0(𝐵𝐸𝐿𝑂𝑁𝐺𝑥==𝑗)𝐴𝑖,𝑥v𝑥\\n=𝑁−1∑︁\\n𝑗=0𝑛−1∑︁\\n𝑥=0(𝐵𝐸𝐿𝑂𝑁𝐺𝑥==𝑗)𝑒𝑥𝑝(𝑃𝑖,𝑥)\\nÍ𝑛−1\\n𝑘=0𝑒𝑥𝑝(𝑃𝑖,𝑘)v𝑥\\n=𝑁−1∑︁\\n𝑗=0𝑛−1∑︁\\n𝑥=0(𝐵𝐸𝐿𝑂𝑁𝐺𝑥==𝑗)𝑒𝑥𝑝(q𝑖·k𝑥)\\nÍ𝑛−1\\n𝑘=0𝑒𝑥𝑝(𝑃𝑖,𝑘)v𝑥\\n=𝑁−1∑︁\\n𝑗=0𝑛−1∑︁\\n𝑥=0(𝐵𝐸𝐿𝑂𝑁𝐺𝑥==𝑗)𝑒𝑥𝑝(q𝑖·rj)\\nÍ𝑛−1\\n𝑘=0𝑒𝑥𝑝(𝑃𝑖,𝑘)v𝑥\\n=𝑁−1∑︁\\n𝑗=0𝑒𝑥𝑝(q𝑖·rj)\\nÍ𝑛−1\\n𝑘=0𝑒𝑥𝑝(𝑃𝑖,𝑘)𝑛−1∑︁\\n𝑥=0(𝐵𝐸𝐿𝑂𝑁𝐺𝑥==𝑗)v𝑥\\n=𝑁−1∑︁\\n𝑗=0𝑒𝑥𝑝(q𝑖·rj)\\nÍ𝑛−1\\n𝑘=0𝑒𝑥𝑝(𝑃𝑖,𝑘)e𝑣𝑗(10)\\nCombining (7), (9) (10), we have oi=ÍN−1\\nj=0ePi,j\\nsievj=eoi.\\nThis concludes that the output of our group attention is identical\\nto vanilla self-attention’s. □\\nA.5 The Proof of Error Bound (Lemma 1)\\nProof. We have\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)=𝑒𝑥𝑝(q𝑖·ek𝑗)\\n𝑒𝑥𝑝(q𝑖·k𝑗)=𝑒𝑥𝑝(q𝑖·(ek𝑗−k𝑗))\\n=𝑒𝑥𝑝(||q𝑖||·||ek𝑗−k𝑗||·𝑐𝑜𝑠(q𝑖,ek𝑗−k𝑗))(11)\\nSo\\n𝑒𝑥𝑝(−𝑑𝑅)≤𝑒𝑥𝑝(𝑃𝑖,𝑗)\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)≤𝑒𝑥𝑝(𝑑𝑅) (12)\\nThen we have:\\n𝐴𝑖,𝑗\\n𝐴𝑖,𝑗=𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑛\\n𝑘=1𝑒𝑥𝑝(𝑃𝑖,𝑘)/𝑒𝑥𝑝(𝑃𝑖,𝑗)Í𝑛\\n𝑘=1𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n=𝑒𝑥𝑝(𝑃𝑖,𝑗)\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)Í𝑛\\n𝑘=1𝑒𝑥𝑝(𝑃𝑖,𝑘)\\nÍ𝑛\\n𝑘=1𝑒𝑥𝑝(𝑃𝑖,𝑘)(13)\\nCombining (12) (13), the error is bounded by\\n𝑒𝑥𝑝(−2𝑑𝑅)≤𝐴𝑖,𝑗\\n𝐴𝑖,𝑗≤𝑒𝑥𝑝(2𝑑𝑅) (14)\\nThus, if d≤ln(𝜖)\\n2R,1\\n𝜖≤Ai,j\\nAi,j≤𝜖. This proves Lemma 1.\\nA.6 The Proof of Merge Operation (Lemma 2)\\nProof. Denote the cluster size of 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘to be𝑛𝑘.After merge-\\ning, the new center will be:\\n𝑐′=Í𝑚\\n𝑖=1𝑛𝑘𝑖𝑐𝑘𝑖Í𝑚\\n𝑖=1𝑛𝑘𝑖For∀𝑖∈[1,𝑚],∀𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘𝑖, it holds that:\\n|𝑥−𝑐′|≤|𝑥−𝑐𝑘𝑖|+|𝑐𝑘𝑖−𝑐′|(𝑇𝑟𝑖𝑎𝑛𝑔𝑙𝑒𝑖𝑛𝑒𝑞𝑢𝑎𝑙𝑖𝑡𝑦 )\\n=|𝑥−𝑐𝑘𝑖|+|Í𝑚\\n𝑗=1𝑛𝑘𝑗Í𝑚\\n𝑗=1𝑛𝑘𝑗𝑐𝑘𝑖−Í𝑚\\n𝑗=1𝑛𝑘𝑗𝑐𝑘𝑗Í𝑚\\n𝑗=1𝑛𝑘𝑗|\\n=|𝑥−𝑐𝑘𝑖|+|Í𝑚\\n𝑗=1𝑛𝑘𝑗(𝑐𝑘𝑖−𝑐𝑘𝑗)\\nÍ𝑚\\n𝑗=1𝑛𝑘𝑗|\\n=|𝑥−𝑐𝑘𝑖|+|Í𝑚\\n𝑗=1𝑛𝑘𝑗(𝑐𝑘𝑖−𝑐𝑘𝑗)|\\nÍ𝑚\\n𝑗=1𝑛𝑘𝑗\\n≤|𝑥−𝑐𝑘𝑖|+Í𝑚\\n𝑗=1𝑛𝑘𝑗|𝑐𝑘𝑖−𝑐𝑘𝑗|\\nÍ𝑚\\n𝑗=1𝑛𝑘𝑗\\n=Í𝑚\\n𝑗=1𝑛𝑘𝑗(|𝑐𝑘𝑖−𝑐𝑘𝑗|+|𝑥−𝑐𝑘𝑖|)\\nÍ𝑚\\n𝑗=1𝑛𝑘𝑗\\n≤Í𝑚\\n𝑗=1𝑛𝑘𝑗𝑑\\nÍ𝑚\\n𝑗=1𝑛𝑘𝑗=𝑑(15)\\nA.7 Downstream Tasks\\nRITA supports a variety of downstream tasks. In this section, we\\nshow that with minimal modification RITA can effectively support\\nclassification, imputation and forecasting tasks. Other unsupervised\\ntasks such as similarity search or clustering are naturally supported\\nby extracting feature embeddings from RITA.\\nA.7.1 Classification\\nTo classify timeseries, we input timeseries to the model as described\\nin Sec. 3 and attach a special token [CLS] as the first input em-\\nbedding. [CLS] ’s embedding acts as the embedding for the entire\\ntimeseries, and the output representation of [CLS] is fed into a\\nclassifier: y=Softmax(WclsZ[CLS]+Bcls), where𝑍[𝐶𝐿𝑆]∈R𝑑is\\nthe output representation of [CLS] , C is the number of classes, and\\nWcls∈RC×d,Bcls∈RCare learnable parameters for classification\\ntask. The result vector 𝑦∈R𝐶represents the possibility that the\\ninput timeseries belongs to each class.\\nWe apply Cross Entropy Loss as the loss function of the classi-\\nfication task [ 13]:L=1\\nCÍC\\ni=1−ˆy(i)log(y(i)), where ˆ𝑦is a binary\\nindicator for ground truth label:\\nˆ𝑦(𝑖)=(\\n1𝑖is ground truth label\\n0𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒(16)\\nA.7.2 Imputation\\nTimeseries are mainly generated by sensors, a common problem\\nof which is missing values. This becomes a challenge when many\\ndownstream analytics require the missing values to be recovered.\\nThe recovering task is imputation.\\nDenote the real timeseries as 𝑇𝑟∈R𝑡×𝑚, the observed timeseries\\nwith missing values as 𝑇𝑜∈R𝑡×𝑚, and the set of missing values’\\npositions as 𝑀. We scale the values of all timeseries to non-negative\\nand use a special value (-1) to indicate missing values:\\n𝑇𝑜(𝑖,𝑗)=(\\n−1(𝑖,𝑗)∈𝑀\\n𝑇𝑟(𝑖,𝑗) (𝑖,𝑗)∉𝑀(17)\\n𝑇𝑜is fed into the RITA as input, and the output representa-\\ntions are concatenated and fed into a Transpose Convolution layer\\nwhich decodes the output embedding vectors from hidden space to\\ntimeseries values, corresponding to the convolution operation in\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c56eaef8-c26f-4b04-af73-ca9333780770', embedding=None, metadata={'page_label': '14', 'file_name': '2306.01926v1.pdf', 'file_path': 'pdfs\\\\2306.01926v1.pdf', 'file_type': 'application/pdf', 'file_size': 2636636, 'creation_date': '2024-01-19', 'last_modified_date': '2024-01-19', 'last_accessed_date': '2024-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='86deb4def749667fc21ddf045b2c649b597cbfa946009359f27841b6a59f8060', text='the input stage, i.e., Y=TransposeCNN(Z1+○Z2+○...+○Zn), where\\n𝑌∈R𝑡×𝑚is the recovered timeseries, and 𝑍𝑖∈R𝑑is the output of\\neach position.\\nHere Mean Square Error is chosen as the loss function [ 51]:\\n𝐿=1\\n|𝑀|Í\\n(𝑖,𝑗)∈𝑀(𝑌(𝑖,𝑗)−𝑇𝑟(𝑖,𝑗))2.\\nA.7.3 Forecasting\\nForecasting can be regarded as a special case of imputation, in\\nwhich all missing values are at the end of timeseries.\\nSo like in imputation task, we scale the timeseries to non-\\nnegative and use a special value (-1) to indicate the values to be\\npredicted:\\n𝑇𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑(𝑖,𝑗)=(\\n𝑇𝑟𝑒𝑎𝑙(𝑖,𝑗)𝑖≤𝑡𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑\\n−1𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒(18)\\nWhere𝑡𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑 is the observed timestamp. Then the output\\nrepresentations are fed into a Transpose Convolution layer using\\nMean Squared Error as loss function, as described above.\\nA.7.4 Other Unsupervised Tasks\\nRITA naturally supports other unsupervised tasks, such as similar-\\nity search and clustering [ 25,31,32], by producing the embedding\\nof one timeseries (output representation of the special token [CLS] ).\\nClustering can be performed on the embeddings with flexible choice\\nof distance metrics. Similarly, a high dimensional similarity search\\nsystem [22, 23, 38] can be built on the embeddings.A.8 Inference Time\\nDataset Length TST[61] Vanilla Performer Linformer Group Attn.\\nWISDM 200 2.18 2.26 2.35 2.22 2.17\\nHHAR 200 1.19 1.23 1.28 1.21 1.18\\nRWHAR 200 1.32 1.37 1.42 1.34 1.31\\nECG 2000 18.44 15.26 5.80 6.08 5.16\\nTable 6: Inference time: Classification on multi-variate data\\n(seconds).\\nDataset Length TST[61] Vanilla Performer Linformer Group Attn.\\nWISDM 200 2.03 2.11 2.19 2.07 2.02\\nHHAR 200 1.11 1.14 1.19 1.12 1.10\\nRWHAR 200 1.23 1.27 1.32 1.25 1.22\\nECG 2000 17.22 14.32 4.73 4.99 4.11\\nMGH 10000 N/A N/A 6.58 6.88 1.35\\nTable 7: Inference time: Imputation on multi-variate data\\n(seconds).\\nIn this section, we present the average inference time on valida-\\ntion sets. The results in Table. 6 and 7 correspond to the average\\ninference time on validation sets of classification and imputation\\ntasks, respectively. Consistent with the results in Section. 6.3, our\\nmethod Group Attn. outperforms the baselines on both classifica-\\ntion and imputation tasks, particularly on the datasets comprising\\nlong timeseries (ECG and MGH).\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
