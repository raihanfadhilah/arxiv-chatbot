Mixtures of Experts Models∗
Isobel Claire Gormley†and Sylvia Frühwirth-Schnatter‡
Abstract
Mixtures of experts models provide a framework in which covariates may be in-
cluded in mixture models. This is achieved by modelling the parameters of the mix-
ture model as functions of the concomitant covariates. Given their mixture model
foundation, mixtures of experts models possess a diverse range of analytic uses, from
clustering observations to capturing parameter heterogeneity in cross-sectional data.
This chapter focuses on delineating the mixture of experts modelling framework and
demonstrates the utility and ﬂexibility of mixtures of experts models as an analytic
tool.
1 Introduction
The terminology mixtures of experts models encapsulates a broad class of mixture models
in which the model parameters are modelled as functions of concomitant covariates. While
the response variable yis modelled via a mixture model, model parameters are modelled as
functions of other, related, covariates xfrom the context under study.
The mixture of experts nomenclature (ME) has its origins in the machine-learning lit-
erature (Jacobs et al., 1991), but mixtures of experts models appear in many diﬀerent
guises, including switching regression models (Quandt, 1972), concomitant variable latent-
class models (Dayton & Macready, 1988), latent class regression models (DeSarbo & Cron,
1988),andmixedmodels(Wang et al.,1996).Li et al.(2011)discussﬁnitesmoothmixtures,
a special case of ME modelling. McLachlan & Peel (2000) and Frühwirth-Schnatter (2006)
provide background to a range of mixtures of experts models; Masoudnia & Ebrahimpour
(2014) survey the ME literature from a machine learning perspective.
The mixture of experts framework facilitates ﬂexible modelling, allowing a wide range of
application. ME models for rank data (Gormley & Murphy, 2008b, 2010a), ME models for
network data (Gormley & Murphy, 2010b), for time series data (Waterhouse et al., 1996;
Huertaet al., 2003; Frühwirth-Schnatter et al., 2012), for non-normal data (Villani et al.
, 2009; Chamroukhi, 2015) and for longitudinal data (Tang & Qu, 2015), among others,
have been developed. Peng et al.(1996) employed a hierarchical mixture of experts model
in a speech recognition context. The general ME framework has also been incorporated
in the mixed membership model setting, giving rise to a mixed membership of experts
model (White & Murphy, 2016), and into the inﬁnite mixture model setting (Rasmussen &
∗A chapter prepared for the forthcoming Handbook of Mixture Analysis
†School of Mathematics and Statistics, Insight Centre for Data Analytics, University College Dublin,
Ireland. claire.gormley@ucd.ie
‡Institute for Statistics and Mathematics, Vienna University of Economics and Business, Austria.
sfruehwi@wu.ac.at
1arXiv:1806.08200v1  [stat.ME]  21 Jun 2018
Ghahramani, 2002). Cluster weighted models (Ingrassia et al., 2015; Subedi et al., 2013;
Gershenfeld, 1997) are also closely related to ME models.
This chapter introduces the generic mixture of experts framework, in Section 2, and
describes approaches to inference for ME models in Section 3. A broad range of illustrative
data analyses are given in Section 4, and an overview of existing softwares which ﬁt ME
models is provided. Section 5 discusses identiﬁability issues for mixtures of experts models.
The chapter concludes with some discussion of the beneﬁts and issues of the ME framework,
and of some areas ripe for future development.
2 The Mixture of Experts Framework
Any mixture model which incorporates covariates or concomitant variables falls within the
mixture of experts framework.
2.1 A mixture of experts model
Lety1,...,ynbe an independent and identically distributed sample of outcome variables
from a population modelled by a Gcomponent ﬁnite mixture model. Depending on the
application context, the outcome variable can be univariate or multivariate, discrete or con-
tinuous, or of a more general structure such as time series or network data. Each component
g(forg= 1,...,G) is modelled by the probability density function fg(·|θg)with parameters
denoted by θg, and has weight ηgwhere/summationtextG
g=1ηg= 1. Observation yi(i= 1,...,n) has
qassociated covariates, which are denoted xi. The ME model extends the standard ﬁnite
mixture model introduced in Chapter 1 of this volume by allowing model parameters to be
functions of the concomitant variables xi:
p(yi|xi) =G/summationdisplay
g=1ηg(xi)fg(yi|θg(xi)). (1)
ME models can be considered as a member of the class of conditional mixture models
(Bishop, 2006); for a given set of covariates xi, the distribution of yiis a ﬁnite mixture
model. Jacobs et al.(1991) consider the component densities fg(yi|θg(xi))as theexperts,
which model diﬀerent parts of the input space, and the component weights ηg(xi)as the
gating networks , hence the mixture of experts terminology.
The models for ηg(xi)and forθg(xi)in (1) vary and are typically application speciﬁc. For
example,Jacobs et al.(1991)modelthecomponentweightsusingamultinomiallogit(MNL)
regression model, and the component densities using generalized linear models. Young &
Hunter (2010) provide further ﬂexibility by allowing the mixing proportions to be modelled
nonparametrically, as a function of the covariates.
2.2 An illustration
A simple simulated data set is employed here to introduce the mixture of experts framework.
Figure 1 shows n= 200two-dimensional continuously valued observations, y1,...,yn, sim-
ulated from an ME model with G= 2components. A single ( q= 1) categorical covariate xi
is associated with each observation representing, for example, gender where level 0 denotes
female. Interest lies in clustering the observations and exploring any relations between the
resulting clusters and the associated covariate.
2
−3 −2 −1 0 1 2 3−3 −2 −1 0 1 2 3
Variable 1Variable 2Figure1: Atwo-dimensionalsimulateddataset, froma G= 2MEmodel. Blackobservations
belong to cluster 1, and grey to cluster 2, based on the MAP clustering from ﬁtting a G= 2
mixture of bivariate Gaussian distributions.
Table 1: Cross tabulation of MAP cluster memberships and the gender covariate for the
simulated data of Figure 1
Female Male
Cluster 1 75 17
Cluster 2 23 85
It is common that a clustering method is implemented on the outcome variables of in-
terest,y1,...,yn, without reference to the covariate information. Once a clustering has been
produced, the user typically probes the clusters to investigate their structure. Interpreta-
tions of the clusters are produced with reference to values of the model parameters within
each cluster and with reference to the covariates that were not used in the construction of
the clusters. Therefore, a natural approach to modelling the data in Figure 1 is to cluster
them by ﬁtting a two component mixture of bivariate Gaussian distributions to y1,...,yn.
Themaximum a posteriori (MAP) cluster membership of each observation resulting from
ﬁtting such a model is also illustrated in Figure 1.
A cross tabulation of the MAP cluster memberships and the gender covariate is given
in Table 1. It is clear that females have a strong presence in cluster 1, and males in cluster
2. However, the mixture of Gaussians model ﬁtted does not incorporate or quantify this
relationship or its associated uncertainty. It is in such a setting that an ME model is useful.
The model from which the data in Figure 1 are simulated is an ME model where
fg(yi|θg) =φ(yi|µg,Σg)is the density of a bivariate Normal distribution and in which the
component weights arise from a multinomial logit model with Gcategories with gender as
covariatexi, i.e.
log/bracketleftBiggηg(xi)
η1(xi)/bracketrightBigg
=γg0+γg1xi, (2)
where cluster 1 is the baseline cluster with γ1= (γ10,γ11)/latticetop= (0,0)/latticetop, andg= 2,...,G.
In our example, where G= 2, model (2) reduces to a binary logit model. The parameter
3
γg1(and its associated uncertainty) quantiﬁes the relationship between the gender covariate
and membership of cluster g, withγg1= 0corresponding to independence between cluster
membership and the gender covariate. Note that such a model easily extends to q > 1
covariatesxi= (xi1,...,xiq)with associated parameter γg= (γg0,...,γgq)/latticetopfor clusterg.
Fitting such an ME model to the simulated data results in a MAP clustering unchanged
from that reported in Table 1 and gives the maximum likelihood estimate ˆγ21= 2.79, with
standard error 0.36. (Details of the maximum likelihood estimation process and standard
error derivation follow in Section 3.1.) Thus, the odds of a male belonging to cluster 2 are
exp(2.79)≈16times greater than the odds of a female belonging to cluster 2. Thus the ME
model has clustering capabilities and provides insight into the type of observation which
characterises each cluster.
2.3 The suite of ME models
The ME model outlined in Section 2.2 involves modelling the component weights as a func-
tion of covariates. This is one model type (termed a simple mixture of experts model ) from
theMEframework.Figure2showsagraphicalmodelrepresentationofthesuiteoffourmod-
els in the ME framework, based on a latent variable representation of the mixture model
(1), involving the latent cluster membership of each observation, denoted zi, wherezi=g
if observation yibelongs to cluster g. The indicator variable zitherefore has a multinomial
distribution with a single trial and probabilities equal to ηg(xi)forg= 1,...,Gand the
latent variable representation reads:
yi|xi,zi=g∼fg(yi|θg(xi)),P(zi=g|xi) =ηg(xi). (3)
This suite of models ranges from a standard mixture of experts regression model (in which
all model parameters are functions of covariates) to the special cases where some of the
model parameters do not depend on covariates. The four models in the ME framework have
the following interpretations, see also Figure 2:
(a) Mixture models, where the outcome variable distribution depends on the latent cluster
membership, denoted z. The model is independent of the covariates x; i.e.p(yi,zi|xi) =
fzi(yi|θzi)ηzi.
(b) Mixtures of regression models, where the outcome variable distribution depends on both
the covariates xand the latent cluster membership variable z; the distribution of the
latent variable is independent of the covariates; i.e. p(yi,zi|xi) =fzi(yi|θzi(xi))ηzi.
(c) Simple mixtures of experts models, where the outcome variable distribution depends on
the latent cluster membership variable zand the distribution of the latent variable z
depends on the covariates x; i.e.p(yi,zi|xi) =fzi(yi|θzi)ηzi(xi).
(d) Standardmixturesofexpertsregressionmodels,wheretheoutcomevariabledistribution
depends on both the covariates xand on the latent cluster membership variable z.
Additionally the distribution of the latent variable zdepends on the covariates x; i.e.
p(yi,zi|xi) =fzi(yi|θzi(xi))ηzi(xi).
ThemannerinwhichthediﬀerentmodelswithintheMEframeworkdependonthecovariates
is typically application speciﬁc. The component weights are usually modelled using a MNL
model, but this need not be the case; Geweke & Keane (2007) employ a model similar to
an ME model, where the component weights have a multinomial probit structure. The form
4
yz x
θη
(a) Mixture modelyz
θη x
(b) Mixture of regressions
model
yx z
θη
(c) Simple mixture of ex-
perts modelyz
θη x
(d) Standard mixture of
experts regression model
Figure 2: The graphical model representation of mixtures of experts models. The diﬀerences
between the four special cases are due to the presence or absence of edges between the
covariatesxand the latent variable zand response variable y. For model (a) p(y,z|x) =
p(y|z)p(z), for model (b) p(y,z|x) =p(y|x,z)p(z), for model (c) p(y,z|x) =p(y|z)p(z|x),
whereas for model (d) p(y,z|x) =p(y|x,z)p(z|x).
of the distribution fg(yi|θg(xi))depends on the type of outcome data under study. The
applications of the ME framework outlined in Section 4 include cases where the outcome
data range from a categorical time series, to rank data, to network data.
3 Statistical Inference for Mixtures of Experts Models
Before illustrating the breadth of the ME framework through illustrative applications in
Section 4, the issue of inference for ME models is addressed. For any ME model that is
underpinned by a ﬁnite mixture model, the approaches to inference outlined in Chapter 2
and Chapter 5 in this volume are applicable. Jacobs et al.(1991) and Jordan & Jacobs
(1994) derive maximum likelihood estimates (MLEs) for ME models via the expectation-
maximisation (EM) algorithm; Gormley & Murphy (2008a) employ the closely related
expectation-minorisation-maximisation (EMM) algorithm. Estimation of the ME model
within the Bayesian framework is detailed, among others, in Peng et al.(1996), Frühwirth-
Schnatter & Kaufmann (2008), Villani et al.(2009), Gormley & Murphy (2010a) and in
Frühwirth-Schnatter et al.(2012) in which Markov chain Monte Carlo methods (Tanner,
1996) are used; Bishop & Svenskn (2003) use variational methods in the Bayesian paradigm
to perform inference for a hierarchical mixture of experts model. Hunter & Young (2012)
present an algorithm for parameter estimation in a semiparametric mixtures of regressions
model setting.
In this section, a general overview of approaches to inference in the ME framework is
provided. Throughout the section, y= (y1,...,yn)will denote the collection of outcome
variables and x= (x1,...,xn)the associated covariates. The latent cluster membership
indicators introduced in (3) are denoted by z= (z1,...,zn), whereasθ={θ1,...,θG}refers
to the collection of the Gcomponent parameters and γ={γ2,...,γG}to the unknown
5
parameters in the Gcomponent weights.
The exact manner in which an ME model is estimated again depends on the nature of the
ME model and the outcome variable. The simple simulated data example of Section 2.2 is
used here to delineate approaches to inference; more detailed application speciﬁc estimation
approaches are outlined in Section 4.
3.1 Maximum likelihood estimation
The EM algorithm (Dempster et al., 1977) provides an eﬃcient approach to deriving MLEs
in ME models. The EM algorithm is most commonly known as a technique to produce MLEs
in settings where the data under study are incomplete or when optimisation of the likelihood
would be simpliﬁed if an additional set of variables were known. The iterative EM algorithm
consists of an expectation (E) step followed by a maximisation (M) step. Generally, during
the E step the conditional expectation of the complete (i.e. observed and unobserved) data
log likelihood is computed, given the data and current parameter values. In the M step the
expected log likelihood is maximised with respect to the model parameters. The imputation
of latent variables often makes maximisation of the expected log likelihood more feasible.
The parameter estimates produced in the M step are then used in a new E step and the
cycle continues until convergence. The parameter estimates produced on convergence are
estimates that achieve a stationary point of the likelihood function of the data, which is at
least a local maximum but may be a saddle point.
The component weights of the simple mixture of experts model outlined in Section 2.2
are given by
ηg(xi|γ) = exp (˜xiγg)/G/summationdisplay
g/prime=1exp (˜xiγg/prime) (4)
where ˜xi= (1,xi)andγg= (γg0,γg1)/latticetop. Note that this is a special case of the multinomial
logit model. For the Normal distribution θg={µg,Σg}and the likelihood function of the
simple mixture of experts model is
L(γ,θ;G) =p(y|x,γ,θ) =n/productdisplay
i=1G/summationdisplay
g=1ηg(xi|γ)φ(yi|µg,Σg),
whereφ(yi|µg,Σg)is the pdf of the d-variate Normal distribution and d= dim(yi). It is
diﬃcult to directly obtain MLEs from this likelihood. To alleviate this, the data are aug-
mented by imputing for each observation yi,i= 1,...,n, the latent group membership
indicatorzi. For the EM algorithm, this latent variable is represented through Gbinary
variables (zi1,...,ziG)wherezig=I(zi=g)takes the value 1 if observation yiis a member
of component gand the value 0 otherwise. This provides the complete data likelihood
Lc(γ,θ,z;G) =p(y,z|x,γ,θ) =n/productdisplay
i=1G/productdisplay
g=1{ηg(xi|γ)φ(yi|µg,Σg)}zig, (5)
the expectation of (the log of) which is obtained in the E step of the EM algorithm. As
the complete data log likelihood is linear in the latent variable, the E step simply consists
of replacing for each i= 1,...,nthe missing data ziwith their expected values ˆzi.In the
M step the complete data log likelihood, computed with the estimates ˆz= (ˆz1,..., ˆzn), is
maximised to provide estimates of the component weight parameters ˆγand the component
parameters ˆθ.
6
Algorithm 1 EM algorithm for a simple Gaussian mixture of experts model
Lets= 0. Choose initial estimates for the component weight parameters
γ(0)= (0,γ(0)
2,...,γ(0)
G)and for the component parameters µ(0)
gand Σ(0)
gforg=
1,...,G.
1E step: fori= 1,...,nandg= 1,...,Gcompute the estimates:
z(s+1)
ig =η(s)
g(xi|γ(s))φ(yi|µ(s)
g,Σ(s)
g)/G/summationdisplay
g/prime=1η(s)
g/prime(xi|γ(s))φ(yi|µ(s)
g/prime,Σ(s)
g/prime).
2M step: Substituting the z(s+1)
igvalues obtained in the E step into the log of the complete
data likelihood (5) forms the so called ‘Q function’
Q=n/summationdisplay
i=1G/summationdisplay
g=1z(s+1)
ig
˜xiγg−log

G/summationdisplay
g/prime=1exp(˜xiγg/prime)


−d/2 log(2π)−1/2 log|Σg|−1/2(yi−µg)/latticetopΣ−1
g(yi−µg)

withd= dim(yi), which is maximised with respect to the model parameters.
(a) The updates of the g= 1,...,Gcomponent means and covariances are, respectively:
µ(s+1)
g =n/summationdisplay
i=1z(s+1)
igyi/n/summationdisplay
i=1z(s+1)
ig
Σ(s+1)
g =n/summationdisplay
i=1z(s+1)
ig(yi−µ(s+1)
g)(yi−µ(s+1)
g)/latticetop/n/summationdisplay
i=1z(s+1)
ig.
(b) The update for the component weight parameters is obtained via a numerical opti-
misation step, such as a Newton-Raphson step, where for g= 2,...,G
γ(s+1)
g =γ(s)
g−(H(γ(s)
g))−1Q/prime(γ(s)
g)
andQ/primeandHdenote the ﬁrst and second derivatives of Qwith respect to γg
respectively. Note that this M step is equivalent to ﬁtting a generalised linear model
with weights provided by the E step.
3 If converged, stop. Otherwise, increment sand return to Step 1.
The EM algorithm for ﬁtting ME models is straightforward in principle, but the M step
is often diﬃcult in practice. This is usually due to a complex component density and/or
component weights model, or a large parameter set. A modiﬁed version of the EM algorithm,
the Expectation and Conditional Maximisation (ECM) algorithm (Meng & Rubin, 1993) is
thereforeoftenemployed.IntheECMalgorithm,theMstepconsistsofaseriesofconditional
maximisation steps. In the context of the simple mixture of experts example considered
here, these maximisations are not straightforward with regard to the γparameters; as in
any MNL model, no closed form expression for the parameter MLEs is available. Thus,
while the conditional M steps for µgandΣg∀g= 1,...,Gare available in closed form,
the conditional M step for γrequires the use of a numerical optimisation technique, or
as in Gormley & Murphy (2008b) the MM algorithm (Hunter & Lange, 2004) in which a
7
minorising function is iteratively maximised and updated. In summary, to ﬁt the simple
mixture of experts example outlined in Section 2.2 the EM algorithm proceeds as described
in Algorithm 1. In the simulated data example, d= 2.
McLachlan & Peel (2000) outline a number of approaches to assessing convergence in
Step 3; typically it is assessed by tracking the change in the log likelihood as the algorithm
proceeds. Standard errors of the resulting parameter estimates are not automatically pro-
duced by the EM algorithm, but they can be approximately computed after convergence,
for example, by computing and inverting the observed information matrix (McLachlan &
Peel, 2000). For a detailed discussion of EM algorithms in a mixture context see Chapter 2
and 3 of this volume.
3.2 Bayesian estimation
Estimation of ME models can be achieved within the Bayesian paradigm, either using a
Markov chain Monte Carlo (MCMC) algorithm or via a variational approach. The reader
is directed to Bishop & Svenskn (2003) for details on the variational approach; this section
focuses on inference using MCMC methods. Both the Gibbs sampler (Geman & Geman,
1984) and the Metropolis-Hastings algorithm (Chib & Greenberg, 1995; Metropolis et al.,
1953) are typically required. Again, the speciﬁc MCMC algorithm, and the form of the prior
distributions, depend on the nature of the ME model under study and on the type of the
response data. As is standard in Bayesian estimation of mixture models (Diebolt & Robert,
1994; Hurn et al., 2003) ﬁtting ME models is greatly simpliﬁed by augmenting the observed
data with the latent group indicator variable zifor each observation yi.
Performing inference on the illustrative simple mixture of experts model of Section 2.2
is again straightforward in principle, but can be diﬃcult in practice. To begin, priors for
the model parameters µg,Σgforg= 1,...,Gandγg(g= 2,...,G) require speciﬁcation.
Positing a conditional d-variate normal prior N(µ0,Λ0)on the group means µg, and an
inverse Wishart prior IW(ν0,S0)on the group covariances Σgprovides conjugacy for these
parameters (Hoﬀ, 2009). The full conditional distributions for these parameters are therefore
available in closed form, and thus Gibbs sampling can be used to draw samples.
A(q+ 1)-variatenormalN(µγ,Λγ)isanintuitivepriorforthecomponentweightparam-
etersγg, but it is non-conjugate. Hence the full conditional distribution is not available in
closed form and a Metropolis-Hastings (MH) step can be applied to sample the component
weight parameters. One sweep of such a Metropolis-within-Gibbs sampler required to ﬁt the
simple mixture of experts model of Section 2.2 in a Bayesian framework is outlined below.
Note that the full conditional distribution of the latent indicator variable zifori= 1,...,n
is also available in closed form and thus a Gibbs step is available, see Algorithm 2.
Sampling the component weight parameters in Step 4 through a MH-algorithm brings
issues such as choosing suitable proposal distributions q(γ∗
g|γg,γ−g)and tuning parameters,
which may make ﬁtting ME models troublesome. Gormley & Murphy (2010b) detail an
approach to deriving proposal distributions with attractive properties, within the context of
an ME model for network data.
Alternatively, Frühwirth-Schnatter et al.(2012) exploit data augmentation of the MNL
model (4) based on the diﬀerenced random utility model representation in the context of
ME models to implement Step 4. As shown by Frühwirth-Schnatter & Frühwirth (2010), for
eachg= 1,...,Gthe MNL model has the following representation as a binary logit model
8
Algorithm 2 MH-within-Gibbs MCMC inference for a simple Gaussian mixture of experts
model
Iterate the following steps for m= 1,...,M:
1 Forg= 1,...,G, drawµgfrom thed-variate normal posterior N(µng,Λng)where
Λng= (Λ−1
0+ngΣ−1
g)−1andµng= Λng(Λ−1
0µ0+ Σ−1
gng¯yg)andng=/summationtextn
i=1I(zi=g)and
ng¯yg=/summationtextn
i=1yiI(zi=g).
2 Forg= 1,...,G, draw ΣgfromIW(νng,Sng)whereνng=ν0+ngandSng=S0+/summationtextn
i=1I(zi=g)(yi−µg)(yi−µg)/latticetop.
3 Fori= 1,...,ndrawzifrom a multinomial distribution M(1,pi1,...,piG)with success
probabilities (pi1,...,piG)where
pig=ηg(xi|γ)φ(yi|µg,Σg)/G/summationdisplay
g/prime=1ηg/prime(xi|γ)φ(yi|µg/prime,Σg/prime).
4 Forg= 2,...,G, the component weight parameters γgare updated via a Metropolis-
Hastings step, while holding the remaining component weight parameters γ−gﬁxed. Typ-
ically, a multivariate normal proposal distribution q(γ∗
g|γg,γ−g)is employed:
(a) Propose γ∗
g∼N (˜µγ,˜Λγ)from a (q+ 1)-variate Normal distribution where ˜µγand
˜Λγare user speciﬁed and might depend on the current value of γ.
(b) IfU∼U[0,1]is such that
U≤min/braceleftBiggp(z|γ∗
g,γ−g,x)p(γ∗
g)q(γg|γ∗
g,γ−g)
p(z|γg,γ−g,x)p(γg)q(γ∗
g|γg,γ−g),1/bracerightBigg
,
then setγg=γ∗
g; otherwise leave γgunchanged.
conditional on knowing λhi= exp (˜xiγh)for allh/negationslash=g:
ugi= ˜xiγg−log(/summationdisplay
h/negationslash=gλhi) +εgi, (6)
Dg
i=I(ugi≥0)
whereugiis a latent variable, εgiare i.i.d. errors following a logistic distribution, and Dg
i=
I(zi=g)is a binary outcome variable indicating whether the group indicator ziis equal to
g. Note that γ1= 0for the baseline, hence λ1i= 1. In a data augmented implementation of
Step 4, the latent variables (u2i,...,uGi)are introduced for each i= 1,...,nas unknowns.
Givenλ2i,...,λGiandzi,(u2i,...,uGi)can be sampled in closed form from exponentially
distributed random variables. Following Scott (2011), natural proposal distributions are
available to implement an MH-step to sample γg|γ−g,z,ugfor allg= 2,...,Gconditional
onug={ug1,...,ugn}from the linear, non-Gaussian regression model (6).
To avoid any MH-step, Frühwirth-Schnatter et al.(2012) apply auxiliary mixture sam-
pling as introduced by Frühwirth-Schnatter & Frühwirth (2010) to (6) and approximate for
eachεgithe logistic distribution by a 10-component scale mixture of Normal distributions
with zero means and parameters (s2
r,wr),r= 1,..., 10. In a second step of data augmenta-
tion, the component indicator rgiis introduced as yet another latent variable. Conditional
on the latent variables ugand the indicators rg={rg1,...,rgn}the binary logit model (6)
reduces to a linear Gaussian regression model. Hence, the posterior γg|γ−g,z,ug,rgis Gaus-
9
sian and a Gibbs step is available to sample γgfor allg= 2,...,Gconditional on ugand
rg. Finally, each component indicator rgiis sampled from a discrete distribution conditional
onugiandγ.
Chapter 13 in this volume details Bayesian estimation of informative regime switching
models which can be regarded as an extension of ME models to hidden Markov models in
time series analysis.
As in any mixture model setting, the so called label switching problem (Stephens, 2000;
Frühwirth-Schnatter, 2011a) must be considered when employing such Gibbs based algo-
rithms, see Chapter 5. This identiﬁability issue, along with others, is discussed in Section 5.
3.3 Model selection
Within the suite of ME models outlined in Section 2.3 the question of which, how and where
covariates are used naturally arises. This is a challenging problem as the space of ME models
is potentially very large, once variable selection for the covariates entering the component
weights and the mixture components is considered. Thus in practice only models where
covariates enter all mixture components and/or all component weights as main eﬀects are
typically considered in order to restrict the size of the model search space. In fact, even for
this reduced model space, there are a maximum of G×2q×2qpossible models to consider.
In ME models involving generalised linear models of covariates, standard variable selection
approaches can be used to ﬁnd the optimal model. Practical approaches to this issue are
detailedintheillustrativeapplicationsofSection4.Notethatthemannerinwhichcovariates
enter the ME model may also be guided by the question of interest in the application under
study.
If the number of components Gis unknown, the model search space increases again.
Approaches such as marginal likelihood evaluation, or information criteria, are useful for
choosing the optimal Gin ME models; the reader is referred to Chapter 7 in this volume
which addresses model selection and selecting the number of components in a mixture model
in great detail.
Marginal likelihood computation for mixtures of experts models
As discussed in Chapter 7, Section 7.2.3.2, highly accurate sampling-based approximations
to the marginal likelihood are available, if Gis not too large. For instance, Frühwirth-
Schnatter & Kaufmann (2008) apply bridge sampling (Frühwirth-Schnatter, 2004) to com-
pute marginal likelihoods for a mixture of experts model with a single covariate (that is
q= 1) with up to four components. Frühwirth-Schnatter (2011b) combines auxiliary mix-
ture sampling (Frühwirth-Schnatter & Wagner, 2008) with importance sampling to compute
marginal likelihoods for mixture of experts models. A detailed summary of this approach is
provided below.
Permutation sampling is applied to ensure that all equivalent modes of the posterior
distribution are visited. Consider a permutation σ∈S(G), where S(G)denotes the set of
theG!permutations of{1,...,G}. To relabel all parameters in a mixture of experts model
according to the permutation σ, deﬁneθ⋆
g=θσ(g)andη⋆
g(˜xi) =ησ(g)(˜xi)forg= 1,...,G.
Special attention has to be given to the correct relabelling of the coeﬃcients γgin the
MNL model when applying the permutation σ. The coeﬃcients (γ1,...,γG)and(γ⋆
1,...,γ⋆
G)
10
deﬁning, respectively, the MNL models ηg(˜xi)andη⋆
g(˜xi)are related through:
˜xiγ⋆
g= log/bracketleftBiggη⋆
g(˜xi)
η⋆
g0(˜xi)/bracketrightBigg
= log/bracketleftBiggησ(g)(˜xi)
ησ(g0)(˜xi)/bracketrightBigg
= log/bracketleftBiggησ(g)(˜xi)
ηg0(˜xi)/bracketrightBigg
−log/bracketleftBiggησ(g0)(˜xi)
ηg0(˜xi)/bracketrightBigg
= ˜xi(γσ(g)−γσ(g0)).
To ensure that the baseline g0(assumed to be equal to g0= 1throughout this chapter)
remains the same, despite relabeling, the coeﬃcients are permuted in the following way:
γ⋆
g=γσ(g)−γσ(g0), g = 1,...,G,
which indeed implies that γ⋆
g0= 0. ForG= 2, the sign of all coeﬃcients of γ2is simply
ﬂipped, ifσ= (2,1)and remains unchanged, otherwise.
Frühwirth-Schnatter & Wagner (2008) discuss various importance sampling estimators
of the marginal likelihood for non-Gaussian models such as logistic models. Using auxiliary
mixture sampling, one of their approaches constructs the importance density from the Gaus-
sian full conditional densities appearing in the augmented Gibbs sampler. This approach is
easily extended to mixture of experts models. As discussed in Section 3.2, auxiliary mixture
sampling yields Gaussian posteriors p(γg|γ−g,z,ug,rg)for the MNL coeﬃcients γgin a mix-
tureofexpertsmodels,conditionalonthelatentutilities ugandthelatentindicators rg.This
allows construction of an importance density qG(θ)as in Chapter 7, Section 7.2.3.2, however
it is essential that qG(θ)covers all symmetric modes of the mixture posterior. A successful
strategy is to apply random permutation sampling, where each sampling step is concluded
by relabelling as described above, using a randomly selected permutation σ∈S(G). The
corresponding importance density reads:
qG(θ) =1
SS/summationdisplay
s=1G/productdisplay
g=2p(γg|γ(s)
−g,u(s)
g,r(s)
g,z(s))G/productdisplay
g=1p(θg|z(s),y), (7)
where{γ(s),u(s)
2,..., u(s)
G,r(s)
2,..., r(s)
G,z(s)},s= 1,...,Sisasubsequenceofposteriordraws.
OnlyifSislargecomparedto G!,thenallsymmetricmodesarecoveredbyrandompermuta-
tion sampling, with the number of visits per mode being on average S/G!. The construction
of this importance density is fully automatic and it is suﬃcient to store the moments of
the various conditional densities (rather than the allocations zand the latent utilities ug
and indicators rgthemselves) during MCMC sampling for later evaluation. This importance
density is used to compute importance sampling estimators of the marginal likelihood, see
the illustrative application in Section 4.1.
4 Illustrative Applications
The utility of ME models is illustrated in this section through the use of several applications.
ME Markov chain models for categorical time series, ME models for ranked preference data,
and ME models for network data, all of which are members of the ME model framework,
are applied.
4.1 Analysing marijuana use through ME Markov chain models
Langet al.(1999) studied data on the marijuana use of 237 teenagers taken from ﬁve annual
waves (1976-80) of the National Youth Survey. The respondents were 13 years old in 1976
11
and reported for ﬁve consecutive years their marijuana use in the past year as a categorical
variable with the three categories “never”, “not more than once a month” and “more than
once a month”. Hence, for i= 1,..., 237, the outcome variable is a categorical time series
yi= (yi0,yi1,...,yi4)with three states, labeled 1 for never-user, 2 for light and 3 for heavy
users.
To identify groups of teenagers with similar marijuana use behaviour, Frühwirth-
Schnatter (2011b) applied a ME approach based on Markov chain models (Frühwirth-
Schnatter et al., 2012) and considered each time series yias a single entity belonging
to one ofGunderlying classes. Various types of ME Markov chain models were applied
to capture dependence in marijuana use over time and to investigate if the gender of the
teenagers can be associated with a certain type of marijuana use.
Given the times series nature of the categorical outcome variable yi, the component
densityfg(·)in the mixture of experts model (1) must have an appropriate form and various
models are considered. Model M1is a standard ﬁnite mixture of time-homogeneous Markov
chainmodelsoforderone(Pamminger&Frühwirth-Schnatter,2010)whereeachcomponent-
speciﬁc density fg(·)in (1) is characterized by a transition matrix ξgwithJ= 3rows
and the weight distribution η1,...,ηGis independent of any covariates. Each row ξg,j·=
(ξg,j1,...,ξg,j3),j= 1,...,J, of the matrix ξgrepresents a probability distribution over the
three categories of marijuana use with
ξg,jk=P(yit=k|yi,t−1=j,zi=g), k = 1,..., 3.
This model is extended in various ways to include covariate information into the transition
behaviour. First, an inhomogeneous model (labelled model M2) is considered, where the
transitionmatrixineachgroupdependsonthegender xioftheteenager.Ifall J= 6possible
combinationsHit= (yi,t−1,xi)of the immediate past yi,t−1at timetand the gender xiare
indexedby j= 1,...,J,thenthecomponent-speciﬁcdensity fg(yi|ξg)in(1)canbedescribed
by a generalized transition matrix ξgwith six rows, with the jth rowξg,j·= (ξg,j1,...,ξg,j3)
describing again the conditional distribution of yit, given that the state of the history Hit
equalsj:
ξg,jk=P(yit=k|Hit=j,zi=g), k = 1,..., 3.
Evidently, the component speciﬁc distribution reads:
fg(yi|ξg) =J/productdisplay
j=13/productdisplay
k=1ξni,jk
g,jk (8)
where, for each time series i,ni,jk=/summationtext4
t=1I(yit=k,Hit=j)is the number of transitions
into statekgiven a history of type j. Note that (8) is formulated conditional on the ﬁrst
observation yi0.
Alternative component-speciﬁc distributions can be constructed, by deﬁning the history
Hitthrough diﬀerent combinations of past values and covariates. Choosing Hit= (yi,t−1,t),
for instance, deﬁnes a time-inhomogeneous Markov chain model, labeled model M3, with
J= 12diﬀerent covariate combinations. This model is able to capture the eﬀect that the
transition behaviour between the states might change as the teenagers grow older.
Themost complexmodel, labelled model M4, extendsmodelM3byassuming additional
dependence on gender, i.e. Hit= (yi,t−1,t,xi), withJ= 24diﬀerent covariate combinations.
Both modelM3andM4are characterised by component-speciﬁc generalized transition
matricesξgwith, respectively, 12 and 24 rows. For each of the models M2,M3,M4, it is
12
Table2:Marijuanadata;marginallikelihood logp(y|Mk)forvariousﬁnitemixturesofhomo-
geneous (M1) and inhomogeneous ( M2,M3,M4) Markov chain models with an increasing
numberGof classes (best values for each model in bold font)
G
Model Covariates J1 2 3
M1 - 3 -605.5 -600.0 -600.3
M2xi6 -610.0 -601.3 -603.6
M3t12 -613.7 -596.5 -599.4
M4t,xi24 -619.8 -602.7 -601.1
assumed that the weight distribution η1,...,ηGis independent of any covariate, leading to
various ﬁnite mixtures of inhomogeneous Markov chain models.
Bayesian inference is carried out for all models M1,...,M4for an increasing number
G= 1,2,3of classes. MCMC estimation as described in Section 3.2 is easily applied, as the
Jrowsξg,j·ofξgare conditionally independent under the conditionally conjugate Dirichlet
priorξg,j·∼D(d0,j1,...,d 0,j3). Given zandy, the generalized transition matrix ξgis sampled
row-by-row from a total of JGDirichlet distributions:
ξg,j·|z,y∼D(d0,j1+ng
j1,...,d 0,j3+ng
j3), j = 1,...,J, g = 1,...,G, (9)
whereng
jk=/summationtext
i:zi=gni,jkis the total number of transitions into state kobserved in class g
given a history of type j.
For model comparison, the marginal likelihood is computed explicitly for G= 1, while
importancesamplingasdescribedinSection3.3isappliedfor G= 2,3,usingtheimportance
density:
qG(θ) =1
SS/summationdisplay
s=1p(η|z(s))G/productdisplay
g=1J/productdisplay
j=1p(ξg,j·|z(s),y),
wherep(ξg,j·|z,y)is equal to the full conditional Dirichlet posterior of ξg,j·given in (9).
Random permutation sampling is applied to ensure that all G!symmetric modes are visited
andS= 10,000. The marginal likelihoods reported in Table 2 select G= 2for all models
except forM4, whereG= 3is selected. Among all models, the marginal likelihood is the
highest for model M3withG= 2classes.
Hence,atime-inhomogeneousMarkovchainmodelwhichdoesnotdependongenderbest
describes the transition behaviour in each class. Table 3 reports the corresponding posterior
means E (ξg,·|y)and E (ηg|y)for each of the two groups. Label switching was resolved by
applyingk-means clustering to a vector constructed from all persistence probabilities at all
time points. Both groups are roughly of equal size, with the ﬁrst group being slightly larger.
A characteristic diﬀerence is evident for the two groups of teenagers. In group 1, never-users
have a high probability ξt,11to remain never-users throughout the whole observation period,
whereas this probability is much smaller for the second group right from the beginning and
drops to only 45% in the last year.
To investigate if gender is associated with group membership, model M3withG= 2
classes is combined with the ME model (4), by including gender as subject-speciﬁc covariate
xias in the example in Section 2.2. This model is labelled model M5. Additionally, a dummy
variableDi0is included, indicating if the teenager used marijuana, light or heavy, in the ﬁrst
year. AsG= 2, the MEmodel (4)reduces to abinary logitmodel withregressioncoeﬃcients
γ2= (γ20,γ21,γ22), each assumed to follow a standard normal prior distribution.
13
Table 3: Marijuana data; ﬁnite mixture of time-inhomogeneous Markov chain models (model
M3) withG= 2classes; the estimated posterior mean E (ξg,·|y)is arranged for each t=
1,..., 4as a3×3matrix; the estimated class sizes ˆηgare equal to the posterior mean E (ηg|y)
t= 1 t= 2 t= 3 t= 4
Group 1 0.93 0.04 0.03 0.89 0.09 0.02 0.90 0.04 0.06 0.93 0.04 0.03
(ˆη1= 0.56)0.50 0.17 0.34 0.10 0.33 0.57 0.17 0.65 0.18 0.20 0.64 0.16
0.22 0.18 0.60 0.10 0.17 0.73 0.04 0.27 0.69 0.15 0.12 0.74
Group 2 0.76 0.21 0.03 0.70 0.24 0.06 0.75 0.18 0.07 0.45 0.43 0.12
(ˆη2= 0.44)0.34 0.15 0.51 0.23 0.39 0.38 0.31 0.41 0.28 0.46 0.43 0.11
0.18 0.23 0.59 0.10 0.22 0.68 0.13 0.15 0.72 0.05 0.10 0.85
Table 4: Marijuana data; ME model with ˜xi= (1,xi,Di0)(modelM5), extending model M3
withG= 2classes. Posterior expectation and 95% HPD region of the component weight
parameters γ2jin the ME model (4)
Covariate ˜xij E(γ2j|y)95% HPD region of γ2j
constant -0.69 (-1.75,0.35)
male (baseline: female) 0.28 (-0.71,1.22)
marijuana use in 1976 (baseline: no) -0.07 (-1.70,1.43)
logp(y|M 5) -598.5
From posterior inference in Table 4, we ﬁnd that male teenagers have a slightly higher
probability to belong to the second group, because E (γ21|y)>0, however, the coeﬃcient γ21
is not signiﬁcantly diﬀerent from 0. Similarly, the initial state from which a teenager started
in 1976 does not have a signiﬁcant inﬂuence on the probability to belong to the second
group. This suggests that the ME time-inhomogeneous Markov chain model actually reduces
to a standard mixture of time-inhomogeneous Markov chain models which is conﬁrmed by
comparing the log marginal likelihood of both models, being equal to -596.5 for a standard
mixture model with G= 2groups, see Table 2, and being equal to -598.5 for an ME model
withG= 2groups, see Table 4.
The marginal likelihood estimator for the ME model is based on importance sampling
using the importance density (7) derived from auxiliary mixture sampling:
qG(θ) =1
SS/summationdisplay
s=1p(γ2|u(s)
2,r(s)
2,z(s))2/productdisplay
g=1J/productdisplay
j=1p(ξg,j·|z(s),y),
wherep(γ2|u2,r2,z)is conditionally Gaussian and p(ξg,j·|z,y)is equal to the full conditional
Dirichlet posterior of ξg,j·given in (9). Again, random permutation sampling is applied to
ensure that the two equivalent modes are visited.
To sum up, this investigation shows that teenagers may, indeed, be clustered into two
groups with diﬀerent behaviour with respect to marijuana use, one being a never-user group,
while the second group has a much higher risk to become a user. Preference for a standard
mixture of Markov chain models over a mixture of experts Markov chain model based on
gender shows that the two types of marijuana use cannot be associated with the gender of
the teenager. Both male and female teenagers have about the same risk to belong to the
second group. Unobserved factors, not the gender, are relevant for membership of a teenager
to one group or the other.
14
Table 5: Covariates recorded for each respondent in the Irish Marketing Surveys poll.
Age Area Gender Government Marital Social
satisfaction status class
– City Housewife No opinion Married AB
Rural Male Not satisﬁed Single C1
Town Non-housewife Satisﬁed Widowed C2
DE
F50+
F50-
4.2 A mixture of experts model for ranked preference data
Mary McAleese served as the eighth President of Ireland from 1997 to 2011 and was elected
under the Single Transferable Vote electoral system. Under this system voters rank, in order
of their preference, some or all of the electoral candidates. The vote counting system which
results in the elimination of candidates and the subsequent election of the President is an
intricateprocessinvolvingthetransferofvotesbetweencandidatesasspeciﬁedbythevoters’
ballots. Details of the electoral system, the counting process and the 1997 Irish presidential
election are given in Coakley & Gallagher (2004), Sinnott (1995), Sinnott (1999) and Marsh
(1999).
The 1997 presidential election race involved ﬁve candidates: Mary Banotti, Mary
McAleese, Derek Nally, Adi Roche and Rosemary Scallon. Derek Nally and Rosemary Scal-
lon were independent candidates while Mary Banotti and Adi Roche were endorsed by the
then current opposition parties Fine Gael and Labour respectively. Mary McAleese was en-
dorsed by the Fianna Fáil party who were in power at that time. In terms of candidate type,
McAleese and Scallon were deemed to be conservative candidates with the other candidates
regarded as liberal. Gormley & Murphy (2008a,b, 2010a,b) provide further details on the
1997 presidential election and on the candidates.
One month prior to election day a survey was conducted by Irish Marketing Surveys on
1083 respondents. Respondents were asked to list some or all of the candidates in order of
preference, as if they were voting on the day of the poll. In addition, pollsters gathered data
on attributes of the respondents as detailed in Table 5.
Interest lies in determining if groups of voters with similar preferences (i.e. voting blocs)
exist within the electorate. If such voting blocs do exist, the inﬂuence the recorded socio-
economic variables may have on the clustering structure and/or on the preferences which
characterize a voting bloc is also of interest. Jointly modelling the rank preference votes and
the covariates through a mixture of experts model for rank preference data when clustering
the electorate provides this insight.
Given the rank nature of the outcome variables or votes yi(i= 1,...,n = 1083) the
component density fg(·)in the mixture of experts model (1) must have an appropriate form.
The Plackett-Luce model (Plackett, 1975; Gormley & Murphy, 2006) (or exploded logit
model) for rank data provides a suitable model; Benter’s model (Benter, 1994) provides
another alternative. Let yi= [c(i,1),...,c (i,mi)]denote the ranked ballot of voter iwhere
c(i,j)denotes the candidate ranked in jth position by voter iandmiis the number of
candidates ranked by voter i. Under the Plackett-Luce model, given that voter iis a member
of voting bloc gand given the ‘support parameter’ pg= (pg1,...,pgM), the probability of
15
Table 6: The model with smallest BIC within each type of mixture of experts model for
ranked preference data applied to the 1997 Irish presidential election data
BICGCovariates
Simple mixture of experts model 8491 4 ηg: Government satisfaction, Age.
Standard mixture of experts 8512 3 ηg: Government satisfaction, Age.
regression model pg: Age
Mixture model 8513 3 –
Mixture of regressions model 8528 1 pg: Government satisfaction
voteri’s ballot is
p(yi|pg) =pg,c(i,1)/summationtextM
s=1pg,c(i,s)·pg,c(i,2)/summationtextM
s=2pg,c(i,s)···pg,c(i,mi)/summationtextM
s=mipg,c(i,s),
whereM= 5denotes the number of candidates in the electoral race. The support parameter
pgj(typically restricted such that/summationtextM
j=1pgj= 1) can be interpreted as the probability of
ranking candidate jﬁrst, out of the currently available choice set. Hence, the Plackett-Luce
model models the ranking of candidates by a voter as a set of independent choices by the
voter, conditional on the cardinality of the choice set being reduced by one after each choice
is made.
In the standard mixture of experts regression model, the parameters of the component
densities are modelled as a function of covariates. Here the support parameters are modelled
as a logistic function of the covariates
log/bracketleftBiggpgj(xi)
pg1(xi)/bracketrightBigg
=βgj0+βgj1xi1+···+βgjqxiq
wherexi= (xi1,...,xiq)is the set of qcovariates associated with voter iandβgj=
(βgj0,...,βgjq)/latticetopare unknown parameters for j= 2,...,M. Note that for identiﬁability
reasons candidate 1 is used as the baseline choice and βg1= (0,..., 0)for allg= 1,...,G.
In the standard mixture of experts regression model, the component weights are also
modelled as a function of covariates, in a similar vein to the example used in Section 2.2,
i.e.
log/bracketleftBiggηg(xi)
η1(xi)/bracketrightBigg
=γg0+γg1xi1+···+γgqxiq,
where voting bloc 1 is used as the baseline voting bloc.
The suite of four ME models in the ME framework (Figure 2) arise from modelling
the component parameters and/or the component weights as functions of covariates, or as
constant with respect to covariates. In this application, each model is ﬁtted in a maximum
likelihood framework using the EM algorithm; approximate standard errors for the model
parameters are derived from the empirical information matrix (McLachlan & Peel, 2000)
after the EM algorithm has converged. Model ﬁtting details for each model are outlined in
Gormley & Murphy (2008a,b, 2010a,b).
Each of the four ME models for rank preference data were ﬁtted to the data from
the electorate in the Irish presidential election poll. A range of models with G= 1,..., 5
was considered and a forward step-wise selection method was employed to choose inﬂuen-
tial covariates. The Bayesian Information Criterion (BIC) (Kass & Raftery, 1995; Schwarz,
16
Voting Bloc 1 Voting Bloc 2 Voting Bloc 3 Voting Bloc 4
Banotti
McAleese
Nally
Roche
Scallon
τ^1=0.19 τ^2=0.16 τ^3=0.35 τ^4=0.30.11 (0.01)
0.28 (0.03)
0.15 (0.02)
0.15 (0.04)
0.31 (0.06)0.13 (0.01)
0.13 (0.01)
0.03 (<0.01)
0.70 (0.01)
0.01 (<0.01)0.17 (<0.01)
0.72 (0.03)
0.04 (<0.01)
0.06 (<0.01)
0.01 (<0.01)0.52 (<0.01)
0.14 (0.01)
0.13 (0.01)
0.15 (0.01)
0.05 (<0.01)Figure 3: A mosaic plot representation of the parameters of the component densities of
the simple mixture of experts model for rank preference data. The width of each block is
proportional to the marginal probability of component membership ( ˆηg=/summationtextn
i=1ηg(xi|ˆγ)/n).
The blocks are divided in proportion to the Plackett-Luce support parameters which are
detailed therein. Standard errors are provided in parentheses.
1978) was used to select the optimal model; this criterion is a penalized likelihood criterion
which rewards model ﬁt while penalizing non-parsimonious models, see also Chapter 7, Sec-
tion 7.2.2 of this volume. Small BIC values indicate a preferable model. Table 6 details the
optimal models for each type of ME model ﬁtted.
Based on the BIC values, the optimal model is a simple mixture of experts model with
four groups where “age“ and “government satisfaction” are important covariates for deter-
mining group or “voting bloc” membership. Under this simple mixture of experts model,
the covariates are not informative within voting blocs, but only in determining voting bloc
membership. The maximum likelihood estimates of the model parameters are reported in
Figure 3 and in Table 7.
The support parameter estimates illustrated in Figure 3 have an interpretation in the
context of the 1997 Irish presidential election. Voting bloc 1 could be characterized as the
“conservative voting bloc” due to its large support parameters for McAleese and Scallon.
Voting bloc 2 has large support for the liberal candidate Adi Roche. Voting bloc 3 is the
17
Table 7: Odds ratios ( exp(γg)/exp(γ1)) for the component weight parameters in the simple
ME model for rank preference data (95% conﬁdence intervals are given in parentheses). The
covariates ‘age’ and ‘government satisfaction level’ were selected as inﬂuential
Age Not satisﬁed Satisﬁed
Voting bloc 2 0.01 (0.00, 0.05) 2.80 (0.77, 10.15) 1.14 (0.42, 3.11)
Voting bloc 3 0.95 (0.32, 2.81) 3.81 (0.90, 16.13) 3.12 (0.94, 10.31)
Voting bloc 4 1.56 (0.35, 6.91) 3.50 (1.07, 11.43) 0.35 (0.12, 0.98)
largestvotingblocintermsofmarginalcomponentweightsandintuitivelyhaslargersupport
parameters for the high proﬁle candidates McAleese and Banotti. These candidates were
endorsed by the two largest political parties in the country at that time. Voters belonging to
voting bloc 4 favor Banotti and have more uniform levels of support for the other candidates.
A detailed discussion of this optimal model is also given in Gormley & Murphy (2008b).
Table 7 details the odds ratios computed from the component weight parameters γ=
{γ2,γ3,γ4}. In the model, voting bloc 1 (the conservative voting bloc) is the baseline voting
bloc andγ1= (0,..., 0)/latticetop. Two covariates were selected as inﬂuential: age and government
satisfaction levels. In the “government satisfaction” covariate, the baseline was chosen to be
“no opinion“.
Interpreting the odds ratios provides insight to the type of voter which characterises
each voting bloc. For example, older (and generally more conservative) voters are much
less likely to belong to the liberal voting bloc 2 than to the conservative voting bloc 1
(exp(γ21) = 0.01). Also, voters with some interest in government are more likely to belong
to voting bloc 3 ( exp(γ32) = 3.81andexp(γ33) = 3.12), the bloc favouring candidates backed
by large government parties, than to belong to the conservative voting bloc 1. Voting bloc
1 had high levels of support for the independent candidate Scallon. The component weight
parameterestimates furtherindicate thatvotersdissatisﬁed with thecurrentgovernmentare
more likely to belong to voting bloc 4 than to voting bloc 1 ( exp(γ42) = 3.50). This is again
intuitive as voting bloc 4 favours Mary Banotti who was backed by the main government
oppositionparty,whilevotingbloc1favoursthegovernmentbackedMaryMcAleese.Further
interpretation of the component weight parameters are given in Gormley & Murphy (2008b).
4.3 A mixture of experts latent position cluster model
The latent position cluster model (Handcock et al., 2007) develops the idea of the latent
socialspace(Hoﬀ et al.,2002)byextendingittoaccommodateclustersofactorsinthelatent
space. Under the latent position cluster model, the latent location of each actor is assumed
to be drawn from a ﬁnite normal mixture model, each component of which represents a
cluster of actors. In contrast, the model outlined in Hoﬀ et al.(2002) assumes that the
latent positions were normally distributed. Thus, the latent position cluster model oﬀers a
more ﬂexible version of the latent space model for modelling heterogeneous social networks.
The latent position cluster model provides a framework in which actor covariates may
be explicitly included in the model – the probability of a link between two actors may be
modelled as a function of both their separation in the latent space and of their relative
covariates. However, the covariates may contribute more to the structure of the network
than solely through the link probabilities – the covariates may inﬂuence both the cluster
membership of an actor and their link probabilities. A latent position cluster model in which
the cluster membership of an actor is modelled as a function of their covariates lies within
18
Table 8: Covariates associated with the 71 lawyers in the US corporate law ﬁrm. The last
category in each categorical covariate is treated as the baseline category in all analyses.
Covariate Levels
Age –
Gender 1 = male
2 = female
Law school 1 = Harvard or Yale
2 = University of Connecticut
3 = other
Oﬃce 1 = Boston
2 = Hartford
3 = Providence
Practice 1 = litigation
2 = corporate
Seniority 1 = partner
2 = associate
Years with the ﬁrm –
the mixture of experts framework.
Speciﬁcally, social network data take the form of a set of relations {yi,j}between a group
ofi,j= 1,...,nactors, represented by an n×nsociomatrix y. Here it is assumed that the
relationyi,jbetween actors iandjis a binary relation, indicating the presence or absence of
a link between the two actors; the mixture of experts latent position cluster model is easily
extended to other forms of relation (such as count data). Covariate data xi= (xi1,...,xiq)
associated with actor iare assumed to be available, where qdenotes the number of observed
covariates.
Each actor iis assumed to have a location wi= (wi1,...,wiD)in theDdimensional
latent social space. The probability of a link between any two actors is assumed to be
independent of all other links in the network, given the latent locations of the actors. Let
xi,j= (xij1,...,xijq)denote anqvector of dyadic speciﬁc covariates where xijk=d(xik,xjk)
is a measure of the similarity in the value of the kth covariate for actors iandj. Given the
link probabilities parameter vector β, the likelihood function is then
p(y|w,x,β) =n/productdisplay
i=1/productdisplay
j/negationslash=ip(yi,j|wi,wj,xi,j,β)
where wis then×Dmatrix of latent locations and xis the matrix of dyadic speciﬁc
covariates. The probability of a link between actors iandjis then modelled using a logistic
regression model where both dyadic speciﬁc covariates and Euclidean distance in the latent
space are covariates:
log/bracketleftBiggP(yi,j= 1)
P(yi,j= 0)/bracketrightBigg
=β0+β1xij1+···+βqxijq−||wi−wj||.
To account for clustering of actor locations in the latent space, it is assumed that the latent
locationswiare drawn from a ﬁnite mixture model. Moreover, in the mixture of experts
latent position cluster model, the latent locations are assumed drawn from a ﬁnite mixture
19
model in which actor covariates may inﬂuence the mixing proportions:
wi∼G/summationdisplay
g=1ηg(xi|γ)φ(wi|µg,σ2
gI)
where
ηg(xi|γ) =exp(γg0+γg1xi1+···+γgqxiq)
/summationtextG
g/prime=1exp(γg/prime0+γg/prime1xi1+···+γg/primeqxiq)
andγ1= (0,..., 0)/latticetop. This model has an intuitive motivation: the covariates of an actor may
inﬂuence their cluster membership, their cluster membership inﬂuences their latent location,
and in turn their latent location determines their link probabilities.
The mixture of experts latent position cluster model can be ﬁtted within the Bayesian
paradigm; as outlined in Section 3.2 a Metropolis-within-Gibbs sampler can be employed
to draw samples from the posterior distribution of interest. Model issues such as likelihood
invariance to distance preserving transformations of the latent space and label switching
mustbeconsideredduringthemodelﬁttingprocess–anapproachtodealingwithsuchmodel
identiﬁability and full model ﬁtting details are available in Gormley & Murphy (2010b). In
this application, model choice concerns not only the number Gof clusters, but also the
dimensionDof the latent space.
An example of the mixture of experts latent position cluster model methodology is pro-
vided here through the analysis of a network data set detailing interactions between a set
of 71 lawyers in a corporate law ﬁrm in the USA (Lazega, 2001). The data include mea-
surements of the coworker network, an advice network and a friendship network. Covariates
associated with each lawyer in the ﬁrm are also included and are detailed in Table 8. Interest
lies in identifying social processes within the ﬁrm such as knowledge sharing and organisa-
tional structures, and examining the potential inﬂuence of covariates on such processes.
Under the ME model framework outlined in Section 2.3, a suite of four mixtures of
experts latent position cluster models is available. This suite of models was ﬁtted to the
advice network; data in this network detail links between lawyers who sought basic profes-
sional advice from each other over the previous twelve months. Gormley & Murphy (2010b)
explore the coworkers network data set and the friendship network data set using similar
methodology. Figure 4 illustrates the resulting latent space locations of the lawyers under
each ﬁtted model with (G,D ) = (2,2). These values were selected using BIC after ﬁtting a
range of latent position cluster models (with no covariates) to the network data only (Hand-
cocket al., 2007). Table 9 details the resulting regression parameter estimates and their
associated uncertainty for the four ﬁtted models.
The models are compared through the AICM, the posterior simulation-based analogue
of Akaike’s Information Criterion (AIC) (Akaike, 1973; Raftery et al., 2007). In this imple-
mentation the optimal model is that with the highest AICM and is the model with covariates
in the link probabilities and in the component weights. The results of the analysis show some
interesting patterns. The coeﬃcients of the covariates in the link probabilities are very simi-
lar in the models (b) and (d) in Table 9. These coeﬃcients indicate that a number of factors
have a positive or negative eﬀect on whether a lawyer asks another for advice. In summary,
lawyers who are similar in seniority, gender, oﬃce location and practice type are more likely
to ask each other for advice. The eﬀects of years and age seem to have a negative eﬀect, but
these variables are correlated with seniority and with each other, so their marginal eﬀects
are more diﬃcult to interpret.
Importantly, the latent positions are very similar in models (a) and (c) which do not
have covariates in the link probabilities and models (b) and (d) which do have covariates
20
−5 0 5−4 −2 0 2 4
Dimension 1Dimension 2(a) Latent position cluster model.
−5 0 5−4 −2 0 2 4
Dimension 1Dimension 2 (b) Mixture of regressions latent position
cluster model.
−5 0 5−4 −2 0 2 4
Dimension 1Dimension 2
(c) Simple latent position cluster model.
−5 0 5−4 −2 0 2 4
Dimension 1Dimension 2 (d) Standard mixture of experts latent po-
sition cluster model.
Figure 4: Estimates of clusters and latent positions of the lawyers from the advice network
data. The ellipses are 50 %posterior sets illustrating the uncertainty in the latent locations.
Lawyers who are members of the same cluster are illustrated using the same shade and
symbol. Observed links between lawyers are also illustrated.
in the link probabilities. This can be explained because of the diﬀerent role that the latent
space plays in the models with covariates in the link probabilities and those that do not
have such covariates. When the covariates are in the link probabilities, the latent space is
modelling the network structure that could not be explained by the link covariates, whereas
in the other case the latent space is modelling much of the network structure.
Interestingly, in the model with the highest AICM value, there are covariates in the
cluster membership probabilities as well as in the link probabilities. This means that the
structure in the latent space, which is modelling what could not be explained directly in
the link probabilities, has structure that can be further explained using the covariates. The
oﬃce location, practice and age of the lawyers retain explanatory power in explaining the
clustering found in the latent social space.
The diﬀerence in the cluster membership coeﬃcients in models (c) and (d) is due to the
diﬀerent interpretation of the latent space in these models. However, it is interesting to note
that in this application the signs of the coeﬃcients are identical because the cluster member-
ships shown for these models in Figure 4(c) and Figure 4(d) are similar; this phenomenon
21
Table9:Posteriormeanparameterestimatesforthefourmixturesofexpertsmodelsﬁttedto
the lawyers advice data as detailed in Figure 4. Standard deviations are given in parentheses.
Note that cluster 1 was used as the baseline cluster in the case of the cluster membership
parameters. Baseline categories for the covariates are detailed in Table 8
Model (a) Model (b) Model (c) Model (d)
Link Probabilities
Intercept 1.26 (0.10) -2.87 (0.17) 1.23 (0.10) -2.65 (0.17)
Age -0.02 (0.004) -0.02 (0.004)
Gender 0.60 (0.09) 0.62 (0.09)
Oﬃce 2.02 (0.10) 1.97 (0.10)
Practice 1.63 (0.10) 1.57 (0.10)
Seniority 0.89 (0.11) 0.81 (0.11)
Years -0.04 (0.005) -0.04 (0.005)
Cluster Memberships
Intercept -1.05 (1.75) 0.94 (0.79) -0.62 (1.23) 1.27 (1.29)
Age -0.09 (0.04) -0.14 (0.06)
Oﬃce (=1) 1.94 (1.02) 2.40 (1.14)
Oﬃce (=2) -2.08 (1.09) -0.97 (1.19)
Practice 3.18 (0.85) 2.14 (1.08)
Latent Space Model
Cluster 1 mean -0.50 (0.52) 0.09 (0.19) -1.09 (0.31) -0.54 (0.21)
0.21 (0.58) -0.09 (0.26) 0.40 (0.28) 0.40 (0.20)
Cluster 1 variance 3.35 (1.29) 2.12 (0.77) 3.19 (0.58) 1.25 (0.34)
Cluster 2 mean 1.66 (0.92) -0.24 (0.20) 2.10 (0.30) 1.32 (0.51)
-0.67 (0.58) 0.35 (0.23) -0.77 (0.30) -0.98 (0.47)
Cluster 2 variance 1.29 (1.58) 0.27 (0.68) 1.16 (0.40) 1.63 (0.69)
AICM -3644.24 -3346.87 -3682.71 -3325.95
does not hold generally (see Gormley & Murphy, 2010b, Section 5.3).
The results of this analysis oﬀer a cautionary message in automatically selecting the
type of mixture of experts latent position cluster model for analyzing the lawyer advice
network. The role of the latent space in the model is very diﬀerent depending on how the
covariates enter the model. So, if the latent space is to be interpreted as a social space that
explains network structure, then the covariates should not directly enter the link probabili-
ties. However, if the latent space is being used to ﬁnd interesting or anomalous structure in
the network that cannot be explained by the covariates, then one should consider allowing
the covariates enter the cluster membership probabilities.
4.4 Software
As demonstrated in this section, the approach to ﬁtting an ME model depends on the
application setting and on the form of the ME model itself. Therefore, a single software
capable of ﬁtting any ME model is not currently available.
22
In R (R Core Team, 2018), the MEclustnet package (Gormley & Murphy, 2018) ﬁts
the mixture of experts latent position cluster model detailed in Section 4.3. The flexmix
package (Grün & Leisch, 2008b) has model ﬁtting capabilities for a range of mixture of
regression models, which include covariates (or concomitant variables), as does the mixreg
package (Turner, 2014). Additionally, mixtools (Benaglia et al., 2009) facilitates ﬁtting
of aG= 2mixture of regressions model in which the component weights are modelled as
an inverse logit function of the covariates. The cluster weighted models which are closely
related to ME models can be ﬁtted using the flexCWM package (Mazza et al., 2017). All
packages are freely available through the Comprehensive R Archive Network (CRAN) at
https://cran.r-project.org .
InMATLAB,the bayesfpackage(Frühwirth-Schnatter,2018)allowstoestimateabroad
rangeofmixturemodelsusingeitherﬁnitemixtures,mixturesofexpertsorMarkovswitching
models as a model for the hidden group indicators z.
In terms of other softwares, the FMMprocedure in SAS also facilitates ME model ﬁtting,
and stand alone softwares such as Latent GOLD (Vermunt & Magidson, 2005) and Mplus
(Muthén & Muthén, 2011) ﬁt closely related latent class models.
5 Identiﬁability of Mixtures of Experts Models
For a ﬁnite mixture distribution one has to distinguish three types of non-identiﬁability
(Frühwirth-Schnatter, 2006, Section 1.3): invariance to relabelling the components of the
mixture distribution (the so-called label switching problem), non-identiﬁability due to po-
tential overﬁtting and generic non-identiﬁability which occurs only for certain classes of
mixture distributions.
Consider a standard mixture distribution with Gcomponents with non-zero weights
η1,...,ηGgenerated by distinct parameters θ1,...,θG. Assume that for all possible realisa-
tionsyfrom this mixture distribution the identity
G/summationdisplay
g=1ηgfg(y|θg) =G⋆/summationdisplay
g=1η⋆
gfg(y|θ⋆
g)
holds where the right-hand side is a mixture distribution from the same family with G⋆
components with non-zero weights η⋆
1,...,η⋆
G⋆generated by distinct parameters θ⋆
1,...,θ⋆
G⋆.
Then generic identiﬁability implies that G⋆=Gand the two mixtures’ parameters
θ= (η1,...,ηG,θ1,...,θG)andθ⋆= (η⋆
1,...,η⋆
G,θ⋆
1,...,θ⋆
G)are identical up to relabelling
the component indices. Common ﬁnite mixture distributions such as Gaussian and Poisson
mixtures are generically identiﬁed, see Teicher (1963), Yakowitz & Spragins (1968), and
Chandra (1977) for a detailed discussion.
Discrete mixtures often suﬀer from generic non-identiﬁability for certain parameter con-
ﬁgurations, well-known examples being mixtures of binomial distributions (see Section 5.1)
and mixtures of multinomial distributions (Grün & Leisch, 2008c). Somewhat unexpect-
edly, mixtures of regression models suﬀer from generic non-identiﬁability (Hennig, 2000;
Grün & Leisch, 2008a), as will be discussed in more detail in Section 5.2. Little is known
about generic identiﬁability of mixtures of experts models and some results are presented
in Section 5.3. However, ensuring generic identiﬁability for general ME models remains a
challenging issue.
Identiﬁability problems for mixture with nonparametric components are discussed in
Chapter 14 of this volume.
23
5.1 Identiﬁability for mixtures of binomials
For binomial mixtures the component densities arise from B(N,π)-distributions, where N
is commonly assumed to be known, whereas πis heterogeneous across the components:
Y∼η1B(N,π 1) +···+ηGB(N,πG). (10)
The probability mass function (pmf) of this mixture takes on N+1diﬀerent support points:
p(y|θ) =P(Y=y|θ) =G/summationdisplay
g=1ηg/parenleftBigg
N
y/parenrightBigg
πy
g(1−πg)N−y, y = 0,1,...,N, (11)
with 2G−1independent parameters θ= (π1,...,πG,η1,...,ηG), withηG= 1−/summationtextG−1
g=1ηg.
Given data y= (y1,...,yn)from mixture (10), the only information available to estimate
θareN(among the N+ 1observed) relative frequencies hn(Y=y)(y= 0,1,...,N). As
n→∞(whileNis ﬁxed),hn(Y=y)converges to P (Y=y|θ)by the law of large numbers,
but the number of support points remains ﬁxed. Hence, the data provide only Nstatistics,
given by the relative frequencies, to estimate 2G−1parameters. Simple counting yields
the following necessary condition for identiﬁability for a binomial mixture, which has been
shown by Teicher (1961) to be also suﬃcient:
2G−1≤N⇔G≤(N+ 1)/2. (12)
Consider, for illustration, a mixture of two binomial distributions,
Y∼η×B(N,π 1) + (1−η)×B(N,π 2), (13)
with three unknown parameters θ= (η,π 1,π2)and assume that the population indeed
contains two diﬀerent groups, i.e. π1/negationslash=π2andη >0. Assuming N= 2obviously violates
condition (12). Lack of identiﬁcation can be veriﬁed directly from the pmf which is diﬀerent
from zero only for the three outcomes y∈{0,1,2}:
P(Y= 0|θ) =η(1−π1)2+ (1−η)(1−π2)2, (14)
P(Y= 1|θ) = 2ηπ1(1−π1) + 2(1−η)π2(1−π2),
P(Y= 2|θ) =ηπ2
1+ (1−η)π2
2.
Since/summationtext
yP(Y=y|θ) = 1, only two linearly independent equations remain to identify the
three parameters (η,π 1,π2). Hence parameters θ= (π1,π2,η)/negationslash=θ⋆= (π⋆
1,π⋆
2,η⋆)fulﬁlling
equations(14)existwhichimplythesamedistributionfor Y,i.e.:P (Y=y|θ) =P(Y=y|θ⋆),
∀y= 0,1,2, but are not related to each other by simple relabelling of the component indices.
Such generic non-identiﬁability severely impacts statistical estimation of the mixture
parameters θfrom observations y= (y1,...,yn), even ifGis known, and goes far be-
yond label switching. Assume, for illustration, that yis the realisation of a random sam-
ple(Y1,...,Yn)from the two-component binomial mixture (13) with N= 2and true pa-
rameterθtrue= (πtrue
1,πtrue
2,ηtrue)and consider the corresponding observed-data likelihood
p(y|θ) =/producttextn
i=1P(Yi=yi|θ).Genericnon-identiﬁabilityoftheunderlyingmixturedistribution
implies that the observed-data likelihood is the same for any pair θ/negationslash=θ⋆of distinct param-
eters satisfying (14), for any possible sample yin the sampling space Y={0,1,2}n, i.e.:
p(y|θ) =p(y|θ⋆),∀y∈Y.Since this holds for arbitrary sample size n= 1,2,..., the true pa-
rameterθtruecannot be recovered, even if n→∞, and both maximum likelihood estimation
as well Bayesian inference suﬀer from non-identiﬁability problems for such a mixture.
24
Figure 5: MCMC inference for data simulated from a mixture of two binomial distributions
withN= 2(left-hand side) and N= 5(right-hand side). Top: scatter plot of π1versusπ2
(truevaluesindicatedbyacircle).Bottom:posteriordrawsofthegroup-speciﬁcprobabilities
π1andπ2after resolving label switching in the scatter plot of π1versusπ2throughk-means
clustering.
Thisexamplemotivatesthefollowingmoreformaldeﬁnitionofgenericnon-identiﬁability.
For a given θ, any subset U(θ)of the parameter space Θof a mixture model, deﬁned as
U(θ) ={θ⋆∈Θ :p(y|θ⋆) =p(y|θ),∀y∈Y},is called a non-identiﬁability set, if it contains
at least one point θ⋆which is not related to θby simple relabelling of the component indices.
Letθtruebe the true parameter value of a mixture model with Gdistinct parameters (i.e.
θg/negationslash=θg/prime, forg/negationslash=g/prime). IfU(θtrue)is a non-identiﬁability set in the sense deﬁned above, then
θtruecannot be recovered from data, even as ngoes to inﬁnity.
Suchgenericnon-identiﬁabilityhasimportantimplicationsforpracticalmixtureanalysis.
For ﬁniten, the observed-data likelihood function p(y|θ)has a ridge close to U(θtrue)instead
ofG!isolated modes and no unique maximum, leading to inconsistent estimates of θtrue. In a
Bayesian framework, this leads to a posterior distribution that does not concentrate around
G!isolated, equivalent modes as nincreases, as for identiﬁable models (see Chapter 4, Sec-
tion 4.3). Rather, the posterior concentrates over the entire non-identiﬁability set U(θtrue)
which has a complex geometry and can be represented as the union of G!symmetric sub-
spaces, see e.g. Figure 5 for a binomial mixture with G= 2andN= 2. The prior p(θ)
provides information beyond the data and might inﬂuence how the posterior concentrates
on each of these G!subspacesU(θtrue), in particular, if the prior p(θ)is not constant over
U(θ).
While generic non-identiﬁability has important practical implications for mixture anal-
ysis, it is rarely as easily diagnosed as for mixtures of binomial distributions and can easily
go unnoticed for more complex mixture models, in particular for maximum likelihood es-
timation, whereas MCMC based Bayesian inference often provides indications of potential
identiﬁability problems, as the following example demonstrates.
25
MCMC inference for an example: a mixture of binomial distributions
For further illustration, we perform MCMC inference (based on 10,000 draws after a burn-in
of 5,000 iterations) for two data sets simulated from a mixture of two binomial distributions
with logitπ1=−1and logitπ2= 1.5using random permutation sampling as explained
in Chapter 5, Section 5.2. We assume that G= 2andNis known, whereas all other pa-
rameters in mixture (13) are unknown. Bayesian inference is based on the following priors:
πg∼U(0,1), and (η1,η2)∼D(1,1). The two data sets were generated with, respectively,
N= 2,n= 250andN= 5,n= 100, implying the same total number n×N= 500of
experiments.
MCMC inference is summarised in Figure 5, showing scatter plot of π1versusπ2for
both values of N. ForN= 5, the mixture is generically identiﬁed and the posterior draws
concentrate around two symmetric modes, centered at the true values (0.269,0.818)and
(0.818,0.269). Non-identiﬁability due to label switching is resolved by applying k-means
clustering to the posterior draws, see the lower part of Figure 5 showing identiﬁed posterior
draws of the group-speciﬁc success probabilities π1andπ2.
ForN= 2, a similar scatter plot of π1versusπ2clearly indicates severe identiﬁability
issues, showing that the posterior draws arise from two symmetric unidentiﬁability sets,
rather than concentrating around two symmetric modes centered at the true values. When
we applyk-means clustering to resolve label switching, we obtain the posterior draws of the
success probabilities π1andπ2shown in the lower part of Figure 5, also indicating problems
with identifying π1andπ2from the data for N= 2.
5.2 Identiﬁability for mixtures of regression models
Consider a mixture of Gregression models for i= 1,...,noutcomesyi, arising from G
diﬀerent groups,
yi|˜xi∼G/summationdisplay
g=1ηgφ(y|µi,g(˜xi),σ2
g) (15)
where for each g= 1,...,G, the group-speciﬁc mean µi,g(˜xi) = ˜xiβgdepends on a group-
speciﬁc regression parameter βgand on the (1×(q+1))-dimensional row vector ˜xicontaining
theqcovariatesxiand a constant. For a ﬁxed design point x= ˜xi, (15) is a standard ﬁnite
Gaussian mixture distribution and as such generically identiﬁed. Hence, if the identity
G/summationdisplay
g=1ηgφ(y|µi,g(x),σ2
g) =G/summationdisplay
g=1η⋆
gφ(y|µ⋆
i,g(x),σ2,⋆
g), (16)
holds, then the two mixtures are related to each other by relabelling, i.e. µ⋆
i,g(x) =
µi,σx(g)(x) =xβσx(g),σ2,⋆
g=σ2
σx(g), andη⋆
g=ησx(g)forg= 1,...,G, for some permuta-
tionσx∈S(G), where S(G)denotes the set of the G!permutations of{1,...,G}. Note
thatσxdepends on the covariate xand that there is no guarantee that σxis identical across
diﬀerent values of xwhich can cause intra-component label switching . One such example is
displayed on the left-hand side of Figure 6.
Nevertheless, assume for the moment that σx≡σ⋆is the same for all possible covariates
x. Then (16) implies ˜xiβ⋆
g= ˜xiβσ⋆(g)for alli= 1,...,nandXβ⋆
g=Xβσ⋆(g)where the rows
of the matrix Xare equal to ˜x1,..., ˜xn. If the usual condition in regression modelling is
satisﬁed that X/latticetopXhas full rank, then it follows immediately that the regression coeﬃcients
are determined up to relabelling: β⋆
g=βσ⋆(g).
26
Figure 6: Data simulated from a mixture of two regression lines under Design 1 (left-hand
side) and Design 2 (right-hand side). The full lines indicate the true underlying model used
to generate 100 data points (black dots). For the unidentiﬁed Design 1 , a second solution
exists which is indicated by the dashed lines.
Hence, generic identiﬁability for a mixture of regressions model can be veriﬁed through
suﬃcient conditions guaranteeing that σxis indeed identical across all values of x. Mathe-
matically, one such condition is the assumption that either the error variances σ2
1,...,σ2
Gor
the weights η1,...,ηGsatisfy a strict order constraint. However, in practice such constraints
are rarely fulﬁlled and forcing an order constraint on one coeﬃcient does not necessarily
prevent label switching for the other coeﬃcients in Bayesian posterior sampling, see e.g.
Frühwirth-Schnatter (2006, Section 2.4).
Hence, several papers focused on conditions for generic identiﬁability through the regres-
sion part of the model (Hennig, 2000; Grün & Leisch, 2008c,a). Assume that the covariates
˜xitakepdiﬀerent values in a design space {x1,...,xp}for the observed outcome yi, for
i= 1,...,n. Identiﬁability through the regression part requires enough variability in the de-
sign space and is guaranteed under so-called coverage conditions . These conditions require
that the number of clusters Gis exceeded by the minimum number of distinct q-dimensional
hyperplanes needed to cover the covariates (excluding the constant). For q= 1, for instance,
the coverage condition is satisﬁed, if the number of design points p(i.e. the number of
distinct values of the univariate covariate) is larger than the number of clusters G. These
identiﬁability conditions go far beyond the usual condition that X/latticetopXhas full rank and are
often violated for regression models with too few design points, a common example being
regression models with 0/1 dummy variables as covariates which are identiﬁable for G= 1,
but not for G> 1, as the following examples with q= 1demonstrate.
For illustration, we consider the following special case of the mixture of regressions model
(15) investigated in Grün & Leisch (2008a, Section 3.1):
yi∼0.5φ(y|µi,1(˜xi),0.1) + 0.5φ(y|µi,2(˜xi),0.1), (17)
with covariate vector ˜xi= (1di)and group-speciﬁc regression parameters β1= (2 2)/latticetopand
β2= (1−2)/latticetop. We consider two diﬀerent regression designs, Design 1 wherediis a 0/1
dummy variable capturing the eﬀect of gender (with female as baseline) and Design 2 where
dicaptures a time eﬀect over 3 periods (with t= 0serving as baseline):
Design 1 :x1=/parenleftBig
1 0/parenrightBig
, x2=/parenleftBig
1 1/parenrightBig
,
Design 2 :x1=/parenleftBig
1 0/parenrightBig
, x2=/parenleftBig
1 1/parenrightBig
, x3=/parenleftBig
1 2/parenrightBig
.
In the following, it is veriﬁed that mixture (17) is generically identiﬁed under Design 2
(which contains three design points), but generically unidentiﬁed under Design 1 (which
contains only two design points).
27
We ﬁrst consider Design 1 . According to (16), µj,1andµj,2are identiﬁed for j= 1and
j= 2up to label switching arising from two permutations σ1andσ2, where we may assume
without loss of generality that σ1is equal to the identity:
x1β1=µ1,1, x 1β2=µ1,2, (18)
x2β1=µ2,σ2(1), x 2β2=µ2,σ2(2).
Ifσ2is identical to σ1, then the original values β1andβ2are recovered through:
β1=X−1
1,2/parenleftBigg
µ1,1
µ2,1/parenrightBigg
=/parenleftBigg
2
2/parenrightBigg
, β 2=X−1
1,2/parenleftBigg
µ1,2
µ2,2/parenrightBigg
=/parenleftBigg
1
−2/parenrightBigg
, (19)
since the design matrix
X1,2=/parenleftBigg
x1
x2/parenrightBigg
=/parenleftBigg
1 0
1 1/parenrightBigg
is invertible. However, as mentioned above, σ2need not be identical to σ1, in which case
σ2(1) = 2,σ2(2) = 1, and a second solution emerges:
β⋆
1=X−1
1,2/parenleftBigg
µ1,1
µ2,2/parenrightBigg
=/parenleftBigg
2
−3/parenrightBigg
, β⋆
2=X−1
1,2/parenleftBigg
µ1,2
µ2,1/parenrightBigg
=/parenleftBigg
1
3/parenrightBigg
.
Evidently, the group-speciﬁc slopes of this second solution are diﬀerent from the original
ones and the un-identiﬁability set U(θtrue)contains two points. The two possible solutions
are depicted in the left-hand side of Figure 6 which also shows a balanced sample of n= 100
observations simulated from mixture (17) under Design 1 .
ForDesign 2 , the ﬁrst two design points are as before and a third point is added with
µ3,1andµ3,2being identiﬁed up to label switching according to a permutation σ3:
x3β1=µ3,σ3(1), x 3β2=µ3,σ3(2). (20)
As only two diﬀerent permutations exist for G= 2, at least two of the three permutations
σ1,σ2, andσ3in (18) and (20) have to be identical (assuming again without loss of gener-
ality thatσ1is equal to the identity). Assume, for example, that σ1=σ2. Then the true
parameters β1andβ2are recovered from (µj,1,µj,2),j= 1,2,as in (19) and can be used to
uniquely predict µ3,1=x3β1andµ3,2=x3β2in both groups. Comparing these predictions
with (20), it is clear that σ3(1) = 1andσ3(2) = 2, henceσ1=σ2=σ3. A similar proof can
be performed for any pair of identical permutations σj=σl,j/negationslash=l, as long as the matrix
X/latticetop
j,l= (x/latticetop
jx/latticetop
l)is invertible and generic identiﬁability of Design 2 follows.
The only possible solution under Design 2 is depicted in the right-hand side of Figure 6
which also shows a balanced sample of n= 100observations simulated from mixture (17)
under this design.
MCMC inference for an example: a mixture of regressions model
For further illustration, we perform MCMC inference (based on 10,000 draws after a burn-in
of 5,000 iterations) for both data sets shown in Figure 6 using random permutation sam-
pling as explained in Chapter 5, Section 5.2. We assume that G= 2is known, whereas
all other parameters in mixture (15) are unknown. Bayesian inference is based on the pri-
ors(η1,η2)∼D (4,4)andβg∼N (0,100×I),σ2
g∼IG (2.5,1.25s2
y)forg= 1,2, where
28
Figure 7: MCMC inference for data simulated from a mixture of two regression models
under the generically un-identiﬁed Design 1 (left-hand side) and under the generically iden-
tiﬁedDesign 2 (right-hand side). Top: scatter plot of the group-speciﬁc slopes β1,2versus
β2,2. Bottom: posterior draws of the group-speciﬁc slopes β1,2andβ2,2after resolving label
switching through k-means clustering in the posterior draws.
s2
yis the data variance of (y1,...,yn). The upper part of Figure 7 shows scatter plots of
the group-speciﬁc slopes β1,2versusβ2,2for both designs which are symmetric due to label
switching.
As expected from Chapter 4, Section 4.3, the posterior draws shown in the upper right-
hand part for the generically identiﬁed Design 2 concentrate around two symmetric modes
corresponding to the true values (2,−2)and(−2,2). Label switching is easily resolved by
applyingk-means clustering to the posterior draws, see the lower right-hand part of Figure 7
showing identiﬁed posterior draws of the group-speciﬁc slopes β1,2andβ2,2for this design.
ForDesign 1 , a similar scatter plot of β1,2versusβ2,2in the upper left-hand part clearly
indicates severe identiﬁability issues. The posterior draws concentrate around four rather
than two modes, with two of them being the symmetric modes corresponding to the true
values (2,−2)and(−2,2). The other two symmetric modes correspond to the second solu-
tion(−3,3)and(3,−3), resulting from generic non-identiﬁability. When we apply k-means
clustering to these posterior draws to resolve label switching, we obtain the posterior draws
of the group-speciﬁc slopes β1,2andβ2,2in the lower left-hand part of Figure 7, showing
intra-component label switching and indicating identiﬁability problems for this design. Since
βg,2switches sign between the two solutions in both groups, it is not possible to recover that
“gender” has a strong positive eﬀect on the outcome in one group and a strong negative
eﬀect in the other group.
5.3 Identiﬁability for mixtures of experts models
Hennig (2000) considers mixtures of regression models where the component sizes can arbi-
trarily depend on covariates and establishes identiﬁability results in the case that the joint
observations of covariates and dependent variable are assumed to be iid and gives suﬃcient
29
identiﬁability conditions for this model. For such a model, the covariates are not assumed
ﬁxed or to occur for a ﬁxed design, but random with a speciﬁc distribution. As opposed
to this, the ME model is deﬁned conditional on the covariates without speciﬁc assumptions
concerning their distribution. We will discuss identiﬁcation for this case.
Consider, as a ﬁrst example, a simple mixture of experts model of Gunivariate Gaussian
distributions for i= 1,...,noutcomesyiarising from Gdiﬀerent groups,
yi|˜xi∼G/summationdisplay
g=1ηg(˜xi)φ(y|µg,σ2
g) (21)
where the group weights ηgdepend on a covariate ˜xiwith group-speciﬁc regression param-
eters, i.e.:
log/bracketleftBiggηg(˜xi)
ηg0(˜xi)/bracketrightBigg
= ˜xiγg, g = 1,...,G, (22)
with baseline g0, whereγg0= 0. Assume that the component densities diﬀer, i.e. θg/negationslash=θ/prime
g,
forg/negationslash=g/prime, whereθg= (µg,σ2
g).
For each ﬁxed design point x= ˜xi, (21) is a standard ﬁnite Gaussian mixture and
therefore generically identiﬁed. Therefore, if the identity
G/summationdisplay
g=1ηg(x)φ(y|µg,σ2
g) =G/summationdisplay
g=1η⋆
g(x)φ(y|µ⋆
g,σ2,⋆
g), (23)
holds, then the two mixtures are related to each other by relabelling, i.e. µ⋆
g=µσx(g),σ2,⋆
g=
σ2
σx(g), andη⋆
g(x) =ησx(g)(x)forg= 1,...,G, for some permutation σx∈S(G). As opposed
to mixtures of regression models, one can show that σx≡σ⋆for all covariate values x.
Assume that σxi/negationslash=σxjfor two covariates ˜xi/negationslash= ˜xjand assume, without loss of generality,
thatσxiis equal to the identity. Consider ﬁrst the case of G= 2. Then (23) implies for
x= ˜xi:
θ⋆
1=/parenleftBigg
µ⋆
1
σ2,⋆
1/parenrightBigg
=θ1, θ⋆
2=/parenleftBigg
µ⋆
2
σ2,⋆
2/parenrightBigg
=θ2,
whereas for x= ˜xj:
θ⋆
1=/parenleftBigg
µ⋆
2
σ2,⋆
2/parenrightBigg
=θ2, θ⋆
2=/parenleftBigg
µ⋆
1
σ2,⋆
1/parenrightBigg
=θ1,
contradicting the assumptions that θ1/negationslash=θ2. A similar proof is possible for G> 2, where the
assumption σxi/negationslash=σxj(assuming again that σ1is equal to the identity) implies for x= ˜xi
thatθ⋆
g=θgfor all components g= 1,...,G, whereas for x= ˜xjat least one component
giexists with θ⋆
gi=θgj, wheregj=σxj(gi)/negationslash=gi. Hence,θgi=θgj, which contradicts the
assumptions that θgi/negationslash=θgj. This implies that σx≡σ⋆for all covariate values x.
Therefore, the weight distribution η1(x),...,ηG(x)is identiﬁed up to relabelling the com-
ponents and identiﬁcation depends on whether γgcan be recovered from the correspond-
ing MNL model (22) given the design matrix X, constructed row-wise from the covariates
˜xi,i= 1,...,n. Standard conditions for identiﬁcation in a MNL model apply, e.g. that
(X/latticetopX)−1exists (McCullagh & Nelder, 1999). It is well-known that identiﬁcation in a logit
and more generally in a MNL model fails under complete separation, see e.g. Heinze (2006).
Hence, a situation where a mixture of experts model is not generically identiﬁed occurs, if
30
certain clusters do not share covariate values with other clusters, see Example 4.2 in Hennig
(2000) for illustration. A rather strong condition ensuring generic identiﬁability for this type
of models is an extended coverage condition (Hennig, 2000) requiring that the number of
clustersGis exceeded by the minimum number of distinct q-dimensional hyperplanes needed
to cover the covariate values (excluding the constant) for eachcluster.
Similar arguments as above apply in general for simple mixtures of experts models of G
probability distributions,
yi∼G/summationdisplay
g=1ηg(˜xi)p(yi|θg). (24)
Provided that the parameters in the MNL model (22) are identiﬁed, it can be shown that
a mixture of experts model is generically identiﬁed, if the corresponding standard ﬁnite
mixture distribution is generically identiﬁed. In this case, any other mixture representation
(24) with parameters θ⋆
gandη⋆
g(˜xi)is identiﬁed up to (the same) label switching according to
a permutation σfor all possible values ˜xi:θ⋆
g=θσ(g)andη⋆
g(˜xi) =ησ(g)(˜xi)forg= 1,...,G.
It follows that mixtures of experts of multivariate Gaussian distributions (as considered
in Section 2.2) and Poisson distributions, among many others, are generically identiﬁed,
provided that parameters in the MNL model (22) are identiﬁed. Since a standard ﬁnite
mixture model is that special case of a mixture of experts model where ˜xi≡1is equal to
the intercept, special care must be exercised when the underlying standard ﬁnite mixture
distribution is generically unidentiﬁed, as might be the case when modelling discrete data. It
is interesting to note that including ˜xiinto the weight function ηg(˜xi)in mixtures of experts
models is possible for models where including ˜xiin the component density p(yi|˜xi,θg)yields
a generically non-identiﬁed model, an example being the regressor ˜xi= (1di), wherediis a
0/1 dummy variable, see Section 5.2.
The situation gets rather complex, when covariates ˜xi(or subsets of these) are included
asregressorsbothintheoutcomedistribution p(yi|˜xi,θg)aswellasintheweightdistribution
ηg(˜xi). The presence of a covariate ˜xiinηg(˜xi)could introduce high discriminative power
among the groups and might lead to identiﬁcation of mixture of regression models which are
not identiﬁed, if ηgis assumed to be independent of the covariates. To our knowledge, generic
identiﬁcation for general mixtures of experts models has not been studied systematically and
would be an interesting venue for future research.
As it is, the only way to investigate, if the chosen mixture model suﬀers from identiﬁabil-
ityproblems is toanalyze theresults obtainedfrom ﬁttingthese models tothe datacarefully.
As the examples in Section 5.1 and 5.2 have shown, weird behaviour of the MCMC draws
in a Bayesian framework are often a sign of identiﬁability problems. On the other hand,
marginal posterior concentration around pronounced modes, veriﬁed for instance through
appropriate scatter plots of MCMC draws for the parameters of interest, indicates that
identiﬁcation might not be an issue for that speciﬁc application.
6 Concluding Remarks
This chapter has outlined the deﬁnition, estimation and application of ME models in a num-
ber of settings clearly demonstrating their utility as an analytical tool. Their demonstrated
use to cluster observations, and to appropriately capture heterogeneity in cross sectional
data, provides only a glimpse of their potential ﬂexibility and utility in a wide range of
settings. The ability of ME models to jointly model response and concomitant variables pro-
vides deeper and more principled insight into the relations between such data in a mixture
model based analysis.
31
On a cautionary note however, when an ME model is employed as an analytic tool,
care must be exercised in how and where covariates enter the ME model framework. The
interpretation of the analysis fundamentally depends on which of the suite of ME models is
invoked. Further, as outlined herein, the identiﬁability of an ME model must be carefully
considered;establishingidentiﬁabilityforMEmodelsisanoutstanding,challengingproblem.
References
Akaike, Hirotogu. 1973. Information theory and an extension of the maximum likelihood
principle. Pages 267–281 of: Petrov, B. N., & Csáki, F. (eds), 2nd International Sympo-
sium Symp. Information Theory . Budapest: Akadémiai Kiadó.
Benaglia, T., Chauveau, D., Hunter, D.R., & Young, D. 2009. mixtools: An R Package for
Analyzing Finite Mixture Models. Journal of Statistical Software ,32(6), 1–29.
Benter, W. 1994. Computer-based Horse Race Handicapping and Wagering Systems: A
Report.Pages 183–198 of: Ziemba, William T., Lo, Victor S., & Haush, Donald B. (eds),
Eﬃciency of Racetrack Betting Markets . San Diego and London: Academic Press.
Bishop, C. M., & Svenskn, M. 2003. Bayesian Hierarchical Mixtures of Experts. Pages 57–
64 of: Proceedings of the Nineteenth Conference on Uncertainty in Artiﬁcial Intelligence .
UAI’03. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.
Bishop, C.M. 2006. Pattern Recognition and Machine Learning . New York: Springer.
Chamroukhi, F. 2015. Non-Normal Mixtures of Experts. ArXiv preprints 1506.06707 , June.
Chandra, Satish. 1977. On the Mixtures of Probability Distributions. Scandinavian Journal
of Statistics ,4, 105–112.
Chib, S., & Greenberg, E. 1995. Understanding the Metropolis-Hastings Algorithm. The
American Statistician ,49, 327–335.
Coakley, J., & Gallagher, M. 2004. Politics in the Republic of Ireland . 4th edn. London:
Routledge in association with PSAI Press.
Dayton, C. M., & Macready, G. B. 1988. Concomitant-variable latent-class models. Journal
of the American Statistical Association ,83(401), 173–178.
Dempster, A. P., Laird, N. M., & Rubin, D. B. 1977. Maximum likelihood from incomplete
data via the EM algorithm (with discussion). Journal of the Royal Statistical Society
Series B,39, 1–38.
DeSarbo, W.S., & Cron, W.L. 1988. A maximum likelihood methodology for clusterwise
linear regression. Journal of Classiﬁcation ,5, 248–282.
Diebolt, J., & Robert, Christian P. 1994. Estimation of Finite Mixture Distributions by
Bayesian Sampling. Journal of the Royal Statistical Society Series B ,56, 363–375.
Frühwirth-Schnatter, S. 2004. Estimating marginal likelihoods for mixture and Markov
switching models using bridge sampling techniques. The Econometrics Journal ,7(1),
143–167.
32
Frühwirth-Schnatter, S. 2011a. Dealing with label switching under model uncertainty. Chap.
10, pages 213–239 of: Mengersen, K., Robert, C. P., & Titterington, D. (eds), Mixture
Estimation and Applications . Chichester: Wiley.
Frühwirth-Schnatter, S. 2011b. Panel Data Analysis - A Survey on Model-Based Clustering
of Time Series. Advances in Data Analysis and Classiﬁcation ,5, 251–280.
Frühwirth-Schnatter, S., & Frühwirth, R. 2010. Data augmentation and MCMC for bi-
nary and multinomial logit models. Pages 111–132 of: Kneib, Thomas, & Tutz, Gerhard
(eds),Statistical Modelling and Regression Structures – Festschrift in Honour of Ludwig
Fahrmeir . Heidelberg: Physica-Verlag.
Frühwirth-Schnatter, S., & Kaufmann, Sylvia. 2008. Model-based clustering of multiple time
series.Journal of Business &Economic Statistics ,26, 78–89.
Frühwirth-Schnatter, S., & Wagner, Helga. 2008. Marginal Likelihoods for Non-Gaussian
Models Using Auxiliary Mixture Sampling. Computational Statistics and Data Analysis ,
52, 4608–4624.
Frühwirth-Schnatter, S., Pamminger, C., Weber, A., & Winter-Ebmer, R. 2012. Labor
marketentryandearningsdynamics:Bayesianinferenceusingmixtures-of-expertsMarkov
chain clustering. Journal of Applied Econometrics ,27(7), 1116–1137.
Frühwirth-Schnatter, Sylvia. 2006. Finite Mixture and Markov Switching Models . New York:
Springer-Verlag.
Frühwirth-Schnatter, Sylvia. 2018. Applied Bayesian Mixture Modelling. Implementations
in MATLAB using the package bayesf Version 4.0 .
Geman, S., & Geman, D. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence ,
6, 721–741.
Gershenfeld, Neil. 1997. Nonlinear Inference and Cluster-Weighted Modeling. Annals of the
New York Academy of Sciences ,808(1), 18–24.
Geweke, J., & Keane, M. 2007. Smoothly mixing regressions. Journal of Econometrics ,
136(1), 252–290.
Gormley, I. C., & Murphy, T. B. 2006. Analysis of Irish third-level college applications data.
Journal of the Royal Statistical Society Series A ,169(2), 361–379.
Gormley, I. C., & Murphy, T. B. 2008a. Exploring voting blocs within the Irish electorate:
A mixture modeling approach. Journal of the American Statistical Association ,103(483),
1014–1027.
Gormley, I. C., & Murphy, T. B. 2008b. A mixture of experts model for rank data with
applications in election studies. The Annals of Applied Statistics ,2(4), 1452–1477.
Gormley, I. C., & Murphy, T. B. 2010a. Clustering ranked preference data using sociode-
mographic covariates. Pages 543–569 of: Hess, S., & Daly, A. (eds), Choice Modelling:
The State-of-the-Art and the State-of-Practice . United Kingdom: Emerald.
33
Gormley, I. C., & Murphy, T. B. 2010b. A Mixture of Experts Latent Position Cluster
Model for Social Network Data. Statistical Methodology ,7(3), 385–405.
Gormley, I. C., & Murphy, T. B. 2018. MEclustnet: ﬁtting the mixture of experts latent
position cluster model. R package version 1.0.
Grün, B., & Leisch, F. 2008a. Finite Mixtures of Generalized Linear Regression Models.
Pages 205–230 of: Shalabh, & Heumann, Christian (eds), Recent Advances in Linear
Models and Related Areas . Springer.
Grün,B.,&Leisch,F.2008b. FlexMixVersion2:Finitemixtureswithconcomitantvariables
and varying and constant parameters. Journal of Statistical Software ,28, 1–35.
Grün, B., & Leisch, F. 2008c. Identiﬁability of Finite Mixtures of Multinomial Logit Models
with Varying and Fixed Eﬀects. Journal of Classiﬁcation ,25, 225–247.
Handcock, M., Raftery, A.E., & Tantrum, J. M. 2007. Model-based clustering for social
networks. Journal of the Royal Statistical Society Series A ,170(2), 301 – 354.
Heinze, Georg. 2006. A comparative investigation of methods for logistic regression with
separated or nearly separated data. Statistics in Medicine ,25, 4216–4226.
Hennig, C. 2000. Identiﬁability of models for clusterwise linear regression. Journal of
Classiﬁcation ,17, 273–296.
Hoﬀ, P. D., Raftery, A. E., & Handcock, M. S. 2002. Latent Space Approaches to Social
Network Analysis. Journal of the American Statistical Association ,97, 1090–1098.
Hoﬀ, P.D. 2009. A First Course in Bayesian Statistical Methods . Springer-Verlag, New
York.
Huerta,G.,Jiang,W.,&Tanner,M.A.2003. Timeseriesmodelingviahierarchicalmixtures.
Statistica Sinica ,13(4), 1097–1118.
Hunter, D. R., & Lange, K. 2004. A tutorial on MM algorithms. The American Statistician ,
58(1), 30–37.
Hunter, David R, & Young, Derek S. 2012. Semiparametric mixtures of regressions. Journal
of Nonparametric Statistics ,24(1), 19–38.
Hurn, M., Justel, A., & Robert, C.P. 2003. Estimating mixtures of regressions. Journal of
Compututional and Graphical Statistics ,12, 1–25.
Ingrassia, Salvatore, Punzo, Antonio, Vittadini, Giorgio, & Minotti, Simona C. 2015. The
generalized linear mixed cluster-weighted model. Journal of Classiﬁcation ,32(1), 85–113.
Jacobs, R.A., Jordan, M.I., Nowlan, S.J., & Hinton, G.E. 1991. Adaptive mixtures of local
experts. Neural Computation ,3, 79–87.
Jordan, M.I., & Jacobs, R.A. 1994. Hierarchical mixtures of experts and the EM algorithm.
Neural Computation ,6, 181–214.
Kass, R.E., & Raftery, A.E. 1995. Bayes factors. Journal of the American Statistical Asso-
ciation,90, 773–795.
34
Lang, J. B., McDonald, J. W., & Smith, P. W. F. 1999. Association-marginal modelling
of multivariate categorical responses: A maximim likelihood approach. Journal of the
American Statistical Association ,94, 1161–71.
Lazega, E. 2001. The Collegial Phenomenon: The Social Mechanisms of Cooperation Among
Peers in a Corporate Law Partnership . Oxford University Press.
Li, F., Villani, M., & Kohn, R. 2011. Modeling conditional densities using ﬁnite smooth
mixtures. Chap. 6, pages 123–144 of: Mengersen, K., Robert, C., & Titterington, M.
(eds),Mixtures: Estimation and Applications . Wiley.
Marsh, M. 1999. The Making of the Eighth President. Pages 215–242 of: Marsh, Michael,
& Mitchell, Paul (eds), How Ireland Voted 1997 . Boulder, CO: Westview and PSAI Press.
Masoudnia, S., & Ebrahimpour, R. 2014. Mixture of experts: a literature survey. Artiﬁcial
Intelligence Review ,42(2), 275–293.
Mazza, A., Punzo, A., & Ingrassia, S. 2017. ﬂexCWM: Flexible Cluster-Weighted Modeling.
R package version 1.7.
McCullagh, P., & Nelder, John A. 1999. Generalized Linear Models . London: Chapman &
Hall.
McLachlan, G, & Peel, D. 2000. Finite Mixture Models . New York: John Wiley.
Meng, X.-L., & Rubin, D. B. 1993. Maximum likelihood estimation via the ECM algorithm:
A general framework. Biometrika ,80(2), 267–278.
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H., & Teller, E. 1953. Equa-
tions of state calculations by fast computing machines. The Journal of Chemical Physics ,
21, 1087–1092.
Muthén, L. K., & Muthén, B. O. 2011. Mplus User’s Guide . 6 edn. Los Angeles, CA: Muthén
and Muthén.
Pamminger, Christoph, & Frühwirth-Schnatter, Sylvia. 2010. Model-based Clustering of
Categorical Time Series. Bayesian Analysis ,5, 345–368.
Peng, F., Jacobs, R. A., & Tanner, M. A. 1996. Bayesian inference in mixtures-of-experts
and hierarchical mixtures-of-experts models with an application to speech recognition.
Journal of the American Statistical Association ,91(435), 953–960.
Plackett, R. L. 1975. The analysis of permutations. Applied Statistics ,24(2), 193–202.
Quandt, R.E. 1972. A new approach to estimating switching regressions. Journal of the
American Statistical Association ,67, 306–310.
R Core Team. 2018. R: A Language and Environment for Statistical Computing . R Foun-
dation for Statistical Computing, Vienna, Austria.
Raftery, A.E., Newton, M.A., Satagopan, J.M., & Krivitsky, P. 2007. Estimating the in-
tegrated likelihood via posterior simulation using the harmonic mean identity (with dis-
cussion). Pages 371–416 of: Bernardo, J.M., Bayarri, M.J., Berger, J.O., Dawid, A.P.,
Heckerman, D., Smith, A.F.M., , & West, M. (eds), Bayesian Statistics 8 . Oxford Uni-
versity Press.
35
Rasmussen, C. E., & Ghahramani, Z. 2002. The inﬁnite mixtures of Gaussian process
experts. Pages 554–560 of: Advances in Neural Information Processing Systems , vol. 12.
MIT Press.
Schwarz, G. 1978. Estimating the dimension of a model. Annals of Statistics ,6, 461–464.
Scott, Steven L. 2011. Data augmentation, frequentist estimation, and the Bayesian analysis
of multinomial logit models. Statistical Papers ,52, 87–109.
Sinnott, R. 1995. Irish voters decide: Voting behaviour in elections and referendums since
1918. Manchester: Manchester University Press.
Sinnott, R. 1999. The Electoral System. Pages 99–126 of: Coakley, John, & Gallagher,
Michael (eds), Politics in the Republic of Ireland , 3rd edn. London: Routledge & PSAI
Press.
Stephens, M. 2000. Bayesian analysis of mixture models with an unknown number of
components—an alternative to reversible jump methods. Annals of Statistics ,28, 40–
74.
Subedi, Sanjeena, Punzo, Antonio, Ingrassia, Salvatore, & McNicholas, Paul D. 2013. Clus-
tering and classiﬁcation via cluster-weighted factor analyzers. Advances in Data Analysis
and Classiﬁcation ,7(1), 5–40.
Tang, X., & Qu, A. 2015. Mixture Modeling for Longitudinal Data. Journal of Computa-
tional and Graphical Statistics ,25, 1117–1137.
Tanner, M. A. 1996. Tools for Statistical Inference: Observed Data and Data Augmentation
Methods. 3 edn. New York: Springer-Verlag.
Teicher, Henry. 1961. Identiﬁability of mixtures. The Annals of Mathematical Statistics ,32,
244–248.
Teicher, Henry. 1963. Identiﬁability of ﬁnite mixtures. The Annals of Mathematical Statis-
tics,34, 1265–1269.
Turner, Rolf. 2014. mixreg: functions to ﬁt mixtures of regressions. R package version 0.0-5.
Vermunt, Jeroen K, & Magidson, Jay. 2005. Latent GOLD 4.0 User’s Guide . Statistical
Innovations Inc.
Villani, Mattias, Kohn, Robert, & Giordani, Paolo. 2009. Regression density Estimation
using smooth adaptive Gaussian mixtures. Journal of Econometrics ,153, 155–173.
Wang, P., Puterman, M.L., Cockburn, I., & Le, N. 1996. Mixed Poisson regression models
with covariate dependent rates. Biometrics ,52, 381–400.
Waterhouse, S., MacKay, D., & Robinson, T. 1996. Bayesian methods for mixtures of
experts. Pages 351–357 of: Advances in Neural Information Processing Systems . Morgan
Kaufmann Publishers.
White, A., & Murphy, T. B. 2016. Mixed-Membership of Experts Stochastic Blockmodel.
Network Science ,4(Apr.), 48–80.
36
Yakowitz, S. J., & Spragins, J. D. 1968. On the Identiﬁability of ﬁnite mixtures. The Annals
of Mathematical Statistics ,39, 209–214.
Young, D. S., & Hunter, D. R. 2010. Mixtures of regressions with predictor-dependent
mixing proportions. Computational Statistics & Data Analysis ,54(10), 2253–2266.
37