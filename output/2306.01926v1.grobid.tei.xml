<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RITA: Group Attention is All You Need for Timeseries Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-02">2 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,108.18,113.95,68.16,5.45"><forename type="first">Jiaming</forename><surname>Liang</surname></persName>
							<email>liangjm@seas.upenn.edu</email>
						</author>
						<author>
							<persName coords="1,286.21,113.95,37.10,5.45;1,323.30,111.06,1.00,4.00"><forename type="first">Lei</forename><surname>Cao</surname></persName>
							<email>lcao@csail.mit.edu</email>
						</author>
						<author>
							<persName coords="1,431.67,113.95,77.96,5.45"><forename type="first">Samuel</forename><surname>Madden</surname></persName>
							<email>madden@csail.mit.edu</email>
						</author>
						<author>
							<persName coords="1,192.49,172.34,62.87,5.45"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
							<email>zives@cis.upenn.edu</email>
						</author>
						<author>
							<persName coords="1,359.36,172.34,57.42,5.45"><forename type="first">Guoliang</forename><surname>Li</surname></persName>
							<email>liguoliang@tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RITA: Group Attention is All You Need for Timeseries Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-02">2 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">A84DCB26BD167B8417A6F60EFC9B59BB</idno>
					<idno type="arXiv">arXiv:2306.01926v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-22T20:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Timeseries analytics is of great importance in many real-world applications. Recently, the Transformer model, popular in natural language processing, has been leveraged to learn high quality feature embeddings from timeseries, core to the performance of various timeseries analytics tasks. However, the quadratic time and space complexities limit Transformers' scalability, especially for long timeseries. To address these issues, we develop a timeseries analytics tool, RITA, which uses a novel attention mechanism, named group attention, to address this scalability issue. Group attention dynamically clusters the objects based on their similarity into a small number of groups and approximately computes the attention at the coarse group granularity. It thus significantly reduces the time and space complexity, yet provides a theoretical guarantee on the quality of the computed attention. The dynamic scheduler of RITA continuously adapts the number of groups and the batch size in the training process, ensuring group attention always uses the fewest groups needed to meet the approximation quality requirement. Extensive experiments on various timeseries datasets and analytics tasks demonstrate that RITA outperforms the state-of-the-art in accuracy and is significantly faster -with speedups of up to 63X.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Motivation. Many data driven applications involve processing massive timeseries data, including IoT <ref type="bibr" coords="1,195.30,499.24,13.36,4.09" target="#b10">[11]</ref>, medical AI <ref type="bibr" coords="1,256.01,499.24,13.36,4.09" target="#b13">[14]</ref>, stock market <ref type="bibr" coords="1,81.34,510.19,13.23,4.09" target="#b26">[27]</ref>, and so on. As such, there is a great need for timeseries analytics, such as forecasting <ref type="bibr" coords="1,159.58,521.15,9.27,4.09" target="#b7">[8]</ref>, classification <ref type="bibr" coords="1,222.30,521.15,13.22,4.09" target="#b19">[20]</ref>, clustering <ref type="bibr" coords="1,278.50,521.15,13.22,4.09" target="#b30">[31]</ref>, similarity search <ref type="bibr" coords="1,114.37,532.11,13.22,4.09" target="#b38">[39]</ref>, and anomaly detection <ref type="bibr" coords="1,214.58,532.11,13.22,4.09" target="#b49">[50]</ref>, with applications ranging from automatically diagnosing diseases <ref type="bibr" coords="1,235.05,543.07,9.52,4.09" target="#b4">[5]</ref>, recognizing human activities <ref type="bibr" coords="1,116.76,554.03,13.36,4.09" target="#b28">[29]</ref>, to stopping financial fraud <ref type="bibr" coords="1,235.03,554.03,13.36,4.09" target="#b58">[59]</ref>.</p><p>Effective feature extraction <ref type="bibr" coords="1,169.31,564.99,14.85,4.09" target="#b39">[40]</ref> lies at the core of almost all these timeseries analytics tasks. Recently researchers <ref type="bibr" coords="1,258.52,575.95,14.85,4.09" target="#b60">[61]</ref> have started leveraging the self-supervised pre-training methodology of Transformers <ref type="bibr" coords="1,104.27,597.87,9.23,4.09" target="#b3">[4,</ref><ref type="bibr" coords="1,115.71,597.87,10.27,4.09" target="#b15">16,</ref><ref type="bibr" coords="1,128.20,597.87,10.05,4.09" target="#b51">52]</ref>, which have proven remarkably successful in natural language processing (NLP), to automatically learn high quality feature embeddings from timeseries. In NLP, self-supervised pre-training exploits the sequential patterns (correlations) among the words in sentences to produce contextualized feature embeddings. Timeseries bear similarity to natural language, because in timeseries data the sequential order among the values (stock price, volume, etc.) over time matters. That is, each value is highly correlated with other values observed before or after it. Therefore, * Corresponding Author pre-training a Transformer model which takes the correlations among different observations into account is a natural idea to learn feature embeddings from timeseries. Indeed, the experiments in <ref type="bibr" coords="1,543.61,251.46,14.60,4.09" target="#b60">[61]</ref> confirm that Transformer-based methods outperform traditional timeseries analytics techniques.</p><p>However, existing work <ref type="bibr" coords="1,417.09,284.34,14.71,4.09" target="#b60">[61]</ref> that directly applies Transformers to learn features from timeseries data have been shown not to be scalable to long timeseries <ref type="bibr" coords="1,418.21,306.26,13.49,4.09" target="#b29">[30]</ref>. The idea of self-attention <ref type="bibr" coords="1,534.80,306.26,14.85,4.09" target="#b51">[52]</ref> is central to pre-training methods in NLP: It computes pairwise correlations among different semantic units in a sequence (in NLP, a sentence); as such, it has quadratic time and space complexity in the length of the input sequence. Such an approach places limits on the model's scalability, especially when handling large sequences, which are common in real-world timeseries applications such as IoT, medical AI, and finance <ref type="bibr" coords="1,418.67,382.97,9.23,4.09" target="#b5">[6,</ref><ref type="bibr" coords="1,429.83,382.97,10.27,4.09" target="#b33">34,</ref><ref type="bibr" coords="1,442.02,382.97,10.05,4.09" target="#b61">62]</ref>. Predictions about timeseries may need to look at months or years of historical data to make accurate predictions, spanning hundreds of thousands of samples. As an example, in collaboration with a research hospital we have been developing a seizure classifier that automatically detects seizures based on EEG signals (timeseries) collected during the clinical observation of patients. As seizures last only a few seconds, we chunk long EEG data into many 2 second segments and detect seizures at a segment level. However, the classification of a particular segment depends on up to 12 hours of prior signal to determine if one 2 second segment indicates seizure or not, because seizure diagnosis needs to consider long-term trends in the EEG data <ref type="bibr" coords="1,500.49,503.52,9.27,4.09" target="#b5">[6]</ref>. The number of segments in 12 hours is more than 21k. This is far larger than the number of semantic units the typical NLP tasks expect. For example, BERT <ref type="bibr" coords="1,378.01,536.40,14.85,4.09" target="#b15">[16]</ref> limits the number of units to 512 and even massive models like GPT-3 <ref type="bibr" coords="1,418.41,547.36,10.55,4.09" target="#b3">[4]</ref> limit the number of units to 2048.</p><p>Although in NLP some lower-complexity methods have been proposed to approximately compute self-attention <ref type="bibr" coords="1,498.07,569.27,13.40,4.09" target="#b9">[10,</ref><ref type="bibr" coords="1,513.52,569.27,10.27,4.09" target="#b25">26,</ref><ref type="bibr" coords="1,525.84,569.27,10.05,4.09" target="#b53">54]</ref>, their performance degrades dramatically when used on timeseries, due to the gap between natural language and timeseries, as we will show in our experiments. Proposed Approach. To tackle the aforementioned problem, we develop RITA, a Transformer-based timeseries analytics tool, which uses a novel attention mechanism, called group attention, to scale to long timeseries.</p><p>Leveraging the periodicity of timeseries, RITA chunks the input timeseries into segments and dynamically clusters the segments into a small number (denoted as 𝑁 ) of groups. Segments in the same group possess similar feature embeddings during the current training iteration, thus enabling them to approximately share the computation of attention. As the timeseries increases in length, more sharing opportunities become available. RITA then computes the self-attention at a group level and produces a compressed group attention matrix. In this way, group attention eliminates both computation and memory bottlenecks in Transformer-style models and thus more scalable to long timeseries.</p><p>However, making this idea effective and efficient in Transformer architectures is challenging for several reasons:</p><p>• Efficiently Producing High Quality Feature Embeddings. Although RITA computes the attention matrix at a group level, to preserve the quality of the feature embeddings, it still has to produce different embeddings for different segments. This is because even if some segments share the attention score temporally, it does not mean they should have the same feature embedding. However, using the group attention matrix, the existing self-attention mechanism will only produce a single feature vector for each group. A naive solution would be to restore the original attention matrix from the group attention matrix. However, in this case we again get an attention matrix with quadratic space complexity. Because GPUs have limited memory, GPU memory will remain a bottleneck in group attention.</p><p>• The Number of Groups N. In RITA, the number of groups 𝑁 is a crucial factor that balances the speed up and the quality of attention approximation. A small 𝑁 will lead to a large speedup, but the approximation errors can also be significant. On the other hand, although a large 𝑁 tends to produce high-quality approximations, it inevitably slows down the training process. Therefore, an appropriate 𝑁 is essential to the performance of group attention. However, 𝑁 depends on the distributional properties of the dataset. Furthermore, like the classical transformer models, RITA stacks multiple attention layers to produce better embeddings. Ideally, different layers should also use different values of 𝑁 . In addition, during the model training phrase, group attention should use different values of 𝑁 at different iterations to adapt to the varying feature embeddings. This makes manually setting appropriate 𝑁 almost impossible.</p><p>• Batch Size. Moreover, as we want to dynamically adjust 𝑁 during training, a fixed batch size is sub-optimal: as 𝑁 decreases, the memory usage of a single sample decreases. This allows a larger batch size which is beneficial, because: (1) it makes full use of GPU memory; (2) high-parallelism across the samples in a big batch brings better performance. Our experimental study shows that doubling the batch size reduces the training time by 30%, while still preserving the quality of the model. Thus, RITA should dynamically adjust batch size as 𝑁 changes.</p><p>To address the above problems, we first propose an embedding aggregation strategy and a customized group softmax function to replace the classical softmax function <ref type="bibr" coords="2,195.57,604.65,13.49,4.09" target="#b51">[52]</ref>. Together they ensure RITA is able to directly use the compressed attention matrix to produce different feature embeddings for different segments. We theoretically show the embeddings RITA produces in this way are identical to those produced by first re-storing the original large attention matrix. Thus RITA is able to produce high quality embeddings without introducing extra overhead. Further, we design a GPU friendly algorithm to group the segments in parallel, effectively minimizing the grouping cost.   Second, we design an adaptive scheduler which dynamically decides an appropriate 𝑁 for each group attention layer during the training process. It starts with a large 𝑁 and iteratively merges groups that are similar to each other. Guided by an error bound on the approximated self-attention that users can tolerate, it automatically determines if two groups are mergeable, performing merging efficiently in a GPU-friendly way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RITA Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale &amp; Input</head><p>Moreover, we propose a learning-based method to model the correlation between the number of groups 𝑁 and the batch size 𝐵. This model is used to predict 𝐵 for a given 𝑁 when training RITA. Specifically, we first sample some 𝑁 values in a reasonable range. For each sampled 𝑁 , we find a batch size that consumes up to a certain percentage of GPU memory in a cost-efficient way. Using a small set of mathematical functions as a prior, RITA learns a model with only a few &lt;N, B&gt; pairs as ground truth labels.</p><p>Our experiments on public timeseries benchmarks and the MGH EEG data <ref type="bibr" coords="2,353.59,514.86,10.44,4.09" target="#b5">[6]</ref> confirm that RITA outperforms state-of-the-art methods in accuracy on various timeseries analytics tasks, while our group attention mechanism achieves a 63X speedup with much less memory required, compared to existing self-attention mechanisms <ref type="bibr" coords="2,341.56,558.70,13.50,4.09" target="#b9">[10,</ref><ref type="bibr" coords="2,357.31,558.70,10.31,4.09" target="#b51">52,</ref><ref type="bibr" coords="2,369.86,558.70,10.13,4.09" target="#b53">54]</ref>. Contributions. The key contributions of this work include:</p><p>• Our group attention mechanism leverages the periodicity of timeseries, reducing the time and space complexity of the selfattention mechanism with accuracy guarantees, allowing RITA to scale to long timeseries data.</p><p>• Guided by an approximation error bound, our adaptive scheduler dynamically adapts the number of groups and the batch size to the distribution properties of the evolving feature embeddings, making group attention efficient and easily tunable.</p><p>• We conduct experiments on various datasets and different analytics tasks, demonstrating that RITA is 4 to 63 times faster than the state-of-the-art while achieving better accuracy when handling long timeseries (length ≥ 2000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We provide some background on the canonical self-attention module in the Transformer <ref type="bibr" coords="3,133.06,114.24,15.54,4.09" target="#b51">[52]</ref>. A self-attention module takes 𝑛 hidden embedding vectors 𝐻 ∈ R 𝑛 * 𝑑 ℎ as input, then projects them to queries (𝑄), keys (𝐾) and values (𝑉 ) and performs Scaled-dot Product Attention, which given input hidden state 𝐻 , is computed by:</p><formula xml:id="formula_0" coords="3,119.38,162.55,175.14,33.63">𝑄 = 𝐻𝑊 𝑄 , 𝐾 = 𝐻𝑊 𝐾 , 𝑉 = 𝐻𝑊 𝑉 𝑂 = 𝐴𝑉 = 𝑆𝑜 𝑓 𝑡𝑀𝑎𝑥 ( 𝑄𝐾 𝑇 √︁ 𝑑 𝑘 )𝑉<label>(1)</label></formula><p>Where Given a matrix 𝑀 ∈ R 𝐿 * 𝑛 , the softmax function normalizes 𝑀 to ensure the sum of each row equals to 1, as shown below.</p><formula xml:id="formula_1" coords="3,83.16,199.91,149.03,10.64">𝑊 𝑄 ∈ R 𝑑 ℎ * 𝑑 𝑘 ,𝑊 𝐾 ∈ R 𝑑 ℎ * 𝑑 𝑘 ,𝑊 𝑉 ∈ R 𝑑 ℎ *</formula><formula xml:id="formula_2" coords="3,109.79,275.04,184.80,24.36">𝑆𝑜 𝑓 𝑡𝑀𝑎𝑥 (𝑀 𝑖,𝑗 ) = 𝑒𝑥𝑝 (𝑀 𝑖,𝑗 ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑀 𝑖,𝑘 )<label>(2)</label></formula><p>Note the attention matrix A is an 𝑛×𝑛 matrix, where 𝑛 represents the number of elements in the input sequence (e.g. words in NLP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RITA OVERVIEW</head><p>Given a collection of unlabeled timeseries, RITA first pre-trains a Transformer-style model to produce high quality feature embeddings for timeseries data. This pre-trained model is then used to support various downstream tasks, similar to BERT <ref type="bibr" coords="3,256.30,385.47,13.45,4.09" target="#b15">[16]</ref>. Next, we overview the model architecture of RITA. We show how RITA supports various downstream tasks in Appendix A.7.</p><p>As shown in Fig. <ref type="figure" coords="3,125.50,418.35,3.01,4.09" target="#fig_1">1</ref>, RITA is consist of two components: (1) Timeaware Convolution Layer (2) RITA Encoder. Time-aware Convolution Layer fills the gap between timeseries and natural language. Despite their high-level similarity, there is a big gap between timeseries and natural language. First, in natural language each word, as a discrete semantic unit, has an independent meaning, while each element in a timeseries is a continuous, numerical value and does not necessarily constitute an independent event. Furthermore, the input sequences are single-channeled in NLP, but often multi-channeled in timeseries (i.e., sensor data often consists of several related channels).</p><p>RITA leverages the classical convolution <ref type="bibr" coords="3,215.93,538.89,14.81,4.09" target="#b27">[28]</ref> strategy to solve this problem. Convolution is widely used to capture the local structures of an image. We use convolution to chunk one input timeseries into a sequence of windows and learn the local structure of each window, similar to the discrete semantic units in natural language. It also discovers the correlations across different channels, thus naturally solving the multi-channel problem.</p><p>More specifically, treating a multi-variate timeseries of length 𝑛 and with 𝑚 variables as an n × m matrix 𝑇 , RITA uses 𝑑 convolution kernels to chunk 𝑇 into n windows and produce one d-dimensional embedding per window using the convolution operation <ref type="bibr" coords="3,257.99,648.48,13.22,4.09" target="#b27">[28]</ref>. Each convolution kernel corresponds to a w × m matrix, where 𝑤 defines the number of timestamps that each convolution kernel covers, identical to the window size in sliding window. RITA Encoder functions as Transformer Encoder as described in the original Transformer work <ref type="bibr" coords="3,165.26,703.28,16.14,4.09" target="#b51">[52]</ref>. It takes the embeddings of 𝑛 semantic units 𝑋 1 , 𝑋 2 , ..., 𝑋 𝑛 (𝑋 𝑖 ∈ 𝑅 𝑑 ) as input (e.g. embeddings of 𝑛 windows for a timeseries), then models the correlations between the semantic units and outputs 𝑌 1 , ..., 𝑌 𝑛 (𝑌 𝑖 ∈ 𝑅 𝑑 ) as the contextaware embedding of each unit.</p><p>What makes RITA Encoder different from Transformer Encoder is that: at the core of Transformer Encoder lies self-attention mechanism which incurs a 𝑂 (𝑛 2 ) time complexity and memory usage. This quadratic cost becomes prohibitive for long timeseries and limits the scalablity of Transformer-based models. To make the attention computation efficient yet high-quality, we replace the canonical self-attention with our proposed group attention. Self-supervised Pretraining. Inspired by the "cloze text" pretraining task in NLP, we designed a mask-and-predict task as the pretraining task for our model. The timeseries is randomly masked and the model should recover the masked values based on corresponding contextual information.</p><p>To be specific, we generate masks on time-stamps, with a mask rate 𝑝. The timeseries is scaled to be non-negative and the values across all the channels on the masked timestamps are set to be -1, an impossible value on normal timestamps. Then the masked timeseries is fed into RITA and the output representation is translated to the recovered timeseries by a Transpose Convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GROUP ATTENTION MECHANISM</head><p>Group attention, a novel and efficient approximate attention mechanism, addresses the performance bottleneck of self-attention in the vanilla Transformer. In this section, we first introduce the framework of group attention and then theoretically establish the bound of its approximation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Idea of Group Attention</head><p>As periodicity is a natural property of timeseries <ref type="bibr" coords="3,512.12,451.22,13.49,4.09" target="#b55">[56]</ref>, similar windows frequently occur. Similar windows result in similar queries/keys for attention computation, bringing opportunities for saving computation.</p><p>As discussed in Sec. 2, 𝐴 𝑖 𝑗 , the attention score of window 𝑖 onto window 𝑗, is determined by the inner product between the query vector of window 𝑖 and the key vector of window 𝑗, that is, 𝑞 𝑖 • 𝑘 𝑗 . Given another window 𝑥, if window 𝑥 has the similar key vector to window 𝑗, that is,</p><formula xml:id="formula_3" coords="3,317.69,536.62,241.50,18.72">𝑘 𝑗 ≈ 𝑘 𝑥 , then 𝑞 𝑖 • 𝑘 𝑗 ≈ 𝑞 𝑖 • 𝑘 𝑥 . In other words, 𝐴 𝑖 𝑗 ≈ 𝐴 𝑖𝑥 when 𝑘 𝑗 ≈ 𝑘 𝑥 .</formula><p>This observation inspires our group attention mechanism. That is, we group the windows by their similarity in keys. Assuming all windows in the same group have the same attention score onto another window 𝑘, we then only compute the attention once by using one single key to represent this group, for example the centroid of the group of keys. This thus saves significant computation cost.</p><p>Better yet, after grouping 𝑛 windows into 𝑁 groups, group attention compresses the attention matrix from an 𝑛×𝑛 matrix to an 𝑛×𝑁 matrix. Because 𝑁 (number of groups) tends to be much smaller than 𝑛 (number of windows) due to the periodicity of timeseries, group attention consumes much less memory than the original self-attention mechanism, successfully eliminating the memory bottleneck. Note that it also doesn't hurt quality all that much, as confirmed in our experiments (Sec. 6.2). We now discuss how to efficiently compute the output feature embeddings using the small compressed group attention matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Problem: Producing Embeddings w/ Group Attention Matrix</head><p>As described in the Background, once we have acquired the attention matrix 𝐴, canonical self-attention computes the output embedding 𝑂 as O = AV . Because 𝐴 is an 𝑛 × 𝑛 matrix and 𝑉 is an 𝑛 × 𝑑 𝑣 matrix, the matrix product operation still produces an 𝑛 × 𝑑 𝑣 matrix 𝑂. That is, it produces a 𝑑 𝑣 dimensional feature vector for each window. However, our group attention will produce an 𝑛 × 𝑁 attention matrix 𝐴 , where 𝑁 corresponds to the number of groups. In this case the matrix product will produce a 𝑁 ×𝑑 𝑣 matrix 𝑂. That is, it produces a feature vector for each group. However, our goal is to produce different embeddings for different windows, because even if some windows share the attention score temporally, it does not mean they should have the same feature embedding. A Naive Solution. A naive solution would be to restore the full attention matrix 𝐴 from the group attention matrix 𝐴. For example, given one group composed of 𝑤𝑖𝑛 𝑖 and 𝑤𝑖𝑛 𝑗 , we map its group attention vector in 𝐴 into two rows that correspond to 𝑤𝑖𝑛 𝑖 and 𝑤𝑖𝑛 𝑗 in 𝐴. However, in this case we again get a 𝑛 × 𝑛 attention matrix; and GPU memory remains a bottleneck in group attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Solution: Embedding Aggregation and Group SoftMax</head><p>Using an embedding aggregation operation and a group softmax function, RITA produces 𝑛 embeddings without restoring the full attention matrix. Fig. <ref type="figure" coords="4,133.18,521.03,4.17,4.09">2</ref> shows the workflow of group attention. Embedding Aggregation. The idea is inspired by the observation on the matrix product operation O = AV conducted on the fully restored attention matrix 𝐴.</p><p>Given an element 𝑂 𝑖,𝑗 of 𝑂 corresponding to the 𝑗 𝑡ℎ dimension of 𝑤𝑖𝑛 𝑖 's feature vector, 𝑂 𝑖,𝑗 = 𝑎 𝑖 •𝑣 𝑗 , where vector a i ∈ R n denotes the 𝑖 𝑡ℎ row of the attention matrix 𝐴 and vector v j ∈ R n denotes the 𝑗 𝑡ℎ dimension of all the 𝑛 feature vectors. Given</p><formula xml:id="formula_4" coords="4,53.80,595.58,240.05,26.10">a i =&lt; a 1 i , a 2 i , • • • , a n i &gt; and v j =&lt; v 1 j , v 2 j , • • • , v n j &gt;, 𝑂 𝑖,𝑗 = n k=1 a k i v k j .</formula><p>As an example, assume 𝑤𝑖𝑛 1 and 𝑤𝑖𝑛 2 belong to the same group</p><formula xml:id="formula_5" coords="4,53.35,633.14,240.70,21.87">𝐺 1 . Then 𝑎 1 𝑖 = 𝑎 2 𝑖 = 𝑎 1 𝑖 , where 𝑎 1 𝑖 ∈ 𝐴 corresponds to the attention of group 𝐺 1 onto 𝑤𝑖𝑛 𝑖 . Therefore, 𝑎 1 𝑖 𝑣 1 𝑗 + 𝑎 2 𝑖 𝑣 2 𝑗 = 𝑎 1 𝑖 (𝑣 1 𝑗 + 𝑣 2 𝑗 ).</formula><p>As an immediate generalization of the above analysis, if we aggregate up the windows that belong to the same group and convert the n-dimensional feature vector 𝑣 𝑗 into a 𝑁 -dimensional group feature vector 𝑣 𝑗 beforehand, we could directly use the group attention vector 𝑎 𝑖 and the group feature vector 𝑣 𝑗 to compute 𝑂 𝑖,𝑗 .</p><p>Using embedding aggregation, RITA is able to produce the feature embedding 𝑂 that is identical to the embedding 𝑂 produced by using the full attention matrix 𝐴 and the embedding matrix 𝑉 . Group Softmax Function. In canonical self-attention the atten-</p><formula xml:id="formula_6" coords="4,317.96,130.12,175.97,19.13">tion matrix 𝐴 is computed as 𝐴 = SoftMax ( QK T √ d k</formula><p>). To compute 𝐴, we have to first compute 𝑄𝐾 𝑇 (denoted as 𝑃) which is an 𝑛 × 𝑛 matrix. Then normalizing the 𝑃 matrix with softmax produces the attention matrix 𝐴.</p><p>Group attention follows the same procedure. But after grouping keys into 𝐾, 𝑄 𝐾 𝑇 produces an 𝑛 × 𝑁 matrix 𝑃. Due to the nonlinearity of the softmax function, applying softmax directly on 𝑃 will result in a group attention matrix 𝐴 from which we are not able to recover a full attention matrix that is identical to first restoring 𝑃 to 𝑃 and then applying softmax on 𝑃. The 𝐴 matrix produced by the latter is desirable, as we want to approximate the original attention matrix as accurately as possible. However, restoring the small 𝑛 × 𝑁 𝑃 matrix is not memory efficient, as it will end up with a full 𝑛 × 𝑛 matrix 𝑃.</p><p>To solve the above problems, we introduce a new group softmax function to replace the original softmax function (Eq. 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝐺𝑟𝑜𝑢𝑝𝑆𝑜 𝑓 𝑡𝑀𝑎𝑥</head><formula xml:id="formula_7" coords="4,410.01,321.48,148.73,24.66">( 𝑃 𝑖,𝑗 ) = 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) 𝑁 -1 𝑘=0 𝑐𝑜𝑢𝑛𝑡 𝑘 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 )<label>(3)</label></formula><p>In Eq. 3, 𝑐𝑜𝑢𝑛𝑡 𝑘 represents the number of windows that Group 𝐺 𝑘 contains. Compared to the original softmax, our group softmax considers each group 𝐺 𝑘 as 𝑐𝑜𝑢𝑛𝑡 𝑘 elements and counts it 𝑐𝑜𝑢𝑛𝑡 𝑘 times when summing up the exponential of each group's 𝑃 𝑖,𝑘 . In this way, the group softmax function operating on the small 𝑃 matrix will produce exactly the same result to the softmax function operating on the full 𝑃 matrix. Theoretical Guarantee. In Appendix A.4, we prove that the group softmax function and the embedding aggregation operation produce the same output feature embedding with the naive method that has to first restore the big full attention matrix.</p><p>We show an efficient implementation of the embedding aggregation operation and group softmax function in Appendix A.2, Alg. 1. Time Complexity. The time complexity of Alg. 1 is 𝑂 (𝑛𝑁𝑑) and the space complexity is 𝑂 (𝑛𝑁 ), while the time and space complexity of the original self-attention mechanism are 𝑂 (𝑛 2 𝑑) and 𝑂 (𝑛 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Error Bound</head><p>Group attention produces a group attention matrix 𝐴 which approximates the attention matrix 𝐴 produced by the classical self-attention with a bounded error, as shown in Lemma 1.</p><p>Lemma 1. Let 𝑅 be the radius of the ball where all key vectors live; 𝑘 𝑖 be the representative of the group that contains key 𝑘 𝑖 . Let 𝐴 denote the full attention matrix restored from 𝐴. Suppose the distance between 𝑘 𝑖 and 𝑘</p><formula xml:id="formula_8" coords="4,327.92,623.22,185.04,27.04">𝑖 (|| k 𝑖 -k 𝑖 ||) satisfies: || k 𝑖 -k 𝑖 || ≤ d. Then ∀ 𝜖 &gt; 1, if d ≤ ln(𝜖 ) 2R , 1 𝜖 ≤ A i,j A i,j ≤ 𝜖</formula><p>Lemma 1 shows that the error bound 𝜖 of the group attention is determined by the distance 𝑑. As discussed in Sec. 5.1, it inspires us to design a strategy to dynamically determine the number of groups 𝑁 -the most critical parameter of group attention. Please refer to Appendix A.5 for the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">GPU Friendly Grouping Method</head><p>In this section, we discuss the implementation of a grouping method. To make group attention efficient and effective, the grouping method has to satisfy the following requirements:</p><p>(1) Tight distance bound: to ensure the approximation quality, the distance between each key and its group representative should be minimized according to Lemma 1.</p><p>(2) Lightweight: to ensure the performance gain, the grouping method must be lightweight, at worst not exceeding the complexity of group attention itself (𝑂 (𝑁𝑛)).</p><p>(3) GPU friendly: to take advantage of GPUs, we prefer a grouping method that mainly consists of matrix operations, which can be efficiently executed on a GPU.</p><p>To satisfy the above requirements, after thorough investigation on various clustering algorithms, we design a GPU friendly Kmeans <ref type="bibr" coords="5,79.59,256.70,14.72,4.09" target="#b34">[35]</ref> as the grouping method.</p><p>First, K-means minimizes the overall distance between any object and its cluster center, hence naturally satisfying Requirement 1.</p><p>Second, given 𝑁 centers, in each iteration the time and space complexity of K-means is 𝑂 (𝑛𝑁 ). Usually, the iteration goes until convergence. However, we observe that rather than seeking a perfect K-means clustering, training a few iterations is sufficient to get a good grouping for group attention, because typically the later iterations only slightly update the clustering and group attention is robust to such imperfection.</p><p>Third, we design a GPU-friendly implementation of K-means. The performance bottleneck of K-means comes from the distance computation between each vector and its center, that is,</p><formula xml:id="formula_9" coords="5,53.80,395.89,241.76,24.98">|v i -c j | = √︃ (v i -c j ) 2 , i ∈ [1, n], j ∈ [1, N ]. The performance bot- tleneck is 𝑣 𝑖 -𝑐 𝑗 .</formula><p>We instead use a different formulation: |𝑣 𝑖 -</p><formula xml:id="formula_10" coords="5,53.40,423.72,211.84,13.85">𝑐 𝑗 | = |v i -c j | = √︃ |v i | 2 + |c j | 2 -2v i • c j , i ∈ [1, n], j ∈ [1, N ]</formula><p>. This is because in this formulation, the performance bottleneck is 𝑣 𝑖 • 𝑐 𝑗 , which could be implemented as a matrix product operation. Although the complexity of the two formulations is the same, in GPUs matrix product is much more efficient than pairwise difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ADAPTIVE SCHEDULER</head><p>Next, we present the adaptive scheduler of RITA which addresses the challenges of determining an appropriate number of groups 𝑁 and accordingly the batch size 𝐵, as described in Introduction.</p><p>Using a dynamic scheduling method we propose, the scheduler automatically determines and adjusts 𝑁 and 𝐵 based on the distributional properties of the feature embeddings produced over the iterative training process, while guaranteed to produce high quality attention approximation that meets the requirement of users. In Sec. 5.1 we show how RITA automatically determines 𝑁 . Then we introduce in Sec. 5.2 the learning-based method which given an 𝑁 , immediately predicts a good batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dynamically Determining the Number of Groups N</head><p>Without loss of generality, we use one group attention module as an example to show how RITA automatically gets an appropriate 𝑁 . The adaptive scheduler of RITA starts with a large 𝑁 and decreases it dynamically. This is because in the training process of RITA, the feature embeddings produced epoch by epoch tend to get stabler and stabler and gradually converge, thus no need to increase 𝑁 . RITA reduces the number of groups by merging similar groups. Intuitively, given two groups, we could measure their similarity based on the distance of their centers. If the distance between their centers is smaller than a distance threshold, then the two groups could be merged. However, setting an appropriate distance threshold seems hard -as difficult as setting an appropriate 𝑁 .</p><p>To solve this problem, RITA leverages the error bound of group attention introduced in Sec. <ref type="bibr" coords="5,425.97,188.21,3.13,4.09" target="#b3">4</ref> </p><formula xml:id="formula_11" coords="5,350.16,283.58,208.58,15.70">𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑘 𝑖 |𝑐 𝑘 𝑖 -𝑐 𝑘 𝑗 | + |𝑥 -𝑐 𝑘 𝑖 | ≤ 𝑑, 𝑖, 𝑗 ∈ [1, 𝑚]<label>(4)</label></formula><p>merging them into one cluster still meets the error bound 𝜖.</p><p>Please refer to Appendix A.6 for the proof. Finding the Mergable Clusters. We formulate the problem of finding mergeable clusters using graph theory:</p><p>(1) each cluster is a node in the graph;</p><p>(2) if 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 and 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 satisfy:</p><formula xml:id="formula_12" coords="5,327.70,377.27,229.82,14.16">𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 |𝑐 𝑖 -𝑐 𝑗 |+|𝑥 -𝑐 𝑖 | ≤ 𝑑, and 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 |𝑐 𝑗 -𝑐 𝑖 |+|𝑥 -𝑐 𝑗 | ≤ 𝑑</formula><p>there is an undirected edge between 𝑛𝑜𝑑𝑒 𝑖 and 𝑛𝑜𝑑𝑒 𝑗 ; In this scenario, finding the maximum number of mergeable clusters is equivalent to finding the minimal clique cover in the corresponding graph, which is an NP-hard problem <ref type="bibr" coords="5,519.86,429.47,13.49,4.09" target="#b23">[24]</ref>. Such heavy computation overhead is not acceptable for RITA. We thus offer a simplified solution:</p><p>(1) Halve the clusters into two sets 𝑆 1 , 𝑆 2 ;</p><p>(2) If 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 ∈ 𝑆 1 and 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 ∈ 𝑆 2 satisfy:</p><formula xml:id="formula_13" coords="5,323.21,481.78,232.65,24.17">𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 |𝑐 𝑖 -𝑐 𝑗 | + |𝑥 -𝑐 𝑖 | ≤ 𝑑, 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 |𝑐 𝑗 -𝑐 𝑖 | + |𝑥 -𝑐 𝑗 | ≤ 𝑑 2 (<label>5</label></formula><formula xml:id="formula_14" coords="5,555.86,502.32,2.82,3.63">)</formula><p>𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 is marked.</p><p>(3) Decrease the number of clusters by counting the masks in 𝑆 2 . In this solution, clusters in 𝑆 1 can be regarded as transfer nodes. If (5) holds for</p><formula xml:id="formula_15" coords="5,317.69,545.72,240.34,19.75">(𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 ∈ 𝑆 1 , 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 1 ∈ 𝑆 2 ) and (𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑖 ∈ 𝑆 1 , 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 2 ∈ 𝑆 2 )</formula><p>, respectively, we have,</p><formula xml:id="formula_16" coords="5,336.69,568.11,221.99,50.24">𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 1 |𝑐 𝑗 1 -𝑐 𝑗 2 | + |𝑥 -𝑐 𝑗 1 | ≤ 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 1 |𝑐 𝑗 1 -𝑐 𝑖 | + |𝑐 𝑖 -𝑐 𝑗 2 | + |𝑥 -𝑐 𝑗 1 | ≤ 𝑚𝑎𝑥 𝑥 ∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑗 1 |𝑐 𝑗 1 -𝑐 𝑖 | + |𝑐 𝑖 -𝑐 𝑗 2 | + |𝑥 -𝑐 𝑗 1 | + |𝑥 -𝑐 𝑗 2 | ≤ 𝑑 (6)</formula><p>Thus (4) holds when merging several clusters in 𝑆 2 with one cluster in 𝑆 1 . As a result, we can greedily merge clusters in 𝑆 2 , as illustrated in step <ref type="bibr" coords="5,380.10,648.48,9.93,4.09" target="#b2">(3)</ref>.</p><p>Assume the number of clusters decreases by 𝐷 after merging, we apply a momentum update <ref type="bibr" coords="5,428.60,670.40,14.60,4.09" target="#b41">[42]</ref> on the number of clusters 𝑁 , as is commonly used in machine learning to smooth the changing of 𝑁 and avoid sample selection bias. To be specific: 𝑁 𝑛𝑒𝑤 = 𝛼 (𝑁 -𝐷) + (1 -𝛼)𝑁 , where 𝛼 is a hyper-parameter for momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dynamically Determining the Batch Size</head><p>Because of the dynamic grouping operation, the computational graph in deep learning training <ref type="bibr" coords="6,165.89,114.24,10.42,4.09" target="#b0">[1]</ref> varies from sample to sample. As a result, it is impossible to precisely compute a batch's GPU memory usage without indeed feeding it into the model. To overcome this problem, RITA learns a batch size prediction function offline; then at the RITA training time, given a number of groups 𝑁 , RITA uses this function to predict a proper batch size.</p><p>When the model architecture and hardware are fixed, the batch size depends on the length of the timeseries 𝐿 and the average group number among all attention module 𝑁 . So RITA samples several (𝐿 𝑖 , 𝑁 𝑖 ) pairs and estimate a proper batch size for each pair.</p><p>More specifically, given a user-defined timeseries maximal length 𝐿 𝑚𝑎𝑥 , we randomly sample integral points (𝐿 𝑖 , 𝑁 𝑖 ) from plane {1 ≤ 𝐿 ≤ 𝐿 𝑚𝑎𝑥 , 1 ≤ 𝑁 ≤ 𝐿}. Then we use a binary search based algorithm to find the maximal batch size 𝐵 𝑖 that consumes less than 90% available GPU memory, aiming to avoid wasting GPU memory and the risks of out of memory (OOM).</p><p>Treating these pairs as ground truth labels, we use function fitting <ref type="bibr" coords="6,78.49,301.64,14.72,4.09" target="#b17">[18]</ref> to learn the batch size predicting function B = f (L, N ), where B is a function of two variables 𝐿 and 𝑁 . Learning the Prediction Function. We apply curve fit from SciPy <ref type="bibr" coords="6,75.20,334.51,14.59,4.09" target="#b52">[53]</ref> as the function fitting tool to fit the two-variable function</p><formula xml:id="formula_17" coords="6,53.71,343.20,190.06,7.93">𝐵 𝑖 = 𝑓 (𝐿 𝑖 , 𝑁 𝑖 ) on plane {1 ≤ 𝐿 ≤ 𝐿 𝑚𝑎𝑥 , 1 ≤ 𝑁 ≤ 𝐿}.</formula><p>We observe that applying one function to the whole plane incurs a huge estimation error. So we develop a dynamic-programming (DP) method to divide the plane into several sub-planes and apply a distinct function to each sub-plane respectively. It is optimal in minimizing the total estimation error on all sub-planes With the learned prediction function 𝑓 , we can estimate a proper batch size for any (𝐿, 𝑁 ) during training, even if it is not seen in the sampled (𝐿 𝑖 , 𝑁 𝑖 ) pairs. The Algorithms and Optimality Proof. Please refer to Appendix A.3 for the pseudo code of the binary search-based algorithm and the description of the DP method for plane-division and the proof for its optimality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>Our experimental study focuses on the following questions:</p><p>1. Effectiveness and efficiency of RITA: How does RITA compare with other Transformer-based methods and traditional timeseries representation learning methods in accuracy and efficiency?</p><p>2. Ablation Study: How do the key techniques of RITA work?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Datasets. We evaluate RITA on classification and imputation tasks using 5 multi-variate and 3 uni-variate timeseries datasets.</p><p>• WISDM <ref type="bibr" coords="6,101.34,626.56,14.77,4.09" target="#b54">[55]</ref> is a popular multivariate timeseries dataset generated from the accelerometer in the mobile phone. The subjects performed 18 daily activities (e.g. walking, jogging). The dataset was collected from 51 subjects and the sampling rate is 20 Hz.</p><p>• HHAR dataset <ref type="bibr" coords="6,123.28,670.40,14.63,4.09" target="#b45">[46]</ref> contains sensing data of accelerometer collected from 9 users performing 5 activities with 12 different smartphones (varying in sampling rate). This increases the complexity of the task and thus can test the model's robustness.</p><p>• RWHAR RealWorld HAR dataset <ref type="bibr" coords="6,457.72,89.58,14.85,4.09" target="#b47">[48]</ref> covers 15 subjects performing 8 locomotion-style activities. Each subject wears the sensors for approximately ten minutes. The sampling rate is 50 Hz.</p><p>• ECG dataset <ref type="bibr" coords="6,379.12,122.46,14.60,4.09" target="#b33">[34]</ref> consists of 10,000 EEG recordings for arrhythmia classification. Each recording has an uncertain length ranging from 6 to 60 seconds sampled at 500 Hz. The ECG recordings correspond to 9 types of heart problems such as atrial fibrillation (AF) and premature atrial contraction (PAC), etc.</p><p>• MGH <ref type="bibr" coords="6,355.77,177.25,10.46,4.09" target="#b5">[6]</ref> is a EEG dataset collected by Mass. General Hospital. Each timeseries corresponds to the EEG data observed from one patient during their stay in ICU for a couple of days. The EEG monitoring produced data with 20 channels. The sampling rate is 200 HZ. So it produces very long timeseries.</p><p>• WISDM*/HHAR*/RWHAR* are three uni-variate datasets derived by picking one channel from WISDM/HHAR/RWHAR. Training/Validation Data Generation. We apply a sliding window on the raw timeseries to get training/validation samples. The size of the sliding window is set as 200 on small datasets (WISDM, HHAR, RWHAR), 2000 on medium size dataset (ECG), and 10,000 on the large dataset (MGH). Table <ref type="table" coords="6,446.69,297.80,4.25,4.09" target="#tab_3">1</ref> shows the statics of the generated datasets. They are randomly split into training/validation set in a proportion of 0.9/0.1. In "pretraining + few-label finetuning" scenario, we use 100 labeled data per class for finetuning. We guarantee that training set does not overlap with the validation set. To evaluate our group attention (referred to as Group Attn.), we develop three baselines by replacing the group attention component in RITA with the classic vanilla Self-Attention <ref type="bibr" coords="6,510.77,473.14,14.59,4.09" target="#b51">[52]</ref>(referred to as Vanilla) and two SOTA methods that reduce the complexity of self-attention by approximation in NLP, namely, Performer <ref type="bibr" coords="6,543.54,495.06,14.66,4.09" target="#b9">[10]</ref> (referred to as Performer) and Linformer <ref type="bibr" coords="6,472.82,506.02,14.68,4.09" target="#b53">[54]</ref> (referred to as Linformer). Similar to our proposed Group Attn., Vanilla, Performer, Linformer all use RITA's time-aware convolution operation (Sec. 3) to turn timeseries segments into input feature vectors. We also compare Group Attn. against GRAIL <ref type="bibr" coords="6,506.23,549.85,13.49,4.09" target="#b39">[40]</ref>, which is the SOTA of the non-deep learning methods for timeseries representation learning. GRAIL supports classification tasks by feeding the learned representations into a Support-Vector Machine <ref type="bibr" coords="6,543.35,582.73,14.85,4.09" target="#b11">[12]</ref> or K-Nearest Neighbor <ref type="bibr" coords="6,407.67,593.69,14.85,4.09" target="#b16">[17]</ref> classifier. Note GRAIL only targets uni-variate timeseries and cannot support imputation tasks. Methodology. We mainly focus on two downstream tasks:</p><p>(1) Classification. First, we train Group Attn. and the baselines with full labels from scratch to test the effectiveness of RITA framework and the approximation quality of our group attention.</p><p>Second, to measure the effectiveness of self-supervised pretraining, we evaluate the accuracy of training on few labeled timeseries with/without pretraining on large scales of unlabeled timeseries. To be specific, we split the training set into a pretraining set and a finetuning set, with very few data in the latter (100 labeled samples per class in our experiment). We train the model on the cloze pretraining task with a mask rate 𝑝 = 0.2. Then we train two classification models using the finetuning set, either based on the pretrained version or from scratch. We repeat the experiment 5 times with random data splits and report the median accuracy.</p><p>(2) Imputation. We run the imputation task on the datasets used in classification as well as the large unlabeled MGH dataset, and measure the mean square error and absolute imputation error. To get timeseries with missing values, we randomly mask the values with an expected mask rate of 𝑝 = 0.2. The masked values are replaced with a special value.</p><p>Finally, to evaluate Group Attn. 's benefit on efficiency, the total time of forward computation, backward propagation, and grouping are measured for all methods in all the experiments.</p><p>To save space, we only report the average training time per epoch here and refer readers to Appendix A.8 for the inference time.</p><p>We first compare against the Transformer-based methods on multi-variate datasets (sec. 6.2, 6.3), then compare against the nondeep learning method GRAIL on uni-variate datasets (sec. 6.4). Configuration. Please refer to Appendix A.1 for the experiment configuration and hyper-parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effectiveness: Transformer-Based Methods</head><p>We first evaluate the quality of the models trained with full labels from scratch. We then show how the pretraining of RITA increases the accuracy of the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">full-label training (Multi-variate classification)</head><p>Results shown in Figure <ref type="figure" coords="7,143.91,527.93,3.38,4.09" target="#fig_3">3</ref>(a) get us the following observations:</p><p>(1) RITA's advantage over TST. On all four datasets for the classification tasks, Group Attn. and the other three baselines that use RITA architecture (Vanilla, Performer, and Linformer) outperform TST. In particular, Group Attn. outperforms TST by 49 percentage points on the ECG dataset (88.48% vs 39.93%) with long timeseries. Two deficiencies in TST may cause its poor performance on the long timeseries. Firstly, TST concatenates the output embedding vector of each time stamp, then uses a linear classifier to do classification on the concatenated vector. When the timeseries is long, the linear classifier has so many parameters that it tends to overfit easily. Secondly, TST replaces Layer Normalization in vanilla Transformer with Batch Normalization. When the timeseries is long, it can only accommodate a small number of timeseries in each batch, leading to bias in Batch Normalization.</p><p>(2) Group-attention's advantage over other attention mechanisms. Group Attn. is better than Performer and Linformer on 3 out of 4 datasets for classification. Although Linformer works slightly better than Group Attn. on the ECG dataset (90.37% vs 88.84%), its performance is the worst in all other cases compared to any other RITA-based methods. Vanilla computes the attention scores precisely. Thus it is expected to work well. However, Group Attn. outperforms Vanilla on WISDM (87.50% vs 86.95%) and is very close to it on other 3 datasets. This suggests that group attention's approximation quality is good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">pretraining + few label finetune (Multi-variate classification)</head><p>The results shown in Table <ref type="table" coords="7,418.74,194.56,4.17,4.09" target="#tab_5">3</ref> get us the following observation:</p><p>(1) Pretraining is effective. Pretraining always leads to better accuracy than training with a few labels from scratch. In particular, on WISDM data all the methods using RITA architecture increase the accuracy by at least 10%. This is impressive considering we do not have a very large unlabeled pre-training set to use.</p><p>(2) RITA's advantage over TST. our Group Attn. and other three baselines using RITA architecture (Vanilla, Performer, and Linformer) significantly outperform TST on all four classification datasets by 25 percentage points.</p><p>(3) Group Attention's advantage over other attention mechanisms. Group Attn. is better than Performer and Linformer on 3 out of 4 datasets. When compared to Vanilla, Group Attn. is better on HHAR and ECG, and comparable on the other two, further confirming its high quality on approximation. Further, we notice that Linformer struggles in this setting: in average its accuracy is worse than Vanilla by 8.22% and Group Attn. by 8.01%. This is because the low-rank projection operation introduces extra model parameters, making Linformer more easily overfit, while overfitting is especially harmful when there are only a few labeled training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">full-dataset training (Multi-variate imputation)</head><p>Similar to classification tasks, the results of imputation tasks (Table .2) show that Group Attn. consistently outperforms the baselines in training time while achieving comparable/better MSE. Again, on the large dataset MGH (length = 10,000), TST and Vanilla fail due to out of memory (OOM) errors. Methods using RITA framework (Group Attn., Performer, Linformer) all achieve very low MSE (are highly accurate). Among them Linformer is the worst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Efficiency: Transformer-based Methods</head><p>We measure the efficiency by the average training time per epoch including the cost of the forward computation + backward propagation and the grouping overhead. We first show the results on all the 5 datasets in Sec. 6.3.1. We then vary the length of the timeseries on the MGH dataset to show group attention's scalability on long timeseries in Sec. 6.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Training Time: All Multi-variate Datasets</head><p>The results in Fig. <ref type="figure" coords="7,387.25,626.56,3.46,4.09" target="#fig_3">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) and Table 2 lead to the below observations:</head><p>(1) Vanilla Self-Attention is not scalable. In average, it takes 2-3 minutes to train one epoch when the length of the timeseries is only 200 (WISDM, HHAR, RWHAR), takes over 15 minutes when the length increases to 2,000 (ECG), and fails on the long MGH data when the length reaches 10,000 due to out of GPU memory.</p><p>(2) Group Attn.'s advantage over all other attention mechanisms. As we have shown in Sec. <ref type="bibr" coords="7,440.07,703.28,3.01,4.09" target="#b5">6</ref>   than Performer and Linformer in classification and imputation tasks, while Group Attn. is always faster than Performer, Linformer, and all other baselines on all 5 multi-variate datasets, thus a win-win.</p><p>(3) The longer the timeseries, the larger the speedup. On the medium sized ECG dataset with a length of 2,000, Group Attn. has a speedup of 3.86/1.36/2.27 compared to Vanilla/Performer/Linformer. When the length increases to 10,000, the speedup on the MGH dataset increases to 6.59/7.48 compared to Performer/Linformer (Vanilla and TST failed in this case) on imputation task (Table . 2). However, even on the short WISDM, HHAR, RWHAR datasets, Group Attn. still consistently outperforms other methods, confirming that it does not introduce much overhead. This is because when the length of the timeseries gets longer, Group Attn. gets more opportunities to find windows with similar properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Training time: Varying the Length</head><p>In this experiment, we truncate the original MGH timseries into sequences with the lengths at 2000/4000/6000/8000/10000, and compare Group Attn. against Vanilla and other attention mechanisms. Vanilla cannot handle sequences longer than 8000.</p><p>The results in Fig. <ref type="figure" coords="8,129.92,637.52,4.09,4.09" target="#fig_4">4</ref> again show that the longer the timeseries, the larger the speed up. With comparable MSE, Group Attn. outperforms Vanilla by 63X. Moreover, as the length increases from 2000 to 10000, the training time of Group Attn. only increases from 31.2 seconds to 54.4 seconds per epoch. The reason is that as the timeseires becomes longer, there are more grouping opportunities because of the similarity of the timeseries segments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison to Non-deep Learning Methods</head><p>We compare against GRAIL, the SOTA of non-deep learning timeseries representation learning. We use the three uni-variate datasets, because GRAIL only targets uni-variate timeseries. Results in Fig. <ref type="figure" coords="8,382.42,424.70,4.25,4.09" target="#fig_6">5</ref> show that on all 3 datasets RITA significantly outperforms GRAIL in accuracy by 45, 16, and 21 percentage points because of the expressive power of Transformer. Moreover, thanks to the GPU-friendly design of RITA, it is at least 2× faster than GRAIL in training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">Adaptive Scheduler</head><p>To evaluate the effectiveness of RITA's adaptive scheduler (Sec. 5), we compare it against a baseline using a fixed group number 𝑁 . We vary 𝑁 and the error bound threshold 𝜖 used by RITA.</p><p>From the results in Table <ref type="table" coords="8,421.33,550.57,4.17,4.09" target="#tab_7">4</ref> we get the following observations:</p><p>(1) Adaptive Scheduler is better than fixed 𝑁 . Training with Adaptive Scheduler already achieves better or comparable performance compared to the best performing 𝑁 . More specifically, on the MGH dataset, dynamic scheduler always achieves better accuracy and is much faster compared to fixed 𝑁 . On the ECG dataset, although fixed 𝑁 is slightly better than adaptive scheduler in accuracy when setting the N as 512, it runs much slower than adaptive scheduler. Of course, finding the best 𝑁 that balances the accuracy and running time requires careful tuning.</p><p>(2) Adaptive Scheduler is tuning free. It is robust on both accuracy and running time when 𝜖 varies, while the results of fixed 𝑁 vary significantly when the value of 𝑁 changes. Therefore, Adaptive Scheduler frees the users from tuning the 𝜖 threshold, while it is hard to find an appropriate 𝑁 for a given dataset.  Table <ref type="table" coords="9,80.85,307.21,3.45,7.70">5</ref>: RITA Pretraining: increasing sizes of pretrain set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">The Sizes of the Pretraining Data</head><p>Next, we evaluate how the number of unlabeled data influences the effectiveness of pretraining. To get empirical results, we pretrain RITA on WISDM dataset with 20%/40%/60%/80% of the pretraining data and finetune each pretrained model with 100 labels per class. The results in Table <ref type="table" coords="9,127.38,396.74,4.13,4.09">5</ref> show that: (1) The more pretraining data, the larger the improvement. The accuracy increases with the sizes of the pretraining data; (2) Marginal utility diminishing.</p><p>The first 20% pretraining data gives a 10.38% improvement accuracy (72.94% vs 62.56%), while the remaining 80% pretraining data only gives an additional improvement of 2.12% (75.06% vs 72.94%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK 7.1 Timeseries Analytics</head><p>There is a great deal of prior work on timeseries analytics methods. This work can be divided into three categories: (1) non-deep learning methods; (2) CNN/RNN-based deep learning methods; and (3) Transformer-based deep learning methods. Traditional Methods. These methods, such as TS-CHIEF <ref type="bibr" coords="9,278.17,549.85,13.49,4.09" target="#b44">[45]</ref>, HIVE-COTE <ref type="bibr" coords="9,100.13,560.81,13.22,4.09" target="#b32">[33]</ref>, ROCKET <ref type="bibr" coords="9,152.53,560.81,14.59,4.09" target="#b14">[15]</ref> have achieved notable performance on public datasets. Despite that, traditional methods suffer from one or more issues: they (1) rely on expert knowledge for feature extraction;</p><p>(2) incur heavy computation cost and are inappropriate for GPU devices; (3) support only uni-variate timeseries; (4) perform classification solely. Some work <ref type="bibr" coords="9,173.34,615.61,14.81,4.09" target="#b60">[61]</ref> shows that the transformedbased methods outperform these traditional methods especially on multi-variate timeseries.</p><p>In particular, as the SOTA of timeseries representation learning, GRAIL <ref type="bibr" coords="9,99.24,659.44,14.85,4.09" target="#b39">[40]</ref> extracts landmarks from data and computes the representations with the combination of the landmarks. However, GRAIL only supports uni-variate timeseries. Our experiments (Sec. 6.4) show that RITA significantly outperforms GRAIL in both effectiveness and efficiency on uni-variate timeseries.</p><p>CNN/RNN-based Deep Learning Methods. CNN-based methods, such as InceptionTime <ref type="bibr" coords="9,402.63,100.54,14.72,4.09" target="#b20">[21]</ref> and Resnet <ref type="bibr" coords="9,461.97,100.54,13.35,4.09" target="#b18">[19]</ref>, are good at classification tasks, but can not handle generative tasks such as forecasting because of the inductive bias of convolution networks. RNN-based methods, such as Brit <ref type="bibr" coords="9,398.28,133.41,10.52,4.09" target="#b6">[7]</ref> and deepAR <ref type="bibr" coords="9,457.22,133.41,13.32,4.09" target="#b43">[44]</ref>, are capable for classification, regression and generation. However, the recurrent structure brings a lot of problems: (1) limiting the model's ability in capturing long-range correlation; (2) notoriously difficult to train <ref type="bibr" coords="9,543.35,166.29,14.85,4.09" target="#b40">[41]</ref> because of gradient vanishing and exploding problem. As a result, such methods can hardly scale to very long timeseries. Transformer-based Deep Learning Methods. Given that Transformer is the best choice for backbone in almost all sequence modeling tasks, some effort has been made to apply Transformer to timeseries analytics. Targeting forecasting of uni-variate timeseries, LogTrans <ref type="bibr" coords="9,355.81,243.00,14.85,4.09" target="#b29">[30]</ref> introduced a log sparsity assumption to attention computation. Informer <ref type="bibr" coords="9,406.06,253.96,14.85,4.09" target="#b61">[62]</ref> pushes LogTrans a step further and scales forecasting to multi-variate timeseries. Autoformer <ref type="bibr" coords="9,527.95,264.92,14.64,4.09" target="#b56">[57]</ref> performs forecasting by decomposing timeseries into two parts, i.e. the trend part and the seasonal part.</p><p>For imputation tasks, CDSA <ref type="bibr" coords="9,434.43,297.80,14.83,4.09" target="#b36">[37]</ref> outperforms statistical methods and the SOTA of RNN-based method Brit <ref type="bibr" coords="9,488.28,308.76,10.66,4.09" target="#b6">[7]</ref> on 3 public and 2 competition datasets. For timeseries classification, AutoTransformer <ref type="bibr" coords="9,346.91,330.67,14.85,4.09" target="#b42">[43]</ref> performs architecture search to adapt to the tasks in different domains. For timeseries anomaly detection, Anomaly Transformer <ref type="bibr" coords="9,367.41,352.59,14.85,4.09" target="#b57">[58]</ref> outperforms many widely-used methods such as OmniAnomaly <ref type="bibr" coords="9,386.26,363.55,13.49,4.09" target="#b46">[47]</ref>, assuming the attention score maps show Gaussian distribution.</p><p>All of these works are designed for specific tasks, rather than functioning as a representation learning framework to serve different downstream tasks. To fill this gap, some researchers proposed a Transformer-based architecture, called TST <ref type="bibr" coords="9,502.65,418.35,13.22,4.09" target="#b60">[61]</ref>. Like RITA, TST supports regression, classification, and unsupervised learning through the "cloze test" pretraining task on timeseries. However, TST directly uses the classical Vanilla self-attention, thus not scalable to long timeseries as shown in our experiments (Sec. 6.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Efficient Transformers</head><p>The need of improving the scalability of Transformers has led to more efficient variations of Transformers, especially for accommodating long text data in NLP <ref type="bibr" coords="9,423.59,538.89,13.36,4.09" target="#b48">[49]</ref>.</p><p>Introducing fixed/random patterns to self-attention mechanism is an intuitive idea. Sparse Transformer <ref type="bibr" coords="9,458.58,560.81,10.43,4.09" target="#b8">[9]</ref> and Longformer <ref type="bibr" coords="9,530.23,560.81,10.43,4.09" target="#b2">[3]</ref> only compute attention at fixed intervals. ETC <ref type="bibr" coords="9,470.81,571.77,10.53,4.09" target="#b1">[2]</ref> and BigBird <ref type="bibr" coords="9,529.05,571.77,14.70,4.09" target="#b59">[60]</ref> use global-local attention: the attention computation is limited within a fixed radius, while some auxiliary tokens are added to attend/get attended globally. The deficiencies of fixed attention patterns are obvious: it heavily depends on users to give an optimal setting.</p><p>To decrease the reliance on human labor, some works seek to introduce learnable/adaptive attention patterns instead of fixed patterns. Reformer <ref type="bibr" coords="9,390.97,648.48,14.85,4.09" target="#b25">[26]</ref> proposed only computing the dominant attention terms based on their observation of sparsity in attention matrix from language/image data. Such sparsity is intuitive in language data, in which a word's attention mainly focuses on the nearby sentences. However, attention in timeseries data shows strong seasonal patterns rather than sparse patterns, mainly as result of the periodicity of timeseries data. Therefore, such works do not work well for timeseries.</p><p>Apart from introducing attention patterns, some works seek to solve this problem with applied mathematics techniques. Linformer <ref type="bibr" coords="10,82.20,133.41,14.85,4.09" target="#b53">[54]</ref> performs a projection to decrease the size of query, key and value matrices before attention computation, because the attention matrix tends to be low-ranked. Performer <ref type="bibr" coords="10,239.30,155.33,14.60,4.09" target="#b9">[10]</ref> uses linear functions to approximate the kernel function softmax, making attention computation commutative. When the sequence length is far greater than the dimension of embedding vectors, Performer benefits from changing the order of matrix multiplication. Linformer and Performer do not depend on the unique properties of language data, thus potentially fitting timeseries better than other techniques, which is why we compared against them in our experiments. However as shown in Sec. 6, our group attention significantly outperforms them in both accuracy and efficiency (training time), because group attention fully leverages the periodicity of timeseries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this work, we presented RITA, an automatic, self-supervised, and scalable timeseries analytics tool. RITA effectively adapts Transformer, popular in NLP, into timeseries analytics. As the key component of RITA, group attention eliminates the performance bottleneck of the classical self-attention mechanisms, thus successfully scaling RITA to highly complex, long timeseries data. Our experiments confirm that RITA significantly speeds up the state-of-the-art by 63X with a better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX: SUPPLEMENTARY MATERIAL A.1 Experiment Configuration and</head><p>Hyper-parameter Settings</p><p>Configuration. All models were trained on an NVIDIA Tesla V100 16GB GPU. All the methods are optimized with AdamW <ref type="bibr" coords="11,533.24,261.46,14.85,4.09" target="#b35">[36]</ref> of which the starting learning rate and weight decay parameter are both 1𝑒 -4 . In full-label training scenario, we train the models for 100 epochs. In "pretraining + few-label finetuning scenario", as the pretrained models require fewer epochs to converge <ref type="bibr" coords="11,509.22,305.29,13.29,4.09" target="#b60">[61]</ref>, we train the model for 50 epochs. For a fair comparison, the baselines use a maximal batch size within GPU's capacity during training.</p><p>As for model hyper-parameter setting, RITA and the baselines use a Transformer structure balancing Vanilla 's accuracy and efficiency: 8-layer stack of 2-head attention with hidden vectors in dimension of 64. Convolution kernel size is set to 5 by default. We set the error bound threshold (𝜖, Sec. 5.1) of Group Attention to 2, as it balances the accuracy and the efficiency in general on all datasets. Because Linformer requires the users to set the sizes of projection matrix, in different settings we choose an accuracyefficiency balancing one among {64,128,256,512}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Efficient Computation of Group Attention Algorithm 1 Efficient Computation of Group Attention</head><p>Require: 𝑄, 𝑉 , 𝑅, 𝐶𝑂𝑈 𝑁𝑇 , 𝐵𝐸𝐿𝑂𝑁 𝐺 Ensure: 𝑄, 𝑉 ∈ R 𝑛 * 𝑑 ,𝑅 ∈ R 𝑁 * 𝑑 ,𝐶𝑂𝑈 𝑁𝑇 ∈ N 𝑁 ,𝐵𝐸𝐿𝑂𝑁 𝐺 ∈ N 𝑛 1: function group_attention(𝑄, 𝑉 , 𝑅)</p><formula xml:id="formula_18" coords="11,322.63,518.53,4.89,3.18">2:</formula><p>for 𝑖 = 0 → 𝑁 -1 do 3:</p><formula xml:id="formula_19" coords="11,322.63,525.78,137.25,19.50">𝑣 𝑖 ← 𝑛-1 𝑗 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑗 == 𝑖 )𝑣 𝑗 4:</formula><p>𝑃 ← 𝑄𝑅 𝑇 5:</p><p>for 𝑖 = 0 → 𝑛 -1 do 6:</p><p>for 𝑗 = 0 → 𝑁 -1 do 7:</p><p>𝑤 𝑖,𝑗 ← 𝑒𝑥𝑝 ( 𝑃 𝑖,𝑗 )𝐶𝑂𝑈 𝑁𝑇 𝑗 8:</p><p>for 𝑖 = 0 → 𝑛 -1 do 9:</p><formula xml:id="formula_20" coords="11,319.39,589.95,88.28,21.18">𝑠 𝑖 ← 𝑁 -1 𝑗 =0 𝑤 𝑖,𝑗<label>10</label></formula><formula xml:id="formula_21" coords="11,324.81,607.95,2.71,3.18">:</formula><p>for 𝑖 = 0 → 𝑛 -1 do 11:</p><formula xml:id="formula_22" coords="11,319.39,613.61,113.42,23.30">𝑜 𝑖 ← 𝑁 -1 𝑗 =0 𝑒𝑥𝑝 ( 𝑃 𝑖,𝑗 ) 𝑠 𝑖 𝑣 𝑗 12:</formula><p>return 𝑂</p><p>In Alg. 1, we denote 𝐶𝑂𝑈 𝑁𝑇 𝑖 to be the size of the 𝑖 𝑡ℎ group, 𝑁 to be the number of groups, r 𝑖 to be the representative key of the 𝑖 𝑡ℎ group and R to be the matrix consisting of all r 𝑖 , 𝐵𝐸𝐿𝑂𝑁𝐺 𝑖 to be the group that k 𝑖 belongs to. 𝑄, 𝑉 are the packing matrices of query vectors and value vectors as described in Sec.2. Alg. 1 outputs the packing matrix 𝑂 for new feature emebddings {𝑜 1 , ..., 𝑜 𝑛 }, where 𝑜 𝑖 corresponds to the feature embedding of 𝑤𝑖𝑛 𝑖 . Lines 2-3 implement the embedding aggregation operation, while   We describe Alg. 3 and intuitively show its optimality. We assume that Scipy <ref type="bibr" coords="12,91.58,670.40,14.60,4.09" target="#b52">[53]</ref> learns an optimal function in Line 4 so that function COST gives the optimal estimation error when fitting the points in set 𝑆. When fitting very few points, we assign an infinite cost to prevent a biased fitting function (Line 2). 𝑔(𝑛) denotes the minimal estimation error for points in sub-plane {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑁 ≤ 𝑛}. In Lines 11-13, we enumerate all possible ways of cutting {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑁 ≤ 𝑛} horizontally into two sub-plane {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑁 ≤ 𝑖} and {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑖 ≤ 𝑁 ≤ 𝑛} by iterating 𝑖 from 1 to n. Choosing the cutting strategy that minimizes estimation error gets us a 𝑔(𝑙 1 ) with minimal estimation error for sub-plane {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑁 ≤ 𝑙 1 }, which is recorded as 𝑓 𝑙 1 ,𝑙 2 in Line 14. 𝑑𝑝 (𝑙) denotes the minimal estimation error for sub-plane {𝐿 ≤ 𝑙 }. We enumerate all the possible ways of cutting {𝐿 ≤ 𝑙 } vertically into two sub-plane {𝐿 ≤ 𝑖} and {𝑖 ≤ 𝐿 ≤ 𝑙 } by iterating 𝑖 from 1 to 𝑙 (Line 17 <ref type="bibr" coords="12,463.94,188.21,14.19,4.09">-19)</ref>. Finally, we have the minimal estimation error for the whole plane as 𝑑𝑝 (𝐿 𝑚𝑎𝑥 ). Based on the above discussion, this algorithm guarantees to not miss any better solution, hence optimal.</p><p>A. <ref type="bibr" coords="12,328.85,246.63,5.45,9.37" target="#b3">4</ref> The Correctness of Group Attention Lemma 3. Assuming the windows belonging to the same group 𝐺 𝑖 have the same key vector, i.e. 𝑘 𝑗 = 𝑟 𝑖 (𝑤𝑖𝑛 𝑗 ∈ 𝐺 𝑖 ), then the feature embedding 𝑂 produced by the original self-attention mechanism is identical to the output of our group attention mechanism implemented in Algorithm 1.</p><p>Proof. Denote 𝑘 𝑗 to be the representative vectors of 𝑘 𝑗 , i.e. 𝑘 𝑗 = 𝑟 𝑖 = 𝑘 𝑗 (𝑤𝑖𝑛 𝑗 ∈ 𝐺 𝑖 ). Algorithm 1 gives that</p><formula xml:id="formula_23" coords="12,362.05,383.05,196.63,52.01">𝑣 𝑖 = 𝑛-1 ∑︁ 𝑗 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑗 == 𝑖 )v 𝑗 , 𝑃 𝑖,𝑗 = q 𝑖 • r 𝑗 𝑠 𝑖 = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 ( 𝑃 𝑖,𝑗 )𝐶𝑂𝑈 𝑁𝑇 𝑗 , 𝑜 𝑖 = 𝑁 -1 ∑︁ 𝑗 =0 𝑃 𝑖,𝑗 𝑠 𝑖 𝑣 𝑗<label>(7)</label></formula><p>By the canonical self-attention mechanism introduced in Sec. 2, we get:</p><formula xml:id="formula_24" coords="12,349.15,491.15,209.53,22.72">𝑃 𝑖,𝑗 = q 𝑖 • k j , 𝐴 𝑖,𝑗 = 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) , o 𝑖 = 𝑛-1 ∑︁ 𝑗 =0 𝐴 𝑖,𝑗 v 𝑗<label>(8)</label></formula><p>With 7 and 8, we have</p><formula xml:id="formula_25" coords="12,345.13,558.08,213.55,151.18">𝑛-1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) = 𝑛-1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (q 𝑖 • k 𝑗 ) = 𝑁 -1 ∑︁ 𝑗 =0 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 )𝑒𝑥𝑝 (q 𝑖 • k 𝑥 ) = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (q 𝑖 • r 𝑗 ) 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 ) = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (q 𝑖 • r 𝑗 )𝐶𝑂𝑈 𝑁𝑇 𝑗 = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 ( 𝑃 𝑖,𝑗 )𝐶𝑂𝑈 𝑁𝑇 𝑗 = 𝑠 𝑖<label>(9)</label></formula><p>Further,</p><formula xml:id="formula_26" coords="13,88.83,104.88,205.69,198.47">o 𝑖 = 𝑛-1 ∑︁ 𝑗 =0 𝐴 𝑖,𝑗 v j = 𝑁 -1 ∑︁ 𝑗 =0 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 )𝐴 𝑖,𝑥 v 𝑥 = 𝑁 -1 ∑︁ 𝑗 =0 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 ) 𝑒𝑥𝑝 (𝑃 𝑖,𝑥 ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) v 𝑥 = 𝑁 -1 ∑︁ 𝑗 =0 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 ) 𝑒𝑥𝑝 (q 𝑖 • k 𝑥 ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) v 𝑥 = 𝑁 -1 ∑︁ 𝑗 =0 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 ) 𝑒𝑥𝑝 (q 𝑖 • r j ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) v 𝑥 = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (q 𝑖 • r j ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) 𝑛-1 ∑︁ 𝑥 =0 (𝐵𝐸𝐿𝑂𝑁 𝐺 𝑥 == 𝑗 )v 𝑥 = 𝑁 -1 ∑︁ 𝑗 =0 𝑒𝑥𝑝 (q 𝑖 • r j ) 𝑛-1 𝑘=0 𝑒𝑥𝑝 (𝑃 𝑖,𝑘 ) 𝑣 𝑗<label>(10)</label></formula><p>Combining ( <ref type="formula" coords="13,109.75,320.29,2.87,4.09" target="#formula_23">7</ref>), (9) (10), we have o i = N -1 j=0 P i,j s i v j = o i . This concludes that the output of our group attention is identical to vanilla self-attention's. □</p><p>A.5 The Proof of Error Bound (Lemma 1)</p><p>Proof. We have</p><formula xml:id="formula_27" coords="13,72.83,395.34,221.76,36.66">𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) = 𝑒𝑥𝑝 (q 𝑖 • k 𝑗 ) 𝑒𝑥𝑝 (q 𝑖 • k 𝑗 ) = 𝑒𝑥𝑝 (q 𝑖 • ( k 𝑗 -k 𝑗 )) = 𝑒𝑥𝑝 (||q 𝑖 || • || k 𝑗 -k 𝑗 || • 𝑐𝑜𝑠 (q 𝑖 , k 𝑗 -k 𝑗 ))<label>(11)</label></formula><p>So</p><formula xml:id="formula_28" coords="13,110.80,455.61,183.78,20.75">𝑒𝑥𝑝 (-𝑑𝑅) ≤ 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) 𝑒𝑥𝑝 (𝑃 𝑖,𝑗 ) ≤ 𝑒𝑥𝑝 (𝑑𝑅)<label>(12)</label></formula><p>Then we have: A i,j ≤ 𝜖. This proves Lemma 1.</p><p>A. <ref type="bibr" coords="13,64.69,645.84,5.45,9.37" target="#b5">6</ref> The Proof of Merge Operation (Lemma 2)</p><p>Proof. Denote the cluster size of 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑘 to be 𝑛 𝑘 .After mergeing, the new center will be: </p><formula xml:id="formula_29" coords="13,143.12,688.30,36.09,15.38">𝑐 ′ = 𝑚 𝑖=1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Downstream Tasks</head><p>RITA supports a variety of downstream tasks. In this section, we show that with minimal modification RITA can effectively support classification, imputation and forecasting tasks. Other unsupervised tasks such as similarity search or clustering are naturally supported by extracting feature embeddings from RITA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.1 Classification</head><p>To classify timeseries, we input timeseries to the model as described in Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.3 Forecasting</head><p>Forecasting can be regarded as a special case of imputation, in which all missing values are at the end of timeseries.</p><p>So like in imputation task, we scale the timeseries to nonnegative and use a special value (-1) to indicate the values to be predicted:</p><formula xml:id="formula_30" coords="14,84.49,220.58,210.10,20.85">𝑇 𝑜𝑏𝑠𝑒𝑟 𝑣𝑒𝑑 (𝑖, 𝑗) = 𝑇 𝑟𝑒𝑎𝑙 (𝑖, 𝑗) 𝑖 ≤ 𝑡 𝑜𝑏𝑠𝑒𝑟 𝑣𝑒𝑑 -1 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒<label>(18)</label></formula><p>Where 𝑡 𝑜𝑏𝑠𝑒𝑟 𝑣𝑒𝑑 is the observed timestamp. Then the output representations are fed into a Transpose Convolution layer using Mean Squared Error as loss function, as described above.</p><p>A.7.4 Other Unsupervised Tasks RITA naturally supports other unsupervised tasks, such as similarity search and clustering <ref type="bibr" coords="14,145.47,311.54,13.46,4.09" target="#b24">[25,</ref><ref type="bibr" coords="14,161.17,311.54,10.30,4.09" target="#b30">31,</ref><ref type="bibr" coords="14,173.70,311.54,10.10,4.09" target="#b31">32]</ref>, by producing the embedding of one timeseries (output representation of the special token [CLS]).</p><p>Clustering can be performed on the embeddings with flexible choice of distance metrics. Similarly, a high dimensional similarity search system <ref type="bibr" coords="14,81.58,355.38,13.50,4.09" target="#b21">[22,</ref><ref type="bibr" coords="14,97.32,355.38,10.31,4.09" target="#b22">23,</ref><ref type="bibr" coords="14,109.87,355.38,11.53,4.09" target="#b37">38]</ref> can be built on the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Inference Time</head><p>Dataset Length TST <ref type="bibr" coords="14,396.99,114.31,13.31,3.20" target="#b60">[61]</ref>    In this section, we present the average inference time on validation sets. The results in Table. 6 and 7 correspond to the average inference time on validation sets of classification and imputation tasks, respectively. Consistent with the results in Section. 6.3, our method Group Attn. outperforms the baselines on both classification and imputation tasks, particularly on the datasets comprising long timeseries (ECG and MGH).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,381.55,286.35,113.07,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: RITA Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,121.45,212.45,104.95,7.70;4,53.80,231.13,236.37,9.37"><head></head><label></label><figDesc>Figure 2: Group Attention 4.2 Computing the Output Feature Embedding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,53.80,205.41,240.20,7.70;7,181.25,92.93,112.06,93.20"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Full-label classification results (multi-variate data).</figDesc><graphic coords="7,181.25,92.93,112.06,93.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,86.80,365.30,174.25,7.70;8,184.99,234.05,112.98,116.19"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Varying the lengths of timeseries.</figDesc><graphic coords="8,184.99,234.05,112.98,116.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,317.96,338.26,241.85,7.70;8,317.72,349.22,52.70,7.70;8,448.00,225.19,91.25,105.93"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison to non-deep learning method (univariate data).</figDesc><graphic coords="8,448.00,225.19,91.25,105.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="13,345.36,381.68,214.36,7.70;13,317.96,392.64,240.25,7.70;13,317.96,403.60,240.25,7.70;13,317.96,414.38,226.83,9.94;13,544.42,414.30,13.78,6.44;13,317.96,426.05,240.25,7.70;13,317.96,436.78,32.28,9.94;13,350.24,435.12,13.63,6.43;13,364.52,436.78,32.68,9.94;13,397.20,435.12,161.00,8.01;13,317.96,447.92,107.08,8.58;13,424.60,447.84,133.61,6.44;13,317.96,461.15,139.51,4.09;13,327.92,472.11,231.80,4.09;13,317.96,479.23,87.96,10.35;13,401.87,485.96,4.48,6.43;13,416.67,479.30,4.48,6.43;13,416.67,485.88,9.69,6.43;13,428.35,481.17,130.08,7.70;13,317.96,494.40,116.96,4.09;13,366.95,513.45,21.06,7.93;13,396.16,509.47,93.12,4.17;13,396.16,522.62,49.06,4.17;13,545.06,515.72,13.68,4.09;13,317.96,537.37,72.52,8.06;13,320.20,550.19,238.00,4.09;13,317.96,561.15,240.48,4.09;13,317.96,572.11,241.63,4.09;13,317.69,583.07,125.70,4.09;13,327.92,594.03,110.43,5.50;13,442.36,591.76,13.46,8.58;13,455.56,589.70,14.32,6.25;13,470.64,594.03,87.55,4.09;13,317.62,604.99,94.07,5.50;13,415.71,602.72,13.75,8.58;13,429.21,600.66,14.32,6.25;13,444.29,604.99,114.87,4.09;13,317.96,615.95,240.24,4.17;13,317.96,626.91,196.92,4.09;13,371.64,648.38,7.92,5.34;13,381.06,645.96,24.47,7.93;13,413.68,639.71,9.87,7.70;13,448.02,639.71,33.94,7.70;13,412.97,655.28,7.18,5.34;13,421.94,652.86,59.66,7.93;13,545.06,648.23,13.68,4.09;13,327.20,670.40,232.51,5.50;13,317.96,679.58,240.42,7.93;13,317.62,692.32,240.58,4.09;13,317.96,703.28,240.24,4.09;14,53.80,87.31,163.57,9.37;14,210.29,87.52,25.50,9.16;14,228.72,87.52,23.09,7.86;14,244.74,87.31,49.31,9.37;14,53.40,98.82,22.24,8.58;14,75.38,96.76,14.32,6.25;14,92.70,101.09,122.38,5.50;14,218.76,98.82,13.46,8.58;14,231.85,98.74,62.19,6.44;14,53.80,112.05,50.79,4.09;14,63.76,123.01,231.34,4.09;14,53.66,132.05,12.77,7.74;14,74.26,131.37,3.38,3.32;14,70.63,131.86,90.02,11.04;14,162.44,131.64,25.96,7.92"><head></head><label></label><figDesc>3 and attach a special token [CLS] as the first input embedding. [CLS]'s embedding acts as the embedding for the entire timeseries, and the output representation of [CLS] is fed into a classifier: y = Softmax (W cls Z [CLS] + B cls ), where 𝑍 [𝐶𝐿𝑆 ] ∈ R 𝑑 is the output representation of [CLS], C is the number of classes, and W cls ∈ R C×d , B cls ∈ R C are learnable parameters for classification task. The result vector 𝑦 ∈ R 𝐶 represents the possibility that the input timeseries belongs to each class.We apply Cross Entropy Loss as the loss function of the classification task<ref type="bibr" coords="13,366.29,483.44,13.60,4.09" target="#b12">[13]</ref>:L = 1 C C i=1 -ŷ(i)log(y(i)), where ŷ is a binary indicator for ground truth label:ŷ (𝑖) = 1 𝑖 is ground truth label 0 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 (16)A.7.2 Imputation Timeseries are mainly generated by sensors, a common problem of which is missing values. This becomes a challenge when many downstream analytics require the missing values to be recovered. The recovering task is imputation.Denote the real timeseries as 𝑇 𝑟 ∈ R 𝑡 ×𝑚 , the observed timeseries with missing values as 𝑇 𝑜 ∈ R 𝑡 ×𝑚 , and the set of missing values' positions as 𝑀. We scale the values of all timeseries to non-negative and use a special value (-1) to indicate missing values:𝑇 𝑜 (𝑖, 𝑗) = -1 (𝑖, 𝑗) ∈ 𝑀 𝑇 𝑟 (𝑖, 𝑗) (𝑖, 𝑗) ∉ 𝑀(17)𝑇 𝑜 is fed into the RITA as input, and the output representations are concatenated and fed into a Transpose Convolution layer which decodes the output embedding vectors from hidden space to timeseries values, corresponding to the convolution operation in the input stage, i.e., Y = TransposeCNN (Z 1 + ○Z 2 + ○... + ○Z n ), where 𝑌 ∈ R 𝑡 ×𝑚 is the recovered timeseries, and 𝑍 𝑖 ∈ R 𝑑 is the output of each position. Here Mean Square Error is chosen as the loss function [51]: 𝐿 = 1 |𝑀 | (𝑖,𝑗 ) ∈𝑀 (𝑌 (𝑖, 𝑗) -𝑇 𝑟 (𝑖, 𝑗)) 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,53.57,199.91,240.71,52.93"><head></head><label></label><figDesc>𝑑 𝑣 are projection matrices for generating 𝑄, 𝐾, 𝑉 . 𝑄 ∈ R 𝑛 * 𝑑 𝑘 is also regarded as the packing of 𝑛 query vectors {𝑞 1 , ..., 𝑞 𝑛 } with dimension 𝑑 𝑘 into a matrix. 𝐾 ∈ R 𝑛 * 𝑑 𝑘 , 𝑉 ∈ R 𝑛 * 𝑑 𝑣 are regarded as the packing of key vectors {𝑘 1 , ..., 𝑘 𝑛 } and value vectors {𝑣 1 , ..., 𝑣 𝑛 } in the same way.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,317.55,188.21,241.63,89.29"><head></head><label></label><figDesc>.3. It only requires users to set an error bound 𝜖, and then uses Lemma 1 to translate 𝜖 to a distance threshold 𝑑. RITA then uses Lemma 2 to determine if merging some given clusters still meets the error bound threshold 𝜖. Lemma 2. Denote 𝑐 𝑘 to be the cluster center of 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑘 . Assume the existing grouping satisfies ∀k, max x ∈cluster k |c kx | ≤ d , thus satisfying an error bound 𝜖 by Lemma 1. If there exist 𝑚 clusters, namely, 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑘 1 , 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑘 2 , ..., 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑘 𝑚 , satisfying that:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,317.62,358.68,242.10,87.73"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="6,337.75,358.68,200.65,43.43"><row><cell>Dataset</cell><cell cols="5">Train. Size Valid. Size Length Channel Classes</cell></row><row><cell>WISDM</cell><cell>28,280</cell><cell>3,112</cell><cell>200</cell><cell>3</cell><cell>18</cell></row><row><cell>HHAR</cell><cell>20,484</cell><cell>2,296</cell><cell>200</cell><cell>3</cell><cell>5</cell></row><row><cell>RWHAR</cell><cell>27,253</cell><cell>3,059</cell><cell>200</cell><cell>3</cell><cell>8</cell></row><row><cell>ECG</cell><cell>31,091</cell><cell>3,551</cell><cell>2000</cell><cell>12</cell><cell>9</cell></row><row><cell>MGH</cell><cell>8,550</cell><cell>950</cell><cell>10000</cell><cell>21</cell><cell>N/A</cell></row></table><note coords="6,397.47,409.80,114.42,7.70;6,317.62,427.74,242.10,7.70;6,317.96,438.70,241.63,7.70"><p><p><p>The statistics of the datasets Alternative Methods. We compare RITA against the SOTA Transformer based timeseries representation learning method TST</p><ref type="bibr" coords="6,542.94,440.26,13.32,4.09" target="#b60">[61]</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,443.08,703.28,115.12,4.09"><head>Table 2 :</head><label>2</label><figDesc>.2, Group Attn. is more accurate Imputation results (multi-variate data). The best results are marked with bold.</figDesc><table coords="8,117.35,83.68,377.30,122.77"><row><cell></cell><cell></cell><cell></cell><cell cols="2">TST [61]</cell><cell cols="2">Vanilla</cell><cell></cell><cell></cell><cell cols="2">Performer</cell><cell cols="2">Linformer</cell><cell>Group Attn.</cell></row><row><cell>Dataset</cell><cell></cell><cell>Length</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>MSE</cell><cell>Time/s</cell><cell>MSE</cell><cell cols="2">Time/s</cell><cell></cell><cell>MSE</cell><cell>Time/s</cell><cell>MSE</cell><cell>Time/s</cell><cell>MSE</cell><cell>Time/s</cell></row><row><cell>WISDM</cell><cell></cell><cell>200</cell><cell>13.30</cell><cell>150.3</cell><cell>3.240</cell><cell cols="2">178.1</cell><cell></cell><cell>3.449</cell><cell>162.6</cell><cell>3.852</cell><cell>141.9</cell><cell>3.277</cell><cell>136.7</cell></row><row><cell>HHAR</cell><cell></cell><cell>200</cell><cell>1.085</cell><cell>78.2</cell><cell>0.2968</cell><cell cols="2">97.4</cell><cell></cell><cell>0.2980</cell><cell>82.6</cell><cell>0.3198</cell><cell>81.1</cell><cell>0.2974</cell><cell>73.3</cell></row><row><cell cols="2">RWHAR</cell><cell>200</cell><cell>0.0882</cell><cell>83.9</cell><cell>0.0478</cell><cell cols="2">108.1</cell><cell></cell><cell>0.0489</cell><cell>89.1</cell><cell>0.0572</cell><cell>98.4</cell><cell>0.0478</cell><cell>81.3</cell></row><row><cell>ECG</cell><cell></cell><cell>2000</cell><cell>0.0905</cell><cell>696.3</cell><cell>0.0037</cell><cell cols="2">857.9</cell><cell></cell><cell>0.0033</cell><cell>270.2</cell><cell>0.0035</cell><cell>291.38</cell><cell>0.0038</cell><cell>164.36</cell></row><row><cell>MGH</cell><cell></cell><cell>10000</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">N/A</cell><cell cols="2">0.00014</cell><cell>356.2</cell><cell>0.00088</cell><cell>404.9</cell><cell>0.00042</cell><cell>54.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TST [61]</cell><cell cols="3">Vanilla</cell><cell></cell><cell cols="2">Performer</cell><cell cols="2">Linformer</cell><cell>Group Attn.</cell></row><row><cell>Dataset</cell><cell cols="2">Pretrain Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Scratch</cell><cell>Pre.</cell><cell cols="2">Scratch</cell><cell>Pre.</cell><cell></cell><cell>Scratch</cell><cell>Pre.</cell><cell>Scratch</cell><cell>Pre.</cell><cell>Scratch</cell><cell>Pre.</cell></row><row><cell>WISDM</cell><cell></cell><cell>62,231</cell><cell>49.13%</cell><cell>50.03%</cell><cell>66.16%</cell><cell></cell><cell cols="2">75.89%</cell><cell>66.09%</cell><cell>73.97%</cell><cell>50.12%</cell><cell>67.44%</cell><cell>62.56%</cell><cell>75.06%</cell></row><row><cell>HHAR</cell><cell></cell><cell>68,294</cell><cell>72.56%</cell><cell>75.30%</cell><cell>75.60%</cell><cell></cell><cell>81.35%</cell><cell></cell><cell>76.52%</cell><cell>80.70%</cell><cell>65.94%</cell><cell>76.52%</cell><cell>76.17%</cell><cell>82.62%</cell></row><row><cell>RWHAR</cell><cell></cell><cell>63,599</cell><cell>69.46%</cell><cell>80.41%</cell><cell>85.68%</cell><cell></cell><cell>91.14%</cell><cell></cell><cell>87.54%</cell><cell>91.33%</cell><cell>81.03%</cell><cell>86.33%</cell><cell>86.13%</cell><cell>89.63%</cell></row><row><cell>ECG</cell><cell></cell><cell>561,358</cell><cell>20.98%</cell><cell>27.99%</cell><cell>42.05%</cell><cell></cell><cell>46.16%</cell><cell></cell><cell>43.34%</cell><cell>45.58%</cell><cell>27.19%</cell><cell>31.34%</cell><cell>42.58%</cell><cell>46.39%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,55.15,212.54,425.14,102.58"><head>Table 3 :</head><label>3</label><figDesc>Pretrain + few-label finetuning results. The best results are marked with bold.</figDesc><table coords="8,55.15,268.25,131.88,46.88"><row><cell>MSE</cell><cell>Training Time/sec</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,90.87,217.29,165.81,82.22"><head>Table 4 :</head><label>4</label><figDesc>Adaptive Scheduling VS Fixed N.</figDesc><table coords="9,115.36,247.71,117.13,51.80"><row><cell cols="2">Pretrain Data size Few-label Accuracy</cell></row><row><cell>N/A</cell><cell>62.56%</cell></row><row><cell>12,446</cell><cell>72.94%</cell></row><row><cell>24,892</cell><cell>72.78%</cell></row><row><cell>37,338</cell><cell>74.10%</cell></row><row><cell>49,784</cell><cell>74.22%</cell></row><row><cell>62,231</cell><cell>75.06%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="12,53.47,111.50,240.58,528.33"><head></head><label></label><figDesc>implement the group softmax function. : function dynamic_programming(𝐿 𝑖 , 𝑁 𝑖 , 𝐿 𝑚𝑎𝑥 ) 6:for 𝑙 1 = 1 → 𝐿 𝑚𝑎𝑥 do 7:for 𝑙 2 = 1 → 𝑙 1 do 8:for𝑛 = 1 → 𝑙 1 do 9: 𝑆 ← 𝑝𝑜𝑖𝑛𝑡𝑠 𝑠𝑒𝑡 𝑖𝑛 {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑁 ≤ 𝑛} 𝑝𝑜𝑖𝑛𝑡𝑠 𝑠𝑒𝑡 𝑖𝑛 {𝑙 2 ≤ 𝐿 ≤ 𝑙 1 , 𝑖 ≤ 𝑁 ≤ 𝑛}</figDesc><table coords="12,53.47,140.82,232.21,499.01"><row><cell cols="4">A.3 The Algorithms and Optimality Proof for</cell></row><row><cell></cell><cell cols="3">Dynamically Determing Batch Size</cell></row><row><cell cols="4">Algorithm 2 Binary Search for Batch Size</cell></row><row><cell cols="2">Require: 𝐿, 𝑁</cell><cell></cell></row><row><cell cols="4">Ensure: 1 ≤ 𝐿 ≤ 𝐿 𝑚𝑎𝑥 , 1 ≤ 𝑁 ≤ 𝐿</cell></row><row><cell cols="4">1: function binary_search(𝐿, 𝑁 )</cell></row><row><cell>2:</cell><cell>𝐿 ← 1</cell><cell></cell></row><row><cell>3:</cell><cell cols="3">𝑅 ← 𝑀𝑎𝑥𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒</cell></row><row><cell>4:</cell><cell cols="3">𝑑𝑎𝑡𝑎 ← 𝑅𝑎𝑛𝑑𝑜𝑚𝑇𝑖𝑚𝑒𝑆𝑒𝑟𝑖𝑒𝑠 𝑖𝑛 𝑙𝑒𝑛𝑔𝑡ℎ 𝐿</cell></row><row><cell>5:</cell><cell cols="2">𝐵 𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙</cell></row><row><cell>6:</cell><cell cols="2">while 𝐿 ≤ 𝑅 do</cell></row><row><cell>7:</cell><cell cols="3">𝐼𝑛𝑝𝑢𝑡 ← 𝑑𝑎𝑡𝑎 × 𝐵 𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙</cell></row><row><cell>8:</cell><cell cols="3">𝑀𝑜𝑑𝑒𝑙𝐹𝑜𝑟𝑤𝑎𝑟𝑑 (𝐼𝑛𝑝𝑢𝑡)</cell></row><row><cell>9:</cell><cell cols="3">𝑀𝑜𝑑𝑒𝑙𝐵𝑎𝑐𝑘𝑤𝑎𝑟𝑑</cell></row><row><cell>10:</cell><cell>𝑢 ←</cell><cell cols="2">𝑃𝑒𝑎𝑘𝑀𝑒𝑚𝑜𝑟 𝑦𝑈 𝑠𝑎𝑔𝑒 𝑇 𝑜𝑡𝑎𝑙𝑀𝑒𝑚𝑜𝑟 𝑦</cell></row><row><cell>11:</cell><cell cols="3">if 0.9 &gt; 𝑢 then</cell></row><row><cell>12:</cell><cell cols="3">𝐿 ← 𝐵 𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙 + 1</cell></row><row><cell>13:</cell><cell cols="3">𝐵 ← 𝐵 𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙</cell></row><row><cell>14:</cell><cell>else</cell><cell></cell></row><row><cell>15:</cell><cell cols="3">𝑅 ← 𝐵 𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙 -1</cell></row><row><cell>16:</cell><cell cols="2">𝐵 𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙 ←</cell><cell>⌊𝐿+𝑅 ⌋ 2</cell></row><row><cell>17:</cell><cell>return 𝐵</cell><cell></cell></row><row><cell cols="4">Algorithm 3 Dynamic Programming for Plane Division</cell></row><row><cell>10:</cell><cell cols="3">𝑔 (𝑛) ← 𝐶𝑂𝑆𝑇 (𝑆 )</cell></row><row><cell>11:</cell><cell cols="3">for 𝑖 = 1 → 𝑛 do</cell></row><row><cell cols="4">12: 𝑆 ← 13: 𝑔 (𝑛) ← 𝑚𝑖𝑛 (𝑔 (𝑛), 𝑔 (𝑖 ) + 𝐶𝑂𝑆𝑇 (𝑆 ) )</cell></row><row><cell>14:</cell><cell cols="2">𝑓 𝑙 2 ,𝑙 1 ← 𝑔 (𝑙 1 )</cell></row><row><cell>15:</cell><cell></cell><cell></cell></row><row><cell>16:</cell><cell cols="2">for 𝑙 = 1 → 𝐿 𝑚𝑎𝑥 do</cell></row><row><cell>17:</cell><cell cols="2">𝑑𝑝 (𝑙 ) ← 𝑓 (1, 𝑙 )</cell></row><row><cell>18:</cell><cell cols="2">for 𝑖 = 1 → 𝑙 do</cell></row><row><cell>19:</cell><cell cols="3">𝑑𝑝 (𝑙 ) ← 𝑚𝑖𝑛 (𝑑𝑝 (𝑙 ), 𝑑𝑝 (𝑖 ) + 𝑓 (𝑖, 𝑙 ) )</cell></row><row><cell></cell><cell cols="2">return 𝑑𝑝 (𝐿 𝑚𝑎𝑥 )</cell></row></table><note coords="12,53.80,452.15,76.50,5.99;12,53.80,459.36,112.19,6.75;12,57.28,468.09,58.83,5.99;12,57.28,475.30,101.33,6.75;12,57.28,484.70,85.41,6.25;12,57.28,492.67,115.62,6.25;12,79.60,500.64,60.31,6.75;12,57.28,512.28,2.44,3.18"><p>Require: 𝐿 𝑖 , 𝑁 𝑖 , 𝐵 𝑖 , 𝐿 𝑚𝑎𝑥 Ensure: 1 ≤ 𝐿 𝑖 ≤ 𝐿 𝑚𝑎𝑥 , 1 ≤ 𝑁 𝑖 ≤ 𝐿 𝑖 1: function cost(S) 2: if |𝑆 | &lt; 𝑀 then return +∞ 3: 𝐿, 𝑁 , 𝐵 ← 𝑝𝑜𝑖𝑛𝑡𝑠 𝑖𝑛 𝑆 4: 𝑓 ← 𝑓 𝑢𝑛𝑐𝑡𝑖𝑜𝑛 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 (𝐵 |𝐿, 𝑁 ) return 𝐸 (𝐵, 𝐿, 𝑁 | 𝑓 ) 5</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="13,173.83,87.31,384.85,624.06"><head></head><label></label><figDesc>𝑛 𝑘 𝑖 𝑐 𝑘 𝑖 𝑚 𝑖=1 𝑛 𝑘 𝑖 For ∀𝑖 ∈ [1, 𝑚], ∀𝑥 ∈ 𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑘 𝑖 , it holds that: |𝑥 -𝑐 ′ | ≤ |𝑥 -𝑐 𝑘 𝑖 | + |𝑐 𝑘 𝑖 -𝑐 ′ | (𝑇 𝑟𝑖𝑎𝑛𝑔𝑙𝑒 𝑖𝑛𝑒𝑞𝑢𝑎𝑙𝑖𝑡 𝑦) = |𝑥 -𝑐 𝑘 𝑖 | + | 𝑚 𝑗 =1 𝑛 𝑘 𝑗 𝑚 𝑗 =1 𝑛 𝑘 𝑗 𝑐 𝑘 𝑖 -𝑚 𝑗 =1 𝑛 𝑘 𝑗 𝑐 𝑘 𝑗 𝑚 𝑗 =1 𝑛 𝑘 𝑗 | = |𝑥 -𝑐 𝑘 𝑖 | + | 𝑚 𝑗 =1 𝑛 𝑘 𝑗 (𝑐 𝑘 𝑖 -𝑐 𝑘 𝑗 ) 𝑐 𝑘 𝑖 | + | 𝑚 𝑗 =1 𝑛 𝑘 𝑗 (𝑐 𝑘 𝑖 -𝑐 𝑘 𝑗 ) | 𝑚 𝑗 =1 𝑛 𝑘 𝑗 ≤ |𝑥-𝑐 𝑘 𝑖 | + 𝑚 𝑗 =1 𝑛 𝑘 𝑗 |𝑐 𝑘 𝑖 -𝑐 𝑘 𝑗 | 𝑚 𝑗 =1 𝑛 𝑘 𝑗 = 𝑚 𝑗 =1 𝑛 𝑘 𝑗 ( |𝑐 𝑘 𝑖 -𝑐 𝑘 𝑗 | + |𝑥 -𝑐 𝑘 𝑖 | )</figDesc><table coords="13,373.52,147.72,185.17,118.02"><row><cell></cell><cell></cell><cell>𝑗 =1 𝑛 𝑘 𝑗 𝑚</cell><cell>|</cell></row><row><cell cols="3">𝑗 =1 𝑛 𝑘 𝑗 = |𝑥 -𝑚</cell><cell>(15)</cell></row><row><cell>≤</cell><cell>𝑚 𝑗 =1 𝑛 𝑘 𝑗 𝑑 𝑗 =1 𝑛 𝑘 𝑗 𝑚</cell><cell>= 𝑑</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="14,324.09,114.31,235.46,38.62"><head></head><label></label><figDesc>Vanilla Performer Linformer Group Attn.</figDesc><table coords="14,324.09,123.42,223.80,29.50"><row><cell>WISDM</cell><cell>200</cell><cell>2.18</cell><cell>2.26</cell><cell>2.35</cell><cell>2.22</cell><cell>2.17</cell></row><row><cell>HHAR</cell><cell>200</cell><cell>1.19</cell><cell>1.23</cell><cell>1.28</cell><cell>1.21</cell><cell>1.18</cell></row><row><cell>RWHAR</cell><cell>200</cell><cell>1.32</cell><cell>1.37</cell><cell>1.42</cell><cell>1.34</cell><cell>1.31</cell></row><row><cell>ECG</cell><cell>2000</cell><cell>18.44</cell><cell>15.26</cell><cell>5.80</cell><cell>6.08</cell><cell>5.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="14,317.66,160.58,241.90,108.94"><head>Table 6 :</head><label>6</label><figDesc>Inference time: Classification on multi-variate data (seconds).</figDesc><table coords="14,324.09,222.14,235.46,47.38"><row><cell cols="7">Dataset Length TST[61] Vanilla Performer Linformer Group Attn.</cell></row><row><cell>WISDM</cell><cell>200</cell><cell>2.03</cell><cell>2.11</cell><cell>2.19</cell><cell>2.07</cell><cell>2.02</cell></row><row><cell>HHAR</cell><cell>200</cell><cell>1.11</cell><cell>1.14</cell><cell>1.19</cell><cell>1.12</cell><cell>1.10</cell></row><row><cell>RWHAR</cell><cell>200</cell><cell>1.23</cell><cell>1.27</cell><cell>1.32</cell><cell>1.25</cell><cell>1.22</cell></row><row><cell>ECG</cell><cell>2000</cell><cell>17.22</cell><cell>14.32</cell><cell>4.73</cell><cell>4.99</cell><cell>4.11</cell></row><row><cell>MGH</cell><cell>10000</cell><cell>N/A</cell><cell>N/A</cell><cell>6.58</cell><cell>6.88</cell><cell>1.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="14,317.66,277.18,240.54,18.66"><head>Table 7 :</head><label>7</label><figDesc>Inference time: Imputation on multi-variate data (seconds).</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,69.23,417.26,225.58,3.18;10,69.23,425.23,225.89,3.18;10,69.23,433.20,224.81,3.18;10,69.23,439.79,135.29,6.16" xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName coords=""><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016).</note>
</biblStruct>

<biblStruct coords="10,69.23,449.14,225.58,3.18;10,69.23,457.11,225.88,3.18;10,69.23,463.70,224.81,6.16;10,69.23,471.67,66.98,6.16" xml:id="b1">
	<analytic>
		<title level="a" type="main">ETC: Encoding Long and Structured Inputs in Transformers</title>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.19</idno>
		<idno type="arXiv">arXiv:2004.08483</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: Encoding long and structured inputs in transformers. arXiv preprint arXiv:2004.08483 (2020).</note>
</biblStruct>

<biblStruct coords="10,69.23,481.02,225.99,3.18;10,69.23,487.61,176.00,6.16" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">Longformer: The longdocument transformer</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long- document transformer. arXiv preprint arXiv:2004.05150 (2020).</note>
</biblStruct>

<biblStruct coords="10,69.23,496.96,225.58,3.18;10,69.23,504.93,224.81,3.18;10,68.99,511.52,225.06,6.16;10,69.23,519.49,147.90,6.16" xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.</note>
</biblStruct>

<biblStruct coords="10,69.23,528.84,224.81,3.18;10,69.23,536.81,225.99,3.18;10,69.23,543.40,224.81,6.16;10,69.23,551.37,78.91,6.16" xml:id="b4">
	<analytic>
		<title level="a" type="main">Time Series Forecasting for Healthcare Diagnosis and Prognostics with the Focus on Cardiovascular Diseases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-10-4361-1_138</idno>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on the Development of Biomedical Engineering in Vietnam (BME6)</title>
		<imprint>
			<publisher>Springer Singapore</publisher>
			<date type="published" when="2017-09-24">2017</date>
			<biblScope unit="page" from="809" to="818" />
		</imprint>
	</monogr>
	<note type="raw_reference">C Bui, N Pham, A Vo, A Tran, A Nguyen, and T Le. 2017. Time series forecasting for healthcare diagnosis and prognostics with the focus on cardiovascular dis- eases. In International conference on the development of biomedical engineering in Vietnam. Springer, 809-818.</note>
</biblStruct>

<biblStruct coords="10,69.23,560.72,224.81,3.18;10,69.23,568.69,225.58,3.18;10,69.23,576.66,224.81,3.18;10,69.23,583.25,225.58,6.16;10,69.23,592.60,14.51,3.18" xml:id="b5">
	<analytic>
		<title level="a" type="main">Smile</title>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sungtae</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yizhou</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wendong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leilani</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Remco</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Westover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Stonebraker</surname></persName>
		</author>
		<idno type="DOI">10.14778/3352063.3352138</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<title level="j" type="abbrev">Proc. VLDB Endow.</title>
		<idno type="ISSN">2150-8097</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2230" to="2241" />
			<date type="published" when="2019-08">2019. 2019</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Lei Cao, Wenbo Tao, Sungtae An, Jing Jin, Yizhou Yan, Xiaoyu Liu, Wendong Ge, Adam Sah, Leilani Battle, Jimeng Sun, Remco Chang, M. Brandon Westover, Samuel Madden, and Michael Stonebraker. 2019. Smile: A System to Support Machine Learning on EEG Data at Scale. Proc. VLDB Endow. 12, 12 (2019), 2230- 2241.</note>
</biblStruct>

<biblStruct coords="10,69.23,600.57,225.63,3.18;10,69.23,607.16,224.81,6.16;10,69.23,615.13,80.14,6.16" xml:id="b6">
	<analytic>
		<title level="a" type="main">Brits: Bidirectional recurrent imputation for time series</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yitan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018. Brits: Bidirectional recurrent imputation for time series. Advances in neural information processing systems 31 (2018).</note>
</biblStruct>

<biblStruct coords="10,69.23,623.10,201.09,6.16" xml:id="b7">
	<monogr>
		<title level="m" type="main">Time-Series Forecasting</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Chatfield</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781420036206</idno>
		<imprint>
			<date type="published" when="2000-10-25">2000</date>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Chris Chatfield. 2000. Time-series forecasting. Chapman and Hall/CRC.</note>
</biblStruct>

<biblStruct coords="10,69.23,632.45,224.81,3.18;10,69.23,639.04,223.19,6.16" xml:id="b8">
	<analytic>
		<title level="a" type="main">arXiv</title>
		<author>
			<persName coords=""><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="DOI">10.1090/mbk/121/79</idno>
		<idno type="arXiv">arXiv:1904.10509</idno>
	</analytic>
	<monogr>
		<title level="m">100 Years of Math Milestones</title>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="2019-06-12">2019. 2019</date>
			<biblScope unit="page" from="433" to="437" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).</note>
</biblStruct>

<biblStruct coords="10,69.23,648.39,225.58,3.18;10,68.99,656.36,225.82,3.18;10,69.23,662.95,224.81,6.16;10,69.23,670.92,66.99,6.16" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<title level="m">Rethinking attention with performers</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint arXiv:2009.14794 (2020).</note>
</biblStruct>

<biblStruct coords="10,69.23,680.27,225.02,3.18;10,69.23,686.86,223.24,6.16" xml:id="b10">
	<analytic>
		<title level="a" type="main">Anomaly Detection for IoT Time-Series Data: A Survey</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
			<idno type="ORCID">0000-0002-0327-1637</idno>
		</author>
		<author>
			<persName><forename type="first">Goksel</forename><surname>Misirli</surname></persName>
			<idno type="ORCID">0000-0002-2454-7188</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhong</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1109/jiot.2019.2958185</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<title level="j" type="abbrev">IEEE Internet Things J.</title>
		<idno type="ISSNe">2372-2541</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="6481" to="6494" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Andrew A Cook, Göksel Mısırlı, and Zhong Fan. 2019. Anomaly detection for IoT time-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 6481-6494.</note>
</biblStruct>

<biblStruct coords="10,69.23,694.83,224.81,6.16;10,69.23,702.80,84.99,6.16" xml:id="b11">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName coords=""><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00994018</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<title level="j" type="abbrev">Mach Learn</title>
		<idno type="ISSN">0885-6125</idno>
		<idno type="ISSNe">1573-0565</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995-09">1995. 1995</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine learning 20, 3 (1995), 273-297.</note>
</biblStruct>

<biblStruct coords="10,333.39,89.10,224.81,6.16;10,333.39,97.07,200.33,6.16" xml:id="b12">
	<analytic>
		<title level="a" type="main">The regression analysis of binary sequences</title>
		<author>
			<persName coords=""><surname>David R Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="1958">1958. 1958</date>
		</imprint>
	</monogr>
	<note type="raw_reference">David R Cox. 1958. The regression analysis of binary sequences. Journal of the Royal Statistical Society: Series B (Methodological) 20, 2 (1958), 215-232.</note>
</biblStruct>

<biblStruct coords="10,333.39,106.42,225.58,3.18;10,333.39,114.39,224.81,3.18;10,333.39,120.98,211.36,6.16" xml:id="b13">
	<analytic>
		<title level="a" type="main">The individual over time: time series applications in health care research</title>
		<author>
			<persName coords=""><surname>Benjamin F Crabtree</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Subhash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Priscilla</forename><forename type="middle">M</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T O'</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of clinical epidemiology</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="260" />
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Benjamin F Crabtree, Subhash C Ray, Priscilla M Schmidt, Patrick T O&apos;Connor, and David D Schmidt. 1990. The individual over time: time series applications in health care research. Journal of clinical epidemiology 43, 3 (1990), 241-260.</note>
</biblStruct>

<biblStruct coords="10,333.39,130.33,225.99,3.18;10,333.39,138.30,224.81,3.18;10,333.39,144.89,162.98,6.16" xml:id="b14">
	<analytic>
		<title level="a" type="main">ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels</title>
		<author>
			<persName coords=""><forename type="first">Angus</forename><surname>Dempster</surname></persName>
			<idno type="ORCID">0000-0002-6809-0482</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">François</forename><surname>Petitjean</surname></persName>
			<idno type="ORCID">0000-0001-5334-3574</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
			<idno type="ORCID">0000-0001-9963-5169</idno>
		</author>
		<idno type="DOI">10.1007/s10618-020-00701-z</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<title level="j" type="abbrev">Data Min Knowl Disc</title>
		<idno type="ISSN">1384-5810</idno>
		<idno type="ISSNe">1573-756X</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1454" to="1495" />
			<date type="published" when="2020-07-13">2020. 2020</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Angus Dempster, François Petitjean, and Geoffrey I. Webb. 2020. ROCKET: excep- tionally fast and accurate time series classification using random convolutional kernels. Data Min. Knowl. Discov. 34, 5 (2020), 1454-1495.</note>
</biblStruct>

<biblStruct coords="10,333.39,154.24,225.63,3.18;10,333.39,162.21,224.81,3.18;10,333.39,168.80,224.81,6.16;10,333.39,176.77,225.57,6.16;10,333.39,184.74,225.57,6.16;10,333.23,194.09,14.51,3.18" xml:id="b15">
	<analytic>
		<title level="a" type="main">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology companion volume of the Proceedings of HLT-NAACL 2003--short papers - NAACL &apos;03</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073483</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
	<note type="raw_reference">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). 4171- 4186.</note>
</biblStruct>

<biblStruct coords="10,333.39,202.06,225.99,3.18;10,333.39,208.65,225.51,6.16;10,333.39,216.62,151.01,6.16" xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties</title>
		<author>
			<persName coords=""><forename type="first">Evelyn</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Hodges</surname></persName>
		</author>
		<idno type="DOI">10.2307/1403797</idno>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review / Revue Internationale de Statistique</title>
		<title level="j" type="abbrev">International Statistical Review / Revue Internationale de Statistique</title>
		<idno type="ISSN">0306-7734</idno>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">238</biblScope>
			<date type="published" when="1989-12">1989. 1989</date>
			<publisher>JSTOR</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Evelyn Fix and Joseph Lawson Hodges. 1989. Discriminatory analysis. Nonpara- metric discrimination: Consistency properties. International Statistical Review/Re- vue Internationale de Statistique 57, 3 (1989), 238-247.</note>
</biblStruct>

<biblStruct coords="10,333.39,224.59,224.81,6.16;10,333.39,232.56,102.08,6.16" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><forename type="middle">George</forename><surname>Guest</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Guest</surname></persName>
		</author>
		<title level="m">Numerical methods of curve fitting</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Philip George Guest and Philip George Guest. 2012. Numerical methods of curve fitting. Cambridge University Press.</note>
</biblStruct>

<biblStruct coords="10,333.39,241.91,224.81,3.18;10,333.39,248.50,224.81,6.16;10,333.39,256.47,110.81,6.16" xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770-778.</note>
</biblStruct>

<biblStruct coords="10,333.39,265.82,225.58,3.18;10,333.39,273.79,224.81,3.18;10,333.39,280.39,190.54,6.16" xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning for time series classification: a review</title>
		<author>
			<persName coords=""><forename type="first">Hassan</forename><surname>Ismail Fawaz</surname></persName>
			<idno type="ORCID">0000-0003-1384-5996</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Germain</forename><surname>Forestier</surname></persName>
			<idno type="ORCID">0000-0002-4960-7554</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lhassane</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-019-00619-1</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<title level="j" type="abbrev">Data Min Knowl Disc</title>
		<idno type="ISSN">1384-5810</idno>
		<idno type="ISSNe">1573-756X</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="917" to="963" />
			<date type="published" when="2019-03-02">2019. 2019</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. 2019. Deep learning for time series classification: a review. Data mining and knowledge discovery 33, 4 (2019), 917-963.</note>
</biblStruct>

<biblStruct coords="10,333.39,289.74,225.58,3.18;10,333.39,297.71,225.99,3.18;10,332.89,305.68,225.45,3.18;10,333.39,312.27,225.58,6.16;10,333.23,321.62,31.30,3.18" xml:id="b20">
	<analytic>
		<title level="a" type="main">InceptionTime: Finding AlexNet for time series classification</title>
		<author>
			<persName coords=""><forename type="first">Hassan</forename><surname>Ismail Fawaz</surname></persName>
			<idno type="ORCID">0000-0003-1384-5996</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Lucas</surname></persName>
			<idno type="ORCID">0000-0002-2021-3076</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Germain</forename><surname>Forestier</surname></persName>
			<idno type="ORCID">0000-0002-4960-7554</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Charlotte</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
			<idno type="ORCID">0000-0002-3694-4703</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
			<idno type="ORCID">0000-0001-9963-5169</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Lhassane</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">François</forename><surname>Petitjean</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-020-00710-y</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<title level="j" type="abbrev">Data Min Knowl Disc</title>
		<idno type="ISSN">1384-5810</idno>
		<idno type="ISSNe">1573-756X</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1936" to="1962" />
			<date type="published" when="2020-09-07">2020. 2020</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier, Daniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar, Pierre- Alain Muller, and François Petitjean. 2020. Inceptiontime: Finding alexnet for time series classification. Data Mining and Knowledge Discovery 34, 6 (2020), 1936-1962.</note>
</biblStruct>

<biblStruct coords="10,333.39,329.59,224.81,3.18;10,333.39,336.18,224.81,6.16;10,333.39,344.15,93.23,6.16" xml:id="b21">
	<analytic>
		<title level="a" type="main">Product Quantization for Nearest Neighbor Search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2010.57</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2010">2010. 2010</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence 33, 1 (2010), 117-128.</note>
</biblStruct>

<biblStruct coords="10,333.39,353.50,224.99,3.18;10,333.39,360.09,195.26,6.16" xml:id="b22">
	<analytic>
		<title level="a" type="main">Billion-Scale Similarity Search with GPUs</title>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
			<idno type="ORCID">0000-0003-2743-0521</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<idno type="DOI">10.1109/tbdata.2019.2921572</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<title level="j" type="abbrev">IEEE Trans. Big Data</title>
		<idno type="ISSNe">2372-2096</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535-547.</note>
</biblStruct>

<biblStruct coords="10,333.39,368.06,224.81,6.16;10,333.39,376.03,123.85,6.16" xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducibility among combinatorial problems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complexity of computer computations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1972">1972</date>
			<biblScope unit="page" from="85" to="103" />
		</imprint>
	</monogr>
	<note type="raw_reference">Richard M Karp. 1972. Reducibility among combinatorial problems. In Complexity of computer computations. Springer, 85-103.</note>
</biblStruct>

<biblStruct coords="10,333.39,385.38,225.88,3.18;10,333.39,393.35,224.81,3.18;10,333.39,399.94,190.74,6.16" xml:id="b24">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction for Fast Similarity Search in Large Time Series Databases</title>
		<author>
			<persName coords=""><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaushik</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharad</forename><surname>Mehrotra</surname></persName>
		</author>
		<idno type="DOI">10.1007/pl00011669</idno>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<title level="j" type="abbrev">Knowledge and Information Systems</title>
		<idno type="ISSN">0219-1377</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="286" />
			<date type="published" when="2001-08">2001. 2001</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Eamonn Keogh, Kaushik Chakrabarti, Michael Pazzani, and Sharad Mehrotra. 2001. Dimensionality reduction for fast similarity search in large time series databases. Knowledge and information Systems 3, 3 (2001), 263-286.</note>
</biblStruct>

<biblStruct coords="10,333.39,409.29,224.81,3.18;10,333.39,415.88,145.59,6.16" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<title level="m">Reformer: The efficient transformer</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451 (2020).</note>
</biblStruct>

<biblStruct coords="10,333.39,425.23,224.81,3.18;10,333.39,431.82,167.44,6.16" xml:id="b26">
	<analytic>
		<title level="a" type="main">Determinants of common stock prices: A time series analysis</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Kraft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of finance</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="417" to="425" />
			<date type="published" when="1977">1977. 1977</date>
		</imprint>
	</monogr>
	<note type="raw_reference">John Kraft and Arthur Kraft. 1977. Determinants of common stock prices: A time series analysis. The journal of finance 32, 2 (1977), 417-425.</note>
</biblStruct>

<biblStruct coords="10,333.39,441.17,225.99,3.18;10,333.39,447.76,224.81,6.16;10,333.39,455.73,225.99,6.16;10,333.39,465.08,225.26,3.18;10,333.39,473.05,178.42,3.18" xml:id="b27">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
		<ptr target="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<title level="j" type="abbrev">Commun. ACM</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<idno type="ISSN">0001-0782</idno>
		<idno type="ISSNe">1557-7317</idno>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2012">2012</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas- sification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein- berger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/ paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</note>
</biblStruct>

<biblStruct coords="10,333.39,481.02,225.99,3.18;10,333.39,487.61,224.81,6.16;10,333.18,496.96,51.70,3.18" xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey on human activity recognition using wearable sensors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oscar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Labrador</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE communications surveys &amp; tutorials</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1192" to="1209" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Oscar D Lara and Miguel A Labrador. 2012. A survey on human activity recog- nition using wearable sensors. IEEE communications surveys &amp; tutorials 15, 3 (2012), 1192-1209.</note>
</biblStruct>

<biblStruct coords="10,333.39,504.93,225.57,3.18;10,333.39,512.90,225.99,3.18;10,333.39,519.49,224.81,6.16;10,333.39,527.46,80.91,6.16" xml:id="b29">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName coords=""><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiyou</forename><surname>Yao Xuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenhu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu-Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle- neck of transformer on time series forecasting. Advances in Neural Information Processing Systems 32 (2019).</note>
</biblStruct>

<biblStruct coords="10,333.39,535.43,224.81,6.16;10,333.39,544.78,69.70,3.18" xml:id="b30">
	<analytic>
		<title level="a" type="main">Clustering of time series data-a survey</title>
		<author>
			<persName coords=""><forename type="first">Warren</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1857" to="1874" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T Warren Liao. 2005. Clustering of time series data-a survey. Pattern recognition 38, 11 (2005), 1857-1874.</note>
</biblStruct>

<biblStruct coords="10,333.39,552.75,224.81,3.18;10,333.39,560.72,224.81,3.18;10,333.39,567.31,224.81,6.16;10,333.39,575.28,43.08,6.16" xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast similarity search in the presence of noise, scaling, and translation in time-series databases</title>
		<author>
			<persName coords=""><forename type="first">Rake&amp;</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">King-Lp</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harpreet S Sawhney Kyuseok</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 21th International Conference on Very Large Data Bases</title>
		<meeting>eeding of the 21th International Conference on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="490" to="501" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rake&amp; Agrawal King-lp Lin and Harpreet S Sawhney Kyuseok Shim. 1995. Fast similarity search in the presence of noise, scaling, and translation in time-series databases. In Proceeding of the 21th International Conference on Very Large Data Bases. 490-501.</note>
</biblStruct>

<biblStruct coords="10,333.39,584.63,224.81,3.18;10,333.13,592.60,225.07,3.18;10,333.39,599.19,223.10,6.16" xml:id="b32">
	<analytic>
		<title level="a" type="main">Time Series Classification with HIVE-COTE</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Lines</surname></persName>
			<idno type="ORCID">0000-0002-1496-5941</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
		<idno type="DOI">10.1145/3182382</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<title level="j" type="abbrev">ACM Trans. Knowl. Discov. Data</title>
		<idno type="ISSN">1556-4681</idno>
		<idno type="ISSNe">1556-472X</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2018-07-05">2018. jul 2018</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Jason Lines, Sarah Taylor, and Anthony Bagnall. 2018. Time Series Classification with HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based Ensembles. ACM Trans. Knowl. Discov. Data 12, 5, Article 52 (jul 2018), 35 pages.</note>
</biblStruct>

<biblStruct coords="10,333.39,608.54,224.81,3.18;10,333.16,616.51,225.04,3.18;10,333.39,624.48,224.81,3.18;10,333.39,631.07,224.81,6.16;10,333.39,639.04,96.57,6.16" xml:id="b33">
	<analytic>
		<title level="a" type="main">An Open Access Database for Evaluating the Algorithms of Electrocardiogram Rhythm and Morphology Abnormality Detection</title>
		<author>
			<persName coords=""><forename type="first">Feifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lina</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoling</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caiyun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shoushui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><forename type="middle">Ng</forename><surname>Yin Kwee</surname></persName>
		</author>
		<idno type="DOI">10.1166/jmihi.2018.2442</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging and Health Informatics</title>
		<title level="j" type="abbrev">j med imaging hlth inform</title>
		<idno type="ISSN">2156-7018</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1368" to="1373" />
			<date type="published" when="2018-09-01">2018. 2018</date>
			<publisher>American Scientific Publishers</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Feifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan Xu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, et al. 2018. An open access database for evaluating the algorithms of electrocardiogram rhythm and morphology abnormality detection. Journal of Medical Imaging and Health Informatics 8, 7 (2018), 1368-1373.</note>
</biblStruct>

<biblStruct coords="10,333.39,647.01,224.81,6.16;10,333.39,654.98,114.58,6.16" xml:id="b34">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName coords=""><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
		<idno type="DOI">10.1109/tit.1982.1056489</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<title level="j" type="abbrev">IEEE Trans. Inform. Theory</title>
		<idno type="ISSN">0018-9448</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982-03">1982. 1982</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on information theory 28, 2 (1982), 129-137.</note>
</biblStruct>

<biblStruct coords="10,333.39,664.33,225.89,3.18;10,333.39,670.92,107.98,6.16" xml:id="b35">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1711.05101</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017).</note>
</biblStruct>

<biblStruct coords="10,333.39,680.27,224.81,3.18;10,333.39,688.24,225.58,3.18;10,333.39,694.83,209.41,6.16" xml:id="b36">
	<monogr>
		<title level="m" type="main">CDSA: cross-dimensional self-attention for multivariate, geo-tagged time series imputation</title>
		<author>
			<persName coords=""><forename type="first">Jiawei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hassan</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Vetro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09904</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, and Shih-Fu Chang. 2019. CDSA: cross-dimensional self-attention for multivariate, geo-tagged time series imputation. arXiv preprint arXiv:1905.09904 (2019).</note>
</biblStruct>

<biblStruct coords="11,69.23,90.48,224.81,3.18;11,69.23,97.07,224.81,6.16;11,69.23,105.04,220.38,6.16" xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitry</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="824" to="836" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence 42, 4 (2018), 824-836.</note>
</biblStruct>

<biblStruct coords="11,69.23,114.39,225.99,3.18;11,69.23,120.98,224.81,6.16;11,69.23,128.95,150.34,6.16" xml:id="b38">
	<analytic>
		<title level="a" type="main">Time series: Similarity search and its applications</title>
		<author>
			<persName coords=""><forename type="first">Tripti</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veena</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Systemics, Cybernetics and Informatics: ICSCI-04</title>
		<meeting>the International Conference on Systemics, Cybernetics and Informatics: ICSCI-04<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="528" to="533" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tripti Negi and Veena Bansal. 2005. Time series: Similarity search and its appli- cations. In Proceedings of the International Conference on Systemics, Cybernetics and Informatics: ICSCI-04, Hyderabad, India. 528-533.</note>
</biblStruct>

<biblStruct coords="11,69.23,138.30,225.99,3.18;11,69.23,144.89,223.26,6.16" xml:id="b39">
	<analytic>
		<title level="a" type="main">GRAIL</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Paparrizos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<idno type="DOI">10.14778/3342263.3342648</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<title level="j" type="abbrev">Proc. VLDB Endow.</title>
		<idno type="ISSN">2150-8097</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1762" to="1777" />
			<date type="published" when="2019-07">2019. 2019</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">John Paparrizos and Michael J Franklin. 2019. Grail: efficient time-series repre- sentation learning. Proceedings of the VLDB Endowment 12, 11 (2019), 1762-1777.</note>
</biblStruct>

<biblStruct coords="11,69.23,154.24,224.99,3.18;11,69.23,160.83,224.81,6.16;11,69.23,168.80,78.26,6.16" xml:id="b40">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
	<note type="raw_reference">Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. In International conference on machine learning. PMLR, 1310-1318.</note>
</biblStruct>

<biblStruct coords="11,69.23,178.15,225.88,3.18;11,69.23,184.74,107.18,6.16" xml:id="b41">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName coords=""><surname>Ning Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ning Qian. 1999. On the momentum term in gradient descent learning algorithms. Neural networks 12, 1 (1999), 145-151.</note>
</biblStruct>

<biblStruct coords="11,69.23,194.09,225.64,3.18;11,68.99,202.06,225.06,3.18;11,69.23,208.65,225.57,6.16;11,69.07,218.00,11.26,3.18" xml:id="b42">
	<analytic>
		<title level="a" type="main">AutoTransformer: Automatic Transformer Architecture Design for Time Series Classification</title>
		<author>
			<persName coords=""><forename type="first">Yankun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinxing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-05933-9_12</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="143" to="155" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yankun Ren, Longfei Li, Xinxing Yang, and Jun Zhou. 2022. AutoTransformer: Automatic Transformer Architecture Design for Time Series Classification. In Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 143- 155.</note>
</biblStruct>

<biblStruct coords="11,69.23,225.97,225.88,3.18;11,69.23,232.56,225.51,6.16;11,69.23,240.53,154.19,6.16" xml:id="b43">
	<analytic>
		<title level="a" type="main">DeepAR: Probabilistic forecasting with autoregressive recurrent networks</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Januschowski</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijforecast.2019.07.001</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<title level="j" type="abbrev">International Journal of Forecasting</title>
		<idno type="ISSN">0169-2070</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1181" to="1191" />
			<date type="published" when="2020-07">2020. 2020</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020. DeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter- national Journal of Forecasting 36, 3 (2020), 1181-1191.</note>
</biblStruct>

<biblStruct coords="11,69.23,249.88,225.88,3.18;11,69.03,257.85,226.09,3.18;11,69.23,264.44,163.59,6.16" xml:id="b44">
	<analytic>
		<title level="a" type="main">TS-CHIEF: a scalable and accurate forest algorithm for time series classification</title>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>Shifaz</surname></persName>
			<idno type="ORCID">0000-0002-4627-8935</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Charlotte</forename><surname>Pelletier</surname></persName>
			<idno type="ORCID">0000-0002-4652-7778</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">François</forename><surname>Petitjean</surname></persName>
			<idno type="ORCID">0000-0001-5334-3574</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
			<idno type="ORCID">0000-0001-9963-5169</idno>
		</author>
		<idno type="DOI">10.1007/s10618-020-00679-8</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<title level="j" type="abbrev">Data Min Knowl Disc</title>
		<idno type="ISSN">1384-5810</idno>
		<idno type="ISSNe">1573-756X</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="742" to="775" />
			<date type="published" when="2020-03-05">2020. 2020</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Ahmed Shifaz, Charlotte Pelletier, François Petitjean, and Geoffrey I. Webb. 2020. TS-CHIEF: a scalable and accurate forest algorithm for time series classification. Data Mining and Knowledge Discovery 34 (2020), 742-775.</note>
</biblStruct>

<biblStruct coords="11,69.23,273.79,225.58,3.18;11,69.23,281.77,225.88,3.18;11,69.23,289.74,225.99,3.18;11,69.23,296.33,224.81,6.16;11,69.23,304.30,136.15,6.16" xml:id="b45">
	<analytic>
		<title level="a" type="main">Smart Devices are Different</title>
		<author>
			<persName coords=""><forename type="first">Allan</forename><surname>Stisen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henrik</forename><surname>Blunck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sourav</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thor</forename><forename type="middle">Siiger</forename><surname>Prentow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikkel</forename><forename type="middle">Baun</forename><surname>Kjærgaard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anind</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Sonne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mads</forename><forename type="middle">Møller</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2809695.2809718</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems</title>
		<meeting>the 13th ACM Conference on Embedded Networked Sensor Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-11">2015</date>
			<biblScope unit="page" from="127" to="140" />
		</imprint>
	</monogr>
	<note type="raw_reference">Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjaergaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen. 2015. Smart devices are different: Assessing and mitigatingmobile sensing het- erogeneities for activity recognition. In Proceedings of the 13th ACM conference on embedded networked sensor systems. 127-140.</note>
</biblStruct>

<biblStruct coords="11,69.23,313.65,224.81,3.18;11,69.23,321.62,224.81,3.18;11,69.23,328.21,224.81,6.16;11,69.23,336.18,141.94,6.16" xml:id="b46">
	<analytic>
		<title level="a" type="main">Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network</title>
		<author>
			<persName coords=""><forename type="first">Ya</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Youjian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenhao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Pei</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330672</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-07-25">2019</date>
			<biblScope unit="page" from="2828" to="2837" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining. 2828-2837.</note>
</biblStruct>

<biblStruct coords="11,69.23,345.53,224.81,3.18;11,69.23,352.12,224.81,6.16;11,69.23,360.09,225.88,6.16;11,69.23,369.44,28.84,3.18" xml:id="b47">
	<analytic>
		<title level="a" type="main">On-body localization of wearable devices: An investigation of position-aware activity recognition</title>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Sztyler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
		<idno type="DOI">10.1109/percom.2016.7456521</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Pervasive Computing and Communications (PerCom)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-03">2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note type="raw_reference">Timo Sztyler and Heiner Stuckenschmidt. 2016. On-body localization of wearable devices: An investigation of position-aware activity recognition. In 2016 IEEE International Conference on Pervasive Computing and Communications (PerCom). IEEE, 1-9.</note>
</biblStruct>

<biblStruct coords="11,69.23,377.41,224.81,3.18;11,69.23,384.00,181.87,6.16" xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient Transformers: A Survey</title>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Tay</surname></persName>
			<idno type="ORCID">0000-0001-6896-4496</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
			<idno type="ORCID">0000-0002-9772-1095</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Dara</forename><surname>Bahri</surname></persName>
			<idno type="ORCID">0000-0003-0144-2911</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
			<idno type="ORCID">0000-0003-4276-6269</idno>
		</author>
		<idno type="DOI">10.1145/3530811</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<title level="j" type="abbrev">ACM Comput. Surv.</title>
		<idno type="ISSN">0360-0300</idno>
		<idno type="ISSNe">1557-7341</idno>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2020">2020. 2020</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient transformers: A survey. ACM Computing Surveys (CSUR) (2020).</note>
</biblStruct>

<biblStruct coords="11,69.23,391.97,224.81,6.16;11,69.23,399.94,212.62,6.16" xml:id="b49">
	<analytic>
		<title level="a" type="main">Anomaly detection on time series</title>
		<author>
			<persName coords=""><forename type="first">Mingyan</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Progress in Informatics and Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="603" to="608" />
		</imprint>
	</monogr>
	<note type="raw_reference">Mingyan Teng. 2010. Anomaly detection on time series. In 2010 IEEE International Conference on Progress in Informatics and Computing, Vol. 1. IEEE, 603-608.</note>
</biblStruct>

<biblStruct coords="11,69.23,409.29,224.99,3.18;11,69.23,415.88,197.06,6.16" xml:id="b50">
	<analytic>
		<title level="a" type="main">An MSE statistic for comparing forecast accuracy across series</title>
		<author>
			<persName coords=""><surname>Patrick A Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Patrick A Thompson. 1990. An MSE statistic for comparing forecast accuracy across series. International Journal of Forecasting 6, 2 (1990), 219-227.</note>
</biblStruct>

<biblStruct coords="11,69.23,425.23,225.58,3.18;11,68.99,433.20,225.06,3.18;11,69.23,439.79,225.51,6.16;11,69.23,447.76,224.81,6.16;11,69.23,455.73,78.71,6.16" xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Con- ference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA. 5998-6008.</note>
</biblStruct>

<biblStruct coords="11,69.23,465.08,224.94,3.18;11,69.23,473.05,225.58,3.18;11,69.12,481.02,226.10,3.18;11,69.23,488.99,225.58,3.18;11,69.23,497.23,225.58,3.18;11,69.23,505.20,225.58,3.18;11,69.23,513.17,225.58,3.18;11,69.23,521.14,225.99,3.18;11,69.23,527.73,225.89,6.16;11,69.23,537.08,119.65,3.18" xml:id="b52">
	<analytic>
		<title level="a" type="main">SciPy 1.0: fundamental algorithms for scientific computing in Python</title>
		<author>
			<persName coords=""><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
			<idno type="ORCID">0000-0002-0300-3333</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Haberland</surname></persName>
			<idno type="ORCID">0000-0003-4806-3601</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
			<idno type="ORCID">0000-0003-2364-6157</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéfan</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
			<idno type="ORCID">0000-0001-9276-1891</idno>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Jarrod</forename><surname>Millman</surname></persName>
			<idno type="ORCID">0000-0002-5263-5070</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolay</forename><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">R J</forename><surname>Nelson</surname></persName>
			<idno type="ORCID">0000-0002-4548-3558</idno>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">İlhan</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Laxalde</surname></persName>
			<idno type="ORCID">0000-0002-5540-4825</idno>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">M</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antônio</forename><forename type="middle">H</forename><surname>Ribeiro</surname></persName>
			<idno type="ORCID">0000-0003-3632-8529</idno>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Van Mulbregt</surname></persName>
			<idno type="ORCID">0000-0002-2382-8308</idno>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><forename type="middle">Pietro</forename><surname>Bardelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Rothberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Hilboll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kloeckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Scopatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Rokem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Nathan</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chad</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Häggström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><forename type="middle">V</forename><surname>Pasechnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Olivetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Lenders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><forename type="middle">A</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gert-Ludwig</forename><surname>Ingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">E</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Audren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irvin</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><forename type="middle">P</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Silterra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Slavič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Buchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><forename type="middle">Vinícius</forename><surname>De Miranda Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joscha</forename><surname>Reimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Harrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Luis Cano</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Kuczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Tritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Newville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Bolingbroke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tartre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Nowaczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Shebanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Pavlyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><forename type="middle">A</forename><surname>Brodtkorb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perry</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Mcgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Feldbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Tygier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sievert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastiano</forename><surname>Vigna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surhud</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadeusz</forename><surname>Pudlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Oshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">J</forename><surname>Pingel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Robitaille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Spura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thouis</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Cera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiziano</forename><surname>Zito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaroslav</forename><forename type="middle">O</forename><surname>Halchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshiki</forename><surname>Vázquez-Baeza</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-019-0686-2</idno>
		<ptr target="https://doi.org/10.1038/s41592-019-0686-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<title level="j" type="abbrev">Nat Methods</title>
		<idno type="ISSN">1548-7091</idno>
		<idno type="ISSNe">1548-7105</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<date type="published" when="2020-02-03">2020</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar- rod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Al- gorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261-272. https://doi.org/10.1038/s41592-019-0686-2</note>
</biblStruct>

<biblStruct coords="11,69.23,545.05,225.99,3.18;11,69.23,551.64,224.81,6.16;11,69.03,560.99,18.66,3.18" xml:id="b53">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName coords=""><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin- former: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 (2020).</note>
</biblStruct>

<biblStruct coords="11,69.23,568.96,224.81,3.18;11,69.23,575.55,225.58,6.16;11,69.07,584.90,44.27,3.18" xml:id="b54">
	<analytic>
		<title level="a" type="main">Smartphone and smartwatch-based biometrics using activities of daily living</title>
		<author>
			<persName coords=""><forename type="first">Kenichi</forename><surname>Gary M Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thaier</forename><surname>Yoneda</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hayajneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="133190" to="133202" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gary M Weiss, Kenichi Yoneda, and Thaier Hayajneh. 2019. Smartphone and smartwatch-based biometrics using activities of daily living. IEEE Access 7 (2019), 133190-133202.</note>
</biblStruct>

<biblStruct coords="11,69.23,592.87,225.88,3.18;11,69.23,600.84,225.88,3.18;11,69.23,607.44,224.81,6.16;11,69.23,615.41,225.58,6.16;11,69.23,624.76,177.81,3.18" xml:id="b55">
	<analytic>
		<title level="a" type="main">RobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection</title>
		<author>
			<persName coords=""><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3448016.3452779</idno>
		<ptr target="https://doi.org/10.1145/3448016.3452779" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 International Conference on Management of Data</title>
		<meeting>the 2021 International Conference on Management of Data<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-06-09">2021</date>
			<biblScope unit="page" from="2328" to="2337" />
		</imprint>
	</monogr>
	<note type="raw_reference">Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke, and Huan Xu. 2021. RobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection. In Proceedings of the 2021 International Conference on Management of Data (Virtual Event, China) (SIGMOD &apos;21). Association for Computing Machinery, New York, NY, USA, 2328-2337. https://doi.org/10.1145/3448016.3452779</note>
</biblStruct>

<biblStruct coords="11,69.23,632.73,225.99,3.18;11,69.23,640.70,225.88,3.18;11,69.23,647.29,211.01,6.16" xml:id="b56">
	<analytic>
		<title level="a" type="main">Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting</title>
		<author>
			<persName coords=""><forename type="first">Haixu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiehui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22419" to="22430" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De- composition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems 34 (2021), 22419-22430.</note>
</biblStruct>

<biblStruct coords="11,69.23,656.64,224.99,3.18;11,69.03,664.61,226.09,3.18;11,69.23,671.20,108.27,6.16" xml:id="b57">
	<monogr>
		<title level="m" type="main">Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy</title>
		<author>
			<persName coords=""><forename type="first">Jiehui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haixu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02642</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy. arXiv preprint arXiv:2110.02642 (2021).</note>
</biblStruct>

<biblStruct coords="11,69.23,680.55,225.05,3.18;11,69.23,687.14,225.51,6.16;11,69.23,695.11,225.88,6.16;11,333.39,90.48,46.03,3.18" xml:id="b58">
	<analytic>
		<title level="a" type="main">A Review of Data Mining-Based Financial Fraud Detection Research</title>
		<author>
			<persName coords=""><forename type="first">Dianmin</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chao-Hsien</forename><surname>Chu</surname></persName>
		</author>
		<idno type="DOI">10.1109/wicom.2007.1352</idno>
	</analytic>
	<monogr>
		<title level="m">2007 International Conference on Wireless Communications, Networking and Mobile Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007-09">2007. 2007</date>
			<biblScope unit="page" from="5519" to="5522" />
		</imprint>
	</monogr>
	<note type="raw_reference">Dianmin Yue, Xiaodan Wu, Yunfeng Wang, Yue Li, and Chao-Hsien Chu. 2007. A review of data mining-based financial fraud detection research. In 2007 Interna- tional Conference on Wireless Communications, Networking and Mobile Computing. Ieee, 5519-5522.</note>
</biblStruct>

<biblStruct coords="11,333.39,98.45,224.81,3.18;11,333.15,106.42,225.82,3.18;11,333.39,113.01,224.81,6.16;11,333.39,120.98,155.19,6.16" xml:id="b59">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName coords=""><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems 33 (2020), 17283-17297.</note>
</biblStruct>

<biblStruct coords="11,333.39,130.33,224.81,3.18;11,333.39,138.30,224.81,3.18;11,333.39,144.89,224.81,6.16;11,333.39,152.86,225.57,6.16;11,333.39,160.83,46.96,6.16" xml:id="b60">
	<analytic>
		<title level="a" type="main">A Transformer-based Framework for Multivariate Time Series Representation Learning</title>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Srideepika</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhaval</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anuradha</forename><surname>Bhamidipaty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467401</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Virtual Event, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-08-14">2021. August 14-18, 2021</date>
			<biblScope unit="page" from="2114" to="2124" />
		</imprint>
	</monogr>
	<note type="raw_reference">George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. 2021. A Transformer-based Framework for Multivariate Time Series Representation Learning. In KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021. 2114-2124.</note>
</biblStruct>

<biblStruct coords="11,333.39,170.18,225.58,3.18;11,333.39,178.15,225.99,3.18;11,333.39,184.74,155.89,6.16" xml:id="b61">
	<analytic>
		<title level="a" type="main">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</title>
		<author>
			<persName coords=""><forename type="first">Haoyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jieqi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wancai</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v35i12.17325</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="11106" to="11115" />
			<date type="published" when="2021-05-18">2021</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se- quence time-series forecasting. In Proceedings of AAAI.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
