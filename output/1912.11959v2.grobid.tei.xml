<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is Attention All What You Need ? -An Empirical Investigation on Convolution-Based Active Memory and Self-Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-30">30 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.71,140.77,85.90,10.75"><forename type="first">Thomas</forename><surname>Dowdell</surname></persName>
							<email>tomjamesdowdell@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Newcastle</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.56,140.77,76.74,10.75"><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
							<email>hongyu.zhang@newcastle.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Newcastle</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Is Attention All What You Need ? -An Empirical Investigation on Convolution-Based Active Memory and Self-Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-30">30 Dec 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">D5141AD7944DCDE0CBD1661081037013</idno>
					<idno type="arXiv">arXiv:1912.11959v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-22T20:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The key to a Transformer model is the self-attention mechanism, which allows the model to analyze an entire sequence in a computationally efficient manner. Recent work has suggested the possibility that general attention mechanisms used by RNNs could be replaced by active-memory mechanisms. In this work, we evaluate whether various activememory mechanisms could replace self-attention in a Transformer. Our experiments suggest that active-memory alone achieves comparable results to the self-attention mechanism for language modelling, but optimal results are mostly achieved by using both active-memory and self-attention mechanisms together. We also note that, for some specific algorithmic tasks, active-memory mechanisms alone outperform both the self attention and a combination of the two.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The previous state-of-the-art sequence model, the recurrent neural network, has been largely supplanted by the Transformer model <ref type="bibr" coords="1,110.95,485.08,85.75,9.53" target="#b31">[Vaswani et al., 2017]</ref>, which is primarily built atop a self-attention mechanism. Given a task to train upon, the self-attention mechanism focuses on one token per attention head within the entire sequence at each time-step; the key to the self-attention mechanism's success is the mechanism's ability to learn which token within the entire sequence to focus on in order to achieve the best results.</p><p>The self-attention mechanism has proven successful on a variety of natural language processing tasks, but has not achieved ubiquitous success. The authors of <ref type="bibr" coords="1,229.81,584.37,62.35,9.53;1,54.00,596.23,42.86,8.64" target="#b15">[Kaiser and Bengio, 2016]</ref> pointed out that an attention mechanism would likely struggle to solve a task which required a model to focus on multiple tokens at a given time-step. Further, the authors of <ref type="bibr" coords="1,88.72,628.21,117.76,9.53" target="#b17">[Kaiser and Sutskever, 2015]</ref> recommended that an attention mechanism could be replaced by active-memory to alleviate these concerns.</p><p>Unlike attention, active-memory allows a model to access and change any and all elements of its memory at each timestep. The active-memory mechanism can access more than one element at each time step. In <ref type="bibr" coords="1,188.59,694.62,104.12,9.53" target="#b15">[Kaiser and Bengio, 2016]</ref>,</p><p>Figure <ref type="figure" coords="1,341.26,283.34,3.49,7.77">1</ref>: The active memory mechanism. In this case, the activememory is implemented in a unidirectional manner, with a kernel size 3.</p><p>the authors used an active-memory system to translate English to French, and was capable of outperforming an RNN model, both with and without an attention mechanism.</p><p>Motivated by the success of attention mechanism <ref type="bibr" coords="1,521.03,358.10,36.97,8.64;1,315.00,369.78,49.44,8.82" target="#b31">[Vaswani et al., 2017]</ref> and active-memory <ref type="bibr" coords="1,447.95,369.06,105.77,9.53" target="#b15">[Kaiser and Bengio, 2016]</ref>, in this paper we investigate the Transformer's self-attention mechanism in comparison to a variety of active-memory mechanisms. We experiment on two types of tasks: the language modeling task and a set of algorithmic tasks.</p><p>For the language modelling task, the self-attention mechanism out-performs an active-memory mechanism used alone by a slim margin. However, a combination of both selfattention and active-memory reliably outperform both mechanisms used alone.</p><p>We also evaluated the self-attention mechanism and various active-memory mechanisms on a variety of algorithmic tasks, which can also be expressed as a sequence modeling task. Across most of the algorithmic tasks tested, the activememory mechanisms achieve equal, or superior, results to a traditional self-attention mechanism. This would appear to vindicate the hypothesis stated by <ref type="bibr" coords="1,450.57,544.80,103.14,9.53" target="#b15">[Kaiser and Bengio, 2016]</ref>, suggesting that the nature of the attention mechanism does indeed limit the effectiveness and accuracy of the model. Finally, we note that, for several algorithmic tasks, the mere addition of the self-attention mechanism hinders results; the active-memory mechanism alone outperforms a combination of the two separate mechanisms. This raises an unsolved problem; it would appear that, for deep learning sequence models, there is still no unambiguous model that can optimally solve all possible problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The Transformer model <ref type="bibr" coords="1,414.97,683.66,90.48,9.53" target="#b31">[Vaswani et al., 2017]</ref> is built with two separate modules, the self-attention mechanism and the feedforward mechanism, which are stacked atop each other for multiple layers. The feedforward mechanism is an intrasequence analysis, where the output for each token in the sequence is dependent only on the token at the same timestep, and independent of all other time-steps. On the other hand, the self-attention mechanism is an inter-sequence analysis, where the output for each time-step is dependent upon the entire sequence. The self-attention mechanism is defined, mathematically, as:</p><formula xml:id="formula_0" coords="2,80.20,166.75,190.11,88.76">Q t , K t , V t = x t y t = concat(head 1,t , head 2,t , . . . , head n,t )W o head i = Attention(Q t W Q i , K t W K i , V t W V i ) Attention(Q, K, V ) = sof tmax( Q t K T t √ d k )V t W o R d k * k,d , W K,Q,V i R d,d k * k</formula><p>The feed-forward module is defined as:</p><formula xml:id="formula_1" coords="2,93.10,281.01,164.30,26.02">y t = W l,2 (max(W l,1 x t + b l,1 , 0)) + b l,2 W l,2 R d,d * 4 , W l,1 R d * 4,d</formula><p>The Transformer model, and its variants <ref type="bibr" coords="2,226.22,311.32,66.49,9.53" target="#b4">[Dai et al., 2019]</ref>, have achieved remarkable results across a variety of natural language processing tasks since its inception <ref type="bibr" coords="2,238.50,333.24,58.50,9.31;2,54.00,344.92,34.05,8.82" target="#b39">[Zhenzhong et al., 2019</ref><ref type="bibr" coords="2,88.05,344.20,84.36,9.53">] [Delvin et al., 2018</ref><ref type="bibr" coords="2,172.42,344.20,87.46,9.53">] [Yang et al., 2019b]</ref> , and are currently investigated heavily by both academia and industry.</p><p>The Neural GPU <ref type="bibr" coords="2,139.69,366.12,118.94,9.53" target="#b10">[Freivalds and Liepins, 2017</ref><ref type="bibr" coords="2,258.63,366.12,38.37,9.53;2,54.00,377.97,71.40,8.64">] [Kaiser and Bengio, 2016</ref><ref type="bibr" coords="2,125.40,377.08,124.74,9.53" target="#b17">] [Kaiser and Sutskever, 2015]</ref>, which introduced an active-memory model, achieved impressive algorithmic results in <ref type="bibr" coords="2,137.44,398.99,117.26,9.53" target="#b17">[Kaiser and Sutskever, 2015]</ref>, and also achieved impressive machine translation results in <ref type="bibr" coords="2,267.68,409.95,29.32,8.64;2,54.00,421.81,73.33,8.64" target="#b15">[Kaiser and Bengio, 2016]</ref>. A Neural GPU contains a CGRU (Convolution Gated Recurrent Unit) module which is iterated repeatedly. This allows the entire sequence to be analyzed in a parallelizable and computationally efficient manner. The CGRU module is defined as:</p><formula xml:id="formula_2" coords="2,73.23,487.24,204.54,38.70">u = sigmoid(U 1 * x + B 1 ) r = sigmoid(U 2 * x + B 2 ) y = u ⊗ x + (1 -u) ⊗ tanh(U 0 * (r ⊗ x) + B 0 ))</formula><p>where U * x refers to applying a convolutional operator over x, using U as a trainable kernel bank and B is a trainable bias vector. The CGRU has, since its introduction, been used in other models <ref type="bibr" coords="2,108.24,563.11,85.91,9.53" target="#b22">[Resende et al., 2016]</ref>.</p><p>Convolutional operators are traditionally used for image processing <ref type="bibr" coords="2,100.06,585.03,77.98,9.53" target="#b0">[Alom et al., 2018]</ref>, and have also been used in relation to sequential analysis in previous papers <ref type="bibr" coords="2,248.58,595.99,48.42,9.31;2,54.00,607.84,23.06,8.64">[Yang et al., 2019a</ref><ref type="bibr" coords="2,77.06,606.95,77.36,9.53">] [Wu et al., 2019</ref><ref type="bibr" coords="2,154.41,606.95,96.16,9.53">] [Gehring et al., 2017</ref><ref type="bibr" coords="2,250.57,606.95,46.43,9.53;2,54.00,618.62,49.34,8.82">] [Dauphin et al., 2016]</ref>. To the best of our knowledge they have not been used explicitly to replace, or augment, the self-attention mechanism. The first sequence-to-sequence model, based on convolutional operators, was, to the best of our knowledge, introduced in <ref type="bibr" coords="2,108.85,661.74,84.11,9.53" target="#b11">[Gehring et al., 2017]</ref>, which replaced the thentraditional LSTM block with a series of convolutions and gated convolutional networks <ref type="bibr" coords="2,175.47,684.56,15.27,8.64">[13]</ref>, and outperformed RNNbased models in terms of both speed and accuracy. However, the model introduced in <ref type="bibr" coords="2,413.90,56.58,87.82,9.53" target="#b11">[Gehring et al., 2017]</ref> was followed shortly afterwards by the Transformer model, which outperformed the convolutional-based model.</p><p>The convolutional self-attention network <ref type="bibr" coords="2,502.71,89.57,55.29,9.31;2,315.00,101.43,27.67,8.64">[Yang et al., 2019a]</ref> was recently introduced, and bares a passing similarity to the traditional convolutional operator described in this paper. The layer of the convolutional self-attention is similar to a traditional self-attention mechanism, but where the key and value tensors are calculated as:</p><formula xml:id="formula_3" coords="2,360.57,165.25,151.86,32.02">K h = (K h i-M/2 , ..., K h i , ..., K h i+M/2 ) V h = (V h i-M/2 , ..., V h i , ..., V h i+M/2</formula><p>) From this point, the convolutional self-attention mechanism acts in an identical manner to the traditional selfattention mechanism. This is in direct comparison to the convolutional operator described in this paper, which explicitly avoids the use of the self-attention mechanism and relies entirely on a purely convolutional operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this paper, we investigate whether various active-memory mechanisms could replace self-attention in a Transformer. We also evaluate the combination of self-attention and activememory mechanisms for language modelling tasks. All the active-memory mechanisms introduced in this paper were inspired by the Neural GPU, as introduced in <ref type="bibr" coords="2,510.20,351.75,47.80,9.53;2,315.00,363.61,65.79,8.64" target="#b17">[Kaiser and Sutskever, 2015]</ref>. The key allure of the Neural GPU is that the inputs of each time-step can be analyzed and altered, and we were inspired to apply a similar form of sequence modelling alongside a self-attention mechanism. We describe various convolution-based active-memory mechanisms in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Convolutional Operators</head><p>The Traditional Convolutional Operator The first, and most simple, active-memory mechanism is the simple convolutional operator. The traditional convolutional operator was formally defined in <ref type="bibr" coords="2,446.13,486.17,65.27,9.53" target="#b2">[Bai et al., 2019]</ref>. If the task requires the sequence to be analyzed in a unidirectional manner, such as the case for language modelling, then a zerosvector of size k -1 is concatenated to the left of the input tensor so that, for the nth output token, the model only has access to the first n input tokens. This feature is crucial to avoid allowing the model 'seeing' forward through the sequence and having access to information that the model, in practice, would not yet have. This has an identical function to the masking operation of the self-attention mechanism.</p><p>If the task can be analyzed in a bidirectional manner, then the model uses a convolutional filter using the SAMEpadding, which allows for the vector to maintain its shape throughout the convolutional operator. However, when the convolutional operator is performed in this manner, the token at time-step t is dependent on the input tokens h</p><formula xml:id="formula_4" coords="2,519.89,651.56,38.11,9.33">[t-k/2,t+k/2] ,</formula><p>where k is the kernel size.</p><p>The primary flaw of a convolutional operator, in comparison to a self-attention mechanism, is that, given n layers where each kernel has a k kernel size, each token can only see k * n -n + 1 or k/2 * n -n + 1 time-steps across for unidirectional and bidirectional tasks respectively. For example, in our experiments on language modeling (Section 4), the kernel size was set to 20 and was iterated over 8 layers. Therefore, at each time-step t, the final output is capable of analyzing the input from 153 previous time-steps, well above the average sequence-size (90 tokens) in the dataset. The self-attention mechanism, in comparison, can see across a theoretically infinite context size, even using only a single layer. Given this information, the self-attention mechanism is capable of handling theoretically greater long-term dependencies than the active-memory mechanism. However, in practice, the ability of an active-memory mechanism to access and change its entire memory could overcome this limitation.</p><p>The convolutional operator is assisted further by the fact that the convolutional operator's complexity grows linearly with the sequence size, while the self-attention mechanism's complexity grows quadratically.</p><p>Numerous papers have noted that, while Transformers are parallelizable and capable of capturing long-range dependencies, the Transformer network suffers from the inability of model tokens in a recurrent manner <ref type="bibr" coords="3,197.69,287.05,72.27,9.53" target="#b33">[Wang et al., 2019</ref><ref type="bibr" coords="3,269.96,287.05,27.04,9.53;3,54.00,298.72,46.50,8.82">] [Hao et al., 2019]</ref>. This is in direct comparison to traditional RNN models, which can capture long-range dependencies, but can struggle to capture long-range dependencies. The use of active-memory, in theory, would accomplish this task, given that the output at time-step t h t is dependent of the inputs x <ref type="bibr" coords="3,58.42,357.01,16.47,6.01">[t-k,t]</ref> where k is the kernel size. Therefore, this operation can, in theory, model recurrence. We did not explicitly test whether this does model recurrence in practice, but will focus on this in future work.</p><p>The convolutional operator is followed by the ReLU activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Persistent-Convolutional Operator</head><p>The Persistent-Convolutional operator is similar to the traditional convolutional operator described above, except that the zeros vector is replaced by the a trainable vector of identical shape to the zeros vector. This allows the operator to, identical to the traditional convolutional operator, maintain an identical shape across the convolution. To keep parameterization to a minimum, the same persistent vector is used across all convolution operators in the entire model. The persistentconvolutional operator is defined as:</p><formula xml:id="formula_5" coords="3,115.31,546.22,119.88,26.74">p ∈ W kernel size-1,hiddensize x = [p, x], y = W * x + b</formula><p>where [.,.] denotes the concatenation function and p is the trainable persistent vector. Persistent vectors have been used previously in language modelling tasks <ref type="bibr" coords="3,220.63,601.16,76.37,9.31;3,54.00,613.01,21.44,8.64" target="#b29">[Sukhbaatar et al., 2019]</ref>, but never as an augmentation for convolutional operators, as far as we know.</p><p>If the model is to be analyzed in a bidirectional manner, rather then a unidirectional manner, then the persistentconvolutional operator can be redefined as: The use of a persistent vector allows for the model to have a permanent memory that, given the fact the vector is trainable, can be expressed in an optimal manner for the model. This is the equivalent of a permanent memory for the deep learning model.</p><formula xml:id="formula_6" coords="3,96.94,677.20,156.62,27.65">p 1 , p 2 ∈ W (kernel size-1)//2,hiddensize x = [p 1 , x, p 2 ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Highway-Convolutional Operator</head><p>The Highway-Convolutional operator is based on the highway network architecture <ref type="bibr" coords="3,418.54,369.19,10.58,8.64">[5]</ref>, which can be defined as:</p><formula xml:id="formula_7" coords="3,380.86,391.23,111.29,39.95">a = U 0 * x + B 0 b = sigmoid(U 1 * x + B 1 ) y = a ⊗ b + x ⊗ (1 -b)</formula><p>The key allure of the highway network, as described in <ref type="bibr" coords="3,315.00,448.21,95.43,9.53" target="#b27">[Srivastava et al., 2015]</ref>, is the fact that a highway network can be trained for a large number of layers, even hundreds of layers, because information can pass, unimpeded, across each layer. The authors of <ref type="bibr" coords="3,422.24,481.09,94.96,9.53" target="#b27">[Srivastava et al., 2015]</ref> described these paths as 'information highways'. The use of these 'information highways' allows information to pass through the self-attention mechanism in an equally efficient manner.</p><p>In this paper, we use the hard-sigmoid function <ref type="bibr" coords="3,512.19,525.03,45.81,9.53;3,315.00,536.89,57.01,8.64" target="#b15">[Kaiser and Bengio, 2016]</ref> to stabilize gradients, which is defined as:</p><formula xml:id="formula_8" coords="3,343.20,565.79,186.60,8.74">y = max(0, min(1, 1.2 * sigmoid(x) -0.1))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-Attention + Convolutional Operators</head><p>The operator calculates the results of the self-attention mechanism and results of the convolutional operator independently, and then adds them together to produce the final output of the operator. This operator would allow the model to analyze the input using both the self-attention mechanism and active-memory mechanism and decide which features from both mechanisms would be most optimal. This approach has the obvious advantage of being able to take the 'best of both worlds', where the optimal features that can only be detected  The loss-per-token of the self-attention mechanism and the active-memory mechanisms on the WT3 dataset, and the difference of loss between the self-attention and the active-memory mechanisms. The lower the loss, the better the model performed. With the exception of the CGRU, all purely active-memory operators achieve a test loss less then 1.2% higher then the self-attention mechanism.</p><p>The optimal models combined the self-attention mechanism and an active-memory mechanism, and achieved a lower test loss than the self-attention mechanism and active-memory mechanisms alone.</p><p>by the self-attention mechanism, and the optimal features that can only be detected by the convolutional operator, are both available to the model. The architecture of a single layer of the "self-attention + convolution" operator is shown in Figure <ref type="figure" coords="4,226.73,326.35,3.74,8.64" target="#fig_0">2</ref>. This architecture, without the convolutional operator, is a simple Transformer layer. The output of the convolutional operator is added, element-wise, to the output of the self-attention mechanism. This allows, hypothetically, for the best-of-bothworlds, where the model has access to the self-attention mechanism and the active-memory mechanism.</p><p>Similarly, we also add the self-attention mechanism to the persistent-convolutional operator and the highwayconvolutional operator, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate the effectiveness of the various convolution-based active-memory mechanisms, we used two separate experiments; a language modelling task that is traditionally associated with attention-based mechanisms <ref type="bibr" coords="4,208.19,495.62,84.52,9.53" target="#b25">[Shoeybi et al., 2019]</ref>, and algorithmic tasks that are associated with active-memory models <ref type="bibr" coords="4,86.84,517.54,118.61,9.53" target="#b17">[Kaiser and Sutskever, 2015]</ref>. The active-memory mechanisms are experimented both independently and alongside a self-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Language Modelling Experimental Setup</head><p>The first task that the operators were tested with was a unidirectional language modelling task; the WikiText-3 (WT3) dataset <ref type="bibr" coords="4,116.20,606.95,83.25,9.53" target="#b20">[Merity et al., 2016]</ref>, tokenized using BPEtokenization <ref type="bibr" coords="4,105.97,617.91,88.64,9.53" target="#b24">[Sennrich et al., 2016]</ref>. The WikiText-3 dataset was sourced entirely from Wikipedia articles, contains over 3.6 millions lines of text, and is split into a training dataset, valid dataset and test dataset. The train dataset contains 103M tokens, while the valid and test dataset contain 250K tokens each.</p><p>The models used were all 8-layer models, with a hidden size of 256 and a filter size of 1024, a vocab size of 32,000, (x, y) 1 0 1 1 + 0 0 1 1 (x + y) 0 0 0 0 0 1 1 1 0 Table 2: The binary addition task. Given two numbers (in this case, the two numbers are 11 and 3), the final output is the binary version of the addition of the two input numbers (in this case, 14). kernel size of 20 and a dropout rate of 0.9. No further regularization was used. The optimizer was the Adam Optimizer <ref type="bibr" coords="4,315.00,164.50,92.67,9.53" target="#b18">[Kingma and Ba, 2014]</ref> and a warmup-learning rate was used, as specified in <ref type="bibr" coords="4,376.07,175.45,88.12,9.53" target="#b31">[Vaswani et al., 2017]</ref>. All models were implemented using Tensorflow, version 1.07, on a V100 GPU card.</p><p>Notable preprocessing was used for analyzing the WikiText-3 dataset; every character was explicitly denoted as lower-case, each hyphenwas replaced by @-@ and punctuation marks, such as fullstops and commas, were seperated by white-space. This was done to discourage the BPE to tokenize sets of characters that included punctuation marks, forcing the model to tokenize sets of characters that were only letters, therefore tokenizing a greater set of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>With the exception of the CGRU operator, all active-memory mechanisms, when combined with the self-attention mechanism, outperformed the self-attention mechanism alone, achieving a lower loss-per-token. This would appear to vindicate the proposition of both this paper and [2], suggesting that, indeed, active-memory mechanisms and self-attention are comparable. However, no model that purely used an active-memory mechanism outperformed the self-attention mechanism for language modelling.</p><p>We note that, if the dropout rate was decreased to 0.7, all operators, with the exception of CGRU, all models achieved superior results to the self-attention mechanism at a the same dropout rate. However, these models did not achieve superior results to the self-attention model with a dropout-rate of 0.9. This would imply that self-attention mechanisms are more sensitive to dropout rates compared to active-memory mechanisms.</p><p>Further, each operator, except for the CGRU operator, benefited from combining it with self-attention, allowing both operators to operate independently and concurrently. The model with the lowest loss-per-token had a self-attention mechanism and a highway-convolutional operator. It is further worth noting that the highway-convolutional operator outperformed both other convolutional operators, both with and without the addition of the self-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithmic Tasks Experimental Setup</head><p>The second experiment for evaluating the active-memory mechanisms was on various algorithmic tasks:</p><p>• Reverse: Given an array X of size L, the model is trained to return the array Y, where Y[0] = X[-1]. In order to effectively perform this task, the model must be capable of analyzing the start of the input vector at the very end, and vice-versa.</p><p>(x, y)</p><p>1 0 1 0 1 × 0 1 1 0 0 (x × y) 0 0 0 1 1 1 1 1 1 0 0 • Sort: Given an array of randomly order integers, the model is trained to return an array that accurately order the input integers. The entire vector must be remembered and analyzed at each time-step.</p><p>• Addition: Given two binary numbers, the model is trained to return an array that represents the addition in the form of a third binary number. An example of the addition task is shown in Table <ref type="table" coords="5,199.62,214.72,3.74,8.96">2</ref>.</p><p>• Multiply: Multiplies two binary numbers, as shown in Table <ref type="table" coords="5,98.30,241.14,3.74,8.96" target="#tab_2">3</ref>.</p><p>• Not: If the input is 1, then not returns 0. Else, the not function returns 1. The output relies only on the input at the current time-step.</p><p>• Remember: Given a series of random numbers of sequence size N , followed by a sequence of zeros of identical size, the model is trained to output a series of zeros of size N , followed by the random numbers. In order for this task to be performed, the model must be able to remember tokens over an increasingly long sequence.</p><p>All data for the algorithmic tasks were generated in an online manner. For three of the tasks, Sort, Addition and Multiply, the model must focus on multiple tokens at every timestep. In comparison, the Reverse task, the Not task and the Remember task only require the model to focus on a single token at every time-step.</p><p>The model that was used for algorithmic tasks contains 4 layers, with a hidden size of 128 units, a filter size of 512 and a kernel size of 20. Each model was trained for a maximum of 100 epochs, where each epoch contains 100 iterations. At the end of each epoch, the model was exposed to an online batch, containing 32 test cases. If the model achieved an accuracy of 100% on the online test batch, the sequence-size of the data is increased, therefore increasing its complexity.</p><p>For the Reverse, Sort, Not and Remember task, when the model achieved a 100% accuracy, the sequence was increased by 1. For the Addition and Multiply task, the sequence was increased by 2.</p><p>The model was initially trained only for sequences that are 5 tokens long and was not introduced to a larger sequence until the model was capable of achieving 100% accuracy on this sequence-size. We found that this form of curriculum learning was essential: if a model was initially trained on a sequence of several dozen tokens, each operator was incapable of achieving a reasonable accuracy.</p><p>The vocabulary size was different for each task. The Reverse task had a vocabulary size of 100, while the Sort task and the Remember task had a vocabulary size of 20. We noted that whenever the vocab size was increased the model would achieve less accurate results. Because all tokens in the Addition, Multiply and Not tasks are either 0, 1, or the separator, the vocabulary size is set to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Each model was tested for each task, and the highest sequence that the model could achieve within 100 epochs was recorded. Each experiment was performed three times, and the average sequence size is presented in Table <ref type="table" coords="5,458.32,123.82,3.74,8.64" target="#tab_3">4</ref>. For example, the selfattention mechanism managed to achieve a 100% accuracy for a sequence of 41 tokens for the Reverse task, but could not achieve a 100% accuracy for both the Sort task and the Addition task for a sequence size of 20 (the Sort task achieved a maximum sequence size of 14, while the Addition task achieved a maximum size of 7). Of the six algorithmic tasks tested, active-memory mechanisms were used, either solely or in combination with the self-attention mechanism, in the best-performing model of five of these tasks. For example, the self-attention mechanism achieved an average sequence size of 41.0 for the Reverse Task and 14.0 for the Sort Task, which are lower than those achieved by the "self-attention + persistent-convolution" mechanism <ref type="bibr" coords="5,466.54,266.29,87.35,8.64;5,315.00,277.25,24.59,8.64">(43.7 and 23.3, respectively)</ref>. Furthermore, for the Addition and Multiply Tasks, the active-memory mechanisms across the board outperformed both the self-attention mechanism and the combination of the self-attention mechanism and the active-memory mechanism. For example, the traditional convolution operator, for the Addition Task, outperformed the self-attention mechanism and the "self-attention + convolutional" mechanism by 34.0 and 4.7 respectively. The results show that the active-memory mechanisms achieve equal, or superior, results to a traditional self-attention mechanism.</p><p>Self-attention, used alone, only performed optimally on the Remember task, and equally well on the Not task. Interestingly, across all models for the Addition and Multiply tasks, the self-attention mechanism reliably led to poor results; not only does the self-attention mechanism, alone, achieve the poorest results, but the combination of the self-attention mechanism and any active-memory system performed worse then the active-memory system alone. This is in direct contrast to the Sort task and the Reverse task, where the combination of self-attention mechanism and the active-memory achieve the best results.</p><p>The self-attention mechanism would, in theory, outperform active-memory mechanisms for the Remember task. This is because, in order to adequately perform the Remember task, the model must be capable of calculating an output based on long-range dependencies, which active-memory cannot match at a large enough sequence length. Other tasks do require a long-range dependency in order to operate well at large sequence sizes, but are dependent on the model performing other tasks as well. For example, the addition task requires to model long range-dependencies and perform binary addition. The self-attention mechanism, although it can learn these long-range dependencies, cannot access all necessary tokens at a given time to adequately perform binary addition. This is vindicated by the experimental results. In Table <ref type="table" coords="5,535.92,651.68,3.74,8.64" target="#tab_3">4</ref>, the self-attention mechanism achieved the highest results on the Remember task. This would suggest that, if the algorithmic task only requires a long-range dependency, then the selfattention mechanism will outperform active-memory mech- The average sequence length that each operator was capable of gaining 100% accuracy within 100 epochs over 3 runs. The higher the sequence size, the better the model learned. For the Reverse and Sort tasks, the combination of self-attention mechanism and persistent-convolution achieved the best results. For the Sort, Addition, and Multiply tasks, the self-attention mechanism was beaten by the active-memory mechanisms. For the Addition and Multiply tasks, the mere use of a self-attention mechanism alongside an active-memory mechanism actively decreased results. The highest possible sequence that can be learned over these epochs is 104 in the Not task. The self-attention mechanism achieved the best result only for the Remember task.</p><p>anisms when used alone. In comparison, the self-attention mechanism is incapable of matching the results of activememory for all other tasks. These findings appear to vindicate the statement made by <ref type="bibr" coords="6,169.11,287.76,66.46,8.64">Kaiser et. al [2]</ref>; whenever the sequential task requires the model to focus on multiple tokens at every time-step, using an attention mechanism will lead to extremely poor results, especially in comparison to active-memory models.</p><p>It is worth noting that, for each of the active-memory mechanisms operating alone, none of the three achieved a 100% accuracy for any sequence over a size of 37 for the Remember task. This is because, given the kernel size of 20 and 4 layers, the model is only capable of seeing 37 time-steps across. Therefore, the model cannot see 37 time-steps across and, therefore, cannot perform the Remember task at this sequence size or any larger sequence size. This displays the importance of utilizing both a self-attention mechanism, which can be utilized for analyzing long-range dependencies, and an active-memory mechanism, which can extract features that the self-attention mechanism cannot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion of Results</head><p>The experiments above suggest that, across most tasks, a combination of a self-attention mechanism and an activememory mechanism, at worst, perform comparably to a purely attention-based model, and at best surpass an attention model, with the exception of the Remember task. However, for some algorithmic tasks, we note that the mere inclusion of a self-attention mechanism actively hinders performance.</p><p>Models that combine both the attention mechanism and active-memory mechanisms outperformed both attentiononly and active-memory-only models for language modelling. This suggests that, for language modelling tasks, both active-memory mechanisms and attention mechanisms are capable of extracting features that the other mechanism is not capable of extracting, and that both mechanisms operate optimally when used alongside each other.</p><p>The findings are further abstracted by studying the effect of various algorithmic tasks; in cases where only a single token needs to be focused on, the self-attention mechanism matches the most ardent active-memory, while active-memory mecha-nisms radically outperform self-attention for other tasks. This would imply that various time-dependencies that cannot be analyzed by a self-attention mechanism can be analyzed by active-memory.</p><p>It is worth noting that, for the Not function, all models learn optimally. This is likely due to the fact that the output of each time-step depends only on the input at this time-step, and each model can analyze this dependency equally efficiently. Also, based on the results of the Remember task, the self-attention mechanism can attain greater long-range dependency in comparison to the active-memory mechanisms.</p><p>Finally, we note that, for the Remember function, both mechanisms, when used alone, outperform the two mechanisms used together. For every other task, a combination of the self-attention and active-memory would improve upon at least one of the mechanisms when used alone. We are unsure exactly what has led to this result. This will require further investigation in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we investigate the Transformer's self-attention mechanism in comparison to a variety of active-memory mechanisms. We experiment on two types of tasks: the language modeling task and the algorithmic task. Our results show that the self-attention mechanism can be improved by an active-memory mechanism alone or by a combination of the two. Our results have implications for wider sequence modeling tasks, which are currently dominated by self-attention based models.</p><p>Our code and models used in experiments are available at: https://github.com/Anon-111/Active-Memory.</p><p>In the future, we will further explore the use of activememory for sequence-to-sequence tasks, such as machine translation. We will also analyze the empirical differences between the studied algorithmic tasks, and investigate why the self-attention mechanism may assist one task but harm another.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,315.00,257.48,243.00,7.77"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Self-Attention + Convolutional Operator Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="1,230.00,102.23,472.62,265.72"><head></head><label></label><figDesc></figDesc><graphic coords="1,230.00,102.23,472.62,265.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,54.00,174.83,28.93,7.77"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,54.00,95.19,243.00,17.74"><head>Table 3 :</head><label>3</label><figDesc>The binary multiplication task. The above example contains two input numbers, 12 and 21, and the output number 252.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,54.00,58.38,465.28,124.22"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table coords="6,92.72,58.38,426.56,101.70"><row><cell>Model</cell><cell cols="4">Reverse Sort Addition Multiply</cell><cell>Not</cell><cell>Remember</cell></row><row><cell>CGRU</cell><cell>7.7</cell><cell>5.7</cell><cell>16.3</cell><cell>9.0</cell><cell>104.0</cell><cell>9.0</cell></row><row><cell>Self-Attention</cell><cell>41.0</cell><cell>14.0</cell><cell>7.0</cell><cell>7.0</cell><cell>104.0</cell><cell>57.0</cell></row><row><cell>Convolution</cell><cell>17.7</cell><cell>20.7</cell><cell>41.0</cell><cell>17.0</cell><cell>104.0</cell><cell>36.0</cell></row><row><cell>Persistent-Convolution</cell><cell>25.0</cell><cell>20.3</cell><cell>38.3</cell><cell>16.3</cell><cell>104.0</cell><cell>35.0</cell></row><row><cell>Highway-Convolution</cell><cell>19.7</cell><cell>16.7</cell><cell>35.7</cell><cell>13.7</cell><cell>104.0</cell><cell>33.0</cell></row><row><cell>Self-Attention + Convolution</cell><cell>41.0</cell><cell>23.3</cell><cell>36.3</cell><cell>12.3</cell><cell>104.0</cell><cell>26.0</cell></row><row><cell>Self-Attention + Persistent-Convolution</cell><cell>43.7</cell><cell>23.3</cell><cell>35.0</cell><cell>12.3</cell><cell>104.0</cell><cell>27.0</cell></row><row><cell>Self-Attention + Highway-Convolution</cell><cell>41.0</cell><cell>20.0</cell><cell>34.3</cell><cell>11.7</cell><cell>104.0</cell><cell>24.7</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,59.20,70.99,71.45,9.53" xml:id="b0">
	<monogr>
		<title level="m" type="main">Figure 1: (A) Forward convolutional units (exploited in this work), (B) recurrent convolutional block, (C) residual convolutional units (exploited in this work) and (D) recurrent residual convolutional units (Alom et al., 2018).</title>
		<author>
			<persName coords=""><surname>Alom</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.1767/fig-1</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alom et al., 2018]</note>
</biblStruct>

<biblStruct coords="7,135.63,71.89,161.36,8.64;7,65.62,82.85,231.38,8.64;7,65.62,93.81,231.38,8.64;7,65.62,104.76,231.38,8.64;7,65.62,115.72,231.38,8.64;7,65.62,126.50,201.06,8.82" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Md</forename><surname>Zahangir Alom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tarek</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Westbery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paheding</forename><surname>Sidike</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mst</forename><surname>Sharmina Nasrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><forename type="middle">C</forename><surname>Van Esesn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijayan</forename><forename type="middle">K</forename><surname>Awwal</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01164</idno>
		<title level="m">The history began from alexnet: A comprehensive survey on deep learning approaches</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Md Zahangir Alom, Tarek M. Taha, Christopher Yakopcic, Stefan Westbery, Paheding Sidike, Mst Sharmina Nasrin, Brian C Van Esesn, Abdul A S. Awwal, and Vijayan K. Asari. The history began from alexnet: A comprehensive survey on deep learning ap- proaches. arXiv preprint arXiv:1803.01164, 2018.</note>
</biblStruct>

<biblStruct coords="7,58.29,140.20,63.51,9.53" xml:id="b2">
	<monogr>
		<title level="m" type="main">Comment on Bai et al.</title>
		<author>
			<persName coords=""><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.5194/amt-2020-445-rc2</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Copernicus GmbH</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Bai et al., 2019]</note>
</biblStruct>

<biblStruct coords="7,126.78,141.09,170.22,8.64;7,65.62,152.05,231.38,8.64;7,65.62,162.83,231.38,8.82;7,65.62,173.79,134.95,8.82" xml:id="b3">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Shaojie Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladlen</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2019.</note>
</biblStruct>

<biblStruct coords="7,58.43,187.48,63.92,9.53" xml:id="b4">
	<monogr>
		<title level="m" type="main">comments on Jiyang Tian et al. (hess-2019-587)</title>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Dai</surname></persName>
		</author>
		<idno type="DOI">10.5194/hess-2019-587-sc1</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Copernicus GmbH</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Dai et al., 2019]</note>
</biblStruct>

<biblStruct coords="7,127.32,188.38,169.67,8.64;7,65.62,199.34,231.38,8.64;7,65.62,210.30,231.38,8.64;7,65.62,221.08,231.38,8.82;7,65.62,232.21,22.42,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</title>
		<author>
			<persName coords=""><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1285</idno>
		<idno type="arXiv">arXiv:1901.02860</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdi- nov. Transformer-XL: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.</note>
</biblStruct>

<biblStruct coords="7,58.70,245.73,83.56,9.53" xml:id="b6">
	<monogr>
		<title level="m" type="main">Referee Comment on manuscript by Dubois-Dauphin et al.</title>
		<author>
			<persName coords=""><surname>Dauphin</surname></persName>
		</author>
		<idno type="DOI">10.5194/cp-2016-64-rc2</idno>
		<imprint>
			<date type="published" when="2016-08-05">2016</date>
			<publisher>Copernicus GmbH</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Dauphin et al., 2016]</note>
</biblStruct>

<biblStruct coords="7,147.25,246.62,149.75,8.64;7,65.62,257.58,231.38,8.64;7,65.62,268.36,231.39,8.82;7,65.62,279.32,100.17,8.82" xml:id="b7">
	<monogr>
		<title level="m" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08083</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language model- ing with gated convolutional networks. arXiv preprint arXiv:1612.08083, 2016.</note>
</biblStruct>

<biblStruct coords="7,58.35,293.01,76.73,9.53" xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Delvin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Delvin et al., 2018]</note>
</biblStruct>

<biblStruct coords="7,140.06,293.91,156.94,8.64;7,65.62,304.87,231.38,8.64;7,65.62,315.83,231.38,8.64;7,65.62,326.61,178.39,8.82" xml:id="b9">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Delvin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jacob Delvin, Ming-Wei Chang, Ken- ton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805, 2018.</note>
</biblStruct>

<biblStruct coords="7,57.99,340.30,239.01,9.53;7,65.62,352.16,231.38,8.64;7,65.62,362.94,222.38,8.82" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Liepins</forename><surname>Freivalds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karlis</forename><surname>Freivalds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renars</forename><surname>Liepins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08727</idno>
		<title level="m">Improving the neural gpu architecture for algorithm learning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Freivalds and Liepins, 2017] Karlis Freivalds and Renars Liepins. Improving the neural gpu architecture for algo- rithm learning. arXiv preprint arXiv:1702.08727, 2017.</note>
</biblStruct>

<biblStruct coords="7,58.50,376.63,82.11,9.53" xml:id="b11">
	<monogr>
		<title level="m" type="main">Review of Gehring et al</title>
		<author>
			<persName coords=""><surname>Gehring</surname></persName>
		</author>
		<idno type="DOI">10.5194/essd-2020-134-rc2</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Copernicus GmbH</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Gehring et al., 2017]</note>
</biblStruct>

<biblStruct coords="7,145.59,377.53,151.41,8.64;7,65.62,388.49,231.38,8.64;7,65.62,399.26,231.38,8.82;7,65.62,410.22,100.17,8.82" xml:id="b12">
	<analytic>
		<title level="a" type="main">A Convolutional Encoder Model for Neural Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p17-1012</idno>
		<idno type="arXiv">arXiv:1705.03122</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017.</note>
</biblStruct>

<biblStruct coords="7,58.98,423.92,65.58,9.53" xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Hao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hao et al., 2019]</note>
</biblStruct>

<biblStruct coords="7,129.54,424.81,167.46,8.64;7,65.62,435.77,231.38,8.64;7,65.62,446.55,231.38,8.82;7,65.62,457.51,100.17,8.82" xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling Recurrence for Transformer</title>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1122</idno>
		<idno type="arXiv">arXiv:1904.03092</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North</title>
		<meeting>the 2019 Conference of the North</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling recurrence for transformer. arXiv preprint arXiv:1904.03092, 2019.</note>
</biblStruct>

<biblStruct coords="7,58.19,471.20,101.50,9.53" xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Bengio</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kaiser and Bengio, 2016]</note>
</biblStruct>

<biblStruct coords="7,164.67,472.10,132.32,8.64;7,65.62,482.88,231.38,8.82;7,65.62,493.84,100.17,8.82" xml:id="b16">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1610.08613</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Lukasz Kaiser and Samy Bengio. Can active memory replace attention. arXiv preprint arXiv:1610.08613, 2016.</note>
</biblStruct>

<biblStruct coords="7,58.19,507.53,238.81,9.53;7,65.62,519.21,231.38,8.82;7,65.62,530.17,100.17,8.82" xml:id="b17">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Sutskever</forename><forename type="middle">;</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1511.08228</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Kaiser and Sutskever, 2015] Lukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.</note>
</biblStruct>

<biblStruct coords="7,59.06,543.86,89.02,9.53" xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditioning Strategies Limit Cellular Injury?</title>
		<author>
			<persName coords=""><forename type="first">Ba</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="DOI">10.4236/wjcd.2014.411065</idno>
	</analytic>
	<monogr>
		<title level="j">World Journal of Cardiovascular Diseases</title>
		<title level="j" type="abbrev">WJCD</title>
		<idno type="ISSN">2164-5329</idno>
		<idno type="ISSNe">2164-5337</idno>
		<imprint>
			<biblScope unit="volume">04</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="539" to="547" />
			<date type="published" when="2014">2014</date>
			<publisher>Scientific Research Publishing, Inc.</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Kingma and Ba, 2014]</note>
</biblStruct>

<biblStruct coords="7,153.06,544.76,143.94,8.64;7,65.62,555.54,231.38,8.82;7,65.62,566.49,129.97,8.82" xml:id="b19">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ba</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</note>
</biblStruct>

<biblStruct coords="7,58.35,580.19,76.73,9.53" xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Merity</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Merity et al., 2016]</note>
</biblStruct>

<biblStruct coords="7,140.06,581.08,156.94,8.64;7,65.62,592.04,231.38,8.64;7,65.62,602.82,194.44,8.82" xml:id="b21">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1609.07843</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.</note>
</biblStruct>

<biblStruct coords="7,319.63,56.58,83.08,9.53" xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Resende</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Resende et al., 2016]</note>
</biblStruct>

<biblStruct coords="7,407.69,57.48,150.31,8.64;7,326.62,68.44,231.38,8.64;7,326.62,79.22,231.38,8.82;7,326.62,90.17,100.17,8.82" xml:id="b23">
	<monogr>
		<title level="m" type="main">One shot generalization in deep generative models</title>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Resende</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daan</forename><surname>Wiestra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05106</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Danilo J. Resende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan Wiestra. One shot generalization in deep generative models. arXiv preprint arXiv:1603.05106, 2016.</note>
</biblStruct>

<biblStruct coords="7,319.30,104.40,238.69,9.53;7,326.62,116.26,231.38,8.64;7,326.62,127.04,231.38,8.82;7,326.62,138.17,22.42,8.64" xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName coords=""><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1162</idno>
		<idno type="arXiv">arXiv:1508.07909</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rico Sennrich</note>
	<note type="raw_reference">Sennrich et al., 2016] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2016.</note>
</biblStruct>

<biblStruct coords="7,319.48,152.08,81.99,9.53" xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Shoeybi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shoeybi et al., 2019]</note>
</biblStruct>

<biblStruct coords="7,406.45,152.98,151.55,8.64;7,326.62,163.94,231.38,8.64;7,326.62,174.90,231.38,8.64;7,326.62,185.68,231.38,8.82;7,326.62,196.64,134.95,8.82" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Mohammad Shoeybi, Mostofa Pat- wary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion pa- rameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.</note>
</biblStruct>

<biblStruct coords="7,318.99,210.86,90.53,9.53" xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Srivastava</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Srivastava et al., 2015]</note>
</biblStruct>

<biblStruct coords="7,414.50,211.76,143.50,8.64;7,326.62,222.54,231.38,8.82;7,326.62,233.50,134.95,8.82" xml:id="b28">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Rupseh</forename><surname>Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1505.00387</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Rupseh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015.</note>
</biblStruct>

<biblStruct coords="7,319.38,247.72,94.40,9.53" xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Sukhbaatar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sukhbaatar et al., 2019]</note>
</biblStruct>

<biblStruct coords="7,418.76,248.62,139.24,8.64;7,326.62,259.58,231.38,8.64;7,326.62,270.54,231.38,8.64;7,326.62,281.32,159.58,8.82" xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive Attention Span in Transformers</title>
		<author>
			<persName coords=""><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1032</idno>
		<idno type="arXiv">arXiv:1907.01470</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. Augmenting self-attention with persistent memory. arXiv preprint arXiv:1907.01470, 2019.</note>
</biblStruct>

<biblStruct coords="7,319.62,295.54,82.99,9.53" xml:id="b31">
	<monogr>
		<title level="m" type="main">Table 1: Comparison of different layers where &lt;i&gt;n&lt;/i&gt; is sequence length, &lt;i&gt;d&lt;/i&gt; is dimension of representation, k is kernel size, and r is neighbourhood size (Vaswani et al., 2017).</title>
		<author>
			<persName coords=""><surname>Vaswani</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerjcs.210/table-1</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Vaswani et al., 2017]</note>
</biblStruct>

<biblStruct coords="7,407.59,296.44,150.40,8.64;7,326.62,307.40,231.38,8.64;7,326.62,318.36,231.38,8.64;7,326.62,329.14,184.47,8.82" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.</note>
</biblStruct>

<biblStruct coords="7,320.26,343.37,71.69,9.53" xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang et al., 2019]</note>
</biblStruct>

<biblStruct coords="7,396.93,344.26,161.06,8.64;7,326.62,355.22,231.38,8.64;7,326.62,366.00,231.38,8.82;7,326.62,377.14,22.42,8.64" xml:id="b34">
	<analytic>
		<title level="a" type="main">A conceptual peer review model for arXiv and other preprint databases</title>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Wang</surname></persName>
			<idno type="ORCID">0000-0001-6217-8865</idno>
		</author>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Zhan</surname></persName>
			<idno type="ORCID">0000-0002-2465-2092</idno>
		</author>
		<idno type="DOI">10.1002/leap.1229</idno>
		<idno type="arXiv">arXiv:1907.05572</idno>
	</analytic>
	<monogr>
		<title level="j">Learned Publishing</title>
		<title level="j" type="abbrev">Learned Publishing</title>
		<idno type="ISSN">0953-1513</idno>
		<idno type="ISSNe">1741-4857</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="219" />
			<date type="published" when="2019-02-06">2019</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Zhiwei Wang, Yao Ma, Zitao Liu, and Jiliang Tang. R-transformer: Recurrent neural network enhanced transformer. arXiv preprint arXiv:1907.05572, 2019.</note>
</biblStruct>

<biblStruct coords="7,320.73,391.19,62.11,9.53" xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu et al., 2019]</note>
</biblStruct>

<biblStruct coords="7,387.83,392.08,170.17,8.64;7,326.62,403.04,231.38,8.64;7,326.62,413.82,231.38,8.82;7,326.62,424.78,100.17,8.82" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10430</idno>
		<title level="m">Pay less attention with lightweight and dynamic attention</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic attention. arXiv preprint arXiv:1901.10430, 2019.</note>
</biblStruct>

<biblStruct coords="7,319.78,439.01,149.67,9.53;7,484.53,439.90,73.46,8.64;7,326.62,450.86,231.38,8.64;7,326.62,461.64,231.38,8.82;7,326.62,472.60,100.17,8.82" xml:id="b37">
	<monogr>
		<title level="m" type="main">Convolutional self-attention networks</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03107</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXix preprint</note>
	<note type="raw_reference">Yang et al., 2019a] Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, and Zhaopeng Tu. Convolutional self-attention networks. arXix preprint arXiv:1904.03107, 2019.</note>
</biblStruct>

<biblStruct coords="7,319.78,486.83,238.22,9.53;7,326.62,498.68,231.38,8.64;7,326.62,509.64,231.38,8.64;7,326.62,520.42,231.38,8.82;7,326.62,531.56,22.42,8.64" xml:id="b38">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yang et al., 2019b] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for lan- guage understanding. arXiv preprint arXiv:1906.08237, 2019.</note>
</biblStruct>

<biblStruct coords="7,319.81,545.61,93.96,9.53" xml:id="b39">
	<monogr>
		<title level="m" type="main">Duan, Zhenzhong</title>
		<author>
			<persName coords=""><surname>Zhenzhong</surname></persName>
		</author>
		<idno type="DOI">10.1093/benz/9780199773787.article.b00054112</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhenzhong et al., 2019]</note>
</biblStruct>

<biblStruct coords="7,418.76,546.50,139.24,8.64;7,326.62,557.46,231.38,8.64;7,326.62,568.42,231.38,8.64;7,326.62,579.20,231.38,8.82;7,326.62,590.16,100.17,8.82" xml:id="b40">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName coords=""><forename type="first">Lan</forename><surname>Zhenzhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Lan Zhenzhong, Mingda Chen, Se- bastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
