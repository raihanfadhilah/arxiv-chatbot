<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention: Marginal Probability is All You Need?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-07">7 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,214.44,143.71,49.58,8.95"><forename type="first">Ryan</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName coords="1,275.76,143.71,100.38,8.95"><forename type="first">Christopher</forename><forename type="middle">L</forename><surname>Buckley</surname></persName>
						</author>
						<title level="a" type="main">Attention: Marginal Probability is All You Need?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-07">7 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">AB445DDFD36FAE25817594AA80FD9166</idno>
					<idno type="arXiv">arXiv:2304.04556v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-22T20:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention mechanisms are a central property of cognitive systems allowing them to selectively deploy cognitive resources in a flexible manner. Attention has been long studied in the neurosciences and there are numerous phenomenological models that try to capture its core properties. Recently attentional mechanisms have become a dominating architectural choice of machine learning and are the central innovation of Transformers. The dominant intuition and formalism underlying their development has drawn on ideas of keys and queries in database management systems. In this work, we propose an alternative Bayesian foundation for attentional mechanisms and show how this unifies different attentional architectures in machine learning. This formulation allows to to identify commonality across different attention ML architectures as well as suggest a bridge to those developed in neuroscience. We hope this work will guide more sophisticated intuitions into the key properties of attention architectures as well suggest new ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Designing neural network architectures with favourable inductive biases lies behind many recent successes in Deep Learning <ref type="bibr" coords="1,118.56,533.87,56.88,8.91" target="#b1">(Baxter, 2000)</ref>. In particular, the attention mechanism has allowed language models to achieve human like generation abilities previously thought impossible <ref type="bibr" coords="1,55.08,569.75,85.91,8.91" target="#b23">(Vaswani et al., 2017)</ref>. The success of the attention mechanism as a domain agnostic architecture has prompted it to be adopted across a huge range of tasks and domains notably reaching state-of-the-art performance in visual reasoning and segmentation tasks <ref type="bibr" coords="1,187.32,617.63,102.97,8.91" target="#b6">(Dosovitskiy et al., 2021;</ref><ref type="bibr" coords="1,54.96,629.51,71.28,8.91" target="#b24">Wang et al., 2022)</ref>. Despite it's success, the role of the attention mechanism remains poorly understood. Indeed, it is unclear to what extent it relates to theories of cognitive attention which inspired it <ref type="bibr" coords="1,342.84,190.43,62.14,8.91" target="#b13">(Lindsay, 2020)</ref>. Here, we aim to provide a parsimonious description grounded in principles of probabilistic inference. This Bayesian perspective provides both a principled method for specifying prior beliefs and reasoning explicitly about the role of the attention variables. Further, understanding the fundamental computation permits us a unified description of different attention mechanisms in the literature. This proceeds in two parts.</p><p>First, we show that 'soft' attention mechanisms (e.g. selfattention, cross-attention, graph attention, which we call transformer attention herafter) can be understood probabilistically as taking an expectation over possible connectivity structures, providing an interesting link between softmax-based attention and marginal likelihood.</p><p>Second, we extend the uncertainty over connectivity to a bayesian setting which, in turn, provides a theoretical grounding for iterative attention mechanisms (slot-attention, perciever and block-slot attention) <ref type="bibr" coords="1,307.08,417.59,95.05,8.91" target="#b14">(Locatello et al., 2020;</ref><ref type="bibr" coords="1,409.20,417.59,77.29,8.91" target="#b21">Singh et al., 2022;</ref><ref type="bibr" coords="1,493.56,417.59,49.17,8.91;1,307.44,429.59,23.24,8.91" target="#b11">Jaegle et al., 2021)</ref> and Modern Continuous Hopfield Networks <ref type="bibr" coords="1,307.08,441.47,91.56,8.91" target="#b17">(Ramsauer et al., 2021)</ref>.</p><p>Additionally, we apply iterative attention to Predictive Coding Networks, an influential theory in computational neuroscience, creating a new theoretical bridge between machine learning and cognitive science.</p><formula xml:id="formula_0" coords="1,318.12,533.45,210.41,54.48">Attention(Q, K, V ) = p(E | Q, K) sof tmax( QW Q W T K K T √ d k ) V = E p(E|Q,K) [V ]</formula><p>A key observation is that the attention matrix can be seen as the posterior distribution over an adjacency structure, E, and the full mechanism as computing an expectation of the value function V (X) over the posterior beliefs about the possible relationships that exist between key and query.</p><p>This formalism provides an alternate Bayesian theoretical framing within which to understand attention models, which contrasts with the original framing in terms of database management systems and data retrieval, providing a unifying framework to describe different attention architectures. Describing their difference only in terms of their edge relationships supporting more effective analysis and development of new architectures. Additionally providing a principled understanding of the difference between hard and soft attention models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>• A unifying probabilistic framework for understanding attention mechanisms.</p><p>• We show self-attention and cross-attention can be seen as computing a marginal likelihood over possible network structures.</p><p>• We show that slot-attention, block-slot-attention and modern continuous hopfield networks can all be seen as collapsed variational inference, where the possible network structures form the collapsed variables.</p><p>• Provide a bridge to Bayesian conceptions of attention from computational neuroscience, through the lens of Predictive Coding Networks.</p><p>• Provide a framework for reasoning about hard attention, and efficient approximations to the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Attention as bi-level optimisation Mapping feed-forward architecture to a minimisation step on a related energy function has been called unfolded optimisation <ref type="bibr" coords="2,235.32,439.31,55.41,8.91;2,55.44,451.31,21.48,8.91" target="#b8">(Frecon et al., 2022)</ref>. Taking this perspective can lead to insights about the inductive biases involved for each architecture. It has been shown that the cross-attention mechanism can be viewed as an optimisation step on the energy function of a form of Hopfield Network <ref type="bibr" coords="2,174.24,499.07,92.40,8.91" target="#b17">(Ramsauer et al., 2021)</ref>, providing a link between attention and associative memory. Whilst <ref type="bibr" coords="2,84.84,523.07,74.72,8.91" target="#b25">(Yang et al., 2022)</ref> extend this view to account for self-attention. Our framework distinguishes hopfield attention, which does not allow an arbritary value matrix, from the standard attention mechanisms. Whilst there remains a strong theoretical connection, it places the Hopfield Energy as an instance of variational free energy, aligning more closely with iterative attention mechanisms such as slotattention.</p><p>Relationship to gaussian mixture model Previous works that have taken a probabilistic perspective on the attention mechanism note the connection to inference in a gaussian mixture model <ref type="bibr" coords="2,119.52,660.47,84.01,8.91" target="#b10">(Gabbur et al., 2021;</ref><ref type="bibr" coords="2,207.60,660.47,82.69,8.91" target="#b16">Nguyen et al., 2022;</ref><ref type="bibr" coords="2,55.44,672.47,70.20,8.91" target="#b5">Ding et al., 2020)</ref>. Indeed <ref type="bibr" coords="2,170.28,672.47,84.68,8.91" target="#b0">(Annabi et al., 2022)</ref> directly show the connection between the Hopfield energy and the variational free energy of a gaussian mixture model. Although gaussian mixture models, a special case of the framework we present here, are enough to explain cross attention they do not capture slot or self-attention. Further our framework allows us to extend the structural inductive biases beyond what can be expressed in a gaussian mixture model and capture the relationship to hard attention.</p><p>Latent alignment and hard attention Several attempts have been made to combine the benefits of soft (differentiability) and hard attention. Most approaches proceed by sampling, e.g., using the REINFORCE estimator <ref type="bibr" coords="2,342.72,183.83,79.04,8.91">(Deng et al., 2018)</ref> or a topK approximation <ref type="bibr" coords="2,307.08,195.83,85.56,8.91" target="#b20">(Shankar et al., 2018)</ref>. The one most similar to ours embeds the full forward-backward algorithm within a forward pass <ref type="bibr" coords="2,327.60,219.71,69.84,8.91" target="#b12">(Kim et al., 2017)</ref>, our approach differs by offering a parsimonious description in terms of marginalisation over an implicit graphical model.</p><p>Collapsed Inference Collapsed variational inference has most notably been employed in topic modelling <ref type="bibr" coords="2,500.28,273.59,42.45,8.91;2,307.44,285.47,21.48,8.91" target="#b22">(Teh et al., 2006)</ref>. To our knowledge, linking collapsed inference to attention in deep learning is completely novel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Transformer Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.0.1.">ATTENTION AS EXPECTATION</head><p>We begin by demonstrating transformer attention is best seen as an expectation over latent variables. In the case of self and cross-attention, the expectation of a neural network with respect to possible adjacency structures.</p><p>Let x = (x 1 , .., x n ) be observed variables, φ be some set of latent variables, and y a variable we need to predict. Given a latent variable model p(y, x, φ) = p(y | x, φ)p(x, φ), where p(y | x, φ) is parameterised by some function v(y, x, φ) e.g. a neural network.</p><p>Our goal is to find p(y | x), however φ are unobserved so we calculate the marginal likelihood.</p><formula xml:id="formula_1" coords="2,358.08,525.81,132.51,20.96">p(y | x) = φ p(φ | x)v(y, x, φ)</formula><p>Importantly, the softmax function is a natural representation for the posterior</p><formula xml:id="formula_2" coords="2,358.80,588.67,131.07,54.98">p(φ | x) = p(x, φ) φ p(x, φ) p(φ | x) = sof tmax(ln p(x, φ))</formula><p>Hence, transformer attention can be seen as weighting v(x, φ) by the posterior distribution p(φ | x).</p><formula xml:id="formula_3" coords="2,331.80,682.29,209.72,37.40">p(y | x) = φ sof tmax(ln p(x, φ))v(y, x, φ) = E p(φ|x) [v(y, x, φ)]<label>(1)</label></formula><p>We claim ( <ref type="formula" coords="3,101.55,70.31,3.87,8.91" target="#formula_3">1</ref>) is exactly the equation underlying self and cross-attention. To make a more direct connection, we present the specific generative models corresponding to them. The latent variables φ are identified as possible relationships, or edges, between each of the observed variables x (keys and queries).</p><p>A natural formalism for modelling these graphical relationships is Markov Random Fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.0.2.">PAIRWISE MARKOV RANDOM FIELDS</head><p>Given a set of random variables X = (X v ) v∈V with probability distribution [p] and a graph G = (V, E). The variables form a pairwise Markov random field (MRF) with respect to G if the joint density function P (X = x) = p(x) factorises as follows</p><formula xml:id="formula_4" coords="3,99.96,267.33,136.50,27.68">p(x) = 1 Z exp v∈V ψ v + e∈E ψ e</formula><p>where Z is the partition function ψ v (x v ) and ψ e = ψ u,v (x u , x v ) are known as the node and edge potentials respectively 1 .</p><p>Beyond the typical set-up, we add a structural prior p(E) over the adjacency structure of the underlying graph.</p><formula xml:id="formula_5" coords="3,83.52,373.65,169.50,45.93">p(x, E) = P (x | E)P (E) = 1 Z p(E) exp v∈V ψ v + e∈E ψ e</formula><p>We briefly remark that (1) respects factorisation of [p] in the following sense; if the distribution admits a factorisation with respect to the latent variables p(x, φ) = i f i (x, φ i ) and v(x, φ) = i v i (x, φ i ) then (applying the linearity of expectation) we may write</p><formula xml:id="formula_6" coords="3,102.96,500.37,186.56,20.73">E p(φ|x) [v(x, φ)] = i E p(φi|x) [v i ]<label>(2)</label></formula><p>Permitting each factor to be marginalised independently.</p><p>In the case of an MRF, such a factorisation is natural. If the distibution over edges factorises into local distributions</p><formula xml:id="formula_7" coords="3,55.44,568.89,235.88,45.52">p(E) = i p(E i ) (using independence properties of the MRF) we can write p(x, E) = 1 Z i f i (x, E i ) where each f i = P (E i ) exp v∈V ψ v e∈Ei ψ e is itself an unnor- malised MRF.</formula><p>To recover cross-attention and self-attention are such models with we need only specify a structural prior and potential functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.0.3.">CROSS ATTENTION</head><p>• Key nodes K = (x 1 , .., x n )</p><p>1 See <ref type="bibr" coords="3,86.76,709.02,65.99,8.02" target="#b19">(Shah et al., 2021)</ref> for a precise definition. • Query nodes Q</p><formula xml:id="formula_8" coords="4,66.84,68.21,222.54,44.52">= (x ′ 1 , ..., x ′ m ) • Structural prior p(E) = m i=1 p(E i ), where E i ∼ U nif orm{(x 1 , x ′ i ), .., (x n , x ′ i )}</formula><p>, such that each query node is uniformly likely to connect to each key node.</p><formula xml:id="formula_9" coords="4,66.84,131.45,183.17,13.08">• Edge potentials ψ(x j , x ′ i ) = x ′T i W T Q W K x j</formula><p>, in effect measuring the similarity of x j and x ′ i under a certain transformation.</p><formula xml:id="formula_10" coords="4,66.84,176.37,184.55,10.76">• Value function V i (K, Q, E i ) = W V x s(Ei)</formula><p>, a linear transformation applied to the node, x s(Ei) , the start of the edge E i .</p><p>Taking the posterior expectation in each of the factors defined in two (2) gives the standard cross-attention mechanism</p><formula xml:id="formula_11" coords="4,55.68,269.81,227.81,108.24">E p(Ei|Q,K) [V i ] = j sof tmax j (x ′T i W T Q W K x j )W V x j E p(E|Q,K) [V ] = sof tmax(Q T W T Q Q K K)W V K 3.0.4. SELF ATTENTION • Nodes K = Q = (x 1 , .., x n ) • Structural prior p(E) = n i=1 p(E → i )</formula><p>, where E → i ∼ U nif orm{(x 1 , x i ), .., (x n , x i )}, such that each node is uniformly likely to connect to every other node.</p><p>• Edge potentials ψ(k j , k i ) = x T i W T Q W K x j , in effect measuring the similarity of x j and x ′ i under a certain transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Value function</head><formula xml:id="formula_12" coords="4,139.08,453.33,112.31,10.76">V i (K, Q, E i ) = W V x s(Ei)</formula><p>, a linear transformation applied to the node, x s(Ei) , the start of the edge E i .</p><p>Again, taking the posterior expectation in each of the factors defined in two (2) gives the standard self-attention mechanism</p><formula xml:id="formula_13" coords="4,61.68,547.09,220.61,46.24">E p(Ei|Q,K) [V i ] = j sof tmax j (x T i W T Q W K x j )W V x j E p(E|Q,K) [V ] = sof tmax(K T W T Q W K K)W V K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Iterative Attention</head><p>We continue by extending attention to full Bayesian inference. In essence applying the attention trick, marginalisation of attention variables, to the variational free energy (a.k.a the ELBO).</p><p>Modern Continuous Hopfield Networks can be seen as a particular instance of this class of system, allowing us to reproduce the 'hopfield attention' updates of <ref type="bibr" coords="4,307.08,70.31,93.68,8.91" target="#b17">(Ramsauer et al., 2021)</ref> within a probabilistic context. Under different structural priors we recover other iterative attention models; slot-attention <ref type="bibr" coords="4,445.80,94.19,92.64,8.91" target="#b14">(Locatello et al., 2020)</ref>, block-slot attention <ref type="bibr" coords="4,395.64,106.19,80.84,8.91" target="#b21">(Singh et al., 2022)</ref> and Perciever <ref type="bibr" coords="4,307.08,118.07,77.64,8.91" target="#b11">(Jaegle et al., 2021)</ref>. Further, we showcase a specific advantage of bayesian attention, hard attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.1.">COLLAPSED INFERENCE</head><p>We present a version of collapsed variational inference <ref type="bibr" coords="4,307.08,184.31,69.56,8.91" target="#b22">(Teh et al., 2006)</ref> showing how this results in a bayesian attention mechanism. The term attention mechanism is apt due to the surprising similarity in form between the variational updates (6) and neural attention mechanism (1).</p><p>Our setting is the latent variable model p(x, z, φ), where x are observed variables, and z, φ, are latent variables. Typically we wish to infer z given x.</p><p>Collapsed inference proceeds by marginalising out the extraneous latent variables φ p(x, z)</p><formula xml:id="formula_14" coords="4,407.40,312.09,134.12,20.96">= φ p(x, z, φ)<label>(3)</label></formula><p>We define a recognition density q(z) ∼ N (z; µ) and optimise the variational free energy with respect to the parameters, µ, of this distribution.</p><formula xml:id="formula_15" coords="4,343.20,391.89,162.48,17.04">min µ F (x, µ) = E q [ln q µ (z) -ln p(x, z)]</formula><p>Under a typical Laplace approximation, we can write the variational free energy as F ≈ -ln p(x, µ)<ref type="foot" coords="4,483.24,426.57,3.49,6.23" target="#foot_1">2</ref> . Substituting in (3) and taking the derivative with respect to the variational parameters yields,</p><formula xml:id="formula_16" coords="4,342.00,472.53,199.52,58.41">F (x, µ) = -ln φ p(x, µ, φ) ∂F ∂µ = - 1 φ p(x, µ, φ) φ ∂ ∂µ p(x, µ, φ)<label>(4)</label></formula><p>Which connects bayesian attention with the standard attention (1). To clarify this, we employ the log-derivative trick, substituting p θ = e ln p θ and re-express (4) in two ways:</p><formula xml:id="formula_17" coords="4,315.24,584.47,226.28,38.23">∂F ∂µ = - φ sof tmax φ (ln p(x, µ, φ)) ∂ ∂µ ln p(x, µ, φ)<label>(5)</label></formula><formula xml:id="formula_18" coords="4,353.64,633.19,187.88,22.85">∂F ∂µ = E p(φ|x,µ) [- ∂ ∂µ ln p(x, µ, φ)]<label>(6)</label></formula><p>The first form reveals the softmax which is ubiquitous in all attention models. The second, suggests the variational update should be evaluated as the expectation of the typical variational gradient (the term within the square brackets) with respect to the posterior over the parameters represented by the random variable φ.</p><p>In other words, bayesian attention is exactly the nueral attention mechanism applied iteratively, where the value function is the variational free energy gradient. We derive updates for a general MRF before again recovering (iterative) attention models in the literature by specifying particular distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.2.">FREE ENERGY OF A MARGINALISED MRF</head><p>Recall the factorised MRF, p(E)</p><formula xml:id="formula_19" coords="5,55.44,225.45,235.65,35.84">= i p(E i ). p(x, E) = 1 Z i f i (x, E i ) with each f i = P (E i ) exp v∈V ψ v e∈Ei ψ e .</formula><p>Independence properties mean the marginalisation necessary for collapsed inference can be simplified</p><formula xml:id="formula_20" coords="5,107.04,299.13,134.79,27.56">E p(x, E) = 1 Z i Ei f i (x, E i )</formula><p>In an inference setting the nodes are partitioned into observed nodes, x, and latent nodes, z. The variational free energy (4) and the associated forms of it's derivative can be expressed</p><formula xml:id="formula_21" coords="5,81.00,399.81,182.09,60.20">F (x, µ, θ) = - i ln Ei f i (x, µ, E i ) ∂F ∂µ j = - i Ei sof tmax(f i (x, µ, E i )) ∂f i ∂µ j</formula><p>Similar to hard attention approaches, the random variable E is an explicit alignment variable. However, unlike hard attention, we avoid inferring E explicitly using the collapsed inference approach outlined above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.3.">QUADRATIC POTENTIALS AND THE CONVEX CONCAVE PROCEDURE</head><p>We follow <ref type="bibr" coords="5,101.28,564.23,94.52,8.91" target="#b17">(Ramsauer et al., 2021)</ref> in using the CCCP to derive a fixed point equation, which necessarily reduces the free energy.</p><p>Assuming the node potentials are quadratic ψ(x i ) = -1 2 x 2 i and the edge potentials have the form ψ(x i , x j ) = x i W x j .</p><formula xml:id="formula_22" coords="5,88.44,638.59,201.08,27.35">µ * j = i Ei sof tmax(g i (x, µ, E i )) ∂g i ∂µ j (7)</formula><p>Where</p><formula xml:id="formula_23" coords="5,84.00,677.73,65.49,11.96">g i = e∈Ei ψ e .</formula><p>By way of the CCCP <ref type="bibr" coords="5,152.16,696.35,113.64,8.91">(Yuille &amp; Rangarajan, 2001)</ref>, this fixed point equation has the property F (x, µ * j , θ) ≤ F (x, µ j , θ) with equality if and only if µ * j is a stationary point of F .</p><p>We follow the 3 in specifying specific structural priors and potential functions to recover different iterative attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.4.">HOPFIELD-STYLE CROSS ATTENTION</head><p>Let the observed x = (x 1 , .., x n ) and latent nodes z = (z 1 , .., z m ) have the following structural prior p(E) = m i=1 p(E i ), where E i ∼ U nif orm{(x 1 , z i ), .., (x n , z i )}. And define edge potentials ψ(x j , z i ) = z i Q T Kx j , Application of ( <ref type="formula" coords="5,348.99,214.31,3.87,8.91">7</ref>)</p><formula xml:id="formula_24" coords="5,330.48,235.73,187.01,22.56">µ * i = j sof tmax j (µ i W T Q W K x j )W T Q W K x j</formula><p>When µ i is initialised to some query ξ the system <ref type="bibr" coords="5,307.08,289.19,94.16,8.91" target="#b17">(Ramsauer et al., 2021)</ref> the fixed point update is given by</p><formula xml:id="formula_25" coords="5,307.44,299.09,234.05,31.83">µ * i (ξ) = E p(Ei|x,ξ) [W T Q W K x t(Ei) ]. When the patterns x are well separated, µ * i (ξ) ≈ W T Q W K x j</formula><p>, where W T Q W K x j is the closest vector and hence can be used as an associative memory. 4.0.5. SLOT ATTENTION Slot attention <ref type="bibr" coords="5,366.12,380.87,92.36,8.91" target="#b14">(Locatello et al., 2020)</ref> is an object centric learning module built on top of an iterative attention mechanism. Here we show this is a simple adjustment of the prior beliefs on our edge set.</p><p>With the same set of nodes and potentials, replace the prior over edges with p(E) = n j=1 p(E j ), E j ∼ U nif orm{(x j , z 1 ), .., (x j , z m )}</p><formula xml:id="formula_26" coords="5,341.64,481.13,164.69,22.56">µ * i = j sof tmax i (µ i Q T Kx j )Q T Kx j</formula><p>Whilst the original slot attention employed an RNN to aid the basic update shown here, the important feature is that the softmax is taken over the 'slots', µ. This forces competition between slots to account for the observed variables, forcing object centric representations. For example, if the observed variables x are image patches, the slots are forced to cluster similar patches together in order increase the overall likelihood of said patches. The word cluster is accurate, in fact there is an exact equivalence between this mechanism and a step of EM on a gaussian mixture model. 4.0.6. BLOCK SLOT ATTENTION <ref type="bibr" coords="5,307.08,672.47,76.40,8.91" target="#b21">(Singh et al., 2022)</ref> suggest combining an associative memory ability with an object-centric slot-like ability and provide an iterative scheme for doing so, alternating between slot-attention and hopfield updates.  ∼ U nif orm{(x j , z 1 ), .., (x j , z m )}, Ẽk ∼ U nif orm{(z 1 , m k ), .., (z m , m k )}, with edge potentials between X and Z given by ψ(x j , z i ) = z i Q T Kx j and between Z and M , ψ(z i , m k ) = z i • m k applying (7) gives</p><formula xml:id="formula_27" coords="6,90.24,474.29,163.49,51.36">µ * i = j sof tmax i (µ i Q T Kx j )Q T Kx j + k sof tmax k (µ i • m k )m k</formula><p>In the original block-slot attention each slot z i is broken into blocks, where each block can access block-specific memories i.e. z k } k≤l . Allowing objects to be represented by slots which in turn disentangle features of each object in different blocks. We presented a single block version above, however it is easy to see that the update extends to the multiple block version applying (7) gives</p><formula xml:id="formula_28" coords="6,90.24,667.85,163.49,51.36">µ * i = j sof tmax i (µ i Q T Kx j )Q T Kx j + k,b sof tmax k (µ (b) i • m (b) k )m (b) k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Predictive Coding Networks</head><p>Predictive Coding Networks (PCN) have emerged as an influential theory in computational neuroscience <ref type="bibr" coords="6,343.32,113.63,93.13,8.91" target="#b18">(Rao &amp; Ballard, 1999;</ref><ref type="bibr" coords="6,443.04,113.63,99.25,8.91" target="#b9">Friston &amp; Kiebel, 2009;</ref><ref type="bibr" coords="6,307.44,125.51,82.44,8.91" target="#b2">Buckley et al., 2017)</ref>. Building on theories of perception as inference and the Bayesian brain, PCNs perform approximate Bayesian inference by minimising the variational free energy which is manifested in the minimisation of local prediction errors. The continuous time dynamics at an individual neuron are given by</p><formula xml:id="formula_29" coords="6,357.36,205.03,134.74,27.82">∂F ∂µ i = - φ - k φ ǫ φ + φ + k φ ǫ φ w φ</formula><p>Where ǫ are prediction errors, w represent synaptic strength and k are node specific precisions representing uncertainty in the generative model <ref type="bibr" coords="6,403.32,268.67,85.92,8.91" target="#b15">(Millidge et al., 2022)</ref>.</p><p>A natural extension is to apply collapsed inference over the set of incoming and out going connection, i.e. a locally factorised prior over possible connectivity. In the notation of the previous section, we have an MRF with a hierarchical structure Z = {Z (0) , ..., Z (l) , ..., Z (N ) } where the prior on edges factorises into layerwise p(E</p><formula xml:id="formula_30" coords="6,307.44,344.33,235.88,104.52">(l) ) = {(z i , z j ) : (z i , z j ) ∈ Z (l-1) × Z (l) } and potential func- tions φ(z i , z j ) = ǫ 2 i,j = k j (z j -w i,j z i ) 2 . ∂F ∂µ i = - φ - sof tmax(-ǫ φ 2 )k φ ǫ φ + φ + sof tmax(-ǫ φ 2 )k φ ǫ φ w φ</formula><p>The resulting dynamics induce a "normalisation" across prediction errors received by a neuron through the softmax function. This dovetails nicely with theories of attention as normalisation in psychology and neuroscience. In contrast previous predictive coding based theories of attention have focused on the precision terms, k, due to their ability to up and down regulate the impact of prediction errors <ref type="bibr" coords="6,307.08,550.43,101.66,8.91" target="#b7">(Feldman &amp; Friston, 2010</ref>). Here we see the softmax term can also perform this regulation, while also exhibiting the fast winner-takes-all dynamics that are associated with cognitive attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Discussion</head><p>In this section we will briefly discuss what can be gained from looking at the attention mechanism as a problem of inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">HARD ATTENTION</head><p>Recall (1) neural attention may be viewed as calculating an expectation over latent variables E p(φ|x) <ref type="bibr" coords="6,467.40,707.61,29.33,9.96">[v(x, φ)</ref>]. Here the mechanism is 'soft' because we weight multiple possibilities of attention variable φ. Hard attention, on the other hand, proceeds with a single sample from p(φ | x). It has been argued this is more biological, more interpretable and has lower computational complexity. Previously the inferior performance of hard-attention has been attributed to it's hard to train, stochastic nature. However, our framing of soft attention as exact marginalisation offers an alternate explanation. Stochastic approximations (hard attention) will always suffer compared with exact marginalisation (soft attention). Further our framework provides a method for seamlessly interchanging hard and soft-attention. Since the distribution p(φ | x) a the categorical distribution, at any point (during training or inference) it is possible to implement hard attention by taking a single sample φ * from p(φ | x) yielding v(x, φ * ).</p><p>There are two issues with this approach to collapsing the attention distribution. First, the single sample will collapse any uncertainty, secondly calculation of p(φ | x), in order to sample, still incurs a quadratic penalty O(n 2 ). However we can employ tools from probability theory to help us analyse the cost of sampling, and linear approximations to the attention distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">EFFICIENT TRANSFORMERS</head><p>Consider some distribution q attempting to approximate p(φ | x) we can quantify the information loss with the relative entropy L[p, q] D KL [q(φ) || p(φ | x)] = H[q] + E q [p(φ | x)]</p><p>In the hard attention approximation a single sample from p is used as an approximation L[p, q] = -ln p(φ * | x) and perhaps intuitively E[L] = H[p] i.e. hard attention is a good approximation when the attention distribution is low-entropy which can be controlled by the temperature parameter (Appendix ??).</p><p>Many of the efficient alternatives to attention, such as lowrank and linear approximations, can be cast as approximating p(φ | x) with q(φ | x) where calculating q is less expensive than exact marginalisation. Estimating L could be used to quantify the relative information loss when using these alternatives. Another direction taken to reduce computational complexity of the attention mechanism is sparsification the attention matrix, which in our framework reduces to adjustments to the prior over edges (Appendix ??).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">NEW DESIGNS</head><p>The main difference between the description presented and previous probabilistic descriptions is to view soft attention as a principled, exact, probabilistic calculation, with respect to an implicit probabilistic model, as opposed to an impoverished approximation. This leads to possibility of designing new attention mechanisms by altering the distribution that the mechanism marginalises over, either by adjusting the structural prior, or the potential functions. We hope this will enable new architectures to be designed in a principled manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,306.96,513.33,236.15,8.13;3,307.44,524.34,235.43,8.02;3,307.44,535.22,233.92,8.05;3,307.44,546.30,234.02,8.02;3,307.44,557.22,113.60,8.02"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Comparison of different attention modules in the literature, the highlighted edges is representative of the marginalisation being performed for the random variable E1, in 1a and 1b all nodes are observed, as opposed to 1c and 1d, where there are latent nodes (indicated in grey).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,117.60,233.61,109.72,8.13"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Block Slot Attention</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,70.68,679.14,220.28,8.02;1,55.44,689.06,206.36,8.05"><p>School of Engineering and Informatics, University of Sussex. Correspondence to: Ryan Singh &lt;rs773@sussex.ac.uk&gt;.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,323.52,699.06,217.77,8.02;4,307.44,709.02,75.97,8.02"><p>See appendix for a more principled derivation taking account of higher order terms</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,307.44,164.63,167.61,8.91;7,491.40,164.63,51.68,8.91;7,317.40,176.51,224.10,8.91;7,317.04,188.51,184.77,8.91;7,522.00,188.51,19.89,8.91;7,317.40,201.50,187.77,7.59;7,317.40,212.39,92.49,8.91" xml:id="b0">
	<analytic>
		<title level="a" type="main">Bidirectional interaction between visual and motor generative models using Predictive Coding and Active Inference</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Annabi</surname></persName>
			<idno type="ORCID">0000-0002-8645-0875</idno>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Pitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Quoy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2021.07.016</idno>
		<idno type="arXiv">arXiv:2210.08013</idno>
		<ptr target="cs" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<title level="j" type="abbrev">Neural Networks</title>
		<idno type="ISSN">0893-6080</idno>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="638" to="656" />
			<date type="published" when="2022-10">October 2022</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Annabi, L., Pitti, A., and Quoy, M. On the Re- lationship Between Variational Inference and Auto-Associative Memory, October 2022. URL http://arxiv.org/abs/2210.08013. arXiv:2210.08013 [cs.</note>
</biblStruct>

<biblStruct coords="7,307.44,233.75,234.25,8.91;7,317.40,245.75,224.10,8.91;7,317.40,257.63,224.49,8.91;7,317.40,270.62,299.62,7.59" xml:id="b1">
	<analytic>
		<title level="a" type="main">A Model of Inductive Bias Learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.731</idno>
		<ptr target="https://www.jair.org/index.php/jair/article/view/1" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<title level="j" type="abbrev">jair</title>
		<idno type="ISSN">1076-9757</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="149" to="198" />
			<date type="published" when="2000-03-01">March 2000</date>
			<publisher>AI Access Foundation</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Baxter, J. A Model of Inductive Bias Learning. Journal of Artificial Intelligence Research, 12:149-198, March 2000. ISSN 1076-9757. doi: 10.1613/jair.731. URL https://www.jair.org/index.php/jair/article/view/1</note>
</biblStruct>

<biblStruct coords="7,307.44,290.87,235.29,8.91;7,317.04,302.87,25.77,8.91;7,359.04,302.87,182.46,8.91;7,317.40,314.87,160.53,8.91;7,495.96,314.95,45.49,8.72;7,317.04,326.75,226.41,8.91;7,317.40,338.75,224.49,8.91;7,317.40,351.74,299.62,7.59" xml:id="b2">
	<analytic>
		<title level="a" type="main">The free energy principle for action and perception: A mathematical review</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">L</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><forename type="middle">Sub</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2017.09.004</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<title level="j" type="abbrev">Journal of Mathematical Psychology</title>
		<idno type="ISSN">0022-2496</idno>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2017-12">December 2017</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Buckley, C. L., Kim, C. S., McGregor, S., and Seth, A. K. The free energy principle for action and perception: A mathematical review. Journal of Mathematical Psychology, 81:55-79, December 2017. ISSN 0022-2496. doi: 10.1016/j.jmp.2017.09.004. URL https://www.sciencedirect.com/science/article/pii/</note>
</biblStruct>

<biblStruct coords="7,307.44,371.99,235.77,8.91;7,317.40,383.99,196.41,8.91;7,533.16,383.99,8.34,8.91;7,316.80,395.94,226.28,8.72;7,317.40,407.87,70.29,8.91" xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Alignment and Variational Attention</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Deng, Y., Kim, Y., Chiu, J., Guo, D., and Rush, A. Latent Alignment and Variational Attention. In Advances in Neural Information Processing Sys- tems, volume 31.</note>
</biblStruct>

<biblStruct coords="7,391.08,407.87,150.81,8.91;7,317.40,420.86,299.62,7.59" xml:id="b4">
	<analytic>
		<title level="a" type="main">Press Ganey Associates Inc.</title>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-349-94186-5_937</idno>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/b69" />
	</analytic>
	<monogr>
		<title level="m">The Grants Register 2018</title>
		<imprint>
			<publisher>Palgrave Macmillan UK</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="602" to="602" />
		</imprint>
	</monogr>
	<note type="raw_reference">Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/hash/b69</note>
</biblStruct>

<biblStruct coords="7,307.44,441.11,235.65,8.91;7,317.04,453.11,226.41,8.91;7,317.40,465.11,19.89,8.91;7,355.44,466.10,187.77,7.59;7,317.40,476.99,111.21,8.91" xml:id="b5">
	<monogr>
		<title level="m" type="main">Attention that does not Explain Away</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14308</idno>
		<ptr target="http://arxiv.org/abs/2009.14308" />
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
	<note type="raw_reference">Ding, N., Fan, X., Lan, Z., Schuurmans, D., and Soricut, R. Attention that does not Explain Away, September 2020. URL http://arxiv.org/abs/2009.14308. arXiv:2009.14308 [cs, stat].</note>
</biblStruct>

<biblStruct coords="7,307.44,498.35,235.41,8.91;7,317.40,510.23,225.80,8.91;7,317.40,522.23,224.10,8.91;7,317.40,534.23,225.80,8.91;7,317.40,546.11,226.05,8.91;7,317.40,558.11,19.89,8.91;7,355.44,559.10,187.77,7.59;7,317.40,570.11,134.97,8.91" xml:id="b6">
	<monogr>
		<title level="m" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<ptr target="http://arxiv.org/abs/2010.11929" />
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Min- derer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An Image is Worth 16x16 Words: Trans- formers for Image Recognition at Scale, June 2021. URL http://arxiv.org/abs/2010.11929. arXiv:2010.11929 [cs] version: 2.</note>
</biblStruct>

<biblStruct coords="7,307.44,591.35,137.01,8.91;7,465.96,591.35,77.24,8.91;7,317.40,603.35,110.25,8.91;7,450.48,603.43,91.14,8.72;7,317.04,615.23,100.77,8.91;7,433.08,615.23,73.89,8.91;7,522.00,615.23,19.89,8.91;7,317.40,628.22,299.62,7.59" xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention, Uncertainty, and Free-Energy</title>
		<author>
			<persName><forename type="first">Harriet</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2010.00215</idno>
		<ptr target="https://www.frontiersin.org/articles/10.3389/fnhum" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<title level="j" type="abbrev">Front. Hum. Neurosci.</title>
		<idno type="ISSN">1662-5161</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2010">2010</date>
			<publisher>Frontiers Media SA</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Feldman, H. and Friston, K. Attention, Uncer- tainty, and Free-Energy. Frontiers in Human Neuroscience, 4, 2010. ISSN 1662-5161. URL https://www.frontiersin.org/articles/10.3389/fnhum</note>
</biblStruct>

<biblStruct coords="7,307.44,648.59,235.76,8.91;7,317.40,660.47,101.49,8.91;7,439.92,660.47,101.62,8.91;7,316.92,672.55,226.28,8.72;7,317.40,684.47,184.89,8.91;7,522.00,684.47,19.89,8.91;7,317.40,697.34,295.41,7.59;7,317.40,708.35,73.05,8.91" xml:id="b8">
	<analytic>
		<title level="a" type="main">Unveiling Groups of Related Tasks in Multi - Task Learning</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Frecon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saverio</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<idno type="DOI">10.1109/icpr48806.2021.9413274</idno>
		<ptr target="https://proceedings.mlr.press/v162/frecon22a.html" />
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="6779" to="6792" />
		</imprint>
	</monogr>
	<note type="raw_reference">Frecon, J., Gasso, G., Pontil, M., and Salzo, S. Breg- man Neural Networks. In Proceedings of the 39th International Conference on Machine Learn- ing, pp. 6779-6792. PMLR, June 2022. URL https://proceedings.mlr.press/v162/frecon22a.html. ISSN: 2640-3498.</note>
</biblStruct>

<biblStruct coords="8,55.44,70.31,234.32,8.91;8,65.40,82.19,113.13,8.91;8,201.36,82.27,89.72,8.72;8,65.40,94.27,225.80,8.72;8,65.40,106.19,182.61,8.91;8,267.84,106.19,21.59,8.91;8,65.40,118.07,46.17,8.91;8,128.40,118.07,125.01,8.91;8,270.00,118.07,19.89,8.91;8,65.40,131.06,319.29,7.59" xml:id="b9">
	<analytic>
		<title level="a" type="main">Predictive coding under the free-energy principle</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Kiebel</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2008.0300</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2666703/" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<title level="j" type="abbrev">Phil. Trans. R. Soc. B</title>
		<idno type="ISSN">0962-8436</idno>
		<idno type="ISSNe">1471-2970</idno>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">1521</biblScope>
			<biblScope unit="page" from="1211" to="1221" />
			<date type="published" when="1521-05">1521. May 2009</date>
			<publisher>The Royal Society</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Friston, K. and Kiebel, S. Predictive coding under the free-energy principle. Philosophical Trans- actions of the Royal Society B: Biological Sci- ences, 364(1521):1211-1221, May 2009. ISSN 0962-8436. doi: 10.1098/rstb.2008.0300. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2666703/.</note>
</biblStruct>

<biblStruct coords="8,55.44,149.99,234.10,8.91;8,65.04,161.99,226.41,8.91;8,65.40,173.99,19.89,8.91;8,103.44,174.98,187.77,7.59;8,65.40,185.87,73.14,8.91" xml:id="b10">
	<monogr>
		<title level="m" type="main">Probabilistic Attention for Interactive Segmentation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gabbur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bilkhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15338</idno>
		<ptr target="http://arxiv.org/abs/2106.15338" />
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gabbur, P., Bilkhu, M., and Movellan, J. Probabilistic Attention for Interactive Segmentation, July 2021. URL http://arxiv.org/abs/2106.15338. arXiv:2106.15338</note>
</biblStruct>

<biblStruct coords="8,55.44,205.91,235.64,8.91;8,65.40,217.79,126.69,8.91;8,207.48,217.79,82.09,8.91;8,65.40,229.79,224.32,8.91;8,65.40,241.87,224.26,8.72;8,65.16,253.67,224.73,8.91;8,65.40,266.66,295.41,7.59;8,65.40,277.67,73.05,8.91" xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceiver: General Perception with Iterative Attention</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/jaegle21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07">July 2021</date>
			<biblScope unit="page" from="4651" to="4664" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zis- serman, A., and Carreira, J. Perceiver: General Perception with Iterative Attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 4651-4664. PMLR, July 2021. URL https://proceedings.mlr.press/v139/jaegle21a.html. ISSN: 2640-3498.</note>
</biblStruct>

<biblStruct coords="8,55.44,297.59,235.77,8.91;8,65.40,309.59,226.05,8.91;8,65.40,321.47,19.89,8.91;8,103.44,322.46,187.77,7.59;8,65.40,333.47,92.49,8.91" xml:id="b12">
	<monogr>
		<title level="m" type="main">Structured Attention Networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00887</idno>
		<ptr target="cs" />
		<imprint>
			<date type="published" when="2017-02">February 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kim, Y., Denton, C., Hoang, L., and Rush, A. M. Structured Attention Networks, February 2017. URL http://arxiv.org/abs/1702.00887. arXiv:1702.00887 [cs.</note>
</biblStruct>

<biblStruct coords="8,55.44,353.39,235.53,8.91;8,65.40,365.39,224.41,8.91;8,65.04,377.39,224.85,8.91;8,65.40,390.26,367.17,7.59" xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention in Psychology, Neuroscience, and Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">Grace</forename><forename type="middle">W</forename><surname>Lindsay</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2020.00029</idno>
		<ptr target="https://www.frontiersin.org/articles/10.3389/fncom.2020.00029" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<title level="j" type="abbrev">Front. Comput. Neurosci.</title>
		<idno type="ISSN">1662-5188</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2020-04-16">2020</date>
			<publisher>Frontiers Media SA</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Lindsay, G. W. Attention in Psychology, Neuroscience, and Machine Learning. Frontiers in Computational Neuroscience, 14, 2020. ISSN 1662-5188. URL https://www.frontiersin.org/articles/10.3389/fncom.2020.00029.</note>
</biblStruct>

<biblStruct coords="8,55.44,409.31,235.64,8.91;8,65.40,421.19,225.80,8.91;8,65.16,433.19,115.41,8.91;8,198.36,433.19,92.96,8.91;8,65.40,445.19,183.33,8.91;8,270.00,445.19,19.89,8.91;8,65.40,458.06,187.77,7.59;8,65.40,469.07,111.21,8.91" xml:id="b14">
	<monogr>
		<title level="m" type="main">Object-Centric Learning with Slot Attention</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15055</idno>
		<ptr target="http://arxiv.org/abs/2006.15055" />
		<imprint>
			<date type="published" when="2020-10">October 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
	<note type="raw_reference">Locatello, F., Weissenborn, D., Unterthiner, T., Ma- hendran, A., Heigold, G., Uszkoreit, J., Doso- vitskiy, A., and Kipf, T. Object-Centric Learn- ing with Slot Attention, October 2020. URL http://arxiv.org/abs/2006.15055. arXiv:2006.15055 [cs, stat].</note>
</biblStruct>

<biblStruct coords="8,55.44,488.99,234.06,8.91;8,65.40,500.99,224.10,8.91;8,65.40,512.99,226.05,8.91;8,65.40,524.87,19.89,8.91;8,103.44,525.86,187.77,7.59;8,65.40,536.87,73.14,8.91" xml:id="b15">
	<monogr>
		<title level="m" type="main">A Theoretical Framework for Inference and Learning in Predictive Coding Networks</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Millidge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bogacz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12316</idno>
		<ptr target="http://arxiv.org/abs/2207.12316" />
		<imprint>
			<date type="published" when="2022-08">August 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Millidge, B., Song, Y., Salvatori, T., Lukasiewicz, T., and Bogacz, R. A Theoretical Framework for Inference and Learning in Predictive Coding Networks, August 2022. URL http://arxiv.org/abs/2207.12316. arXiv:2207.12316</note>
</biblStruct>

<biblStruct coords="8,55.44,556.79,235.53,8.91;8,65.40,568.79,224.10,8.91;8,65.40,580.79,42.93,8.91;8,128.52,580.79,162.68,8.91;8,65.40,592.67,102.93,8.91;8,188.64,592.67,100.90,8.91;8,64.92,604.75,226.28,8.72;8,65.40,616.67,188.61,8.91;8,270.00,616.67,19.89,8.91;8,65.40,629.54,295.41,7.59;8,65.40,640.55,73.05,8.91" xml:id="b16">
	<analytic>
		<title level="a" type="main">A Probabilistic Framework for Pruning Transformers Via a Finite Admixture of Keys</title>
		<author>
			<persName><forename type="first">Tan</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duy</forename><forename type="middle">Khuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dung</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Tran-The</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nhat</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">J</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp49357.2023.10096107</idno>
		<ptr target="https://proceedings.mlr.press/v162/nguyen22c.html.ISSN:2640-3498" />
	</analytic>
	<monogr>
		<title level="m">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="16595" to="16621" />
		</imprint>
	</monogr>
	<note type="raw_reference">Nguyen, T. M., Nguyen, T. M., Le, D. D. D., Nguyen, D. K., Tran, V.-A., Baraniuk, R., Ho, N., and Osher, S. Improving Transformers with Proba- bilistic Attention Keys. In Proceedings of the 39th International Conference on Machine Learn- ing, pp. 16595-16621. PMLR, June 2022. URL https://proceedings.mlr.press/v162/nguyen22c.html. ISSN: 2640-3498.</note>
</biblStruct>

<biblStruct coords="8,55.44,660.47,235.41,8.91;8,65.40,672.47,225.45,8.91;8,65.40,684.47,225.33,8.91;8,65.40,696.35,225.81,8.91;8,65.40,708.35,226.05,8.91;8,317.40,70.31,19.89,8.91;8,355.44,71.30,187.77,7.59;8,317.40,82.19,111.21,8.91" xml:id="b17">
	<monogr>
		<title level="m" type="main">Hopfield Networks is All You Need</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schäfl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pavlović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Sandve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02217</idno>
		<ptr target="http://arxiv.org/abs/2008.02217" />
		<imprint>
			<date type="published" when="2021-04">April 2021</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
	<note type="raw_reference">Ramsauer, H., Schäfl, B., Lehner, J., Seidl, P., Widrich, M., Adler, T., Gruber, L., Holzleitner, M., Pavlović, M., Sandve, G. K., Greiff, V., Kreil, D., Kopp, M., Klambauer, G., Brandstetter, J., and Hochreiter, S. Hopfield Networks is All You Need, April 2021. URL http://arxiv.org/abs/2008.02217. arXiv:2008.02217 [cs, stat].</note>
</biblStruct>

<biblStruct coords="8,307.44,103.67,155.85,8.91;8,479.04,103.67,64.16,8.91;8,317.40,115.67,225.92,8.91;8,317.40,127.67,225.80,8.91;8,317.40,139.55,21.33,8.91;8,357.00,139.55,184.98,8.91;8,316.68,151.55,225.21,8.91;8,317.40,164.54,247.53,7.59;8,317.40,175.43,192.33,8.91" xml:id="b18">
	<analytic>
		<title level="a" type="main">Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</title>
		<author>
			<persName><forename type="first">Rajesh</forename><forename type="middle">P N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
		<idno type="DOI">10.1038/4580</idno>
		<ptr target="https://www.nature.com/articles/nn0199_79.Number:1Publisher" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<title level="j" type="abbrev">Nat Neurosci</title>
		<idno type="ISSN">1097-6256</idno>
		<idno type="ISSNe">1546-1726</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1999-01">January 1999</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Rao, R. P. N. and Ballard, D. H. Predictive cod- ing in the visual cortex: a functional interpre- tation of some extra-classical receptive-field ef- fects. Nature Neuroscience, 2(1):79-87, January 1999. ISSN 1546-1726. doi: 10.1038/4580. URL https://www.nature.com/articles/nn0199_79. Number: 1 Publisher: Nature Publishing Group.</note>
</biblStruct>

<biblStruct coords="8,307.44,196.91,173.13,8.91;8,497.64,196.91,45.68,8.91;8,317.40,208.91,225.81,8.91;8,317.40,220.79,225.80,8.91;8,317.40,232.86,225.33,8.72;8,317.40,244.79,177.57,8.91;8,522.00,244.79,19.89,8.91;8,317.40,257.66,283.41,7.59;8,317.40,268.67,73.05,8.91" xml:id="b19">
	<analytic>
		<title level="a" type="main">On Learning Continuous Pairwise Markov Random Fields</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wornell</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v130/shah21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 24th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>The 24th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-03">March 2021</date>
			<biblScope unit="page" from="1153" to="1161" />
		</imprint>
	</monogr>
	<note type="raw_reference">Shah, A., Shah, D., and Wornell, G. On Learn- ing Continuous Pairwise Markov Random Fields. In Proceedings of The 24th International Con- ference on Artificial Intelligence and Statistics, pp. 1153-1161. PMLR, March 2021. URL https://proceedings.mlr.press/v130/shah21a.html. ISSN: 2640-3498.</note>
</biblStruct>

<biblStruct coords="8,307.44,290.15,234.42,8.91;8,317.40,302.03,224.10,8.91;8,317.04,314.10,226.16,8.72;8,317.40,326.03,225.68,8.91;8,317.40,337.91,225.92,8.91;8,317.40,349.91,224.49,8.91;8,317.40,362.90,200.25,7.59" xml:id="b20">
	<analytic>
		<title level="a" type="main">Surprisingly Easy Hard-Attention for Sequence to Sequence Learning</title>
		<author>
			<persName><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1065</idno>
		<ptr target="https://aclanthology.org/D18-1065" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="page" from="640" to="645" />
		</imprint>
	</monogr>
	<note type="raw_reference">Shankar, S., Garg, S., and Sarawagi, S. Surprisingly Easy Hard-Attention for Sequence to Sequence Learning. In Proceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pp. 640-645, Brus- sels, Belgium, October 2018. Association for Computa- tional Linguistics. doi: 10.18653/v1/D18-1065. URL https://aclanthology.org/D18-1065.</note>
</biblStruct>

<biblStruct coords="8,307.44,383.39,157.29,8.91;8,482.04,383.39,61.16,8.91;8,317.40,395.27,177.81,8.91;8,522.00,395.27,19.89,8.91;8,317.40,408.26,187.77,7.59;8,317.40,419.15,73.14,8.91" xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural Block-Slot Representations</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2020.00029</idno>
		<idno>arXiv:</idno>
		<ptr target="2211.01177" />
		<imprint>
			<date type="published" when="2022-11">November 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Singh, G., Kim, Y., and Ahn, S. Neural Block- Slot Representations, November 2022. URL http://arxiv.org/abs/2211.01177. arXiv:2211.01177</note>
</biblStruct>

<biblStruct coords="8,307.44,440.63,234.06,8.91;8,317.04,452.63,224.53,8.91;8,317.40,464.63,224.22,8.91;8,317.04,476.51,224.85,8.91;8,317.40,489.50,299.62,7.59" xml:id="b22">
	<analytic>
		<title level="a" type="main">A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/7503.003.0174</idno>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1353" to="1360" />
		</imprint>
	</monogr>
	<note type="raw_reference">Teh, Y., Newman, D., and Welling, M. A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation. In Advances in Neural Information Processing Systems, volume 19. MIT Press, 2006. URL https://proceedings.neurips.cc/paper_files/paper/2</note>
</biblStruct>

<biblStruct coords="8,307.44,509.99,235.29,8.91;8,317.16,521.87,225.81,8.91;8,317.40,533.87,5.85,8.91;8,340.68,533.87,202.77,8.91;8,317.40,545.87,19.89,8.91;8,355.44,546.86,187.77,7.59;8,317.40,557.75,73.14,8.91" xml:id="b23">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention Is All You Need, December 2017. URL http://arxiv.org/abs/1706.03762. arXiv:1706.03762</note>
</biblStruct>

<biblStruct coords="8,307.44,579.23,235.41,8.91;8,317.40,591.23,225.33,8.91;8,317.40,603.23,157.65,8.91;8,491.04,603.23,50.38,8.91;8,317.40,615.11,225.68,8.91;8,317.40,627.11,226.05,8.91;8,317.40,638.99,19.89,8.91;8,355.44,639.98,187.77,7.59;8,317.40,650.99,73.14,8.91" xml:id="b24">
	<analytic>
		<title level="a" type="main">Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiliang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kriti</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owais</forename><forename type="middle">Khan</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhojit</forename><surname>Som</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52729.2023.01838</idno>
		<idno type="arXiv">arXiv:2208.10442</idno>
		<ptr target="http://arxiv.org/abs/2208.10442" />
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-08">August 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O. K., Singhal, S., Som, S., and Wei, F. Image as a Foreign Language: BEiT Pretraining for All Vi- sion and Vision-Language Tasks, August 2022. URL http://arxiv.org/abs/2208.10442. arXiv:2208.10442</note>
</biblStruct>

<biblStruct coords="8,307.44,672.47,171.33,8.91;8,498.12,672.47,45.32,8.91;8,317.40,684.47,226.05,8.91;8,317.40,696.35,19.89,8.91;8,355.44,697.34,187.77,7.59;8,317.40,708.35,92.49,8.91" xml:id="b25">
	<monogr>
		<title level="m" type="main">Transformers from an Optimization Perspective</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.13891</idno>
		<ptr target="http://arxiv.org/abs/2205.13891" />
		<imprint>
			<date type="published" when="2022-05">May 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yang, Y., Huang, Z., and Wipf, D. Transform- ers from an Optimization Perspective, May 2022. URL http://arxiv.org/abs/2205.13891. arXiv:2205.13891 [cs].</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
