<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">All the attention you need: Global-local, spatial-channel attention for image retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-16">16 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,126.04,164.94,85.56,10.37"><forename type="first">Chull</forename><forename type="middle">Hwan</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Rennes</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.41,164.94,39.51,10.37"><forename type="first">Hye</forename><surname>Joo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Rennes</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.91,164.94,19.92,10.37;1,253.09,180.83,51.05,7.77"><forename type="first">Han</forename><forename type="middle">Odd</forename><surname>Concepts</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Rennes</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.42,164.94,73.97,10.37"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Rennes</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">All the attention you need: Global-local, spatial-channel attention for image retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-16">16 Jul 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">74C3ED5A317EEC98581242178860A8D4</idno>
					<idno type="arXiv">arXiv:2107.08000v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-22T20:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address representation learning for large-scale instance-level image retrieval. Apart from backbone, training pipelines and loss functions, popular approaches have focused on different spatial pooling and attention mechanisms, which are at the core of learning a powerful global image representation. There are different forms of attention according to the interaction of elements of the feature tensor (local and global) and the dimensions where it is applied (spatial and channel). Unfortunately, each study addresses only one or two forms of attention and applies it to different problems like classification, detection or retrieval.</p><p>We present global-local attention module (GLAM), which is attached at the end of a backbone network and incorporates all four forms of attention: local and global, spatial and channel. We obtain a new feature tensor and, by spatial pooling, we learn a powerful embedding for image retrieval. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance-level image retrieval is at the core of visual representation learning and is connected with many problems of visual recognition and machine learning, for instance metric learning <ref type="bibr" coords="1,115.58,553.18,15.77,8.64" target="#b29">[30,</ref><ref type="bibr" coords="1,134.48,553.18,11.83,8.64" target="#b25">26]</ref>, few-shot learning <ref type="bibr" coords="1,226.99,553.18,16.60,8.64" target="#b41">[42]</ref> and unsupervised learning <ref type="bibr" coords="1,125.30,565.13,10.58,8.64">[8]</ref>. Many large-scale open datasets <ref type="bibr" coords="1,275.57,565.13,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="1,50.11,577.09,12.45,8.64" target="#b36">37,</ref><ref type="bibr" coords="1,64.60,577.09,12.45,8.64" target="#b15">16,</ref><ref type="bibr" coords="1,79.08,577.09,12.45,8.64" target="#b28">29,</ref><ref type="bibr" coords="1,93.57,577.09,11.83,8.64" target="#b52">53]</ref>, and competitions 1 have accelerated progress in instance-level image retrieval, which has been transformed by deep learning <ref type="bibr" coords="1,150.27,601.00,10.58,8.64" target="#b2">[3]</ref>.</p><p>Many studies on instance-level image retrieval focus on learning features from convolutional neural networks (CNN), while others focus on re-ranking, for instance by graph-based methods <ref type="bibr" coords="1,137.06,649.18,15.27,8.64" target="#b10">[11]</ref>. The former can be distinguished according to feature types: local descriptors, reminiscent of SIFT <ref type="bibr" coords="1,72.77,673.09,15.27,8.64">[27]</ref>, where an image is mapped to a few hundred vectors; and global descriptors, where an image is mapped to a 1 https://www.kaggle.com/c/landmark-retrieval-2020 single vector. In fact, deep learning has brought global descriptors with astounding performance, while allowing efficient search. Our study belongs to this type.</p><p>Studies on global descriptors have focused on spatial pooling <ref type="bibr" coords="1,342.36,280.74,10.79,8.64">[2,</ref><ref type="bibr" coords="1,356.21,280.74,11.83,8.64" target="#b36">37]</ref>. The need for compact, discriminative representations that are resistant to clutter has naturally given rise to spatial attention methods <ref type="bibr" coords="1,445.06,304.65,15.77,8.64" target="#b23">[24,</ref><ref type="bibr" coords="1,464.39,304.65,11.83,8.64" target="#b27">28]</ref>. Different kinds of attention have been studied in many areas of computer vision research. There is also channel attention <ref type="bibr" coords="1,501.52,328.56,15.77,8.64" target="#b19">[20,</ref><ref type="bibr" coords="1,520.13,328.56,7.38,8.64" target="#b8">9]</ref>; local attention, applied independently to elements of the representation (feature map) <ref type="bibr" coords="1,413.97,352.47,15.77,8.64" target="#b53">[54,</ref><ref type="bibr" coords="1,433.06,352.47,12.04,8.64" target="#b24">25]</ref>; global attention, based on interaction between elements <ref type="bibr" coords="1,441.56,364.43,15.77,8.64" target="#b51">[52,</ref><ref type="bibr" coords="1,460.21,364.43,7.38,8.64" target="#b8">9]</ref>; and combinations thereof. Unfortunately, each study has been limited to one or two kinds of attention only; attention is not always learned; and applications vary.</p><p>It is the objective of our work to perform a comprehensive study of all forms of attention above, apply them to instance-level image retrieval and provide a detailed account of their interaction and impact on performance. As shown in Figure <ref type="figure" coords="1,377.13,471.76,3.74,8.64" target="#fig_0">1</ref>, we collect contextual information from images with both local and global attention, giving rise to two parallel network streams. Importantly, each operates on both spatial locations and feature channels. Local attention is about individual locations and channels; global is about interaction between locations and between channels. The extracted information is separately embedded in local and global attention feature maps, which are combined in a global-local attention feature map before pooling.</p><p>Our contributions can be summarized as follows:</p><p>1. We propose a novel network that consists of both global and local attention for image retrieval. This is the first study that employs both mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Each of the global and local attention mechanisms</head><p>comprises both spatial and channel attention. 3. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks. </p><formula xml:id="formula_0" coords="2,72.87,94.81,445.09,137.66">A l c c × 1 × 1 × + F l c A l s 1 × h × w × + F l × c × h × w F × + c × h × w F gl A g c c × c × F g c A g s hw × hw × + F g</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Instance-level image retrieval Studies on instance-level image retrieval can be roughly, but not exclusively, divided into three types: (1) studies on global descriptors <ref type="bibr" coords="2,50.11,419.82,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="2,64.20,419.82,12.45,8.64" target="#b15">16,</ref><ref type="bibr" coords="2,79.95,419.82,12.45,8.64" target="#b23">24,</ref><ref type="bibr" coords="2,95.70,419.82,12.45,8.64" target="#b52">53,</ref><ref type="bibr" coords="2,111.45,419.82,7.47,8.64">2,</ref><ref type="bibr" coords="2,122.22,419.82,12.04,8.64" target="#b36">37]</ref>; (2) studies on local descriptors and geometry-based re-ranking <ref type="bibr" coords="2,161.15,431.77,15.77,8.64" target="#b28">[29,</ref><ref type="bibr" coords="2,179.87,431.77,12.45,8.64" target="#b44">45,</ref><ref type="bibr" coords="2,195.29,431.77,12.45,8.64">40,</ref><ref type="bibr" coords="2,210.69,431.77,12.04,8.64" target="#b52">53]</ref>; (3) re-ranking by graph-based methods <ref type="bibr" coords="2,153.51,443.73,15.77,8.64" target="#b10">[11,</ref><ref type="bibr" coords="2,172.95,443.73,12.45,8.64" target="#b20">21,</ref><ref type="bibr" coords="2,189.06,443.73,11.83,8.64" target="#b54">55]</ref>. The first two types of studies focus on the feature representation, while the last type focuses on re-ranking extracted features.</p><p>Studies on global descriptors focus on spatial pooling of CNN feature maps into vectors, including MAC <ref type="bibr" coords="2,267.28,493.30,15.27,8.64" target="#b37">[38]</ref>, SPoC [2], CroW <ref type="bibr" coords="2,124.67,505.25,15.27,8.64" target="#b23">[24]</ref>, R-MAC <ref type="bibr" coords="2,184.95,505.25,15.77,8.64">[48,</ref><ref type="bibr" coords="2,205.18,505.25,12.45,8.64" target="#b14">15,</ref><ref type="bibr" coords="2,222.10,505.25,11.83,8.64" target="#b15">16]</ref>, GeM <ref type="bibr" coords="2,267.28,505.25,15.27,8.64" target="#b36">[37]</ref>, and NetVLAD <ref type="bibr" coords="2,112.86,517.21,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="2,126.79,517.21,11.83,8.64" target="#b24">25]</ref>, as well as learning the representation <ref type="bibr" coords="2,68.57,529.16,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="2,82.32,529.16,12.45,8.64" target="#b14">15,</ref><ref type="bibr" coords="2,97.72,529.16,12.45,8.64" target="#b15">16,</ref><ref type="bibr" coords="2,113.13,529.16,12.45,8.64" target="#b35">36,</ref><ref type="bibr" coords="2,128.54,529.16,11.83,8.64" target="#b36">37]</ref>. Studies before deep learning dominated image retrieval were mostly based on local descriptors like SIFT [27] and bag-of-words representation <ref type="bibr" coords="2,258.97,553.07,16.60,8.64" target="#b31">[32]</ref> or aggregated descriptors like VLAD <ref type="bibr" coords="2,189.53,565.03,16.60,8.64">[22]</ref> or ASMK <ref type="bibr" coords="2,250.47,565.03,15.27,8.64" target="#b45">[46]</ref>. Local descriptors have been revived in deep learning, e.g. with DELF <ref type="bibr" coords="2,77.51,588.94,15.27,8.64" target="#b28">[29]</ref>, DELG <ref type="bibr" coords="2,128.14,588.94,11.62,8.64" target="#b4">[5]</ref> and ASMK extensions <ref type="bibr" coords="2,234.80,588.94,15.77,8.64" target="#b44">[45,</ref><ref type="bibr" coords="2,253.06,588.94,11.83,8.64" target="#b46">47]</ref>.</p><p>We focus on learning a global descriptor in this work, because it is the most efficient in terms of storage and search. However, our generic attention mechanism produces a feature tensor and could be applicable to local descriptors as well, if global pooling were replaced by local feature detection. Re-ranking methods are complementary to the representation and we do not consider them in this work.</p><p>Attention Attention mechanisms have been first proposed in image classification studies focusing on channel at-</p><formula xml:id="formula_1" coords="2,311.68,365.21,232.15,23.53">METHOD LOCAL GLOBAL LRN RET Spatial Channel Spatial Channel</formula><p>SENet <ref type="bibr" coords="2,337.10,396.94,14.94,7.77" target="#b19">[20]</ref> ECA-Net <ref type="bibr" coords="2,347.56,407.90,14.94,7.77" target="#b50">[51]</ref> GCNet <ref type="bibr" coords="2,339.10,418.85,10.45,7.77" target="#b5">[6]</ref> CBAM <ref type="bibr" coords="2,339.79,429.81,14.94,7.77" target="#b53">[54]</ref> GE <ref type="bibr" coords="2,325.65,440.77,14.94,7.77" target="#b18">[19]</ref> NL-Net <ref type="bibr" coords="2,341.58,451.73,14.94,7.77" target="#b51">[52]</ref> AA-Net <ref type="bibr" coords="2,342.57,462.69,10.45,7.77" target="#b3">[4]</ref> SAN [59] N 3 Net [34] A 2 -Net <ref type="bibr" coords="2,340.25,495.57,10.45,7.77" target="#b8">[9]</ref> GSoP <ref type="bibr" coords="2,334.62,506.53,14.94,7.77" target="#b13">[14]</ref> OnA <ref type="bibr" coords="2,331.12,522.49,14.94,7.77" target="#b22">[23]</ref> AGeM <ref type="bibr" coords="2,338.23,533.45,14.94,7.77" target="#b16">[17]</ref> CroW <ref type="bibr" coords="2,335.61,544.40,14.94,7.77" target="#b23">[24]</ref> CRN <ref type="bibr" coords="2,332.13,555.36,14.94,7.77" target="#b24">[25]</ref> DELF <ref type="bibr" coords="2,336.11,566.32,14.94,7.77" target="#b28">[29]</ref> DELG <ref type="bibr" coords="2,337.60,577.28,10.45,7.77" target="#b4">[5]</ref> Tolias et al. <ref type="bibr" coords="2,355.58,588.24,14.94,7.77" target="#b46">[47]</ref> SOLAR <ref type="bibr" coords="2,343.09,599.20,14.94,7.77" target="#b27">[28]</ref> Ours Table <ref type="table" coords="2,332.96,638.64,3.88,8.64">1</ref>: Related work on attention. LRN: learned; RET: applied to instance-level image retrieval.</p><p>tention <ref type="bibr" coords="2,341.77,692.56,15.77,8.64" target="#b19">[20,</ref><ref type="bibr" coords="2,362.77,692.56,12.45,8.64" target="#b50">51,</ref><ref type="bibr" coords="2,380.45,692.56,7.19,8.64" target="#b5">6]</ref>, spatial attention <ref type="bibr" coords="2,469.48,692.56,16.60,8.64" target="#b18">[19]</ref> or both, like CBAM <ref type="bibr" coords="2,341.45,704.51,15.27,8.64" target="#b53">[54]</ref>. In image retrieval, CroW <ref type="bibr" coords="2,471.61,704.51,16.60,8.64" target="#b23">[24]</ref> also employs  both spatial and channel attention and can be seen as a precursor of CBAM, but, like other studies of spatial attention on retrieval <ref type="bibr" coords="3,100.60,292.43,15.77,8.64" target="#b40">[41,</ref><ref type="bibr" coords="3,120.29,292.43,12.45,8.64" target="#b22">23,</ref><ref type="bibr" coords="3,136.66,292.43,11.83,8.64" target="#b16">17]</ref>, it is not learned. CRN <ref type="bibr" coords="3,253.13,292.43,16.60,8.64" target="#b24">[25]</ref> applies spatial attention for feature reweighting and is learned. Learned spatial attention mechanisms are common for local descriptors <ref type="bibr" coords="3,96.32,328.30,15.77,8.64" target="#b28">[29,</ref><ref type="bibr" coords="3,114.58,328.30,7.47,8.64" target="#b4">5,</ref><ref type="bibr" coords="3,124.54,328.30,11.83,8.64" target="#b46">47]</ref>.</p><formula xml:id="formula_2" coords="3,126.61,80.35,80.54,134.60">c × h × w c × 1 × 1 c × 1 × 1 F A l c</formula><p>We call the above methods local attention, in the sense that elements of the feature tensor (channels / spatial locations), are weighted independently, based on contextual information obtained by pooling or learned. By constrast, by global attention we refer to mechanisms that model interaction between elements of the feature tensor, for example between channels or between locations.</p><p>In image classification, non-local neural network (NL-Net) <ref type="bibr" coords="3,69.93,438.70,16.60,8.64" target="#b51">[52]</ref> is maybe the first global attention mechanism, followed by similar studies <ref type="bibr" coords="3,151.15,450.65,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="3,164.85,450.65,12.45,8.64">59,</ref><ref type="bibr" coords="3,180.21,450.65,11.83,8.64" target="#b33">34]</ref>. It is global spatial attention, allowing interaction between any pair of spatial locations. Similarly, there are studies of global channel attention, allowing interaction between channels <ref type="bibr" coords="3,227.17,486.52,10.79,8.64" target="#b8">[9,</ref><ref type="bibr" coords="3,240.72,486.52,11.83,8.64" target="#b13">14]</ref>. Global attention has focused mostly on image recognition and has been applied to either spatial or channel attention so far, not both. In image retrieval, SOLAR <ref type="bibr" coords="3,182.84,522.38,16.60,8.64" target="#b27">[28]</ref> is a direct application of the global spatial attention mechanism of <ref type="bibr" coords="3,227.49,534.34,15.27,8.64" target="#b51">[52]</ref>.</p><p>Table <ref type="table" coords="3,88.33,547.70,4.98,8.64">1</ref> attempts to categorize related work on attention according to whether attention is local or global, spatial or channel, whether it is learned and whether it is applied to instance-level image retrieval. We observe that all methods limit to one or two forms of attention only. Of those studies that focus on image retrieval, many are not learned <ref type="bibr" coords="3,83.41,619.43,15.77,8.64" target="#b22">[23,</ref><ref type="bibr" coords="3,103.15,619.43,12.45,8.64" target="#b16">17,</ref><ref type="bibr" coords="3,119.57,619.43,11.83,8.64" target="#b23">24]</ref>, and of those that are, some are designed for local descriptors <ref type="bibr" coords="3,160.79,631.38,15.77,8.64" target="#b28">[29,</ref><ref type="bibr" coords="3,179.05,631.38,11.83,8.64" target="#b46">47]</ref>.</p><p>By contrast, we provide a comprehensive study of all forms of attention, global and local, spatial and channel, to obtain a learned representation in the form of a tensor that can be used in any way. We spatially pool it into a global descriptor and we study the relative gain of different forms of attention in image retrieval. </p><formula xml:id="formula_3" coords="3,348.59,79.75,169.70,155.30">feature map conv 1 × 1 conv 3 × 3 conv 5 × 5 conv 7 × 7 concat conv 1 × 1 attention map c × h × w 4c × h × w 1 × h × w c × h × w dilated conv F F A l s</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Global-local attention</head><p>We design a global-local attention module (GLAM), which is attached at the end of a backbone network. Figure <ref type="figure" coords="3,540.13,342.70,4.98,8.64" target="#fig_0">1</ref> illustrates its main components. We are given a c × h × w feature tensor F, where c is the number of channels, and h × w is the spatial resolution. Local attention collects context from the image and applies pooling to obtain a c × 1 × 1 local channel attention map A l c and a 1 × h × w local spatial attention map A l s . Global attention allows interaction between channels, resulting in a c × c global channel attention map A g c , and between spatial locations, resulting in a hw × hw global spatial attention map A g s . The feature maps produced by the two attention streams are combined with the original one by a learned fusion mechanism into the global-local attention feature map F gl before being spatially pooled into a global image descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Local attention</head><p>We extract an 1D channel and a 2D spatial attention map to weigh the feature map in the corresponding dimensions.</p><p>Local channel attention Following ECA-Net <ref type="bibr" coords="3,507.75,567.83,15.27,8.64" target="#b50">[51]</ref>, this attention captures local channel information. As shown in Figure <ref type="figure" coords="3,337.05,591.74,3.74,8.64" target="#fig_2">2</ref>, we are given a c × h × w feature tensor F from our backbone. We first reduce it to a c × 1 × 1 tensor by global average pooling (GAP). Channel attention is then captured by a 1D convolution of kernel size k along the channel dimension, where k controls the extent of cross-channel interaction. This is followed by a sigmoid function, resulting in the c × 1 × 1 local channel attention map A l c . Local spatial attention Inspired by the inception module <ref type="bibr" coords="3,323.00,692.56,16.60,8.64" target="#b42">[43]</ref> and similar to <ref type="bibr" coords="3,397.24,692.56,15.27,8.64" target="#b24">[25]</ref>, this attention map captures local spatial information at different scales. As shown in Figure <ref type="figure" coords="3,537.64,704.51,3.74,8.64" target="#fig_3">3</ref>, given the same c × h × w feature tensor F from our backbone, we obtain a new tensor F with channels reduced to c , using a 1 × 1 convolution. We then extract local spatial contextual information using convolutional filters of kernel size 3 × 3, 5 × 5, and 7 × 7, which are efficiently implemented by 3 × 3 dilated convolutions <ref type="bibr" coords="4,204.91,352.04,10.79,8.64" target="#b6">[7,</ref><ref type="bibr" coords="4,218.78,352.04,13.28,8.64" target="#b56">57]</ref> with dilation parameter 1, 2, and 3 respectively. The resulting features, along with one obtained by 1 × 1 convolution on F , are concatenated into a 4c × h × w tensor. Finally, we obtain the 1 × h × w local spatial attention map A l s by a 1 × 1 convolution that reduces the channel dimension to 1.</p><formula xml:id="formula_4" coords="4,58.65,79.75,212.64,162.91">feature map GAP conv1d(k) conv1d(k) sigmoid sigmoid × × softmax attention feature map 1 × c 1 × c 1 × c Qc c × c hw × c Vc A g c c × h × w 1 × c 1 × c Kc F Gc</formula><p>The middle column of Figure <ref type="figure" coords="4,180.43,423.77,4.98,8.64" target="#fig_7">6</ref> shows heat maps of local spatial attention, localizing target objects in images.</p><p>Local attention feature map We use the local channel attention map A l c to weigh F in the channel dimension</p><formula xml:id="formula_5" coords="4,127.45,479.86,158.91,12.69">F l c := F A l c + F.<label>(1)</label></formula><p>We then use local spatial attention map A l s to weigh F l c in the spatial dimensions, resulting in the c × h × w local attention feature map</p><formula xml:id="formula_6" coords="4,125.17,540.05,161.19,12.69">F l = F l c A l s + F l c .<label>(2)</label></formula><p>Here, A B denotes an element-wise multiplication of tensors A and B, with broadcasting when one tensor is smaller. We adopt the choice of applying channel followed by spatial attention from convolutional block attention module CBAM <ref type="bibr" coords="4,82.00,608.39,15.27,8.64" target="#b53">[54]</ref>. However, apart from computing A l s at different scales, both attention maps are obtained from the original tensor F rather than sequentially. In addition, both (1) and (2) include residual connections, while CBAM includes a single residual connection over both steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Global attention</head><p>We extract two matrices capturing global pairwise channel and spatial interaction to weigh the feature map. Global channel attention We introduce a global channel attention mechanism that captures global channel interaction. This mechanism is based on the non-local neural network <ref type="bibr" coords="4,332.01,320.43,15.27,8.64" target="#b51">[52]</ref>, but with the idea of 1D convolution from ECA-Net <ref type="bibr" coords="4,325.65,332.39,15.27,8.64" target="#b50">[51]</ref>. As shown in Figure <ref type="figure" coords="4,427.37,332.39,3.74,8.64" target="#fig_4">4</ref>, we are given the c × h × w feature tensor F from our backbone. We apply GAP and squeeze spatial dimensions, followed by a 1D convolution of kernel size k and a sigmoid function, to obtain 1×c query Q c and key K c tensors. The value tensor V c is obtained by mere reshaping of F to hw×c, without GAP. Next, we form the outer product of K c and Q c , followed by softmax over channels to obtain a c × c global channel attention map</p><formula xml:id="formula_7" coords="4,313.07,79.75,225.13,154.58">feature map conv 1 × 1 conv 1 × 1 conv 1 × 1 × × softmax conv 1 × 1 attention feature map c × hw Qs hw × hw c × h × w c × hw Vs c × h × w A g s c × h × w c × hw Kc F Gs</formula><formula xml:id="formula_8" coords="4,375.04,432.62,170.07,12.69">A g c = softmax(K c Q c ).<label>(3)</label></formula><p>Finally, this attention map is multiplied with V c and the matrix product V c A g c is reshaped back to c × h × w to give the global channel attention feature map G c . In GSoP <ref type="bibr" coords="4,511.81,477.86,16.60,8.64" target="#b13">[14]</ref> and A 2 -Net <ref type="bibr" coords="4,340.49,489.81,10.58,8.64" target="#b8">[9]</ref>, a c × c global channel attention map is obtained by multiplication of hw × c matrices; (3) is more efficient, using only an outer product of 1 × c vectors.</p><p>Global spatial attention Since ordinary convolution applies only a local neighborhood at a time, it cannot capture global contextual information. Thus, we apply non-local filtering <ref type="bibr" coords="4,334.90,566.03,15.27,8.64" target="#b51">[52]</ref>, which is a form of self-attention <ref type="bibr" coords="4,485.29,566.03,16.60,8.64" target="#b48">[49]</ref> in the spatial dimensions. As shown in Figure <ref type="figure" coords="4,464.00,577.98,3.74,8.64" target="#fig_5">5</ref>, we are given the same c × h × w feature tensor F from our backbone. By using three 1 × 1 convolutions, which reduce channels to c , and flattening spatial dimensions to hw, we obtain c × hw query Q s , key K s , and value V s tensors, where each column is a feature vector corresponding to a particular spatial location. We capture pairwise similarities of these vectors by matrix multiplication of K s and Q s , followed by softmax over locations to obtain a hw × hw global spatial attention map:</p><formula xml:id="formula_9" coords="4,376.97,702.12,168.14,12.69">A g s = softmax(K s Q s ).<label>(4)</label></formula><p>This attention map is multiplied with V s and the matrix product V s A g s is reshaped back to c × h × w by expanding the spatial dimensions. Finally, using a 1 × 1 convolution, which increases channels back to c, we obtain the c × h × w global spatial attention feature map G s .</p><p>The right column of Figure <ref type="figure" coords="5,171.58,135.25,4.98,8.64" target="#fig_7">6</ref> shows heat maps for global spatial attention, localizing target objects in images.</p><p>Global attention feature map We use the global channel attention feature map F c to weigh F element-wise</p><formula xml:id="formula_10" coords="5,138.06,194.45,148.30,12.69">F g c = F G c .<label>(5)</label></formula><p>We then use global spatial attention feature map G s to weigh F g c element-wise, resulting in the c × h × w global attention feature map</p><formula xml:id="formula_11" coords="5,123.64,260.85,158.85,12.69">F g = F g c G s + F g c . (<label>6</label></formula><formula xml:id="formula_12" coords="5,282.49,263.24,3.87,8.64">)</formula><p>Similarly to F l in ( <ref type="formula" coords="5,130.31,284.49,3.87,8.64" target="#formula_5">1</ref>) and (2), we apply channel attention first, followed by spatial attention. However, unlike (1), there is no residual connection in ( <ref type="formula" coords="5,194.22,308.40,3.53,8.64" target="#formula_10">5</ref>). This choice is supported by early experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Global-local attention</head><p>Feature fusion As shown in Figure <ref type="figure" coords="5,209.04,357.94,3.74,8.64" target="#fig_0">1</ref>, we combine the local and global attention feature maps, F l and F g , with the original feature F. While concatenation and summation are common operations for feature combination, we use a weighted average with weights w l , w g , w respectively, obtained by softmax over three learnable scalar parameters, to obtain a c × h × w global-local attention feature map</p><formula xml:id="formula_13" coords="5,112.28,448.52,174.08,11.72">F gl = w l F l + w g F l + wF.<label>(7)</label></formula><p>EfficientDet <ref type="bibr" coords="5,101.66,472.16,16.60,8.64" target="#b43">[44]</ref> has shown that this is the most effective, among a number of choices, for fusion of features across different scales.</p><p>Pooling We apply GeM <ref type="bibr" coords="5,157.16,512.51,15.27,8.64" target="#b36">[37]</ref>, a learnable spatial pooling mechanism, to feature map F gl (7), followed by a fullyconnected (FC) layer with dropout and batch normalization. The final embedding is obtained by 2 -normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Training set There are a number of open landmark datasets commonly used for training in image retrieval studies, including neural code (NC) <ref type="bibr" coords="5,177.06,632.78,10.58,8.64" target="#b2">[3]</ref>, neural code clean (NCclean) <ref type="bibr" coords="5,76.81,644.74,15.27,8.64" target="#b15">[16]</ref>, as well as Google Landmarks v1 (GLDv1) <ref type="bibr" coords="5,269.77,644.74,16.60,8.64" target="#b28">[29]</ref> and v2 (GLDv2) <ref type="bibr" coords="5,124.96,656.69,15.27,8.64" target="#b52">[53]</ref>. Table <ref type="table" coords="5,174.89,656.69,4.98,8.64" target="#tab_0">2</ref> shows relevant statistics. These datasets can be categorized into noisy and clean. The clean sets were obtained from the original noisy sets for more effective training <ref type="bibr" coords="5,143.06,692.56,15.77,8.64" target="#b15">[16,</ref><ref type="bibr" coords="5,161.26,692.56,11.83,8.64" target="#b52">53]</ref>. The original noisy datasets are much larger, but they have high intra-class variability.  Evaluation set and metrics We use four common evaluation datasets for landmark image retrieval: Oxford5k (Ox5k) <ref type="bibr" coords="5,340.21,582.02,15.27,8.64" target="#b31">[32]</ref>, Paris6k (Par6k) <ref type="bibr" coords="5,426.50,582.02,15.27,8.64" target="#b32">[33]</ref>, as well as Revisited Oxford (ROxford or ROxf) and Paris (RParis or RPar) <ref type="bibr" coords="5,526.02,593.98,15.27,8.64" target="#b34">[35]</ref>.</p><p>ROxford and RParis are used with and without one million distractors (R1M) <ref type="bibr" coords="5,383.09,617.89,16.60,8.64" target="#b27">[28]</ref> and evaluated using the Medium and Hard protocols <ref type="bibr" coords="5,371.27,629.84,15.27,8.64" target="#b34">[35]</ref>. We evaluate using mean Average Precision (mAP) and mean precision at 10 (mP@10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We train on 8 TITAN RTX 2080Ti GPUs. All models are pre-trained on ImageNet <ref type="bibr" coords="5,409.96,692.56,16.60,8.64" target="#b38">[39]</ref> and implemented in PyTorch <ref type="bibr" coords="5,308.86,704.51,15.27,8.64" target="#b30">[31]</ref>. For fair comparisons, we set a training environment similar to the those of compared studies <ref type="bibr" coords="6,208.56,356.98,15.77,8.64" target="#b55">[56,</ref><ref type="bibr" coords="6,226.41,356.98,12.45,8.64" target="#b52">53,</ref><ref type="bibr" coords="6,240.95,356.98,12.45,8.64" target="#b27">28,</ref><ref type="bibr" coords="6,255.48,356.98,11.83,8.64" target="#b34">35]</ref>. We employ ResNet101 <ref type="bibr" coords="6,130.93,368.93,16.60,8.64" target="#b17">[18]</ref> as a backbone model. The kernel size k of ECANet in subsection 3.1 is set to 3. The parameter p of GeM in subsection 3.3 is set to 3 and the dimension d of final embeddings to 512. We adopt ArcFace <ref type="bibr" coords="6,267.27,404.80,15.27,8.64" target="#b9">[10]</ref>, a cosine-softmax based loss, with a margin of 0.3. We use stochastic gradient descent with initial learning rate 10 -3 , momentum 0.9 and weight decay 10 -5 .</p><p>We adopt the batch sampling of Yokoo et al. <ref type="bibr" coords="6,242.80,452.62,16.60,8.64" target="#b55">[56]</ref> where mini-batch samples with similar aspect ratios are resized to a particular size. Here, we use a batch size of 64. For image augmentation, we apply scaling, random cropping, and varied illumination. At inference, we apply a multi-resolution representation <ref type="bibr" coords="6,109.04,512.40,16.60,8.64" target="#b15">[16]</ref> to query and database images.</p><p>Our method is denoted as GLAM (global-local attention module). Using the backbone model alone is referred to as baseline. It is compatible with recent models based on ResNet101-GeM trained with ArcFace <ref type="bibr" coords="6,219.84,560.22,15.77,8.64" target="#b52">[53,</ref><ref type="bibr" coords="6,238.16,560.22,11.83,8.64" target="#b27">28]</ref>. Adding our local attention (subsection 3.1) to the baseline model is denoted +local, while adding our global attention (subsection 3.2) is denoted +global. Since we focus on representation learning, we do not consider post-processing methods like geometry-based re-ranking <ref type="bibr" coords="6,177.70,619.99,15.77,8.64" target="#b28">[29,</ref><ref type="bibr" coords="6,196.01,619.99,12.45,8.64">40,</ref><ref type="bibr" coords="6,211.01,619.99,13.28,8.64" target="#b52">53]</ref> or graph-based re-ranking <ref type="bibr" coords="6,94.10,631.95,15.77,8.64" target="#b10">[11,</ref><ref type="bibr" coords="6,112.36,631.95,12.45,8.64" target="#b20">21,</ref><ref type="bibr" coords="6,127.30,631.95,11.83,8.64" target="#b54">55]</ref>     <ref type="bibr" coords="6,308.86,607.42,15.77,8.64" target="#b52">[53,</ref><ref type="bibr" coords="6,326.64,607.42,11.83,8.64" target="#b27">28]</ref>. All models use ResNet101-GeM. Red: best results. Blue: GLAM higher than SOLAR <ref type="bibr" coords="6,447.50,619.38,16.60,8.64" target="#b27">[28]</ref> on GLDv1-noisy.</p><p>GLDv2-noisy has 2.6 times more images than GLDv2clean, the latter is superior by a large margin. This shows that, in training, a cleaner dataset can be more important than a larger one. By contrast, NC-clean has the worst performance despite being clean, aparently because it is  <ref type="bibr" coords="7,115.63,169.30,11.04,6.05" target="#b15">[16,</ref><ref type="bibr" coords="7,128.42,169.30,9.30,6.05" target="#b34">35]</ref> NC-clean 2048 86.1 94.5 60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0 GeM-R101-Siamese <ref type="bibr" coords="7,114.21,177.27,11.04,6.05" target="#b36">[37,</ref><ref type="bibr" coords="7,126.99,177.27,9.30,6.05" target="#b34">35]</ref> SfM  <ref type="bibr" coords="7,483.51,291.23,16.60,8.64" target="#b52">[53]</ref> is the only model other than ours trained on GLDv2-clean, while <ref type="bibr" coords="7,266.76,303.19,16.60,8.64" target="#b27">[28]</ref> is trained on GLDv1-noisy and compared in Table <ref type="table" coords="7,488.70,303.19,3.74,8.64" target="#tab_2">3</ref>.</p><p>too small. To achieve best possible performance, we use GLDv2-clean as a training set in the remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons on same training set</head><p>It is common to compare methods regardless of training sets as more become available, e.g., <ref type="bibr" coords="7,113.18,389.08,15.77,8.64" target="#b34">[35,</ref><ref type="bibr" coords="7,133.13,389.08,11.83,8.64" target="#b27">28]</ref>. Since GLDv2-clean is relatively new, Weyand et al. <ref type="bibr" coords="7,126.46,401.04,15.27,8.64" target="#b52">[53]</ref>, which introduced the dataset, is the only study that has trained the same backbone with the same settings (ResNet101-GeM with ArcFace) on GLDv2-clean.</p><p>Our baseline is lower than <ref type="bibr" coords="7,153.98,436.90,15.27,8.64" target="#b52">[53]</ref>, because our dimensinality is 512, while other models based on ResNet101 use 2048. Yet, Table <ref type="table" coords="7,74.57,460.81,4.98,8.64" target="#tab_2">3</ref> shows that our best model trained on GLDv2-clean outperforms <ref type="bibr" coords="7,102.53,472.77,16.60,8.64" target="#b52">[53]</ref> by a large margin. But the most important comparison is with SOLAR <ref type="bibr" coords="7,186.51,484.72,15.27,8.64" target="#b27">[28]</ref>, also based on selfattention, which has trained ResNet101-GeM on GLDv1noisy. On this training set, our best model clearly outperforms <ref type="bibr" coords="7,75.85,520.59,16.60,8.64" target="#b27">[28]</ref> despite lower dimensionality. With this model, we outperform previous best methods on most benchmarks except mP@10 on RParis (medium) and RParis+R1M (medium), where we are outperformed by <ref type="bibr" coords="7,62.83,692.56,15.77,8.64" target="#b36">[37,</ref><ref type="bibr" coords="7,81.37,692.56,11.83,8.64" target="#b34">35]</ref>. These results demonstrate that our approach is effective for landmark image retrieval. Figure <ref type="figure" coords="7,231.55,704.51,4.98,8.64" target="#fig_8">7</ref> shows some </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with state of the art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>Our ablation study uses the Google Landmark v2 clean dataset (GLDv2-clean) <ref type="bibr" coords="7,403.18,692.56,16.60,8.64" target="#b52">[53]</ref> for training, which is shown to be the most effective in Table <ref type="table" coords="7,428.60,704.51,3.74,8.64" target="#tab_2">3</ref> Table <ref type="table" coords="8,75.59,378.35,3.88,8.64">9</ref>: mAP comparison of using multiresolution representation (Multi) or not (Single) on query or database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of attention modules</head><p>We ablate the effect of our local and global attention networks as well as their combination. Table <ref type="table" coords="8,114.63,448.54,4.98,8.64">5</ref> shows the results, which are more finegrained than those of Table <ref type="table" coords="8,161.33,460.50,3.74,8.64" target="#tab_4">4</ref>. In particular, it shows the effect of the channel and spatial variants of both local and global attention. We observe that, when used alone, the channel and spatial variants of local attention are harmful in most cases. Even the combination, baseline+local, is not always effective. By contrast, when used alone, the channel and spatial variants of global attention are mostly beneficial, especially the latter. Their combination, baseline+global, is impressive, bringing gain of up to 7.5%. Importantly, the combination baseline+global+local improves further by up to another 2.8%. This result shows the necessity of local attention in the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CBAM vs. our local spatial attention</head><p>We experiment with the local spatial attention of CBAM <ref type="bibr" coords="8,219.33,620.83,15.27,8.64" target="#b53">[54]</ref>. CBAM applies average and max-pooling to input features and concatenates the two for spatial attention. We apply this variant to our local spatial attention module for comparison.</p><p>For the CBAM style module, we keep the overall design of our module as shown in Figure <ref type="figure" coords="8,188.37,680.60,3.74,8.64" target="#fig_3">3</ref>, but apply average and max-pooling to each of the four convolutional layer outputs before concatenation. Table <ref type="table" coords="8,165.22,704.51,4.98,8.64">6</ref> shows that the CBAM style module is considerably worse than ours on all benchmarks except Paris6k, where it is only slightly better.</p><p>Concatenation vs. sum for feature fusion We use a softmax-based weighted average of local and global attention feature maps with the original feature map <ref type="bibr" coords="8,505.77,127.78,10.58,8.64" target="#b6">(7)</ref>. Here, we compare this weighted average with weighted concatenation, where concatenation replaces the sum operation in <ref type="bibr" coords="8,319.87,163.65,10.58,8.64" target="#b6">(7)</ref>. As shown in Table <ref type="table" coords="8,416.73,163.65,3.74,8.64">7</ref>, the weighted average outperforms the weighted concatenation.</p><p>Fixed-size vs. group-size sampling Numerous studies have proposed methods for constructing batches according to image size for efficient training. For instance, Gordo et al. <ref type="bibr" coords="8,322.61,227.91,15.27,8.64" target="#b15">[16]</ref>, DELF <ref type="bibr" coords="8,373.61,227.91,15.27,8.64" target="#b28">[29]</ref>, and Yokoo et al. <ref type="bibr" coords="8,467.94,227.91,16.60,8.64" target="#b55">[56]</ref> employed different image sizes per batch for training instead of a single fixed size. We adopt the method of Yokoo et al., which constructs a batch with images of similar aspect ratio, so that the images can be resized to a size with an aspect ratio that is similar to their own. We call this method group-size sampling. Table <ref type="table" coords="8,358.60,299.64,4.98,8.64">8</ref> compares fixed-size (224 × 224) with groupsize sampling. We observe that maintaining aspect ratios by using dynamic input sizes is much more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-resolution</head><p>We use the multi-resolution representation <ref type="bibr" coords="8,326.95,351.94,16.60,8.64" target="#b15">[16]</ref> for the final feature of an image at inference time. This method: (1) resizes an image into multiple scales; (2) extracts features from the resized images; and (3) averages the features to obtain the final feature of the image. The method is applied to both query and database images to enhance ranking results, especially for small target objects. Table <ref type="table" coords="8,333.32,423.67,4.98,8.64">9</ref> compares the four cases of applying this method or not to query or database images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced a novel approach that extracts global and local contextual information using attention mechanisms for instance-level image retrieval. It is manifested as a network architecture consisting of global and local attention components, each operating on both spatial and channel dimensions. This constitutes a comprehensive study and empirical evaluation of all four forms of attention that have previously been studied only in isolation. Our findings indicate that the gain (or loss) brought by one form of attention alone strongly depends on the presence of the others, with the maximum gain appearing when all forms are present. The output is a modified feature tensor that can be used in any way, for instance with local feature detection instead of spatial pooling for image retrieval.</p><p>With the advent of vision transformers <ref type="bibr" coords="8,476.36,644.74,15.77,8.64" target="#b11">[12,</ref><ref type="bibr" coords="8,494.48,644.74,13.28,8.64" target="#b57">58]</ref> and their recent application to image retrieval <ref type="bibr" coords="8,460.81,656.69,15.27,8.64" target="#b12">[13]</ref>, attention is expected to play a more and more significant role in vision. According to our classification, transformers perform global spatial attention alone. It is of great interest to investigate the role of the other forms of attention, where our approach may yield a basic building block of such architectures. One may even envision an extension to language models, where transformers originate from <ref type="bibr" coords="9,223.31,99.39,15.27,8.64" target="#b49">[50]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,50.11,269.84,495.00,8.82;2,50.11,281.98,495.00,8.64;2,50.11,293.75,485.13,8.82;2,535.25,292.04,2.52,6.12;2,535.25,298.12,3.56,6.12;2,539.30,293.93,5.81,8.64;2,50.11,305.71,65.28,8.82;2,115.39,303.99,2.52,6.12;2,115.39,310.07,3.76,6.12;2,119.65,305.71,84.01,8.82;2,203.66,303.99,3.89,6.12;2,203.66,310.07,3.56,6.12;2,208.29,305.71,94.76,8.82;2,303.05,303.99,3.89,6.12;2,303.05,310.07,3.76,6.12;2,307.68,305.54,213.55,8.99;2,521.23,303.99,2.52,6.12;2,524.29,305.89,20.82,8.64;2,50.11,317.84,37.99,8.64;2,88.10,315.95,3.89,6.12;2,92.73,317.49,389.44,8.99;2,482.18,315.95,6.66,6.12;2,489.37,317.84,55.74,8.64;2,50.11,329.80,336.78,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local attention (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention (based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (A l c ), local spatial (A l s ), global channel (A g c ) and global spatial (A g s ). The input feature map F is weighted into local (F l ) and global (F g ) attention feature maps, which are fused with F to yield the global-local attention feature map F gl . The diagram is abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,100.86,232.24,134.75,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Local channel attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,308.86,252.35,236.25,8.64;3,308.86,264.30,236.25,8.64;3,308.86,275.94,130.23,8.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Local spatial attention. Convolutional layers in blue implemented by dilated convolutions with kernel size 3 × 3 and dilation factors 1, 3, 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,98.64,261.82,139.19,8.64"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Global channel attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="4,359.88,253.48,134.22,8.64"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Global spatial attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="5,308.86,423.62,236.25,8.82;5,308.86,435.75,236.25,8.64;5,308.86,447.71,236.25,8.64;5,308.86,459.66,98.53,8.64;5,323.78,287.46,66.15,110.25"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Local and global spatial attention. Left: input images. Middle: local spatial attention heat maps. Right: global spatial attention heat maps. Red (blue) means higher (lower) attention weight.</figDesc><graphic coords="5,323.78,287.46,66.15,110.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="6,50.11,299.59,495.00,8.64;6,50.11,311.55,495.00,8.64;6,50.11,323.50,162.38,8.64;6,50.11,72.00,495.00,215.75"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Examples of our ranking results. In each row, the first image on the left (pink dotted outline) is a query image with a target object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images for the query; red solid outline: negative.</figDesc><graphic coords="6,50.11,72.00,495.00,215.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="6,139.13,631.95,3.94,8.64"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="6,50.11,649.64,88.59,9.85;6,50.11,668.26,236.25,9.03;6,50.11,680.60,236.25,8.64"><head>4. 3 .</head><label>3</label><figDesc>Benchmarking Noisy vs. clean training sets We begin by training our best model (baseline+local+global) on all training sets of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,50.11,358.92,451.41,354.23"><head>Table 2 ,</head><label>2</label><figDesc>except NC-noisy because some images are currently unavailable. As shown in Table3, even though</figDesc><table coords="6,352.24,358.92,149.29,78.53"><row><cell>TRAIN SET</cell><cell cols="2">#IMAGES #CLASSES</cell></row><row><cell>NC-noisy</cell><cell>213,678</cell><cell>672</cell></row><row><cell>NC-clean</cell><cell>27,965</cell><cell>581</cell></row><row><cell>SfM-120k</cell><cell>117,369</cell><cell>713</cell></row><row><cell cols="2">GLDv1-noisy 1,225,029</cell><cell>14, 951</cell></row><row><cell cols="2">GLDv2-noisy 4,132,914</cell><cell>203,094</cell></row><row><cell cols="2">GLDv2-clean 1,580,470</cell><cell>81,313</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,309.55,453.16,234.87,115.11"><head>Table 2 :</head><label>2</label><figDesc>Statistics of different training sets.</figDesc><table coords="6,309.55,483.43,234.87,84.84"><row><cell>METHOD</cell><cell cols="2">TRAIN SET DIM OXF5K PAR6K</cell><cell>RMEDIUM RHARD</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ROxf RPar ROxf RPar</cell></row><row><cell cols="4">GeM-Siamese [37, 35] SfM-120k 2048 87.8 92.7 64.7 77.2 38.5 56.3</cell></row><row><cell>SOLAR [28]</cell><cell>GLDv1-noisy 2048 -</cell><cell>-</cell><cell>69.9 81.6 47.9 64.5</cell></row><row><cell>GLDv2 [53]</cell><cell>GLDv2-clean 2048 -</cell><cell>-</cell><cell>74.2 84.9 51.6 70.3</cell></row><row><cell>GLAM (Ours)</cell><cell cols="3">NC-clean 512 77.8 85.8 51.6 68.1 20.9 44.7</cell></row><row><cell></cell><cell cols="3">GLDv1-noisy 512 92.8 95.0 73.7 83.5 49.8 69.4</cell></row><row><cell></cell><cell cols="3">GLDv2-noisy 512 93.3 95.3 75.7 86.0 53.1 73.8</cell></row><row><cell></cell><cell cols="3">GLDv2-clean 512 94.2 95.6 78.6 88.5 60.2 76.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,308.86,583.51,236.25,20.59"><head>Table 3 :</head><label>3</label><figDesc>mAP comparison of our best model (base-line+local+global) trained on different training sets against</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,50.11,177.00,495.00,122.87"><head>Table 4 :</head><label>4</label><figDesc>mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16: VGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet).</figDesc><table coords="7,55.14,177.00,484.95,75.08"><row><cell></cell><cell cols="12">-120k 2048 87.8 92.7 64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3</cell></row><row><cell>AGeM-R101-Siamese [17]</cell><cell>SfM-120k 2048</cell><cell>-</cell><cell>-</cell><cell>67.0 -</cell><cell>-</cell><cell>-78.1 -</cell><cell>-</cell><cell>-40.7 -</cell><cell>-</cell><cell>-57.3 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">SOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048</cell><cell>-</cell><cell>-</cell><cell cols="9">69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6</cell></row><row><cell>DELG-GeM-R101-ArcFace [5]</cell><cell>GLDv1-noisy 2048</cell><cell>-</cell><cell>-</cell><cell cols="9">73.2 -54.8 -82.4 -61.8 -51.2 -30.3 -64.7 -35.5 -</cell></row><row><cell>GeM-R101-ArcFace [53]</cell><cell>GLDv2-clean 2048</cell><cell>-</cell><cell>-</cell><cell>74.2 -</cell><cell>-</cell><cell>-84.9 -</cell><cell>-</cell><cell>-51.6 -</cell><cell>-</cell><cell>-70.3 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="13">GLAM-GeM-R101-ArcFace baseline GLDv2-clean 512 91.9 94.5 72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7</cell></row><row><cell>+local</cell><cell cols="12">GLDv2-clean 512 91.2 95.4 73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1</cell></row><row><cell>+global</cell><cell cols="12">GLDv2-clean 512 92.3 95.3 77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0</cell></row><row><cell>+global+local</cell><cell cols="12">GLDv2-clean 512 94.2 95.6 78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0</cell></row></table><note coords="7,355.35,277.38,2.72,6.12;7,358.57,278.96,186.54,8.96;7,50.11,291.05,430.57,8.82"><p>* : dimension d = 256 [2]. mP: mP@10. Red: best results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,50.11,537.14,236.25,116.23"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="7,50.11,537.14,236.25,116.23"><row><cell>shows the</cell></row><row><cell>performance of four variants of our model, i.e. baseline</cell></row><row><cell>with or without local/global attention, and compares them</cell></row><row><cell>against state-of-the-art (SOTA) methods based on global de-</cell></row><row><cell>scriptors without re-ranking on the complete set of bench-</cell></row><row><cell>marks, including distractors. Both local and global atten-</cell></row><row><cell>tion bring significant gain over the baseline. The effect</cell></row><row><cell>of global is stronger, while the gain of the two is addi-</cell></row><row><cell>tive in the combination. The best results are achieved by</cell></row><row><cell>the global-local attention network (baseline+global+local).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,432.34,704.51,3.74,8.64"><head></head><label></label><figDesc>.</figDesc><table coords="8,50.11,77.13,236.25,285.51"><row><cell>METHOD</cell><cell></cell><cell cols="3">OXF5K PAR6K</cell><cell>RMEDIUM</cell><cell>RHARD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ROxf RPar ROxf RPar</cell></row><row><cell cols="2">Concatenate</cell><cell>89.5</cell><cell cols="2">95.1</cell><cell>73.6 86.5 54.0 73.7</cell></row><row><cell cols="2">Sum (Ours)</cell><cell>94.2</cell><cell cols="2">95.6</cell><cell>78.6 88.5 60.2 76.8</cell></row><row><cell cols="5">Table 7: mAP comparison between weighted concatenation</cell></row><row><cell cols="5">and weighted average for feature fusion.</cell></row><row><cell>METHOD</cell><cell></cell><cell cols="3">OXF5K PAR6K</cell><cell>RMEDIUM</cell><cell>RHARD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ROxf RPar ROxf RPar</cell></row><row><cell>Fixed-size</cell><cell></cell><cell></cell><cell>76.1</cell><cell>82.6 55.7 68.4 29.2 47.5</cell></row><row><cell cols="4">Group-size (Ours) 94.2</cell><cell>95.6 78.6 88.5 60.2 76.8</cell></row><row><cell cols="5">Table 8: mAP comparison between fixed-size (224 × 224)</cell></row><row><cell cols="5">and group-size sampling methods.</cell></row><row><cell cols="5">QUERY DATABASE OXF5K PAR6K</cell><cell>RMEDIUM RHARD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ROxf RPar ROxf RPar</cell></row><row><cell>Single</cell><cell cols="2">Single</cell><cell>93.3</cell><cell>95.2 76.9 87.1 58.6 74.7</cell></row><row><cell>Multi</cell><cell cols="2">Single</cell><cell>93.9</cell><cell>95.4 78.0 87.7 59.0 75.5</cell></row><row><cell>Single</cell><cell cols="2">Multi</cell><cell>93.6</cell><cell>95.6 77.0 87.8 57.1 76.0</cell></row><row><cell>Multi</cell><cell cols="2">Multi</cell><cell>94.2</cell><cell>95.6 78.6 88.5 60.2 76.8</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,70.04,144.27,216.32,7.77;9,70.03,155.23,216.33,7.77;9,70.03,166.03,192.98,7.93" xml:id="b0">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN Architecture for Weakly Supervised Place Recognition</title>
		<author>
			<persName><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.572</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Relja Arandjelović, Petr Gronat, Akihiko Torii, Tomas Pa- jdla, and Josef Sivic. NetVLAD: CNN architecture for weakly supervised place recognition. In CVPR, 2016.</note>
</biblStruct>

<biblStruct coords="9,70.04,178.74,216.32,7.77;9,70.03,189.54,209.45,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main">Aggregating Local Deep Features for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Artem Babenko and Victor Lempitsky. Aggregating Local Deep Features for Image Retrieval. In ICCV, 2015. 1, 2, 7</note>
</biblStruct>

<biblStruct coords="9,70.04,202.25,216.32,7.77;9,70.03,213.21,216.33,7.77;9,70.03,224.01,73.22,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural Codes for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anton</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandr</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10590-1_38</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2014</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
	<note type="raw_reference">Artem Babenko, Anton Slesarev, Alexandr Chigorin, and Victor Lempitsky. Neural Codes for Image Retrieval. In ECCV, 2014. 1, 2, 5</note>
</biblStruct>

<biblStruct coords="9,70.04,236.72,216.32,7.77;9,70.03,247.68,216.33,7.77;9,70.03,258.48,98.76,7.93" xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention Augmented Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00338</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. Attention augmented convolutional net- works. In ICCV, 2019. 2, 3</note>
</biblStruct>

<biblStruct coords="9,70.04,271.19,216.33,7.77;9,70.03,281.99,216.33,7.93;9,70.03,293.11,22.42,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main">Unifying Deep Local and Global Features for Image Search</title>
		<author>
			<persName coords=""><forename type="first">Bingyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">André</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58565-5_43</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="726" to="743" />
		</imprint>
	</monogr>
	<note type="raw_reference">Bingyi Cao, André Araujo, and Jack Sim. Unifying deep local and global features for image search. In ECCV, 2020. 2, 3, 7</note>
</biblStruct>

<biblStruct coords="9,70.04,305.66,216.32,7.77;9,70.03,316.62,216.33,7.77;9,70.03,327.42,134.99,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main">GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond</title>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2019.00246</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10">2019</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. GCNet: Non-Local Networks Meet Squeeze-Excitation Net- works and Beyond. In ICCV, 2019. 2</note>
</biblStruct>

<biblStruct coords="9,70.04,340.13,216.32,7.77;9,70.03,351.09,216.33,7.77;9,70.03,361.89,216.33,7.93;9,70.03,373.01,20.17,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_49</idno>
		<idno type="arXiv">arXiv:1706.05587</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for seman- tic image segmentation. arXiv preprint arXiv:1706.05587, 2017.</note>
</biblStruct>

<biblStruct coords="9,70.04,385.56,216.32,7.77;9,70.03,396.52,216.33,7.77;9,70.03,407.32,156.62,7.93" xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName coords=""><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 1</note>
</biblStruct>

<biblStruct coords="9,70.04,420.03,216.32,7.77;9,70.03,430.99,216.33,7.77;9,70.03,441.79,99.86,7.93" xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph-Based Global Reasoning Networks</title>
		<author>
			<persName coords=""><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00052</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi Feng. Aˆ2-nets: Double attention networks. In NeurIPS, 2018. 1, 2, 3, 4</note>
</biblStruct>

<biblStruct coords="9,70.04,454.50,216.32,7.77;9,70.03,465.46,216.33,7.77;9,70.03,476.26,132.84,7.93" xml:id="b9">
	<analytic>
		<title level="a" type="main">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</title>
		<author>
			<persName coords=""><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00482</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. ArcFace: Additive Angular Margin Loss for Deep Face Recognition. In CVPR, 2019. 6</note>
</biblStruct>

<biblStruct coords="9,70.04,488.97,216.32,7.77;9,70.03,499.77,156.21,7.93" xml:id="b10">
	<analytic>
		<title level="a" type="main">Diffusion Processes for Retrieval Revisited</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2013.174</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-06">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael Donoser and Horst Bischof. Diffusion Processes for Retrieval Revisited. In CVPR, 2013. 1, 2, 6</note>
</biblStruct>

<biblStruct coords="9,70.04,512.49,216.32,7.77;9,70.03,523.44,216.33,7.77;9,70.03,534.40,216.33,7.77;9,70.03,545.36,216.33,7.77;9,70.03,556.16,216.33,7.93;9,70.03,567.12,97.87,7.93" xml:id="b11">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<idno>. 8</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 8</note>
</biblStruct>

<biblStruct coords="9,70.04,579.83,216.32,7.77;9,70.03,590.79,216.33,7.77;9,70.03,601.75,118.68,7.77" xml:id="b12">
	<monogr>
		<title level="m" type="main">Training vision transformers for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note type="raw_reference">Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Hervé Jégou. Training vision transformers for image re- trieval. Technical report, 2021. 8</note>
</biblStruct>

<biblStruct coords="9,70.04,614.30,216.32,7.77;9,70.03,625.10,216.33,7.93;9,70.03,636.22,45.82,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main">Global Second-Order Pooling Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">Zilin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiangtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00314</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global second-order pooling convolutional networks. In CVPR, 2019. 2, 3, 4</note>
</biblStruct>

<biblStruct coords="9,70.04,648.77,216.32,7.77;9,70.03,659.73,216.33,7.77;9,70.03,670.53,130.22,7.93" xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Image Retrieval: Learning Global Representations for Image Search</title>
		<author>
			<persName coords=""><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_15</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="241" to="257" />
		</imprint>
	</monogr>
	<note type="raw_reference">Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar- lus. Deep image retrieval: Learning global representations for image search. In ECCV, 2016. 2</note>
</biblStruct>

<biblStruct coords="9,70.04,683.24,216.32,7.77;9,70.03,694.20,216.33,7.77;9,70.03,705.00,154.68,7.93" xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Deep Visual Representations for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-017-1016-8</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<title level="j" type="abbrev">Int J Comput Vis</title>
		<idno type="ISSN">0920-5691</idno>
		<idno type="ISSNe">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="254" />
			<date type="published" when="2017-06-05">2017</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar- lus. End-to-end learning of deep visual representations for image retrieval. IJCV, 2017. 1, 2, 5, 6, 7, 8</note>
</biblStruct>

<biblStruct coords="9,328.79,76.13,216.32,7.77;9,328.78,86.92,216.33,7.94;9,328.78,97.88,147.10,7.94" xml:id="b16">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Yinzheng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuanpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinbin</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1811.00202</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018">2018</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yinzheng Gu, Chuanpeng Li, and Jinbin Xie. Attention- aware generalized mean pooling for image retrieval. arXiv preprint arXiv:1811.00202, 2018. 2, 3, 7</note>
</biblStruct>

<biblStruct coords="9,328.79,110.65,216.32,7.77;9,328.78,121.44,216.33,7.93;9,328.78,132.56,27.89,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 6</note>
</biblStruct>

<biblStruct coords="9,328.79,145.17,216.32,7.77;9,328.78,156.12,216.33,7.77;9,328.78,166.92,174.29,7.93" xml:id="b18">
	<analytic>
		<title level="a" type="main">Gather-excite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea Vedaldi. Gather-excite: Exploiting feature context in con- volutional neural networks. In NeurIPS, 2018. 2</note>
</biblStruct>

<biblStruct coords="9,328.79,179.69,216.32,7.77;9,328.78,190.48,201.60,7.93" xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Hu</surname></persName>
			<idno type="ORCID">0000-0002-5150-1003</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Shen</surname></persName>
			<idno type="ORCID">0000-0002-2283-4976</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
			<idno type="ORCID">0000-0003-1732-9198</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Gang</forename><surname>Sun</surname></persName>
			<idno type="ORCID">0000-0001-6913-6799</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Enhua</forename><surname>Wu</surname></persName>
			<idno type="ORCID">0000-0002-2174-1428</idno>
		</author>
		<idno type="DOI">10.1109/tpami.2019.2913372</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">1939-3539</idno>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2011" to="2023" />
			<date type="published" when="2018">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. Squeeze-and-Excitation Networks. In CVPR, 2018. 1, 2</note>
</biblStruct>

<biblStruct coords="9,328.79,203.25,216.32,7.77;9,328.78,214.21,216.33,7.77;9,328.78,225.17,216.33,7.77;9,328.78,235.96,73.46,7.93" xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient Diffusion on Region Manifolds: Recovering Small Objects with Compact CNN Representations</title>
		<author>
			<persName coords=""><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teddy</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.105</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon, and Ondrej Chum. Efficient diffusion on region manifolds: Recovering small objects with compact cnn representations. In CVPR, 2017. 2, 6</note>
</biblStruct>

<biblStruct coords="9,328.79,248.73,216.32,7.77;9,328.78,259.69,216.33,7.77;9,328.78,270.48,131.40,7.93" xml:id="b21">
	<analytic>
		<title level="a" type="main">Aggregating Local Image Descriptors into Compact Codes</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2011.235</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">2160-9292</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2011">2011</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid. Aggregating local image descriptors into com- pact codes. PAMI, (99):1-1, 2011. 2</note>
</biblStruct>

<biblStruct coords="9,328.79,283.25,216.33,7.77;9,328.78,294.21,216.33,7.77;9,328.78,305.00,103.82,7.93" xml:id="b22">
	<analytic>
		<title level="a" type="main">Class Weighted Convolutional Features for Visual Instance Search</title>
		<author>
			<persName coords=""><forename type="first">Albert</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Giro-I-Nieto</surname></persName>
		</author>
		<idno type="DOI">10.5244/c.31.144</idno>
	</analytic>
	<monogr>
		<title level="m">Procedings of the British Machine Vision Conference 2017</title>
		<meeting>edings of the British Machine Vision Conference 2017</meeting>
		<imprint>
			<publisher>British Machine Vision Association</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Albert Jimenez, Jose M. Alvarez, and Xavier Giró-i-Nieto. Class weighted convolutional features for visual instance search. In BMVC, 2017. 2, 3</note>
</biblStruct>

<biblStruct coords="9,328.79,317.77,216.32,7.77;9,328.78,328.73,216.33,7.77;9,328.78,339.52,148.41,7.93" xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-Dimensional Weighting for Aggregated Deep Convolutional Features</title>
		<author>
			<persName coords=""><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clayton</forename><surname>Mellina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46604-0_48</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="685" to="701" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yannis Kalantidis, Clayton Mellina, and Simon Osindero. Crossdimensional weighting for aggregated deep convolu- tional features. In ECCV, 2016. 1, 2, 3, 7</note>
</biblStruct>

<biblStruct coords="9,328.79,352.29,216.32,7.77;9,328.78,363.25,216.33,7.77;9,328.78,374.04,132.72,7.93" xml:id="b24">
	<analytic>
		<title level="a" type="main">Learned Contextual Feature Reweighting for Image Geo-Localization</title>
		<author>
			<persName coords=""><forename type="first">Jin</forename><surname>Hyo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan-Michael</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm. Learned Contextual Feature Reweighting for Image Geo- Localization. In CVPR, 2017. 1, 2, 3</note>
</biblStruct>

<biblStruct coords="9,328.79,386.81,216.32,7.77;9,328.78,397.61,216.33,7.93" xml:id="b25">
	<analytic>
		<title level="a" type="main">Proxy Anchor Loss for Deep Metric Learning</title>
		<author>
			<persName coords=""><forename type="first">Sungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00330</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric learning. In CVPR, 2020.</note>
</biblStruct>

<biblStruct coords="9,328.79,421.33,216.32,7.77;9,328.78,432.13,143.12,7.93" xml:id="b26">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">David G. Lowe. Distinctive image features from scale- invariant keypoints. In IJCV, 2004. 1, 2</note>
</biblStruct>

<biblStruct coords="9,328.79,444.89,216.32,7.77;9,328.78,455.85,216.33,7.77;9,328.78,466.65,172.37,7.93" xml:id="b27">
	<analytic>
		<title level="a" type="main">SOLAR: Second-Order Loss and Attention for Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Tony</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yurun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58595-2_16</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="253" to="270" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian Mikolajczyk. SOLAR: Second-Order Loss and Attention for Image Retrieval. In ECCV, 2020. 1, 2, 3, 5, 6, 7</note>
</biblStruct>

<biblStruct coords="9,328.79,479.41,216.32,7.77;9,328.78,490.37,216.33,7.77;9,328.78,501.17,202.33,7.93" xml:id="b28">
	<analytic>
		<title level="a" type="main">Large-Scale Image Retrieval with Attentive Deep Local Features</title>
		<author>
			<persName coords=""><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.374</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, and Bohyung Han. Large Scale Image Retrieval with Atten- tive Deep Local Features. In ICCV, 2017. 1, 2, 3, 5, 6, 8</note>
</biblStruct>

<biblStruct coords="9,328.79,513.93,216.32,7.77;9,328.78,524.89,216.33,7.77;9,328.78,535.69,109.81,7.93" xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Metric Learning via Lifted Structured Feature Embedding</title>
		<author>
			<persName><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.434</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In CVPR, 2016. 1</note>
</biblStruct>

<biblStruct coords="9,328.79,548.45,216.32,7.77;9,328.78,559.41,216.33,7.77;9,328.78,570.37,216.33,7.77;9,328.78,581.33,216.33,7.77;9,328.78,592.29,216.33,7.77;9,328.78,603.25,216.33,7.77;9,328.78,614.04,216.33,7.93;9,328.78,625.16,27.89,7.77" xml:id="b30">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An im- perative style, high-performance deep learning. In NeurIPS, 2019. 5</note>
</biblStruct>

<biblStruct coords="9,328.79,637.77,216.32,7.77;9,328.78,648.72,216.33,7.77;9,328.78,659.52,168.24,7.93" xml:id="b31">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2007.383172</idno>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007-06">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Object retrieval with large vocabularies and fast spatial matching. In CVPR, 2007. 2, 5</note>
</biblStruct>

<biblStruct coords="9,328.79,672.29,216.32,7.77;9,328.78,683.24,216.33,7.77;9,328.78,694.04,216.33,7.93;9,328.78,705.16,27.89,7.77" xml:id="b32">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2008.4587635</idno>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008-06">2008</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Lost in quantization:Improving particu- lar object retrieval in large scale image databases. In CVPR, 2008. 5</note>
</biblStruct>

<biblStruct coords="10,70.04,76.13,216.33,7.77;10,70.03,86.92,109.22,7.94" xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural nearest neighbors networks</title>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tobias Plötz and Stefan Roth. Neural nearest neighbors net- works. In NeurIPS, 2018. 2, 3</note>
</biblStruct>

<biblStruct coords="10,70.04,98.95,216.33,7.77;10,70.03,109.90,216.33,7.77;10,70.03,120.70,216.33,7.93;10,70.03,131.82,22.42,7.77" xml:id="b34">
	<analytic>
		<title level="a" type="main">Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00598</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Filip Radenović, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondřej Chum. Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking. In CVPR, 2018. 5, 6, 7</note>
</biblStruct>

<biblStruct coords="10,70.04,143.68,216.33,7.77;10,70.03,154.64,216.33,7.77;10,70.03,165.44,149.52,7.93" xml:id="b35">
	<analytic>
		<title level="a" type="main">CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples</title>
		<author>
			<persName coords=""><forename type="first">Filip</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ondřej</forename><surname>Chum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_1</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
	<note type="raw_reference">Filip Radenović, Giorgos Tolias, and Ondřej Chum. CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples. In ECCV, 2016. 2, 7</note>
</biblStruct>

<biblStruct coords="10,70.04,177.46,216.33,7.77;10,70.03,188.42,216.33,7.77;10,70.03,199.22,103.54,7.93" xml:id="b36">
	<analytic>
		<title level="a" type="main">Fine-Tuning CNN Image Retrieval with No Human Annotation</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
			<idno type="ORCID">0000-0002-7122-2765</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
			<idno type="ORCID">0000-0001-7042-1810</idno>
		</author>
		<idno type="DOI">10.1109/tpami.2018.2846566</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">1939-3539</idno>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
			<date type="published" when="2019-07-01">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Filip Radenović, Giorgos Tolias, and Ondřej Chum. Fine- Tuning CNN Image Retrieval with No Human Annotation. In TPAMI, 2019. 1, 2, 5, 6, 7</note>
</biblStruct>

<biblStruct coords="10,70.04,211.24,216.32,7.77;10,70.03,222.20,216.33,7.77;10,70.03,233.00,142.17,7.93" xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual Instance Retrieval with Deep Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson, and Atsuto Maki. Visual Instance Retrieval with Deep Con- volutional Networks. In CoRR, 2015. 2</note>
</biblStruct>

<biblStruct coords="10,70.04,245.02,216.32,7.77;10,70.03,255.98,216.33,7.77;10,70.03,266.94,216.33,7.77;10,70.03,277.90,216.33,7.77;10,70.03,288.69,216.33,7.93" xml:id="b38">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<title level="j" type="abbrev">Int J Comput Vis</title>
		<idno type="ISSN">0920-5691</idno>
		<idno type="ISSNe">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-04-11">2015</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. In International booktitle of Computer Vision, 2015.</note>
</biblStruct>

<biblStruct coords="10,70.04,311.68,216.33,7.77;10,70.03,322.47,216.33,7.93;10,70.03,333.59,36.85,7.77" xml:id="b39">
	<analytic>
		<title level="a" type="main">Local Features and Visual Words Emerge in Activations</title>
		<author>
			<persName><forename type="first">Oriane</forename><surname>Simeoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.01192</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Oriane Siméoni, Yannis Avrithis, and Ondrej Chum. Local features and visual words emerge in activations. In CVPR, 2019. 2, 6</note>
</biblStruct>

<biblStruct coords="10,70.04,345.45,216.33,7.77;10,70.03,356.25,216.33,7.93;10,70.03,367.21,142.21,7.93" xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph-based particular object discovery</title>
		<author>
			<persName><forename type="first">Oriane</forename><surname>Siméoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
			<idno type="ORCID">0000-0002-1856-4790</idno>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Chum</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00138-019-01005-z</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<title level="j" type="abbrev">Machine Vision and Applications</title>
		<idno type="ISSN">0932-8092</idno>
		<idno type="ISSNe">1432-1769</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="243" to="254" />
			<date type="published" when="2019-02-08">3 2019</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">O. Siméoni, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum. Graph-based particular object discovery. Machine Vision and Applications, 30(2):243-254, 3 2019. 3</note>
</biblStruct>

<biblStruct coords="10,70.04,379.23,216.32,7.77;10,70.03,390.03,204.11,7.93" xml:id="b41">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName coords=""><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp- ical networks for few-shot learning. In NeurIPS, 2017. 1</note>
</biblStruct>

<biblStruct coords="10,70.04,402.05,216.32,7.77;10,70.03,413.01,216.33,7.77;10,70.03,423.97,216.33,7.77;10,70.03,434.77,115.76,7.93" xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><surname>Wei Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Yangqing Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">2015</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015. 3</note>
</biblStruct>

<biblStruct coords="10,70.04,446.79,216.32,7.77;10,70.03,457.59,211.68,7.93" xml:id="b43">
	<analytic>
		<title level="a" type="main">EfficientDet: Scalable and Efficient Object Detection</title>
		<author>
			<persName coords=""><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01079</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Mingxing Tan, Ruoming Pang, and Quoc V. Le. EfficientDet: Scalable and Efficient Object Detection. In CVPR, 2020. 5</note>
</biblStruct>

<biblStruct coords="10,70.04,469.61,216.32,7.77;10,70.03,480.57,216.33,7.77;10,70.03,491.37,117.02,7.93" xml:id="b44">
	<analytic>
		<title level="a" type="main">Detect-To-Retrieve: Efficient Regional Aggregation for Image Search</title>
		<author>
			<persName coords=""><forename type="first">Marvin</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00525</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack Sim. Detect-to-retrieve: Efficient regional aggregation for image search. In CVPR, 2019. 2</note>
</biblStruct>

<biblStruct coords="10,70.04,503.39,216.33,7.77;10,70.03,514.35,216.33,7.77;10,70.03,525.15,90.87,7.93" xml:id="b45">
	<analytic>
		<title level="a" type="main">To Aggregate or Not to aggregate: Selective Match Kernels for Image Search</title>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2013.177</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-12">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Giorgios Tolias, Yannis Avrithis, and Hervé Jégou. To aggre- gate or not to aggregate: Selective match kernels for image search. In ICCV, 2013. 2</note>
</biblStruct>

<biblStruct coords="10,70.04,537.17,216.32,7.77;10,70.03,548.13,216.33,7.77;10,70.03,558.93,120.27,7.93" xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning and Aggregating Deep Local Descriptors for Instance-Level Recognition</title>
		<author>
			<persName coords=""><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Jenicek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ondřej</forename><surname>Chum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_27</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="460" to="477" />
		</imprint>
	</monogr>
	<note type="raw_reference">Giorgos Tolias, Tomas Jenicek, and Ondřej Chum. Learn- ing and aggregating deep local descriptors for instance-level recognition. In ECCV, 2020. 2, 3</note>
</biblStruct>

<biblStruct coords="10,70.04,570.95,216.33,7.77;10,70.03,581.91,216.33,7.77;10,70.03,592.71,61.51,7.93" xml:id="b47">
	<analytic>
		<title level="a" type="main">Particular object retrieval with integral max-pooling of CNN activations</title>
		<author>
			<persName coords=""><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronan</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Giorgos Tolias, Ronan Sicre, and Hervé Jégou. Particular ob- ject retrieval with integral max-pooling of CNN activations. In ICLR, 2016. 2</note>
</biblStruct>

<biblStruct coords="10,70.04,604.73,216.32,7.77;10,70.03,615.69,216.33,7.77;10,70.03,626.48,212.42,7.93" xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 4</note>
</biblStruct>

<biblStruct coords="10,70.04,638.51,216.32,7.77;10,70.03,649.47,216.33,7.77;10,70.03,660.26,212.42,7.93" xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 9</note>
</biblStruct>

<biblStruct coords="10,70.04,672.29,216.32,7.77;10,70.03,683.24,216.33,7.77;10,70.03,694.20,216.33,7.77;10,70.03,705.00,72.72,7.93" xml:id="b50">
	<analytic>
		<title level="a" type="main">ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Banggu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01155</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang- meng Zuo, and Qinghua Hu. ECA-Net: Efficient Chan- nel Attention for Deep Convolutional Neural Networks. In CVPR, 2020. 2, 3, 4</note>
</biblStruct>

<biblStruct coords="10,328.79,76.12,216.32,7.77;10,328.78,86.92,216.33,7.94;10,328.78,98.04,13.45,7.77" xml:id="b51">
	<analytic>
		<title level="a" type="main">Non-local Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00813</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim- ing He. Non-local Neural Networks. In CVPR, 2018. 1, 2, 3, 4</note>
</biblStruct>

<biblStruct coords="10,328.79,110.00,216.32,7.77;10,328.78,120.96,216.33,7.77;10,328.78,131.75,216.33,7.93;10,328.78,142.87,63.75,7.77" xml:id="b52">
	<analytic>
		<title level="a" type="main">Google Landmarks Dataset v2 – A Large-Scale Benchmark for Instance-Level Recognition and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bingyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00265</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google Landmarks Dataset v2 -A Large-Scale Benchmark for Instance-Level Recognition and Retrieval. In CVPR, 2020. 1, 2, 5, 6, 7</note>
</biblStruct>

<biblStruct coords="10,328.79,154.83,216.32,7.77;10,328.78,165.79,216.33,7.77;10,328.78,176.59,82.19,7.93" xml:id="b53">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional Block Attention Module</title>
		<author>
			<persName coords=""><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_1</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. CBAM: Convolutional Block Attention Module. In ECCV, 2018. 1, 2, 4, 8</note>
</biblStruct>

<biblStruct coords="10,328.79,188.70,216.32,7.77;10,328.78,199.66,216.33,7.77;10,328.78,210.46,216.33,7.93;10,328.78,221.58,4.48,7.77" xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing</title>
		<author>
			<persName coords=""><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin’ichi</forename><surname>Satoh</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33019087</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="9087" to="9094" />
			<date type="published" when="2019-07-17">2019</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and Shin&apos;ichi Satoh. Efficient image retrieval via decoupling dif- fusion into online and offline processing. In AAAI, 2019. 2, 6</note>
</biblStruct>

<biblStruct coords="10,328.79,233.53,216.32,7.77;10,328.78,244.49,216.33,7.77;10,328.78,255.29,193.53,7.93" xml:id="b55">
	<analytic>
		<title level="a" type="main">Two-stage Discriminative Re-ranking for Large-scale Landmark Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Shuhei</forename><surname>Yokoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kohei</forename><surname>Ozaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw50498.2020.00514</idno>
		<idno type="arXiv">arXiv:2003.11211</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi Iizuka. Two-stage Discriminative Re-ranking for Large-scale Landmark Retrieval. In arXiv:2003.11211, 2020. 6, 8</note>
</biblStruct>

<biblStruct coords="10,328.79,267.41,216.32,7.77;10,328.78,278.21,133.37,7.93" xml:id="b56">
	<analytic>
		<title level="a" type="main">Dilated Residual Networks</title>
		<author>
			<persName coords=""><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.75</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated residual networks. In CVPR, 2017. 4</note>
</biblStruct>

<biblStruct coords="10,328.79,290.32,216.32,7.77;10,328.78,301.28,216.33,7.77;10,328.78,312.24,216.33,7.77;10,328.78,323.04,181.96,7.93" xml:id="b57">
	<analytic>
		<title level="a" type="main">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</title>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francis</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00060</idno>
		<idno type="arXiv">arXiv:2101.11986</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens- to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.</note>
</biblStruct>

<biblStruct coords="10,328.79,335.15,216.32,7.77;10,328.78,345.95,206.18,7.93" xml:id="b58">
	<analytic>
		<title level="a" type="main">Exploring Self-Attention for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01009</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. 2, 3</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
